{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c2b8f09-14b7-4084-bffa-4341f4806e8a",
   "metadata": {},
   "source": [
    "* Keras-based model (not PyTorch) - The bigest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78bc58d8-aa3a-4965-8dd1-8c47a5ba8592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Jaber\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:204: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "Found 7936 validated image filenames belonging to 6 classes.\n",
      "Found 1984 validated image filenames belonging to 6 classes.\n",
      "Found 2480 validated image filenames belonging to 6 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jaber\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10\n",
      "Batch 1/992 ━━━━━━━━━━━━━━━━━━━━ 15:35:42\n",
      "Accuracy: 0.0000 - Loss: 1.6865\n",
      "\n",
      "Batch 2/992 ━━━━━━━━━━━━━━━━━━━━ 15:35:53\n",
      "Accuracy: 0.1250 - Loss: 2.8387\n",
      "\n",
      "Batch 3/992 ━━━━━━━━━━━━━━━━━━━━ 15:36:03\n",
      "Accuracy: 0.2083 - Loss: 4.0671\n",
      "\n",
      "Batch 4/992 ━━━━━━━━━━━━━━━━━━━━ 15:36:14\n",
      "Accuracy: 0.2812 - Loss: 3.5803\n",
      "\n",
      "Batch 5/992 ━━━━━━━━━━━━━━━━━━━━ 15:36:25\n",
      "Accuracy: 0.2750 - Loss: 3.4327\n",
      "\n",
      "Batch 6/992 ━━━━━━━━━━━━━━━━━━━━ 15:36:36\n",
      "Accuracy: 0.2708 - Loss: 3.3700\n",
      "\n",
      "Batch 7/992 ━━━━━━━━━━━━━━━━━━━━ 15:36:46\n",
      "Accuracy: 0.2857 - Loss: 3.2943\n",
      "\n",
      "Batch 8/992 ━━━━━━━━━━━━━━━━━━━━ 15:36:56\n",
      "Accuracy: 0.3125 - Loss: 3.0776\n",
      "\n",
      "Batch 9/992 ━━━━━━━━━━━━━━━━━━━━ 15:37:07\n",
      "Accuracy: 0.3333 - Loss: 2.8945\n",
      "\n",
      "Batch 10/992 ━━━━━━━━━━━━━━━━━━━━ 15:37:17\n",
      "Accuracy: 0.3125 - Loss: 2.9569\n",
      "\n",
      "Batch 11/992 ━━━━━━━━━━━━━━━━━━━━ 15:37:28\n",
      "Accuracy: 0.3068 - Loss: 2.8177\n",
      "\n",
      "Batch 12/992 ━━━━━━━━━━━━━━━━━━━━ 15:37:38\n",
      "Accuracy: 0.2917 - Loss: 2.7474\n",
      "\n",
      "Batch 13/992 ━━━━━━━━━━━━━━━━━━━━ 15:37:49\n",
      "Accuracy: 0.2981 - Loss: 2.7376\n",
      "\n",
      "Batch 14/992 ━━━━━━━━━━━━━━━━━━━━ 15:37:59\n",
      "Accuracy: 0.2857 - Loss: 2.6752\n",
      "\n",
      "Batch 15/992 ━━━━━━━━━━━━━━━━━━━━ 15:38:10\n",
      "Accuracy: 0.2750 - Loss: 2.6217\n",
      "\n",
      "Batch 16/992 ━━━━━━━━━━━━━━━━━━━━ 15:38:21\n",
      "Accuracy: 0.2656 - Loss: 2.5853\n",
      "\n",
      "Batch 17/992 ━━━━━━━━━━━━━━━━━━━━ 15:38:31\n",
      "Accuracy: 0.2868 - Loss: 2.5124\n",
      "\n",
      "Batch 18/992 ━━━━━━━━━━━━━━━━━━━━ 15:38:42\n",
      "Accuracy: 0.2917 - Loss: 2.4968\n",
      "\n",
      "Batch 19/992 ━━━━━━━━━━━━━━━━━━━━ 15:38:52\n",
      "Accuracy: 0.2961 - Loss: 2.4582\n",
      "\n",
      "Batch 20/992 ━━━━━━━━━━━━━━━━━━━━ 15:39:03\n",
      "Accuracy: 0.3000 - Loss: 2.4147\n",
      "\n",
      "Batch 21/992 ━━━━━━━━━━━━━━━━━━━━ 15:39:13\n",
      "Accuracy: 0.3036 - Loss: 2.3938\n",
      "\n",
      "Batch 22/992 ━━━━━━━━━━━━━━━━━━━━ 15:39:24\n",
      "Accuracy: 0.3182 - Loss: 2.3367\n",
      "\n",
      "Batch 23/992 ━━━━━━━━━━━━━━━━━━━━ 15:39:34\n",
      "Accuracy: 0.3207 - Loss: 2.3158\n",
      "\n",
      "Batch 24/992 ━━━━━━━━━━━━━━━━━━━━ 15:39:45\n",
      "Accuracy: 0.3229 - Loss: 2.3132\n",
      "\n",
      "Batch 25/992 ━━━━━━━━━━━━━━━━━━━━ 15:39:55\n",
      "Accuracy: 0.3200 - Loss: 2.3221\n",
      "\n",
      "Batch 26/992 ━━━━━━━━━━━━━━━━━━━━ 15:40:06\n",
      "Accuracy: 0.3173 - Loss: 2.3164\n",
      "\n",
      "Batch 27/992 ━━━━━━━━━━━━━━━━━━━━ 15:40:16\n",
      "Accuracy: 0.3102 - Loss: 2.3217\n",
      "\n",
      "Batch 28/992 ━━━━━━━━━━━━━━━━━━━━ 15:40:27\n",
      "Accuracy: 0.3080 - Loss: 2.2986\n",
      "\n",
      "Batch 29/992 ━━━━━━━━━━━━━━━━━━━━ 15:40:38\n",
      "Accuracy: 0.3147 - Loss: 2.2707\n",
      "\n",
      "Batch 30/992 ━━━━━━━━━━━━━━━━━━━━ 15:40:48\n",
      "Accuracy: 0.3125 - Loss: 2.2609\n",
      "\n",
      "Batch 31/992 ━━━━━━━━━━━━━━━━━━━━ 15:40:58\n",
      "Accuracy: 0.3024 - Loss: 2.2622\n",
      "\n",
      "Batch 32/992 ━━━━━━━━━━━━━━━━━━━━ 15:41:09\n",
      "Accuracy: 0.3125 - Loss: 2.2280\n",
      "\n",
      "Batch 33/992 ━━━━━━━━━━━━━━━━━━━━ 15:41:20\n",
      "Accuracy: 0.3106 - Loss: 2.2241\n",
      "\n",
      "Batch 34/992 ━━━━━━━━━━━━━━━━━━━━ 15:41:31\n",
      "Accuracy: 0.3235 - Loss: 2.1902\n",
      "\n",
      "Batch 35/992 ━━━━━━━━━━━━━━━━━━━━ 15:41:42\n",
      "Accuracy: 0.3214 - Loss: 2.1749\n",
      "\n",
      "Batch 36/992 ━━━━━━━━━━━━━━━━━━━━ 15:41:53\n",
      "Accuracy: 0.3264 - Loss: 2.1580\n",
      "\n",
      "Batch 37/992 ━━━━━━━━━━━━━━━━━━━━ 15:42:04\n",
      "Accuracy: 0.3243 - Loss: 2.1524\n",
      "\n",
      "Batch 38/992 ━━━━━━━━━━━━━━━━━━━━ 15:42:15\n",
      "Accuracy: 0.3289 - Loss: 2.1348\n",
      "\n",
      "Batch 39/992 ━━━━━━━━━━━━━━━━━━━━ 15:42:26\n",
      "Accuracy: 0.3301 - Loss: 2.1279\n",
      "\n",
      "Batch 40/992 ━━━━━━━━━━━━━━━━━━━━ 15:42:37\n",
      "Accuracy: 0.3313 - Loss: 2.1179\n",
      "\n",
      "Batch 41/992 ━━━━━━━━━━━━━━━━━━━━ 15:42:47\n",
      "Accuracy: 0.3293 - Loss: 2.1261\n",
      "\n",
      "Batch 42/992 ━━━━━━━━━━━━━━━━━━━━ 15:42:58\n",
      "Accuracy: 0.3274 - Loss: 2.1177\n",
      "\n",
      "Batch 43/992 ━━━━━━━━━━━━━━━━━━━━ 15:43:08\n",
      "Accuracy: 0.3227 - Loss: 2.1163\n",
      "\n",
      "Batch 44/992 ━━━━━━━━━━━━━━━━━━━━ 15:43:19\n",
      "Accuracy: 0.3182 - Loss: 2.1138\n",
      "\n",
      "Batch 45/992 ━━━━━━━━━━━━━━━━━━━━ 15:43:29\n",
      "Accuracy: 0.3222 - Loss: 2.1086\n",
      "\n",
      "Batch 46/992 ━━━━━━━━━━━━━━━━━━━━ 15:43:40\n",
      "Accuracy: 0.3179 - Loss: 2.1087\n",
      "\n",
      "Batch 47/992 ━━━━━━━━━━━━━━━━━━━━ 15:43:50\n",
      "Accuracy: 0.3138 - Loss: 2.1121\n",
      "\n",
      "Batch 48/992 ━━━━━━━━━━━━━━━━━━━━ 15:44:00\n",
      "Accuracy: 0.3177 - Loss: 2.0926\n",
      "\n",
      "Batch 49/992 ━━━━━━━━━━━━━━━━━━━━ 15:44:11\n",
      "Accuracy: 0.3189 - Loss: 2.0752\n",
      "\n",
      "Batch 50/992 ━━━━━━━━━━━━━━━━━━━━ 15:44:22\n",
      "Accuracy: 0.3200 - Loss: 2.0726\n",
      "\n",
      "Batch 51/992 ━━━━━━━━━━━━━━━━━━━━ 15:44:33\n",
      "Accuracy: 0.3186 - Loss: 2.0638\n",
      "\n",
      "Batch 52/992 ━━━━━━━━━━━━━━━━━━━━ 15:44:44\n",
      "Accuracy: 0.3221 - Loss: 2.0584\n",
      "\n",
      "Batch 53/992 ━━━━━━━━━━━━━━━━━━━━ 15:44:55\n",
      "Accuracy: 0.3208 - Loss: 2.0584\n",
      "\n",
      "Batch 54/992 ━━━━━━━━━━━━━━━━━━━━ 15:45:06\n",
      "Accuracy: 0.3218 - Loss: 2.0597\n",
      "\n",
      "Batch 55/992 ━━━━━━━━━━━━━━━━━━━━ 15:45:17\n",
      "Accuracy: 0.3182 - Loss: 2.0590\n",
      "\n",
      "Batch 56/992 ━━━━━━━━━━━━━━━━━━━━ 15:45:27\n",
      "Accuracy: 0.3147 - Loss: 2.0586\n",
      "\n",
      "Batch 57/992 ━━━━━━━━━━━━━━━━━━━━ 15:45:38\n",
      "Accuracy: 0.3136 - Loss: 2.0589\n",
      "\n",
      "Batch 58/992 ━━━━━━━━━━━━━━━━━━━━ 15:45:48\n",
      "Accuracy: 0.3082 - Loss: 2.0523\n",
      "\n",
      "Batch 59/992 ━━━━━━━━━━━━━━━━━━━━ 15:45:59\n",
      "Accuracy: 0.3114 - Loss: 2.0465\n",
      "\n",
      "Batch 60/992 ━━━━━━━━━━━━━━━━━━━━ 15:46:09\n",
      "Accuracy: 0.3104 - Loss: 2.0452\n",
      "\n",
      "Batch 61/992 ━━━━━━━━━━━━━━━━━━━━ 15:46:21\n",
      "Accuracy: 0.3094 - Loss: 2.0444\n",
      "\n",
      "Batch 62/992 ━━━━━━━━━━━━━━━━━━━━ 15:46:33\n",
      "Accuracy: 0.3105 - Loss: 2.0371\n",
      "\n",
      "Batch 63/992 ━━━━━━━━━━━━━━━━━━━━ 15:46:44\n",
      "Accuracy: 0.3155 - Loss: 2.0249\n",
      "\n",
      "Batch 64/992 ━━━━━━━━━━━━━━━━━━━━ 15:46:54\n",
      "Accuracy: 0.3145 - Loss: 2.0205\n",
      "\n",
      "Batch 65/992 ━━━━━━━━━━━━━━━━━━━━ 15:47:05\n",
      "Accuracy: 0.3154 - Loss: 2.0140\n",
      "\n",
      "Batch 66/992 ━━━━━━━━━━━━━━━━━━━━ 15:47:15\n",
      "Accuracy: 0.3201 - Loss: 2.0030\n",
      "\n",
      "Batch 67/992 ━━━━━━━━━━━━━━━━━━━━ 15:47:25\n",
      "Accuracy: 0.3190 - Loss: 1.9999\n",
      "\n",
      "Batch 68/992 ━━━━━━━━━━━━━━━━━━━━ 15:47:35\n",
      "Accuracy: 0.3180 - Loss: 1.9964\n",
      "\n",
      "Batch 69/992 ━━━━━━━━━━━━━━━━━━━━ 15:47:45\n",
      "Accuracy: 0.3188 - Loss: 1.9894\n",
      "\n",
      "Batch 70/992 ━━━━━━━━━━━━━━━━━━━━ 15:47:56\n",
      "Accuracy: 0.3161 - Loss: 1.9876\n",
      "\n",
      "Batch 71/992 ━━━━━━━━━━━━━━━━━━━━ 15:48:06\n",
      "Accuracy: 0.3134 - Loss: 1.9866\n",
      "\n",
      "Batch 72/992 ━━━━━━━━━━━━━━━━━━━━ 15:48:16\n",
      "Accuracy: 0.3125 - Loss: 1.9867\n",
      "\n",
      "Batch 73/992 ━━━━━━━━━━━━━━━━━━━━ 15:48:26\n",
      "Accuracy: 0.3116 - Loss: 1.9828\n",
      "\n",
      "Batch 74/992 ━━━━━━━━━━━━━━━━━━━━ 15:48:37\n",
      "Accuracy: 0.3125 - Loss: 1.9773\n",
      "\n",
      "Batch 75/992 ━━━━━━━━━━━━━━━━━━━━ 15:48:47\n",
      "Accuracy: 0.3100 - Loss: 1.9732\n",
      "\n",
      "Batch 76/992 ━━━━━━━━━━━━━━━━━━━━ 15:48:57\n",
      "Accuracy: 0.3059 - Loss: 1.9765\n",
      "\n",
      "Batch 77/992 ━━━━━━━━━━━━━━━━━━━━ 15:49:07\n",
      "Accuracy: 0.3068 - Loss: 1.9728\n",
      "\n",
      "Batch 78/992 ━━━━━━━━━━━━━━━━━━━━ 15:49:17\n",
      "Accuracy: 0.3109 - Loss: 1.9668\n",
      "\n",
      "Batch 79/992 ━━━━━━━━━━━━━━━━━━━━ 15:49:27\n",
      "Accuracy: 0.3070 - Loss: 1.9686\n",
      "\n",
      "Batch 80/992 ━━━━━━━━━━━━━━━━━━━━ 15:49:37\n",
      "Accuracy: 0.3094 - Loss: 1.9614\n",
      "\n",
      "Batch 81/992 ━━━━━━━━━━━━━━━━━━━━ 15:49:47\n",
      "Accuracy: 0.3148 - Loss: 1.9553\n",
      "\n",
      "Batch 82/992 ━━━━━━━━━━━━━━━━━━━━ 15:49:57\n",
      "Accuracy: 0.3186 - Loss: 1.9470\n",
      "\n",
      "Batch 83/992 ━━━━━━━━━━━━━━━━━━━━ 15:50:07\n",
      "Accuracy: 0.3178 - Loss: 1.9421\n",
      "\n",
      "Batch 84/992 ━━━━━━━━━━━━━━━━━━━━ 15:50:18\n",
      "Accuracy: 0.3140 - Loss: 1.9442\n",
      "\n",
      "Batch 85/992 ━━━━━━━━━━━━━━━━━━━━ 15:50:28\n",
      "Accuracy: 0.3147 - Loss: 1.9396\n",
      "\n",
      "Batch 86/992 ━━━━━━━━━━━━━━━━━━━━ 15:50:38\n",
      "Accuracy: 0.3154 - Loss: 1.9353\n",
      "\n",
      "Batch 87/992 ━━━━━━━━━━━━━━━━━━━━ 15:50:48\n",
      "Accuracy: 0.3175 - Loss: 1.9298\n",
      "\n",
      "Batch 88/992 ━━━━━━━━━━━━━━━━━━━━ 15:50:59\n",
      "Accuracy: 0.3196 - Loss: 1.9226\n",
      "\n",
      "Batch 89/992 ━━━━━━━━━━━━━━━━━━━━ 15:51:09\n",
      "Accuracy: 0.3174 - Loss: 1.9199\n",
      "\n",
      "Batch 90/992 ━━━━━━━━━━━━━━━━━━━━ 15:51:19\n",
      "Accuracy: 0.3167 - Loss: 1.9168\n",
      "\n",
      "Batch 91/992 ━━━━━━━━━━━━━━━━━━━━ 15:51:29\n",
      "Accuracy: 0.3159 - Loss: 1.9155\n",
      "\n",
      "Batch 92/992 ━━━━━━━━━━━━━━━━━━━━ 15:51:39\n",
      "Accuracy: 0.3152 - Loss: 1.9128\n",
      "\n",
      "Batch 93/992 ━━━━━━━━━━━━━━━━━━━━ 15:51:49\n",
      "Accuracy: 0.3145 - Loss: 1.9072\n",
      "\n",
      "Batch 94/992 ━━━━━━━━━━━━━━━━━━━━ 15:51:59\n",
      "Accuracy: 0.3152 - Loss: 1.9013\n",
      "\n",
      "Batch 95/992 ━━━━━━━━━━━━━━━━━━━━ 15:52:10\n",
      "Accuracy: 0.3145 - Loss: 1.8961\n",
      "\n",
      "Batch 96/992 ━━━━━━━━━━━━━━━━━━━━ 15:52:20\n",
      "Accuracy: 0.3151 - Loss: 1.8921\n",
      "\n",
      "Batch 97/992 ━━━━━━━━━━━━━━━━━━━━ 15:52:30\n",
      "Accuracy: 0.3183 - Loss: 1.8865\n",
      "\n",
      "Batch 98/992 ━━━━━━━━━━━━━━━━━━━━ 15:52:41\n",
      "Accuracy: 0.3151 - Loss: 1.8910\n",
      "\n",
      "Batch 99/992 ━━━━━━━━━━━━━━━━━━━━ 15:52:51\n",
      "Accuracy: 0.3157 - Loss: 1.8881\n",
      "\n",
      "Batch 100/992 ━━━━━━━━━━━━━━━━━━━━ 15:53:01\n",
      "Accuracy: 0.3175 - Loss: 1.8830\n",
      "\n",
      "Batch 101/992 ━━━━━━━━━━━━━━━━━━━━ 15:53:11\n",
      "Accuracy: 0.3181 - Loss: 1.8799\n",
      "\n",
      "Batch 102/992 ━━━━━━━━━━━━━━━━━━━━ 15:53:21\n",
      "Accuracy: 0.3186 - Loss: 1.8773\n",
      "\n",
      "Batch 103/992 ━━━━━━━━━━━━━━━━━━━━ 15:53:31\n",
      "Accuracy: 0.3167 - Loss: 1.8779\n",
      "\n",
      "Batch 104/992 ━━━━━━━━━━━━━━━━━━━━ 15:53:41\n",
      "Accuracy: 0.3185 - Loss: 1.8756\n",
      "\n",
      "Batch 105/992 ━━━━━━━━━━━━━━━━━━━━ 15:53:52\n",
      "Accuracy: 0.3202 - Loss: 1.8738\n",
      "\n",
      "Batch 106/992 ━━━━━━━━━━━━━━━━━━━━ 15:54:02\n",
      "Accuracy: 0.3172 - Loss: 1.8798\n",
      "\n",
      "Batch 107/992 ━━━━━━━━━━━━━━━━━━━━ 15:54:12\n",
      "Accuracy: 0.3189 - Loss: 1.8734\n",
      "\n",
      "Batch 108/992 ━━━━━━━━━━━━━━━━━━━━ 15:54:23\n",
      "Accuracy: 0.3183 - Loss: 1.8721\n",
      "\n",
      "Batch 109/992 ━━━━━━━━━━━━━━━━━━━━ 15:54:34\n",
      "Accuracy: 0.3165 - Loss: 1.8706\n",
      "\n",
      "Batch 110/992 ━━━━━━━━━━━━━━━━━━━━ 15:54:45\n",
      "Accuracy: 0.3170 - Loss: 1.8699\n",
      "\n",
      "Batch 111/992 ━━━━━━━━━━━━━━━━━━━━ 15:54:56\n",
      "Accuracy: 0.3176 - Loss: 1.8718\n",
      "\n",
      "Batch 112/992 ━━━━━━━━━━━━━━━━━━━━ 15:55:08\n",
      "Accuracy: 0.3181 - Loss: 1.8703\n",
      "\n",
      "Batch 113/992 ━━━━━━━━━━━━━━━━━━━━ 15:55:22\n",
      "Accuracy: 0.3186 - Loss: 1.8717\n",
      "\n",
      "Batch 114/992 ━━━━━━━━━━━━━━━━━━━━ 15:55:34\n",
      "Accuracy: 0.3169 - Loss: 1.8729\n",
      "\n",
      "Batch 115/992 ━━━━━━━━━━━━━━━━━━━━ 15:55:45\n",
      "Accuracy: 0.3163 - Loss: 1.8712\n",
      "\n",
      "Batch 116/992 ━━━━━━━━━━━━━━━━━━━━ 15:55:56\n",
      "Accuracy: 0.3147 - Loss: 1.8701\n",
      "\n",
      "Batch 117/992 ━━━━━━━━━━━━━━━━━━━━ 15:56:08\n",
      "Accuracy: 0.3130 - Loss: 1.8714\n",
      "\n",
      "Batch 118/992 ━━━━━━━━━━━━━━━━━━━━ 15:56:21\n",
      "Accuracy: 0.3157 - Loss: 1.8686\n",
      "\n",
      "Batch 119/992 ━━━━━━━━━━━━━━━━━━━━ 15:56:31\n",
      "Accuracy: 0.3183 - Loss: 1.8642\n",
      "\n",
      "Batch 120/992 ━━━━━━━━━━━━━━━━━━━━ 15:56:42\n",
      "Accuracy: 0.3198 - Loss: 1.8626\n",
      "\n",
      "Batch 121/992 ━━━━━━━━━━━━━━━━━━━━ 15:56:52\n",
      "Accuracy: 0.3202 - Loss: 1.8590\n",
      "\n",
      "Batch 122/992 ━━━━━━━━━━━━━━━━━━━━ 15:57:03\n",
      "Accuracy: 0.3207 - Loss: 1.8631\n",
      "\n",
      "Batch 123/992 ━━━━━━━━━━━━━━━━━━━━ 15:57:14\n",
      "Accuracy: 0.3211 - Loss: 1.8629\n",
      "\n",
      "Batch 124/992 ━━━━━━━━━━━━━━━━━━━━ 15:57:25\n",
      "Accuracy: 0.3196 - Loss: 1.8618\n",
      "\n",
      "Batch 125/992 ━━━━━━━━━━━━━━━━━━━━ 15:57:35\n",
      "Accuracy: 0.3190 - Loss: 1.8643\n",
      "\n",
      "Batch 126/992 ━━━━━━━━━━━━━━━━━━━━ 15:57:47\n",
      "Accuracy: 0.3204 - Loss: 1.8616\n",
      "\n",
      "Batch 127/992 ━━━━━━━━━━━━━━━━━━━━ 15:57:59\n",
      "Accuracy: 0.3199 - Loss: 1.8655\n",
      "\n",
      "Batch 128/992 ━━━━━━━━━━━━━━━━━━━━ 15:58:10\n",
      "Accuracy: 0.3203 - Loss: 1.8651\n",
      "\n",
      "Batch 129/992 ━━━━━━━━━━━━━━━━━━━━ 15:58:22\n",
      "Accuracy: 0.3198 - Loss: 1.8608\n",
      "\n",
      "Batch 130/992 ━━━━━━━━━━━━━━━━━━━━ 15:58:32\n",
      "Accuracy: 0.3212 - Loss: 1.8595\n",
      "\n",
      "Batch 131/992 ━━━━━━━━━━━━━━━━━━━━ 15:58:44\n",
      "Accuracy: 0.3206 - Loss: 1.8610\n",
      "\n",
      "Batch 132/992 ━━━━━━━━━━━━━━━━━━━━ 15:58:56\n",
      "Accuracy: 0.3182 - Loss: 1.8627\n",
      "\n",
      "Batch 133/992 ━━━━━━━━━━━━━━━━━━━━ 15:59:08\n",
      "Accuracy: 0.3167 - Loss: 1.8669\n",
      "\n",
      "Batch 134/992 ━━━━━━━━━━━━━━━━━━━━ 15:59:19\n",
      "Accuracy: 0.3162 - Loss: 1.8716\n",
      "\n",
      "Batch 135/992 ━━━━━━━━━━━━━━━━━━━━ 15:59:29\n",
      "Accuracy: 0.3167 - Loss: 1.8670\n",
      "\n",
      "Batch 136/992 ━━━━━━━━━━━━━━━━━━━━ 15:59:39\n",
      "Accuracy: 0.3180 - Loss: 1.8622\n",
      "\n",
      "Batch 137/992 ━━━━━━━━━━━━━━━━━━━━ 15:59:50\n",
      "Accuracy: 0.3203 - Loss: 1.8570\n",
      "\n",
      "Batch 138/992 ━━━━━━━━━━━━━━━━━━━━ 16:00:00\n",
      "Accuracy: 0.3207 - Loss: 1.8548\n",
      "\n",
      "Batch 139/992 ━━━━━━━━━━━━━━━━━━━━ 16:00:11\n",
      "Accuracy: 0.3219 - Loss: 1.8514\n",
      "\n",
      "Batch 140/992 ━━━━━━━━━━━━━━━━━━━━ 16:00:21\n",
      "Accuracy: 0.3232 - Loss: 1.8478\n",
      "\n",
      "Batch 141/992 ━━━━━━━━━━━━━━━━━━━━ 16:00:32\n",
      "Accuracy: 0.3227 - Loss: 1.8476\n",
      "\n",
      "Batch 142/992 ━━━━━━━━━━━━━━━━━━━━ 16:00:48\n",
      "Accuracy: 0.3231 - Loss: 1.8453\n",
      "\n",
      "Batch 143/992 ━━━━━━━━━━━━━━━━━━━━ 16:01:00\n",
      "Accuracy: 0.3217 - Loss: 1.8459\n",
      "\n",
      "Batch 144/992 ━━━━━━━━━━━━━━━━━━━━ 16:01:15\n",
      "Accuracy: 0.3220 - Loss: 1.8461\n",
      "\n",
      "Batch 145/992 ━━━━━━━━━━━━━━━━━━━━ 16:01:27\n",
      "Accuracy: 0.3233 - Loss: 1.8424\n",
      "\n",
      "Batch 146/992 ━━━━━━━━━━━━━━━━━━━━ 16:01:38\n",
      "Accuracy: 0.3228 - Loss: 1.8417\n",
      "\n",
      "Batch 147/992 ━━━━━━━━━━━━━━━━━━━━ 16:01:48\n",
      "Accuracy: 0.3231 - Loss: 1.8394\n",
      "\n",
      "Batch 148/992 ━━━━━━━━━━━━━━━━━━━━ 16:01:59\n",
      "Accuracy: 0.3226 - Loss: 1.8405\n",
      "\n",
      "Batch 149/992 ━━━━━━━━━━━━━━━━━━━━ 16:02:09\n",
      "Accuracy: 0.3221 - Loss: 1.8406\n",
      "\n",
      "Batch 150/992 ━━━━━━━━━━━━━━━━━━━━ 16:02:20\n",
      "Accuracy: 0.3225 - Loss: 1.8379\n",
      "\n",
      "Batch 151/992 ━━━━━━━━━━━━━━━━━━━━ 16:02:30\n",
      "Accuracy: 0.3245 - Loss: 1.8329\n",
      "\n",
      "Batch 152/992 ━━━━━━━━━━━━━━━━━━━━ 16:02:40\n",
      "Accuracy: 0.3240 - Loss: 1.8342\n",
      "\n",
      "Batch 153/992 ━━━━━━━━━━━━━━━━━━━━ 16:02:51\n",
      "Accuracy: 0.3252 - Loss: 1.8362\n",
      "\n",
      "Batch 154/992 ━━━━━━━━━━━━━━━━━━━━ 16:03:01\n",
      "Accuracy: 0.3255 - Loss: 1.8336\n",
      "\n",
      "Batch 155/992 ━━━━━━━━━━━━━━━━━━━━ 16:03:11\n",
      "Accuracy: 0.3282 - Loss: 1.8298\n",
      "\n",
      "Batch 156/992 ━━━━━━━━━━━━━━━━━━━━ 16:03:21\n",
      "Accuracy: 0.3293 - Loss: 1.8283\n",
      "\n",
      "Batch 157/992 ━━━━━━━━━━━━━━━━━━━━ 16:03:32\n",
      "Accuracy: 0.3312 - Loss: 1.8241\n",
      "\n",
      "Batch 158/992 ━━━━━━━━━━━━━━━━━━━━ 16:03:43\n",
      "Accuracy: 0.3323 - Loss: 1.8232\n",
      "\n",
      "Batch 159/992 ━━━━━━━━━━━━━━━━━━━━ 16:03:54\n",
      "Accuracy: 0.3325 - Loss: 1.8221\n",
      "\n",
      "Batch 160/992 ━━━━━━━━━━━━━━━━━━━━ 16:04:05\n",
      "Accuracy: 0.3328 - Loss: 1.8218\n",
      "\n",
      "Batch 161/992 ━━━━━━━━━━━━━━━━━━━━ 16:04:16\n",
      "Accuracy: 0.3354 - Loss: 1.8175\n",
      "\n",
      "Batch 162/992 ━━━━━━━━━━━━━━━━━━━━ 16:04:27\n",
      "Accuracy: 0.3364 - Loss: 1.8149\n",
      "\n",
      "Batch 163/992 ━━━━━━━━━━━━━━━━━━━━ 16:04:39\n",
      "Accuracy: 0.3374 - Loss: 1.8126\n",
      "\n",
      "Batch 164/992 ━━━━━━━━━━━━━━━━━━━━ 16:04:50\n",
      "Accuracy: 0.3369 - Loss: 1.8140\n",
      "\n",
      "Batch 165/992 ━━━━━━━━━━━━━━━━━━━━ 16:05:01\n",
      "Accuracy: 0.3386 - Loss: 1.8111\n",
      "\n",
      "Batch 166/992 ━━━━━━━━━━━━━━━━━━━━ 16:05:13\n",
      "Accuracy: 0.3381 - Loss: 1.8120\n",
      "\n",
      "Batch 167/992 ━━━━━━━━━━━━━━━━━━━━ 16:05:23\n",
      "Accuracy: 0.3383 - Loss: 1.8103\n",
      "\n",
      "Batch 168/992 ━━━━━━━━━━━━━━━━━━━━ 16:05:34\n",
      "Accuracy: 0.3378 - Loss: 1.8124\n",
      "\n",
      "Batch 169/992 ━━━━━━━━━━━━━━━━━━━━ 16:05:44\n",
      "Accuracy: 0.3388 - Loss: 1.8104\n",
      "\n",
      "Batch 170/992 ━━━━━━━━━━━━━━━━━━━━ 16:05:55\n",
      "Accuracy: 0.3390 - Loss: 1.8083\n",
      "\n",
      "Batch 171/992 ━━━━━━━━━━━━━━━━━━━━ 16:06:06\n",
      "Accuracy: 0.3377 - Loss: 1.8096\n",
      "\n",
      "Batch 172/992 ━━━━━━━━━━━━━━━━━━━━ 16:06:17\n",
      "Accuracy: 0.3379 - Loss: 1.8085\n",
      "\n",
      "Batch 173/992 ━━━━━━━━━━━━━━━━━━━━ 16:06:28\n",
      "Accuracy: 0.3374 - Loss: 1.8066\n",
      "\n",
      "Batch 174/992 ━━━━━━━━━━━━━━━━━━━━ 16:06:40\n",
      "Accuracy: 0.3398 - Loss: 1.8019\n",
      "\n",
      "Batch 175/992 ━━━━━━━━━━━━━━━━━━━━ 16:06:51\n",
      "Accuracy: 0.3407 - Loss: 1.7985\n",
      "\n",
      "Batch 176/992 ━━━━━━━━━━━━━━━━━━━━ 16:07:01\n",
      "Accuracy: 0.3409 - Loss: 1.7964\n",
      "\n",
      "Batch 177/992 ━━━━━━━━━━━━━━━━━━━━ 16:07:11\n",
      "Accuracy: 0.3404 - Loss: 1.7986\n",
      "\n",
      "Batch 178/992 ━━━━━━━━━━━━━━━━━━━━ 16:07:22\n",
      "Accuracy: 0.3392 - Loss: 1.7987\n",
      "\n",
      "Batch 179/992 ━━━━━━━━━━━━━━━━━━━━ 16:07:32\n",
      "Accuracy: 0.3387 - Loss: 1.7997\n",
      "\n",
      "Batch 180/992 ━━━━━━━━━━━━━━━━━━━━ 16:07:42\n",
      "Accuracy: 0.3396 - Loss: 1.7969\n",
      "\n",
      "Batch 181/992 ━━━━━━━━━━━━━━━━━━━━ 16:07:53\n",
      "Accuracy: 0.3391 - Loss: 1.7966\n",
      "\n",
      "Batch 182/992 ━━━━━━━━━━━━━━━━━━━━ 16:08:04\n",
      "Accuracy: 0.3420 - Loss: 1.7917\n",
      "\n",
      "Batch 183/992 ━━━━━━━━━━━━━━━━━━━━ 16:08:14\n",
      "Accuracy: 0.3422 - Loss: 1.7885\n",
      "\n",
      "Batch 184/992 ━━━━━━━━━━━━━━━━━━━━ 16:08:25\n",
      "Accuracy: 0.3438 - Loss: 1.7847\n",
      "\n",
      "Batch 185/992 ━━━━━━━━━━━━━━━━━━━━ 16:08:36\n",
      "Accuracy: 0.3432 - Loss: 1.7832\n",
      "\n",
      "Batch 186/992 ━━━━━━━━━━━━━━━━━━━━ 16:08:47\n",
      "Accuracy: 0.3441 - Loss: 1.7821\n",
      "\n",
      "Batch 187/992 ━━━━━━━━━━━━━━━━━━━━ 16:08:57\n",
      "Accuracy: 0.3436 - Loss: 1.7799\n",
      "\n",
      "Batch 188/992 ━━━━━━━━━━━━━━━━━━━━ 16:09:08\n",
      "Accuracy: 0.3431 - Loss: 1.7785\n",
      "\n",
      "Batch 189/992 ━━━━━━━━━━━━━━━━━━━━ 16:09:18\n",
      "Accuracy: 0.3439 - Loss: 1.7759\n",
      "\n",
      "Batch 190/992 ━━━━━━━━━━━━━━━━━━━━ 16:09:29\n",
      "Accuracy: 0.3441 - Loss: 1.7765\n",
      "\n",
      "Batch 191/992 ━━━━━━━━━━━━━━━━━━━━ 16:09:39\n",
      "Accuracy: 0.3442 - Loss: 1.7742\n",
      "\n",
      "Batch 192/992 ━━━━━━━━━━━━━━━━━━━━ 16:09:50\n",
      "Accuracy: 0.3438 - Loss: 1.7719\n",
      "\n",
      "Batch 193/992 ━━━━━━━━━━━━━━━━━━━━ 16:10:00\n",
      "Accuracy: 0.3452 - Loss: 1.7676\n",
      "\n",
      "Batch 194/992 ━━━━━━━━━━━━━━━━━━━━ 16:10:11\n",
      "Accuracy: 0.3460 - Loss: 1.7653\n",
      "\n",
      "Batch 195/992 ━━━━━━━━━━━━━━━━━━━━ 16:10:22\n",
      "Accuracy: 0.3455 - Loss: 1.7635\n",
      "\n",
      "Batch 196/992 ━━━━━━━━━━━━━━━━━━━━ 16:10:32\n",
      "Accuracy: 0.3457 - Loss: 1.7618\n",
      "\n",
      "Batch 197/992 ━━━━━━━━━━━━━━━━━━━━ 16:10:43\n",
      "Accuracy: 0.3477 - Loss: 1.7570\n",
      "\n",
      "Batch 198/992 ━━━━━━━━━━━━━━━━━━━━ 16:10:53\n",
      "Accuracy: 0.3485 - Loss: 1.7559\n",
      "\n",
      "Batch 199/992 ━━━━━━━━━━━━━━━━━━━━ 16:11:05\n",
      "Accuracy: 0.3499 - Loss: 1.7529\n",
      "\n",
      "Batch 200/992 ━━━━━━━━━━━━━━━━━━━━ 16:11:15\n",
      "Accuracy: 0.3506 - Loss: 1.7509\n",
      "\n",
      "Batch 201/992 ━━━━━━━━━━━━━━━━━━━━ 16:11:26\n",
      "Accuracy: 0.3514 - Loss: 1.7493\n",
      "\n",
      "Batch 202/992 ━━━━━━━━━━━━━━━━━━━━ 16:11:36\n",
      "Accuracy: 0.3527 - Loss: 1.7474\n",
      "\n",
      "Batch 203/992 ━━━━━━━━━━━━━━━━━━━━ 16:11:47\n",
      "Accuracy: 0.3528 - Loss: 1.7457\n",
      "\n",
      "Batch 204/992 ━━━━━━━━━━━━━━━━━━━━ 16:11:57\n",
      "Accuracy: 0.3523 - Loss: 1.7448\n",
      "\n",
      "Batch 205/992 ━━━━━━━━━━━━━━━━━━━━ 16:12:08\n",
      "Accuracy: 0.3530 - Loss: 1.7445\n",
      "\n",
      "Batch 206/992 ━━━━━━━━━━━━━━━━━━━━ 16:12:18\n",
      "Accuracy: 0.3538 - Loss: 1.7439\n",
      "\n",
      "Batch 207/992 ━━━━━━━━━━━━━━━━━━━━ 16:12:29\n",
      "Accuracy: 0.3545 - Loss: 1.7420\n",
      "\n",
      "Batch 208/992 ━━━━━━━━━━━━━━━━━━━━ 16:12:40\n",
      "Accuracy: 0.3552 - Loss: 1.7416\n",
      "\n",
      "Batch 209/992 ━━━━━━━━━━━━━━━━━━━━ 16:12:51\n",
      "Accuracy: 0.3565 - Loss: 1.7386\n",
      "\n",
      "Batch 210/992 ━━━━━━━━━━━━━━━━━━━━ 16:13:02\n",
      "Accuracy: 0.3560 - Loss: 1.7380\n",
      "\n",
      "Batch 211/992 ━━━━━━━━━━━━━━━━━━━━ 16:13:13\n",
      "Accuracy: 0.3555 - Loss: 1.7376\n",
      "\n",
      "Batch 212/992 ━━━━━━━━━━━━━━━━━━━━ 16:13:23\n",
      "Accuracy: 0.3561 - Loss: 1.7352\n",
      "\n",
      "Batch 213/992 ━━━━━━━━━━━━━━━━━━━━ 16:13:34\n",
      "Accuracy: 0.3574 - Loss: 1.7323\n",
      "\n",
      "Batch 214/992 ━━━━━━━━━━━━━━━━━━━━ 16:13:44\n",
      "Accuracy: 0.3563 - Loss: 1.7315\n",
      "\n",
      "Batch 215/992 ━━━━━━━━━━━━━━━━━━━━ 16:13:55\n",
      "Accuracy: 0.3564 - Loss: 1.7305\n",
      "\n",
      "Batch 216/992 ━━━━━━━━━━━━━━━━━━━━ 16:14:06\n",
      "Accuracy: 0.3571 - Loss: 1.7277\n",
      "\n",
      "Batch 217/992 ━━━━━━━━━━━━━━━━━━━━ 16:14:17\n",
      "Accuracy: 0.3577 - Loss: 1.7270\n",
      "\n",
      "Batch 218/992 ━━━━━━━━━━━━━━━━━━━━ 16:14:29\n",
      "Accuracy: 0.3584 - Loss: 1.7250\n",
      "\n",
      "Batch 219/992 ━━━━━━━━━━━━━━━━━━━━ 16:14:44\n",
      "Accuracy: 0.3590 - Loss: 1.7232\n",
      "\n",
      "Batch 220/992 ━━━━━━━━━━━━━━━━━━━━ 16:14:58\n",
      "Accuracy: 0.3591 - Loss: 1.7229\n",
      "\n",
      "Batch 221/992 ━━━━━━━━━━━━━━━━━━━━ 16:15:10\n",
      "Accuracy: 0.3597 - Loss: 1.7215\n",
      "\n",
      "Batch 222/992 ━━━━━━━━━━━━━━━━━━━━ 16:15:22\n",
      "Accuracy: 0.3604 - Loss: 1.7191\n",
      "\n",
      "Batch 223/992 ━━━━━━━━━━━━━━━━━━━━ 16:15:36\n",
      "Accuracy: 0.3615 - Loss: 1.7155\n",
      "\n",
      "Batch 224/992 ━━━━━━━━━━━━━━━━━━━━ 16:15:50\n",
      "Accuracy: 0.3622 - Loss: 1.7154\n",
      "\n",
      "Batch 225/992 ━━━━━━━━━━━━━━━━━━━━ 16:16:01\n",
      "Accuracy: 0.3622 - Loss: 1.7134\n",
      "\n",
      "Batch 226/992 ━━━━━━━━━━━━━━━━━━━━ 16:16:13\n",
      "Accuracy: 0.3634 - Loss: 1.7114\n",
      "\n",
      "Batch 227/992 ━━━━━━━━━━━━━━━━━━━━ 16:16:24\n",
      "Accuracy: 0.3634 - Loss: 1.7101\n",
      "\n",
      "Batch 228/992 ━━━━━━━━━━━━━━━━━━━━ 16:16:34\n",
      "Accuracy: 0.3651 - Loss: 1.7055\n",
      "\n",
      "Batch 229/992 ━━━━━━━━━━━━━━━━━━━━ 16:16:46\n",
      "Accuracy: 0.3657 - Loss: 1.7036\n",
      "\n",
      "Batch 230/992 ━━━━━━━━━━━━━━━━━━━━ 16:16:56\n",
      "Accuracy: 0.3668 - Loss: 1.7024\n",
      "\n",
      "Batch 231/992 ━━━━━━━━━━━━━━━━━━━━ 16:17:08\n",
      "Accuracy: 0.3663 - Loss: 1.7019\n",
      "\n",
      "Batch 232/992 ━━━━━━━━━━━━━━━━━━━━ 16:17:19\n",
      "Accuracy: 0.3675 - Loss: 1.7005\n",
      "\n",
      "Batch 233/992 ━━━━━━━━━━━━━━━━━━━━ 16:17:31\n",
      "Accuracy: 0.3691 - Loss: 1.6975\n",
      "\n",
      "Batch 234/992 ━━━━━━━━━━━━━━━━━━━━ 16:17:43\n",
      "Accuracy: 0.3697 - Loss: 1.6976\n",
      "\n",
      "Batch 235/992 ━━━━━━━━━━━━━━━━━━━━ 16:17:54\n",
      "Accuracy: 0.3707 - Loss: 1.6961\n",
      "\n",
      "Batch 236/992 ━━━━━━━━━━━━━━━━━━━━ 16:18:06\n",
      "Accuracy: 0.3713 - Loss: 1.6946\n",
      "\n",
      "Batch 237/992 ━━━━━━━━━━━━━━━━━━━━ 16:18:21\n",
      "Accuracy: 0.3718 - Loss: 1.6928\n",
      "\n",
      "Batch 238/992 ━━━━━━━━━━━━━━━━━━━━ 16:18:33\n",
      "Accuracy: 0.3734 - Loss: 1.6897\n",
      "\n",
      "Batch 239/992 ━━━━━━━━━━━━━━━━━━━━ 16:18:47\n",
      "Accuracy: 0.3740 - Loss: 1.6884\n",
      "\n",
      "Batch 240/992 ━━━━━━━━━━━━━━━━━━━━ 16:19:01\n",
      "Accuracy: 0.3740 - Loss: 1.6869\n",
      "\n",
      "Batch 241/992 ━━━━━━━━━━━━━━━━━━━━ 16:19:14\n",
      "Accuracy: 0.3750 - Loss: 1.6853\n",
      "\n",
      "Batch 242/992 ━━━━━━━━━━━━━━━━━━━━ 16:19:25\n",
      "Accuracy: 0.3755 - Loss: 1.6841\n",
      "\n",
      "Batch 243/992 ━━━━━━━━━━━━━━━━━━━━ 16:19:36\n",
      "Accuracy: 0.3760 - Loss: 1.6831\n",
      "\n",
      "Batch 244/992 ━━━━━━━━━━━━━━━━━━━━ 16:19:50\n",
      "Accuracy: 0.3765 - Loss: 1.6809\n",
      "\n",
      "Batch 245/992 ━━━━━━━━━━━━━━━━━━━━ 16:20:00\n",
      "Accuracy: 0.3776 - Loss: 1.6794\n",
      "\n",
      "Batch 246/992 ━━━━━━━━━━━━━━━━━━━━ 16:20:14\n",
      "Accuracy: 0.3775 - Loss: 1.6783\n",
      "\n",
      "Batch 247/992 ━━━━━━━━━━━━━━━━━━━━ 16:20:28\n",
      "Accuracy: 0.3780 - Loss: 1.6766\n",
      "\n",
      "Batch 248/992 ━━━━━━━━━━━━━━━━━━━━ 16:20:42\n",
      "Accuracy: 0.3790 - Loss: 1.6754\n",
      "\n",
      "Batch 249/992 ━━━━━━━━━━━━━━━━━━━━ 16:20:56\n",
      "Accuracy: 0.3790 - Loss: 1.6733\n",
      "\n",
      "Batch 250/992 ━━━━━━━━━━━━━━━━━━━━ 16:21:08\n",
      "Accuracy: 0.3795 - Loss: 1.6731\n",
      "\n",
      "Batch 251/992 ━━━━━━━━━━━━━━━━━━━━ 16:21:22\n",
      "Accuracy: 0.3800 - Loss: 1.6724\n",
      "\n",
      "Batch 252/992 ━━━━━━━━━━━━━━━━━━━━ 16:21:36\n",
      "Accuracy: 0.3810 - Loss: 1.6716\n",
      "\n",
      "Batch 253/992 ━━━━━━━━━━━━━━━━━━━━ 16:21:49\n",
      "Accuracy: 0.3824 - Loss: 1.6686\n",
      "\n",
      "Batch 254/992 ━━━━━━━━━━━━━━━━━━━━ 16:22:03\n",
      "Accuracy: 0.3824 - Loss: 1.6688\n",
      "\n",
      "Batch 255/992 ━━━━━━━━━━━━━━━━━━━━ 16:22:17\n",
      "Accuracy: 0.3838 - Loss: 1.6670\n",
      "\n",
      "Batch 256/992 ━━━━━━━━━━━━━━━━━━━━ 16:22:29\n",
      "Accuracy: 0.3848 - Loss: 1.6651\n",
      "\n",
      "Batch 257/992 ━━━━━━━━━━━━━━━━━━━━ 16:22:43\n",
      "Accuracy: 0.3838 - Loss: 1.6666\n",
      "\n",
      "Batch 258/992 ━━━━━━━━━━━━━━━━━━━━ 16:22:56\n",
      "Accuracy: 0.3842 - Loss: 1.6643\n",
      "\n",
      "Batch 259/992 ━━━━━━━━━━━━━━━━━━━━ 16:23:09\n",
      "Accuracy: 0.3851 - Loss: 1.6614\n",
      "\n",
      "Batch 260/992 ━━━━━━━━━━━━━━━━━━━━ 16:23:21\n",
      "Accuracy: 0.3851 - Loss: 1.6608\n",
      "\n",
      "Batch 261/992 ━━━━━━━━━━━━━━━━━━━━ 16:23:34\n",
      "Accuracy: 0.3855 - Loss: 1.6599\n",
      "\n",
      "Batch 262/992 ━━━━━━━━━━━━━━━━━━━━ 16:23:47\n",
      "Accuracy: 0.3860 - Loss: 1.6587\n",
      "\n",
      "Batch 263/992 ━━━━━━━━━━━━━━━━━━━━ 16:24:03\n",
      "Accuracy: 0.3869 - Loss: 1.6575\n",
      "\n",
      "Batch 264/992 ━━━━━━━━━━━━━━━━━━━━ 16:24:19\n",
      "Accuracy: 0.3873 - Loss: 1.6578\n",
      "\n",
      "Batch 265/992 ━━━━━━━━━━━━━━━━━━━━ 16:24:34\n",
      "Accuracy: 0.3873 - Loss: 1.6575\n",
      "\n",
      "Batch 266/992 ━━━━━━━━━━━━━━━━━━━━ 16:24:49\n",
      "Accuracy: 0.3872 - Loss: 1.6567\n",
      "\n",
      "Batch 267/992 ━━━━━━━━━━━━━━━━━━━━ 16:25:05\n",
      "Accuracy: 0.3858 - Loss: 1.6583\n",
      "\n",
      "Batch 268/992 ━━━━━━━━━━━━━━━━━━━━ 16:25:20\n",
      "Accuracy: 0.3853 - Loss: 1.6583\n",
      "\n",
      "Batch 269/992 ━━━━━━━━━━━━━━━━━━━━ 16:25:35\n",
      "Accuracy: 0.3852 - Loss: 1.6574\n",
      "\n",
      "Batch 270/992 ━━━━━━━━━━━━━━━━━━━━ 16:25:50\n",
      "Accuracy: 0.3866 - Loss: 1.6556\n",
      "\n",
      "Batch 271/992 ━━━━━━━━━━━━━━━━━━━━ 16:26:05\n",
      "Accuracy: 0.3879 - Loss: 1.6547\n",
      "\n",
      "Batch 272/992 ━━━━━━━━━━━━━━━━━━━━ 16:26:21\n",
      "Accuracy: 0.3888 - Loss: 1.6522\n",
      "\n",
      "Batch 273/992 ━━━━━━━━━━━━━━━━━━━━ 16:26:36\n",
      "Accuracy: 0.3897 - Loss: 1.6507\n",
      "\n",
      "Batch 274/992 ━━━━━━━━━━━━━━━━━━━━ 16:26:51\n",
      "Accuracy: 0.3901 - Loss: 1.6501\n",
      "\n",
      "Batch 275/992 ━━━━━━━━━━━━━━━━━━━━ 16:27:06\n",
      "Accuracy: 0.3900 - Loss: 1.6496\n",
      "\n",
      "Batch 276/992 ━━━━━━━━━━━━━━━━━━━━ 16:27:21\n",
      "Accuracy: 0.3895 - Loss: 1.6483\n",
      "\n",
      "Batch 277/992 ━━━━━━━━━━━━━━━━━━━━ 16:27:36\n",
      "Accuracy: 0.3899 - Loss: 1.6476\n",
      "\n",
      "Batch 278/992 ━━━━━━━━━━━━━━━━━━━━ 16:27:52\n",
      "Accuracy: 0.3907 - Loss: 1.6453\n",
      "\n",
      "Batch 279/992 ━━━━━━━━━━━━━━━━━━━━ 16:28:07\n",
      "Accuracy: 0.3925 - Loss: 1.6426\n",
      "\n",
      "Batch 280/992 ━━━━━━━━━━━━━━━━━━━━ 16:28:20\n",
      "Accuracy: 0.3933 - Loss: 1.6405\n",
      "\n",
      "Batch 281/992 ━━━━━━━━━━━━━━━━━━━━ 16:28:32\n",
      "Accuracy: 0.3932 - Loss: 1.6387\n",
      "\n",
      "Batch 282/992 ━━━━━━━━━━━━━━━━━━━━ 16:28:44\n",
      "Accuracy: 0.3927 - Loss: 1.6388\n",
      "\n",
      "Batch 283/992 ━━━━━━━━━━━━━━━━━━━━ 16:28:55\n",
      "Accuracy: 0.3931 - Loss: 1.6375\n",
      "\n",
      "Batch 284/992 ━━━━━━━━━━━━━━━━━━━━ 16:29:07\n",
      "Accuracy: 0.3926 - Loss: 1.6395\n",
      "\n",
      "Batch 285/992 ━━━━━━━━━━━━━━━━━━━━ 16:29:20\n",
      "Accuracy: 0.3930 - Loss: 1.6386\n",
      "\n",
      "Batch 286/992 ━━━━━━━━━━━━━━━━━━━━ 16:29:32\n",
      "Accuracy: 0.3929 - Loss: 1.6366\n",
      "\n",
      "Batch 287/992 ━━━━━━━━━━━━━━━━━━━━ 16:29:43\n",
      "Accuracy: 0.3937 - Loss: 1.6346\n",
      "\n",
      "Batch 288/992 ━━━━━━━━━━━━━━━━━━━━ 16:29:55\n",
      "Accuracy: 0.3941 - Loss: 1.6334\n",
      "\n",
      "Batch 289/992 ━━━━━━━━━━━━━━━━━━━━ 16:30:06\n",
      "Accuracy: 0.3949 - Loss: 1.6306\n",
      "\n",
      "Batch 290/992 ━━━━━━━━━━━━━━━━━━━━ 16:30:17\n",
      "Accuracy: 0.3953 - Loss: 1.6306\n",
      "\n",
      "Batch 291/992 ━━━━━━━━━━━━━━━━━━━━ 16:30:28\n",
      "Accuracy: 0.3956 - Loss: 1.6293\n",
      "\n",
      "Batch 292/992 ━━━━━━━━━━━━━━━━━━━━ 16:30:43\n",
      "Accuracy: 0.3947 - Loss: 1.6296\n",
      "\n",
      "Batch 293/992 ━━━━━━━━━━━━━━━━━━━━ 16:30:58\n",
      "Accuracy: 0.3951 - Loss: 1.6277\n",
      "\n",
      "Batch 294/992 ━━━━━━━━━━━━━━━━━━━━ 16:31:08\n",
      "Accuracy: 0.3946 - Loss: 1.6266\n",
      "\n",
      "Batch 295/992 ━━━━━━━━━━━━━━━━━━━━ 16:31:19\n",
      "Accuracy: 0.3962 - Loss: 1.6236\n",
      "\n",
      "Batch 296/992 ━━━━━━━━━━━━━━━━━━━━ 16:31:31\n",
      "Accuracy: 0.3974 - Loss: 1.6212\n",
      "\n",
      "Batch 297/992 ━━━━━━━━━━━━━━━━━━━━ 16:31:43\n",
      "Accuracy: 0.3981 - Loss: 1.6205\n",
      "\n",
      "Batch 298/992 ━━━━━━━━━━━━━━━━━━━━ 16:31:53\n",
      "Accuracy: 0.3993 - Loss: 1.6182\n",
      "\n",
      "Batch 299/992 ━━━━━━━━━━━━━━━━━━━━ 16:32:05\n",
      "Accuracy: 0.3997 - Loss: 1.6167\n",
      "\n",
      "Batch 300/992 ━━━━━━━━━━━━━━━━━━━━ 16:32:16\n",
      "Accuracy: 0.4000 - Loss: 1.6152\n",
      "\n",
      "Batch 301/992 ━━━━━━━━━━━━━━━━━━━━ 16:32:26\n",
      "Accuracy: 0.4007 - Loss: 1.6143\n",
      "\n",
      "Batch 302/992 ━━━━━━━━━━━━━━━━━━━━ 16:32:36\n",
      "Accuracy: 0.4011 - Loss: 1.6153\n",
      "\n",
      "Batch 303/992 ━━━━━━━━━━━━━━━━━━━━ 16:32:46\n",
      "Accuracy: 0.4010 - Loss: 1.6159\n",
      "\n",
      "Batch 304/992 ━━━━━━━━━━━━━━━━━━━━ 16:32:57\n",
      "Accuracy: 0.4017 - Loss: 1.6129\n",
      "\n",
      "Batch 305/992 ━━━━━━━━━━━━━━━━━━━━ 16:33:07\n",
      "Accuracy: 0.4020 - Loss: 1.6110\n",
      "\n",
      "Batch 306/992 ━━━━━━━━━━━━━━━━━━━━ 16:33:17\n",
      "Accuracy: 0.4032 - Loss: 1.6087\n",
      "\n",
      "Batch 307/992 ━━━━━━━━━━━━━━━━━━━━ 16:33:27\n",
      "Accuracy: 0.4027 - Loss: 1.6089\n",
      "\n",
      "Batch 308/992 ━━━━━━━━━━━━━━━━━━━━ 16:33:37\n",
      "Accuracy: 0.4026 - Loss: 1.6095\n",
      "\n",
      "Batch 309/992 ━━━━━━━━━━━━━━━━━━━━ 16:33:48\n",
      "Accuracy: 0.4041 - Loss: 1.6076\n",
      "\n",
      "Batch 310/992 ━━━━━━━━━━━━━━━━━━━━ 16:33:58\n",
      "Accuracy: 0.4048 - Loss: 1.6055\n",
      "\n",
      "Batch 311/992 ━━━━━━━━━━━━━━━━━━━━ 16:34:08\n",
      "Accuracy: 0.4043 - Loss: 1.6064\n",
      "\n",
      "Batch 312/992 ━━━━━━━━━━━━━━━━━━━━ 16:34:19\n",
      "Accuracy: 0.4042 - Loss: 1.6044\n",
      "\n",
      "Batch 313/992 ━━━━━━━━━━━━━━━━━━━━ 16:34:29\n",
      "Accuracy: 0.4034 - Loss: 1.6055\n",
      "\n",
      "Batch 314/992 ━━━━━━━━━━━━━━━━━━━━ 16:34:39\n",
      "Accuracy: 0.4037 - Loss: 1.6039\n",
      "\n",
      "Batch 315/992 ━━━━━━━━━━━━━━━━━━━━ 16:34:49\n",
      "Accuracy: 0.4048 - Loss: 1.6014\n",
      "\n",
      "Batch 316/992 ━━━━━━━━━━━━━━━━━━━━ 16:35:00\n",
      "Accuracy: 0.4055 - Loss: 1.5996\n",
      "\n",
      "Batch 317/992 ━━━━━━━━━━━━━━━━━━━━ 16:35:10\n",
      "Accuracy: 0.4058 - Loss: 1.5985\n",
      "\n",
      "Batch 318/992 ━━━━━━━━━━━━━━━━━━━━ 16:35:22\n",
      "Accuracy: 0.4057 - Loss: 1.5986\n",
      "\n",
      "Batch 319/992 ━━━━━━━━━━━━━━━━━━━━ 16:35:35\n",
      "Accuracy: 0.4056 - Loss: 1.5981\n",
      "\n",
      "Batch 320/992 ━━━━━━━━━━━━━━━━━━━━ 16:35:45\n",
      "Accuracy: 0.4066 - Loss: 1.5957\n",
      "\n",
      "Batch 321/992 ━━━━━━━━━━━━━━━━━━━━ 16:36:00\n",
      "Accuracy: 0.4077 - Loss: 1.5939\n",
      "\n",
      "Batch 322/992 ━━━━━━━━━━━━━━━━━━━━ 16:36:13\n",
      "Accuracy: 0.4092 - Loss: 1.5918\n",
      "\n",
      "Batch 323/992 ━━━━━━━━━━━━━━━━━━━━ 16:36:27\n",
      "Accuracy: 0.4094 - Loss: 1.5910\n",
      "\n",
      "Batch 324/992 ━━━━━━━━━━━━━━━━━━━━ 16:36:39\n",
      "Accuracy: 0.4097 - Loss: 1.5899\n",
      "\n",
      "Batch 325/992 ━━━━━━━━━━━━━━━━━━━━ 16:36:52\n",
      "Accuracy: 0.4096 - Loss: 1.5913\n",
      "\n",
      "Batch 326/992 ━━━━━━━━━━━━━━━━━━━━ 16:37:03\n",
      "Accuracy: 0.4107 - Loss: 1.5894\n",
      "\n",
      "Batch 327/992 ━━━━━━━━━━━━━━━━━━━━ 16:37:13\n",
      "Accuracy: 0.4113 - Loss: 1.5876\n",
      "\n",
      "Batch 328/992 ━━━━━━━━━━━━━━━━━━━━ 16:37:24\n",
      "Accuracy: 0.4116 - Loss: 1.5867\n",
      "\n",
      "Batch 329/992 ━━━━━━━━━━━━━━━━━━━━ 16:37:35\n",
      "Accuracy: 0.4119 - Loss: 1.5853\n",
      "\n",
      "Batch 330/992 ━━━━━━━━━━━━━━━━━━━━ 16:37:45\n",
      "Accuracy: 0.4125 - Loss: 1.5832\n",
      "\n",
      "Batch 331/992 ━━━━━━━━━━━━━━━━━━━━ 16:37:56\n",
      "Accuracy: 0.4135 - Loss: 1.5806\n",
      "\n",
      "Batch 332/992 ━━━━━━━━━━━━━━━━━━━━ 16:38:07\n",
      "Accuracy: 0.4153 - Loss: 1.5776\n",
      "\n",
      "Batch 333/992 ━━━━━━━━━━━━━━━━━━━━ 16:38:19\n",
      "Accuracy: 0.4152 - Loss: 1.5771\n",
      "\n",
      "Batch 334/992 ━━━━━━━━━━━━━━━━━━━━ 16:38:31\n",
      "Accuracy: 0.4150 - Loss: 1.5772\n",
      "\n",
      "Batch 335/992 ━━━━━━━━━━━━━━━━━━━━ 16:38:43\n",
      "Accuracy: 0.4153 - Loss: 1.5759\n",
      "\n",
      "Batch 336/992 ━━━━━━━━━━━━━━━━━━━━ 16:38:55\n",
      "Accuracy: 0.4156 - Loss: 1.5748\n",
      "\n",
      "Batch 337/992 ━━━━━━━━━━━━━━━━━━━━ 16:39:10\n",
      "Accuracy: 0.4158 - Loss: 1.5735\n",
      "\n",
      "Batch 338/992 ━━━━━━━━━━━━━━━━━━━━ 16:39:22\n",
      "Accuracy: 0.4164 - Loss: 1.5719\n",
      "\n",
      "Batch 339/992 ━━━━━━━━━━━━━━━━━━━━ 16:39:33\n",
      "Accuracy: 0.4167 - Loss: 1.5722\n",
      "\n",
      "Batch 340/992 ━━━━━━━━━━━━━━━━━━━━ 16:39:44\n",
      "Accuracy: 0.4165 - Loss: 1.5705\n",
      "\n",
      "Batch 341/992 ━━━━━━━━━━━━━━━━━━━━ 16:39:54\n",
      "Accuracy: 0.4168 - Loss: 1.5697\n",
      "\n",
      "Batch 342/992 ━━━━━━━━━━━━━━━━━━━━ 16:40:05\n",
      "Accuracy: 0.4167 - Loss: 1.5690\n",
      "\n",
      "Batch 343/992 ━━━━━━━━━━━━━━━━━━━━ 16:40:16\n",
      "Accuracy: 0.4173 - Loss: 1.5692\n",
      "\n",
      "Batch 344/992 ━━━━━━━━━━━━━━━━━━━━ 16:40:27\n",
      "Accuracy: 0.4172 - Loss: 1.5691\n",
      "\n",
      "Batch 345/992 ━━━━━━━━━━━━━━━━━━━━ 16:40:38\n",
      "Accuracy: 0.4170 - Loss: 1.5691\n",
      "\n",
      "Batch 346/992 ━━━━━━━━━━━━━━━━━━━━ 16:40:49\n",
      "Accuracy: 0.4176 - Loss: 1.5678\n",
      "\n",
      "Batch 347/992 ━━━━━━━━━━━━━━━━━━━━ 16:40:59\n",
      "Accuracy: 0.4186 - Loss: 1.5650\n",
      "\n",
      "Batch 348/992 ━━━━━━━━━━━━━━━━━━━━ 16:41:10\n",
      "Accuracy: 0.4195 - Loss: 1.5633\n",
      "\n",
      "Batch 349/992 ━━━━━━━━━━━━━━━━━━━━ 16:41:22\n",
      "Accuracy: 0.4208 - Loss: 1.5608\n",
      "\n",
      "Batch 350/992 ━━━━━━━━━━━━━━━━━━━━ 16:41:34\n",
      "Accuracy: 0.4211 - Loss: 1.5602\n",
      "\n",
      "Batch 351/992 ━━━━━━━━━━━━━━━━━━━━ 16:41:44\n",
      "Accuracy: 0.4217 - Loss: 1.5590\n",
      "\n",
      "Batch 352/992 ━━━━━━━━━━━━━━━━━━━━ 16:41:55\n",
      "Accuracy: 0.4219 - Loss: 1.5601\n",
      "\n",
      "Batch 353/992 ━━━━━━━━━━━━━━━━━━━━ 16:42:06\n",
      "Accuracy: 0.4221 - Loss: 1.5588\n",
      "\n",
      "Batch 354/992 ━━━━━━━━━━━━━━━━━━━━ 16:42:17\n",
      "Accuracy: 0.4220 - Loss: 1.5575\n",
      "\n",
      "Batch 355/992 ━━━━━━━━━━━━━━━━━━━━ 16:42:28\n",
      "Accuracy: 0.4225 - Loss: 1.5569\n",
      "\n",
      "Batch 356/992 ━━━━━━━━━━━━━━━━━━━━ 16:42:39\n",
      "Accuracy: 0.4235 - Loss: 1.5549\n",
      "\n",
      "Batch 357/992 ━━━━━━━━━━━━━━━━━━━━ 16:42:50\n",
      "Accuracy: 0.4244 - Loss: 1.5523\n",
      "\n",
      "Batch 358/992 ━━━━━━━━━━━━━━━━━━━━ 16:43:01\n",
      "Accuracy: 0.4246 - Loss: 1.5514\n",
      "\n",
      "Batch 359/992 ━━━━━━━━━━━━━━━━━━━━ 16:43:11\n",
      "Accuracy: 0.4255 - Loss: 1.5496\n",
      "\n",
      "Batch 360/992 ━━━━━━━━━━━━━━━━━━━━ 16:43:22\n",
      "Accuracy: 0.4264 - Loss: 1.5482\n",
      "\n",
      "Batch 361/992 ━━━━━━━━━━━━━━━━━━━━ 16:43:33\n",
      "Accuracy: 0.4266 - Loss: 1.5475\n",
      "\n",
      "Batch 362/992 ━━━━━━━━━━━━━━━━━━━━ 16:43:44\n",
      "Accuracy: 0.4275 - Loss: 1.5457\n",
      "\n",
      "Batch 363/992 ━━━━━━━━━━━━━━━━━━━━ 16:43:57\n",
      "Accuracy: 0.4280 - Loss: 1.5447\n",
      "\n",
      "Batch 364/992 ━━━━━━━━━━━━━━━━━━━━ 16:44:08\n",
      "Accuracy: 0.4286 - Loss: 1.5432\n",
      "\n",
      "Batch 365/992 ━━━━━━━━━━━━━━━━━━━━ 16:44:20\n",
      "Accuracy: 0.4288 - Loss: 1.5417\n",
      "\n",
      "Batch 366/992 ━━━━━━━━━━━━━━━━━━━━ 16:44:31\n",
      "Accuracy: 0.4300 - Loss: 1.5393\n",
      "\n",
      "Batch 367/992 ━━━━━━━━━━━━━━━━━━━━ 16:44:41\n",
      "Accuracy: 0.4312 - Loss: 1.5370\n",
      "\n",
      "Batch 368/992 ━━━━━━━━━━━━━━━━━━━━ 16:44:54\n",
      "Accuracy: 0.4321 - Loss: 1.5358\n",
      "\n",
      "Batch 369/992 ━━━━━━━━━━━━━━━━━━━━ 16:45:06\n",
      "Accuracy: 0.4322 - Loss: 1.5355\n",
      "\n",
      "Batch 370/992 ━━━━━━━━━━━━━━━━━━━━ 16:45:16\n",
      "Accuracy: 0.4334 - Loss: 1.5335\n",
      "\n",
      "Batch 371/992 ━━━━━━━━━━━━━━━━━━━━ 16:45:27\n",
      "Accuracy: 0.4340 - Loss: 1.5319\n",
      "\n",
      "Batch 372/992 ━━━━━━━━━━━━━━━━━━━━ 16:45:38\n",
      "Accuracy: 0.4345 - Loss: 1.5310\n",
      "\n",
      "Batch 373/992 ━━━━━━━━━━━━━━━━━━━━ 16:45:48\n",
      "Accuracy: 0.4347 - Loss: 1.5307\n",
      "\n",
      "Batch 374/992 ━━━━━━━━━━━━━━━━━━━━ 16:45:59\n",
      "Accuracy: 0.4348 - Loss: 1.5297\n",
      "\n",
      "Batch 375/992 ━━━━━━━━━━━━━━━━━━━━ 16:46:10\n",
      "Accuracy: 0.4340 - Loss: 1.5317\n",
      "\n",
      "Batch 376/992 ━━━━━━━━━━━━━━━━━━━━ 16:46:21\n",
      "Accuracy: 0.4345 - Loss: 1.5313\n",
      "\n",
      "Batch 377/992 ━━━━━━━━━━━━━━━━━━━━ 16:46:32\n",
      "Accuracy: 0.4357 - Loss: 1.5291\n",
      "\n",
      "Batch 378/992 ━━━━━━━━━━━━━━━━━━━━ 16:46:42\n",
      "Accuracy: 0.4362 - Loss: 1.5279\n",
      "\n",
      "Batch 379/992 ━━━━━━━━━━━━━━━━━━━━ 16:46:53\n",
      "Accuracy: 0.4354 - Loss: 1.5294\n",
      "\n",
      "Batch 380/992 ━━━━━━━━━━━━━━━━━━━━ 16:47:03\n",
      "Accuracy: 0.4362 - Loss: 1.5272\n",
      "\n",
      "Batch 381/992 ━━━━━━━━━━━━━━━━━━━━ 16:47:15\n",
      "Accuracy: 0.4357 - Loss: 1.5265\n",
      "\n",
      "Batch 382/992 ━━━━━━━━━━━━━━━━━━━━ 16:47:26\n",
      "Accuracy: 0.4368 - Loss: 1.5242\n",
      "\n",
      "Batch 383/992 ━━━━━━━━━━━━━━━━━━━━ 16:47:37\n",
      "Accuracy: 0.4377 - Loss: 1.5221\n",
      "\n",
      "Batch 384/992 ━━━━━━━━━━━━━━━━━━━━ 16:47:48\n",
      "Accuracy: 0.4375 - Loss: 1.5209\n",
      "\n",
      "Batch 385/992 ━━━━━━━━━━━━━━━━━━━━ 16:47:59\n",
      "Accuracy: 0.4380 - Loss: 1.5201\n",
      "\n",
      "Batch 386/992 ━━━━━━━━━━━━━━━━━━━━ 16:48:10\n",
      "Accuracy: 0.4385 - Loss: 1.5191\n",
      "\n",
      "Batch 387/992 ━━━━━━━━━━━━━━━━━━━━ 16:48:22\n",
      "Accuracy: 0.4386 - Loss: 1.5186\n",
      "\n",
      "Batch 388/992 ━━━━━━━━━━━━━━━━━━━━ 16:48:33\n",
      "Accuracy: 0.4391 - Loss: 1.5171\n",
      "\n",
      "Batch 389/992 ━━━━━━━━━━━━━━━━━━━━ 16:48:44\n",
      "Accuracy: 0.4393 - Loss: 1.5169\n",
      "\n",
      "Batch 390/992 ━━━━━━━━━━━━━━━━━━━━ 16:48:55\n",
      "Accuracy: 0.4401 - Loss: 1.5149\n",
      "\n",
      "Batch 391/992 ━━━━━━━━━━━━━━━━━━━━ 16:49:05\n",
      "Accuracy: 0.4399 - Loss: 1.5139\n",
      "\n",
      "Batch 392/992 ━━━━━━━━━━━━━━━━━━━━ 16:49:18\n",
      "Accuracy: 0.4407 - Loss: 1.5124\n",
      "\n",
      "Batch 393/992 ━━━━━━━━━━━━━━━━━━━━ 16:49:29\n",
      "Accuracy: 0.4415 - Loss: 1.5104\n",
      "\n",
      "Batch 394/992 ━━━━━━━━━━━━━━━━━━━━ 16:49:39\n",
      "Accuracy: 0.4416 - Loss: 1.5095\n",
      "\n",
      "Batch 395/992 ━━━━━━━━━━━━━━━━━━━━ 16:49:49\n",
      "Accuracy: 0.4427 - Loss: 1.5078\n",
      "\n",
      "Batch 396/992 ━━━━━━━━━━━━━━━━━━━━ 16:49:59\n",
      "Accuracy: 0.4432 - Loss: 1.5071\n",
      "\n",
      "Batch 397/992 ━━━━━━━━━━━━━━━━━━━━ 16:50:10\n",
      "Accuracy: 0.4443 - Loss: 1.5058\n",
      "\n",
      "Batch 398/992 ━━━━━━━━━━━━━━━━━━━━ 16:50:20\n",
      "Accuracy: 0.4444 - Loss: 1.5053\n",
      "\n",
      "Batch 399/992 ━━━━━━━━━━━━━━━━━━━━ 16:50:31\n",
      "Accuracy: 0.4442 - Loss: 1.5058\n",
      "\n",
      "Batch 400/992 ━━━━━━━━━━━━━━━━━━━━ 16:50:41\n",
      "Accuracy: 0.4453 - Loss: 1.5035\n",
      "\n",
      "Batch 401/992 ━━━━━━━━━━━━━━━━━━━━ 16:50:52\n",
      "Accuracy: 0.4451 - Loss: 1.5027\n",
      "\n",
      "Batch 402/992 ━━━━━━━━━━━━━━━━━━━━ 16:51:03\n",
      "Accuracy: 0.4453 - Loss: 1.5029\n",
      "\n",
      "Batch 403/992 ━━━━━━━━━━━━━━━━━━━━ 16:51:15\n",
      "Accuracy: 0.4454 - Loss: 1.5025\n",
      "\n",
      "Batch 404/992 ━━━━━━━━━━━━━━━━━━━━ 16:51:25\n",
      "Accuracy: 0.4462 - Loss: 1.5005\n",
      "\n",
      "Batch 405/992 ━━━━━━━━━━━━━━━━━━━━ 16:51:35\n",
      "Accuracy: 0.4466 - Loss: 1.4990\n",
      "\n",
      "Batch 406/992 ━━━━━━━━━━━━━━━━━━━━ 16:51:46\n",
      "Accuracy: 0.4467 - Loss: 1.4984\n",
      "\n",
      "Batch 407/992 ━━━━━━━━━━━━━━━━━━━━ 16:51:56\n",
      "Accuracy: 0.4478 - Loss: 1.4963\n",
      "\n",
      "Batch 408/992 ━━━━━━━━━━━━━━━━━━━━ 16:52:06\n",
      "Accuracy: 0.4479 - Loss: 1.4952\n",
      "\n",
      "Batch 409/992 ━━━━━━━━━━━━━━━━━━━━ 16:52:19\n",
      "Accuracy: 0.4487 - Loss: 1.4937\n",
      "\n",
      "Batch 410/992 ━━━━━━━━━━━━━━━━━━━━ 16:52:33\n",
      "Accuracy: 0.4485 - Loss: 1.4928\n",
      "\n",
      "Batch 411/992 ━━━━━━━━━━━━━━━━━━━━ 16:52:47\n",
      "Accuracy: 0.4486 - Loss: 1.4917\n",
      "\n",
      "Batch 412/992 ━━━━━━━━━━━━━━━━━━━━ 16:53:05\n",
      "Accuracy: 0.4490 - Loss: 1.4906\n",
      "\n",
      "Batch 413/992 ━━━━━━━━━━━━━━━━━━━━ 16:53:17\n",
      "Accuracy: 0.4498 - Loss: 1.4884\n",
      "\n",
      "Batch 414/992 ━━━━━━━━━━━━━━━━━━━━ 16:53:29\n",
      "Accuracy: 0.4502 - Loss: 1.4867\n",
      "\n",
      "Batch 415/992 ━━━━━━━━━━━━━━━━━━━━ 16:53:41\n",
      "Accuracy: 0.4497 - Loss: 1.4868\n",
      "\n",
      "Batch 416/992 ━━━━━━━━━━━━━━━━━━━━ 16:53:53\n",
      "Accuracy: 0.4501 - Loss: 1.4860\n",
      "\n",
      "Batch 417/992 ━━━━━━━━━━━━━━━━━━━━ 16:54:05\n",
      "Accuracy: 0.4502 - Loss: 1.4864\n",
      "\n",
      "Batch 418/992 ━━━━━━━━━━━━━━━━━━━━ 16:54:16\n",
      "Accuracy: 0.4504 - Loss: 1.4860\n",
      "\n",
      "Batch 419/992 ━━━━━━━━━━━━━━━━━━━━ 16:54:27\n",
      "Accuracy: 0.4502 - Loss: 1.4856\n",
      "\n",
      "Batch 420/992 ━━━━━━━━━━━━━━━━━━━━ 16:54:39\n",
      "Accuracy: 0.4503 - Loss: 1.4853\n",
      "\n",
      "Batch 421/992 ━━━━━━━━━━━━━━━━━━━━ 16:54:51\n",
      "Accuracy: 0.4507 - Loss: 1.4840\n",
      "\n",
      "Batch 422/992 ━━━━━━━━━━━━━━━━━━━━ 16:55:03\n",
      "Accuracy: 0.4520 - Loss: 1.4815\n",
      "\n",
      "Batch 423/992 ━━━━━━━━━━━━━━━━━━━━ 16:55:17\n",
      "Accuracy: 0.4527 - Loss: 1.4801\n",
      "\n",
      "Batch 424/992 ━━━━━━━━━━━━━━━━━━━━ 16:55:28\n",
      "Accuracy: 0.4531 - Loss: 1.4783\n",
      "\n",
      "Batch 425/992 ━━━━━━━━━━━━━━━━━━━━ 16:55:40\n",
      "Accuracy: 0.4532 - Loss: 1.4771\n",
      "\n",
      "Batch 426/992 ━━━━━━━━━━━━━━━━━━━━ 16:55:51\n",
      "Accuracy: 0.4533 - Loss: 1.4767\n",
      "\n",
      "Batch 427/992 ━━━━━━━━━━━━━━━━━━━━ 16:56:02\n",
      "Accuracy: 0.4540 - Loss: 1.4750\n",
      "\n",
      "Batch 428/992 ━━━━━━━━━━━━━━━━━━━━ 16:56:13\n",
      "Accuracy: 0.4547 - Loss: 1.4731\n",
      "\n",
      "Batch 429/992 ━━━━━━━━━━━━━━━━━━━━ 16:56:24\n",
      "Accuracy: 0.4551 - Loss: 1.4724\n",
      "\n",
      "Batch 430/992 ━━━━━━━━━━━━━━━━━━━━ 16:56:35\n",
      "Accuracy: 0.4549 - Loss: 1.4709\n",
      "\n",
      "Batch 431/992 ━━━━━━━━━━━━━━━━━━━━ 16:56:45\n",
      "Accuracy: 0.4545 - Loss: 1.4710\n",
      "\n",
      "Batch 432/992 ━━━━━━━━━━━━━━━━━━━━ 16:56:55\n",
      "Accuracy: 0.4549 - Loss: 1.4710\n",
      "\n",
      "Batch 433/992 ━━━━━━━━━━━━━━━━━━━━ 16:57:05\n",
      "Accuracy: 0.4555 - Loss: 1.4690\n",
      "\n",
      "Batch 434/992 ━━━━━━━━━━━━━━━━━━━━ 16:57:15\n",
      "Accuracy: 0.4559 - Loss: 1.4671\n",
      "\n",
      "Batch 435/992 ━━━━━━━━━━━━━━━━━━━━ 16:57:26\n",
      "Accuracy: 0.4560 - Loss: 1.4663\n",
      "\n",
      "Batch 436/992 ━━━━━━━━━━━━━━━━━━━━ 16:57:36\n",
      "Accuracy: 0.4567 - Loss: 1.4649\n",
      "\n",
      "Batch 437/992 ━━━━━━━━━━━━━━━━━━━━ 16:57:46\n",
      "Accuracy: 0.4574 - Loss: 1.4634\n",
      "\n",
      "Batch 438/992 ━━━━━━━━━━━━━━━━━━━━ 16:57:56\n",
      "Accuracy: 0.4563 - Loss: 1.4640\n",
      "\n",
      "Batch 439/992 ━━━━━━━━━━━━━━━━━━━━ 16:58:07\n",
      "Accuracy: 0.4567 - Loss: 1.4626\n",
      "\n",
      "Batch 440/992 ━━━━━━━━━━━━━━━━━━━━ 16:58:17\n",
      "Accuracy: 0.4571 - Loss: 1.4615\n",
      "\n",
      "Batch 441/992 ━━━━━━━━━━━━━━━━━━━━ 16:58:28\n",
      "Accuracy: 0.4578 - Loss: 1.4602\n",
      "\n",
      "Batch 442/992 ━━━━━━━━━━━━━━━━━━━━ 16:58:38\n",
      "Accuracy: 0.4581 - Loss: 1.4594\n",
      "\n",
      "Batch 443/992 ━━━━━━━━━━━━━━━━━━━━ 16:58:48\n",
      "Accuracy: 0.4582 - Loss: 1.4586\n",
      "\n",
      "Batch 444/992 ━━━━━━━━━━━━━━━━━━━━ 16:58:59\n",
      "Accuracy: 0.4578 - Loss: 1.4588\n",
      "\n",
      "Batch 445/992 ━━━━━━━━━━━━━━━━━━━━ 16:59:09\n",
      "Accuracy: 0.4581 - Loss: 1.4581\n",
      "\n",
      "Batch 446/992 ━━━━━━━━━━━━━━━━━━━━ 16:59:19\n",
      "Accuracy: 0.4588 - Loss: 1.4570\n",
      "\n",
      "Batch 447/992 ━━━━━━━━━━━━━━━━━━━━ 16:59:29\n",
      "Accuracy: 0.4586 - Loss: 1.4574\n",
      "\n",
      "Batch 448/992 ━━━━━━━━━━━━━━━━━━━━ 16:59:39\n",
      "Accuracy: 0.4590 - Loss: 1.4563\n",
      "\n",
      "Batch 449/992 ━━━━━━━━━━━━━━━━━━━━ 16:59:50\n",
      "Accuracy: 0.4594 - Loss: 1.4561\n",
      "\n",
      "Batch 450/992 ━━━━━━━━━━━━━━━━━━━━ 17:00:00\n",
      "Accuracy: 0.4600 - Loss: 1.4548\n",
      "\n",
      "Batch 451/992 ━━━━━━━━━━━━━━━━━━━━ 17:00:10\n",
      "Accuracy: 0.4601 - Loss: 1.4545\n",
      "\n",
      "Batch 452/992 ━━━━━━━━━━━━━━━━━━━━ 17:00:21\n",
      "Accuracy: 0.4602 - Loss: 1.4536\n",
      "\n",
      "Batch 453/992 ━━━━━━━━━━━━━━━━━━━━ 17:00:31\n",
      "Accuracy: 0.4605 - Loss: 1.4522\n",
      "\n",
      "Batch 454/992 ━━━━━━━━━━━━━━━━━━━━ 17:00:41\n",
      "Accuracy: 0.4609 - Loss: 1.4516\n",
      "\n",
      "Batch 455/992 ━━━━━━━━━━━━━━━━━━━━ 17:00:52\n",
      "Accuracy: 0.4607 - Loss: 1.4507\n",
      "\n",
      "Batch 456/992 ━━━━━━━━━━━━━━━━━━━━ 17:01:02\n",
      "Accuracy: 0.4608 - Loss: 1.4500\n",
      "\n",
      "Batch 457/992 ━━━━━━━━━━━━━━━━━━━━ 17:01:12\n",
      "Accuracy: 0.4609 - Loss: 1.4494\n",
      "\n",
      "Batch 458/992 ━━━━━━━━━━━━━━━━━━━━ 17:01:23\n",
      "Accuracy: 0.4610 - Loss: 1.4486\n",
      "\n",
      "Batch 459/992 ━━━━━━━━━━━━━━━━━━━━ 17:01:33\n",
      "Accuracy: 0.4611 - Loss: 1.4486\n",
      "\n",
      "Batch 460/992 ━━━━━━━━━━━━━━━━━━━━ 17:01:43\n",
      "Accuracy: 0.4611 - Loss: 1.4479\n",
      "\n",
      "Batch 461/992 ━━━━━━━━━━━━━━━━━━━━ 17:01:53\n",
      "Accuracy: 0.4618 - Loss: 1.4473\n",
      "\n",
      "Batch 462/992 ━━━━━━━━━━━━━━━━━━━━ 17:02:03\n",
      "Accuracy: 0.4621 - Loss: 1.4460\n",
      "\n",
      "Batch 463/992 ━━━━━━━━━━━━━━━━━━━━ 17:02:14\n",
      "Accuracy: 0.4625 - Loss: 1.4449\n",
      "\n",
      "Batch 464/992 ━━━━━━━━━━━━━━━━━━━━ 17:02:24\n",
      "Accuracy: 0.4628 - Loss: 1.4440\n",
      "\n",
      "Batch 465/992 ━━━━━━━━━━━━━━━━━━━━ 17:02:35\n",
      "Accuracy: 0.4637 - Loss: 1.4423\n",
      "\n",
      "Batch 466/992 ━━━━━━━━━━━━━━━━━━━━ 17:02:45\n",
      "Accuracy: 0.4638 - Loss: 1.4418\n",
      "\n",
      "Batch 467/992 ━━━━━━━━━━━━━━━━━━━━ 17:02:55\n",
      "Accuracy: 0.4647 - Loss: 1.4398\n",
      "\n",
      "Batch 468/992 ━━━━━━━━━━━━━━━━━━━━ 17:03:06\n",
      "Accuracy: 0.4647 - Loss: 1.4391\n",
      "\n",
      "Batch 469/992 ━━━━━━━━━━━━━━━━━━━━ 17:03:16\n",
      "Accuracy: 0.4654 - Loss: 1.4376\n",
      "\n",
      "Batch 470/992 ━━━━━━━━━━━━━━━━━━━━ 17:03:26\n",
      "Accuracy: 0.4652 - Loss: 1.4376\n",
      "\n",
      "Batch 471/992 ━━━━━━━━━━━━━━━━━━━━ 17:03:36\n",
      "Accuracy: 0.4652 - Loss: 1.4369\n",
      "\n",
      "Batch 472/992 ━━━━━━━━━━━━━━━━━━━━ 17:03:46\n",
      "Accuracy: 0.4661 - Loss: 1.4354\n",
      "\n",
      "Batch 473/992 ━━━━━━━━━━━━━━━━━━━━ 17:03:57\n",
      "Accuracy: 0.4662 - Loss: 1.4351\n",
      "\n",
      "Batch 474/992 ━━━━━━━━━━━━━━━━━━━━ 17:04:07\n",
      "Accuracy: 0.4670 - Loss: 1.4334\n",
      "\n",
      "Batch 475/992 ━━━━━━━━━━━━━━━━━━━━ 17:04:17\n",
      "Accuracy: 0.4674 - Loss: 1.4329\n",
      "\n",
      "Batch 476/992 ━━━━━━━━━━━━━━━━━━━━ 17:04:28\n",
      "Accuracy: 0.4677 - Loss: 1.4328\n",
      "\n",
      "Batch 477/992 ━━━━━━━━━━━━━━━━━━━━ 17:04:39\n",
      "Accuracy: 0.4686 - Loss: 1.4310\n",
      "\n",
      "Batch 478/992 ━━━━━━━━━━━━━━━━━━━━ 17:04:49\n",
      "Accuracy: 0.4689 - Loss: 1.4300\n",
      "\n",
      "Batch 479/992 ━━━━━━━━━━━━━━━━━━━━ 17:05:00\n",
      "Accuracy: 0.4695 - Loss: 1.4286\n",
      "\n",
      "Batch 480/992 ━━━━━━━━━━━━━━━━━━━━ 17:05:11\n",
      "Accuracy: 0.4701 - Loss: 1.4268\n",
      "\n",
      "Batch 481/992 ━━━━━━━━━━━━━━━━━━━━ 17:05:22\n",
      "Accuracy: 0.4704 - Loss: 1.4268\n",
      "\n",
      "Batch 482/992 ━━━━━━━━━━━━━━━━━━━━ 17:05:34\n",
      "Accuracy: 0.4710 - Loss: 1.4257\n",
      "\n",
      "Batch 483/992 ━━━━━━━━━━━━━━━━━━━━ 17:05:45\n",
      "Accuracy: 0.4710 - Loss: 1.4262\n",
      "\n",
      "Batch 484/992 ━━━━━━━━━━━━━━━━━━━━ 17:05:56\n",
      "Accuracy: 0.4718 - Loss: 1.4240\n",
      "\n",
      "Batch 485/992 ━━━━━━━━━━━━━━━━━━━━ 17:06:08\n",
      "Accuracy: 0.4719 - Loss: 1.4230\n",
      "\n",
      "Batch 486/992 ━━━━━━━━━━━━━━━━━━━━ 17:06:21\n",
      "Accuracy: 0.4722 - Loss: 1.4221\n",
      "\n",
      "Batch 487/992 ━━━━━━━━━━━━━━━━━━━━ 17:06:32\n",
      "Accuracy: 0.4728 - Loss: 1.4205\n",
      "\n",
      "Batch 488/992 ━━━━━━━━━━━━━━━━━━━━ 17:06:45\n",
      "Accuracy: 0.4734 - Loss: 1.4188\n",
      "\n",
      "Batch 489/992 ━━━━━━━━━━━━━━━━━━━━ 17:06:59\n",
      "Accuracy: 0.4737 - Loss: 1.4181\n",
      "\n",
      "Batch 490/992 ━━━━━━━━━━━━━━━━━━━━ 17:07:14\n",
      "Accuracy: 0.4742 - Loss: 1.4162\n",
      "\n",
      "Batch 491/992 ━━━━━━━━━━━━━━━━━━━━ 17:07:29\n",
      "Accuracy: 0.4748 - Loss: 1.4156\n",
      "\n",
      "Batch 492/992 ━━━━━━━━━━━━━━━━━━━━ 17:07:43\n",
      "Accuracy: 0.4754 - Loss: 1.4148\n",
      "\n",
      "Batch 493/992 ━━━━━━━━━━━━━━━━━━━━ 17:07:59\n",
      "Accuracy: 0.4759 - Loss: 1.4131\n",
      "\n",
      "Batch 494/992 ━━━━━━━━━━━━━━━━━━━━ 17:08:13\n",
      "Accuracy: 0.4765 - Loss: 1.4114\n",
      "\n",
      "Batch 495/992 ━━━━━━━━━━━━━━━━━━━━ 17:08:25\n",
      "Accuracy: 0.4770 - Loss: 1.4097\n",
      "\n",
      "Batch 496/992 ━━━━━━━━━━━━━━━━━━━━ 17:08:36\n",
      "Accuracy: 0.4773 - Loss: 1.4094\n",
      "\n",
      "Batch 497/992 ━━━━━━━━━━━━━━━━━━━━ 17:08:48\n",
      "Accuracy: 0.4774 - Loss: 1.4097\n",
      "\n",
      "Batch 498/992 ━━━━━━━━━━━━━━━━━━━━ 17:09:03\n",
      "Accuracy: 0.4779 - Loss: 1.4084\n",
      "\n",
      "Batch 499/992 ━━━━━━━━━━━━━━━━━━━━ 17:09:14\n",
      "Accuracy: 0.4782 - Loss: 1.4073\n",
      "\n",
      "Batch 500/992 ━━━━━━━━━━━━━━━━━━━━ 17:09:24\n",
      "Accuracy: 0.4782 - Loss: 1.4067\n",
      "\n",
      "Batch 501/992 ━━━━━━━━━━━━━━━━━━━━ 17:09:36\n",
      "Accuracy: 0.4785 - Loss: 1.4064\n",
      "\n",
      "Batch 502/992 ━━━━━━━━━━━━━━━━━━━━ 17:09:46\n",
      "Accuracy: 0.4788 - Loss: 1.4053\n",
      "\n",
      "Batch 503/992 ━━━━━━━━━━━━━━━━━━━━ 17:09:56\n",
      "Accuracy: 0.4794 - Loss: 1.4044\n",
      "\n",
      "Batch 504/992 ━━━━━━━━━━━━━━━━━━━━ 17:10:07\n",
      "Accuracy: 0.4792 - Loss: 1.4039\n",
      "\n",
      "Batch 505/992 ━━━━━━━━━━━━━━━━━━━━ 17:10:19\n",
      "Accuracy: 0.4795 - Loss: 1.4025\n",
      "\n",
      "Batch 506/992 ━━━━━━━━━━━━━━━━━━━━ 17:10:33\n",
      "Accuracy: 0.4792 - Loss: 1.4025\n",
      "\n",
      "Batch 507/992 ━━━━━━━━━━━━━━━━━━━━ 17:10:45\n",
      "Accuracy: 0.4786 - Loss: 1.4030\n",
      "\n",
      "Batch 508/992 ━━━━━━━━━━━━━━━━━━━━ 17:10:58\n",
      "Accuracy: 0.4791 - Loss: 1.4020\n",
      "\n",
      "Batch 509/992 ━━━━━━━━━━━━━━━━━━━━ 17:11:12\n",
      "Accuracy: 0.4801 - Loss: 1.4001\n",
      "\n",
      "Batch 510/992 ━━━━━━━━━━━━━━━━━━━━ 17:11:25\n",
      "Accuracy: 0.4806 - Loss: 1.3993\n",
      "\n",
      "Batch 511/992 ━━━━━━━━━━━━━━━━━━━━ 17:11:40\n",
      "Accuracy: 0.4807 - Loss: 1.3991\n",
      "\n",
      "Batch 512/992 ━━━━━━━━━━━━━━━━━━━━ 17:11:50\n",
      "Accuracy: 0.4810 - Loss: 1.3992\n",
      "\n",
      "Batch 513/992 ━━━━━━━━━━━━━━━━━━━━ 17:12:01\n",
      "Accuracy: 0.4815 - Loss: 1.3984\n",
      "\n",
      "Batch 514/992 ━━━━━━━━━━━━━━━━━━━━ 17:12:12\n",
      "Accuracy: 0.4813 - Loss: 1.3984\n",
      "\n",
      "Batch 515/992 ━━━━━━━━━━━━━━━━━━━━ 17:12:23\n",
      "Accuracy: 0.4820 - Loss: 1.3966\n",
      "\n",
      "Batch 516/992 ━━━━━━━━━━━━━━━━━━━━ 17:12:33\n",
      "Accuracy: 0.4823 - Loss: 1.3956\n",
      "\n",
      "Batch 517/992 ━━━━━━━━━━━━━━━━━━━━ 17:12:43\n",
      "Accuracy: 0.4828 - Loss: 1.3953\n",
      "\n",
      "Batch 518/992 ━━━━━━━━━━━━━━━━━━━━ 17:12:54\n",
      "Accuracy: 0.4826 - Loss: 1.3958\n",
      "\n",
      "Batch 519/992 ━━━━━━━━━━━━━━━━━━━━ 17:13:05\n",
      "Accuracy: 0.4829 - Loss: 1.3950\n",
      "\n",
      "Batch 520/992 ━━━━━━━━━━━━━━━━━━━━ 17:13:19\n",
      "Accuracy: 0.4829 - Loss: 1.3952\n",
      "\n",
      "Batch 521/992 ━━━━━━━━━━━━━━━━━━━━ 17:13:30\n",
      "Accuracy: 0.4827 - Loss: 1.3948\n",
      "\n",
      "Batch 522/992 ━━━━━━━━━━━━━━━━━━━━ 17:13:41\n",
      "Accuracy: 0.4832 - Loss: 1.3945\n",
      "\n",
      "Batch 523/992 ━━━━━━━━━━━━━━━━━━━━ 17:13:52\n",
      "Accuracy: 0.4833 - Loss: 1.3939\n",
      "\n",
      "Batch 524/992 ━━━━━━━━━━━━━━━━━━━━ 17:14:03\n",
      "Accuracy: 0.4833 - Loss: 1.3933\n",
      "\n",
      "Batch 525/992 ━━━━━━━━━━━━━━━━━━━━ 17:14:14\n",
      "Accuracy: 0.4843 - Loss: 1.3915\n",
      "\n",
      "Batch 526/992 ━━━━━━━━━━━━━━━━━━━━ 17:14:26\n",
      "Accuracy: 0.4848 - Loss: 1.3904\n",
      "\n",
      "Batch 527/992 ━━━━━━━━━━━━━━━━━━━━ 17:14:37\n",
      "Accuracy: 0.4855 - Loss: 1.3888\n",
      "\n",
      "Batch 528/992 ━━━━━━━━━━━━━━━━━━━━ 17:14:49\n",
      "Accuracy: 0.4863 - Loss: 1.3879\n",
      "\n",
      "Batch 529/992 ━━━━━━━━━━━━━━━━━━━━ 17:15:01\n",
      "Accuracy: 0.4872 - Loss: 1.3862\n",
      "\n",
      "Batch 530/992 ━━━━━━━━━━━━━━━━━━━━ 17:15:15\n",
      "Accuracy: 0.4875 - Loss: 1.3852\n",
      "\n",
      "Batch 531/992 ━━━━━━━━━━━━━━━━━━━━ 17:15:29\n",
      "Accuracy: 0.4882 - Loss: 1.3841\n",
      "\n",
      "Batch 532/992 ━━━━━━━━━━━━━━━━━━━━ 17:15:42\n",
      "Accuracy: 0.4887 - Loss: 1.3829\n",
      "\n",
      "Batch 533/992 ━━━━━━━━━━━━━━━━━━━━ 17:15:53\n",
      "Accuracy: 0.4892 - Loss: 1.3820\n",
      "\n",
      "Batch 534/992 ━━━━━━━━━━━━━━━━━━━━ 17:16:09\n",
      "Accuracy: 0.4890 - Loss: 1.3820\n",
      "\n",
      "Batch 535/992 ━━━━━━━━━━━━━━━━━━━━ 17:16:23\n",
      "Accuracy: 0.4888 - Loss: 1.3820\n",
      "\n",
      "Batch 536/992 ━━━━━━━━━━━━━━━━━━━━ 17:16:34\n",
      "Accuracy: 0.4890 - Loss: 1.3817\n",
      "\n",
      "Batch 537/992 ━━━━━━━━━━━━━━━━━━━━ 17:16:47\n",
      "Accuracy: 0.4895 - Loss: 1.3801\n",
      "\n",
      "Batch 538/992 ━━━━━━━━━━━━━━━━━━━━ 17:16:59\n",
      "Accuracy: 0.4898 - Loss: 1.3793\n",
      "\n",
      "Batch 539/992 ━━━━━━━━━━━━━━━━━━━━ 17:17:12\n",
      "Accuracy: 0.4900 - Loss: 1.3789\n",
      "\n",
      "Batch 540/992 ━━━━━━━━━━━━━━━━━━━━ 17:17:25\n",
      "Accuracy: 0.4900 - Loss: 1.3785\n",
      "\n",
      "Batch 541/992 ━━━━━━━━━━━━━━━━━━━━ 17:17:36\n",
      "Accuracy: 0.4903 - Loss: 1.3784\n",
      "\n",
      "Batch 542/992 ━━━━━━━━━━━━━━━━━━━━ 17:17:46\n",
      "Accuracy: 0.4910 - Loss: 1.3765\n",
      "\n",
      "Batch 543/992 ━━━━━━━━━━━━━━━━━━━━ 17:17:59\n",
      "Accuracy: 0.4915 - Loss: 1.3757\n",
      "\n",
      "Batch 544/992 ━━━━━━━━━━━━━━━━━━━━ 17:18:09\n",
      "Accuracy: 0.4922 - Loss: 1.3743\n",
      "\n",
      "Batch 545/992 ━━━━━━━━━━━━━━━━━━━━ 17:18:19\n",
      "Accuracy: 0.4927 - Loss: 1.3728\n",
      "\n",
      "Batch 546/992 ━━━━━━━━━━━━━━━━━━━━ 17:18:29\n",
      "Accuracy: 0.4931 - Loss: 1.3720\n",
      "\n",
      "Batch 547/992 ━━━━━━━━━━━━━━━━━━━━ 17:18:40\n",
      "Accuracy: 0.4934 - Loss: 1.3722\n",
      "\n",
      "Batch 548/992 ━━━━━━━━━━━━━━━━━━━━ 17:18:54\n",
      "Accuracy: 0.4936 - Loss: 1.3713\n",
      "\n",
      "Batch 549/992 ━━━━━━━━━━━━━━━━━━━━ 17:19:09\n",
      "Accuracy: 0.4943 - Loss: 1.3694\n",
      "\n",
      "Batch 550/992 ━━━━━━━━━━━━━━━━━━━━ 17:19:22\n",
      "Accuracy: 0.4945 - Loss: 1.3687\n",
      "\n",
      "Batch 551/992 ━━━━━━━━━━━━━━━━━━━━ 17:19:33\n",
      "Accuracy: 0.4946 - Loss: 1.3677\n",
      "\n",
      "Batch 552/992 ━━━━━━━━━━━━━━━━━━━━ 17:19:44\n",
      "Accuracy: 0.4950 - Loss: 1.3663\n",
      "\n",
      "Batch 553/992 ━━━━━━━━━━━━━━━━━━━━ 17:19:59\n",
      "Accuracy: 0.4957 - Loss: 1.3644\n",
      "\n",
      "Batch 554/992 ━━━━━━━━━━━━━━━━━━━━ 17:20:12\n",
      "Accuracy: 0.4959 - Loss: 1.3633\n",
      "\n",
      "Batch 555/992 ━━━━━━━━━━━━━━━━━━━━ 17:20:23\n",
      "Accuracy: 0.4962 - Loss: 1.3632\n",
      "\n",
      "Batch 556/992 ━━━━━━━━━━━━━━━━━━━━ 17:20:35\n",
      "Accuracy: 0.4969 - Loss: 1.3618\n",
      "\n",
      "Batch 557/992 ━━━━━━━━━━━━━━━━━━━━ 17:20:46\n",
      "Accuracy: 0.4975 - Loss: 1.3604\n",
      "\n",
      "Batch 558/992 ━━━━━━━━━━━━━━━━━━━━ 17:21:00\n",
      "Accuracy: 0.4980 - Loss: 1.3592\n",
      "\n",
      "Batch 559/992 ━━━━━━━━━━━━━━━━━━━━ 17:21:16\n",
      "Accuracy: 0.4982 - Loss: 1.3587\n",
      "\n",
      "Batch 560/992 ━━━━━━━━━━━━━━━━━━━━ 17:21:31\n",
      "Accuracy: 0.4987 - Loss: 1.3574\n",
      "\n",
      "Batch 561/992 ━━━━━━━━━━━━━━━━━━━━ 17:21:47\n",
      "Accuracy: 0.4991 - Loss: 1.3563\n",
      "\n",
      "Batch 562/992 ━━━━━━━━━━━━━━━━━━━━ 17:22:03\n",
      "Accuracy: 0.4998 - Loss: 1.3549\n",
      "\n",
      "Batch 563/992 ━━━━━━━━━━━━━━━━━━━━ 17:22:19\n",
      "Accuracy: 0.5002 - Loss: 1.3534\n",
      "\n",
      "Batch 564/992 ━━━━━━━━━━━━━━━━━━━━ 17:22:35\n",
      "Accuracy: 0.5002 - Loss: 1.3527\n",
      "\n",
      "Batch 565/992 ━━━━━━━━━━━━━━━━━━━━ 17:22:51\n",
      "Accuracy: 0.5004 - Loss: 1.3522\n",
      "\n",
      "Batch 566/992 ━━━━━━━━━━━━━━━━━━━━ 17:23:09\n",
      "Accuracy: 0.5011 - Loss: 1.3511\n",
      "\n",
      "Batch 567/992 ━━━━━━━━━━━━━━━━━━━━ 17:23:25\n",
      "Accuracy: 0.5015 - Loss: 1.3500\n",
      "\n",
      "Batch 568/992 ━━━━━━━━━━━━━━━━━━━━ 17:23:35\n",
      "Accuracy: 0.5020 - Loss: 1.3489\n",
      "\n",
      "Batch 569/992 ━━━━━━━━━━━━━━━━━━━━ 17:23:46\n",
      "Accuracy: 0.5024 - Loss: 1.3479\n",
      "\n",
      "Batch 570/992 ━━━━━━━━━━━━━━━━━━━━ 17:24:02\n",
      "Accuracy: 0.5029 - Loss: 1.3477\n",
      "\n",
      "Batch 571/992 ━━━━━━━━━━━━━━━━━━━━ 17:24:18\n",
      "Accuracy: 0.5033 - Loss: 1.3466\n",
      "\n",
      "Batch 572/992 ━━━━━━━━━━━━━━━━━━━━ 17:24:34\n",
      "Accuracy: 0.5035 - Loss: 1.3459\n",
      "\n",
      "Batch 573/992 ━━━━━━━━━━━━━━━━━━━━ 17:24:50\n",
      "Accuracy: 0.5039 - Loss: 1.3450\n",
      "\n",
      "Batch 574/992 ━━━━━━━━━━━━━━━━━━━━ 17:25:04\n",
      "Accuracy: 0.5041 - Loss: 1.3441\n",
      "\n",
      "Batch 575/992 ━━━━━━━━━━━━━━━━━━━━ 17:25:19\n",
      "Accuracy: 0.5048 - Loss: 1.3434\n",
      "\n",
      "Batch 576/992 ━━━━━━━━━━━━━━━━━━━━ 17:25:30\n",
      "Accuracy: 0.5052 - Loss: 1.3428\n",
      "\n",
      "Batch 577/992 ━━━━━━━━━━━━━━━━━━━━ 17:25:46\n",
      "Accuracy: 0.5056 - Loss: 1.3417\n",
      "\n",
      "Batch 578/992 ━━━━━━━━━━━━━━━━━━━━ 17:26:01\n",
      "Accuracy: 0.5058 - Loss: 1.3407\n",
      "\n",
      "Batch 579/992 ━━━━━━━━━━━━━━━━━━━━ 17:26:17\n",
      "Accuracy: 0.5058 - Loss: 1.3401\n",
      "\n",
      "Batch 580/992 ━━━━━━━━━━━━━━━━━━━━ 17:26:33\n",
      "Accuracy: 0.5060 - Loss: 1.3393\n",
      "\n",
      "Batch 581/992 ━━━━━━━━━━━━━━━━━━━━ 17:26:44\n",
      "Accuracy: 0.5067 - Loss: 1.3378\n",
      "\n",
      "Batch 582/992 ━━━━━━━━━━━━━━━━━━━━ 17:26:54\n",
      "Accuracy: 0.5067 - Loss: 1.3377\n",
      "\n",
      "Batch 583/992 ━━━━━━━━━━━━━━━━━━━━ 17:27:04\n",
      "Accuracy: 0.5069 - Loss: 1.3371\n",
      "\n",
      "Batch 584/992 ━━━━━━━━━━━━━━━━━━━━ 17:27:14\n",
      "Accuracy: 0.5075 - Loss: 1.3364\n",
      "\n",
      "Batch 585/992 ━━━━━━━━━━━━━━━━━━━━ 17:27:24\n",
      "Accuracy: 0.5081 - Loss: 1.3353\n",
      "\n",
      "Batch 586/992 ━━━━━━━━━━━━━━━━━━━━ 17:27:35\n",
      "Accuracy: 0.5079 - Loss: 1.3359\n",
      "\n",
      "Batch 587/992 ━━━━━━━━━━━━━━━━━━━━ 17:27:45\n",
      "Accuracy: 0.5081 - Loss: 1.3352\n",
      "\n",
      "Batch 588/992 ━━━━━━━━━━━━━━━━━━━━ 17:27:55\n",
      "Accuracy: 0.5083 - Loss: 1.3348\n",
      "\n",
      "Batch 589/992 ━━━━━━━━━━━━━━━━━━━━ 17:28:05\n",
      "Accuracy: 0.5081 - Loss: 1.3348\n",
      "\n",
      "Batch 590/992 ━━━━━━━━━━━━━━━━━━━━ 17:28:15\n",
      "Accuracy: 0.5085 - Loss: 1.3336\n",
      "\n",
      "Batch 591/992 ━━━━━━━━━━━━━━━━━━━━ 17:28:26\n",
      "Accuracy: 0.5089 - Loss: 1.3324\n",
      "\n",
      "Batch 592/992 ━━━━━━━━━━━━━━━━━━━━ 17:28:36\n",
      "Accuracy: 0.5095 - Loss: 1.3315\n",
      "\n",
      "Batch 593/992 ━━━━━━━━━━━━━━━━━━━━ 17:28:47\n",
      "Accuracy: 0.5093 - Loss: 1.3312\n",
      "\n",
      "Batch 594/992 ━━━━━━━━━━━━━━━━━━━━ 17:29:00\n",
      "Accuracy: 0.5095 - Loss: 1.3304\n",
      "\n",
      "Batch 595/992 ━━━━━━━━━━━━━━━━━━━━ 17:29:10\n",
      "Accuracy: 0.5095 - Loss: 1.3300\n",
      "\n",
      "Batch 596/992 ━━━━━━━━━━━━━━━━━━━━ 17:29:22\n",
      "Accuracy: 0.5101 - Loss: 1.3289\n",
      "\n",
      "Batch 597/992 ━━━━━━━━━━━━━━━━━━━━ 17:29:33\n",
      "Accuracy: 0.5107 - Loss: 1.3277\n",
      "\n",
      "Batch 598/992 ━━━━━━━━━━━━━━━━━━━━ 17:29:45\n",
      "Accuracy: 0.5107 - Loss: 1.3272\n",
      "\n",
      "Batch 599/992 ━━━━━━━━━━━━━━━━━━━━ 17:29:56\n",
      "Accuracy: 0.5111 - Loss: 1.3264\n",
      "\n",
      "Batch 600/992 ━━━━━━━━━━━━━━━━━━━━ 17:30:07\n",
      "Accuracy: 0.5108 - Loss: 1.3270\n",
      "\n",
      "Batch 601/992 ━━━━━━━━━━━━━━━━━━━━ 17:30:18\n",
      "Accuracy: 0.5106 - Loss: 1.3272\n",
      "\n",
      "Batch 602/992 ━━━━━━━━━━━━━━━━━━━━ 17:30:31\n",
      "Accuracy: 0.5110 - Loss: 1.3263\n",
      "\n",
      "Batch 603/992 ━━━━━━━━━━━━━━━━━━━━ 17:30:42\n",
      "Accuracy: 0.5116 - Loss: 1.3248\n",
      "\n",
      "Batch 604/992 ━━━━━━━━━━━━━━━━━━━━ 17:30:54\n",
      "Accuracy: 0.5116 - Loss: 1.3246\n",
      "\n",
      "Batch 605/992 ━━━━━━━━━━━━━━━━━━━━ 17:31:06\n",
      "Accuracy: 0.5124 - Loss: 1.3232\n",
      "\n",
      "Batch 606/992 ━━━━━━━━━━━━━━━━━━━━ 17:31:18\n",
      "Accuracy: 0.5128 - Loss: 1.3223\n",
      "\n",
      "Batch 607/992 ━━━━━━━━━━━━━━━━━━━━ 17:31:35\n",
      "Accuracy: 0.5130 - Loss: 1.3215\n",
      "\n",
      "Batch 608/992 ━━━━━━━━━━━━━━━━━━━━ 17:31:50\n",
      "Accuracy: 0.5136 - Loss: 1.3205\n",
      "\n",
      "Batch 609/992 ━━━━━━━━━━━━━━━━━━━━ 17:32:04\n",
      "Accuracy: 0.5140 - Loss: 1.3195\n",
      "\n",
      "Batch 610/992 ━━━━━━━━━━━━━━━━━━━━ 17:32:19\n",
      "Accuracy: 0.5145 - Loss: 1.3183\n",
      "\n",
      "Batch 611/992 ━━━━━━━━━━━━━━━━━━━━ 17:32:31\n",
      "Accuracy: 0.5145 - Loss: 1.3181\n",
      "\n",
      "Batch 612/992 ━━━━━━━━━━━━━━━━━━━━ 17:32:42\n",
      "Accuracy: 0.5149 - Loss: 1.3174\n",
      "\n",
      "Batch 613/992 ━━━━━━━━━━━━━━━━━━━━ 17:32:53\n",
      "Accuracy: 0.5155 - Loss: 1.3164\n",
      "\n",
      "Batch 614/992 ━━━━━━━━━━━━━━━━━━━━ 17:33:03\n",
      "Accuracy: 0.5159 - Loss: 1.3154\n",
      "\n",
      "Batch 615/992 ━━━━━━━━━━━━━━━━━━━━ 17:33:15\n",
      "Accuracy: 0.5161 - Loss: 1.3145\n",
      "\n",
      "Batch 616/992 ━━━━━━━━━━━━━━━━━━━━ 17:33:25\n",
      "Accuracy: 0.5164 - Loss: 1.3140\n",
      "\n",
      "Batch 617/992 ━━━━━━━━━━━━━━━━━━━━ 17:33:36\n",
      "Accuracy: 0.5164 - Loss: 1.3136\n",
      "\n",
      "Batch 618/992 ━━━━━━━━━━━━━━━━━━━━ 17:33:48\n",
      "Accuracy: 0.5168 - Loss: 1.3128\n",
      "\n",
      "Batch 619/992 ━━━━━━━━━━━━━━━━━━━━ 17:33:59\n",
      "Accuracy: 0.5172 - Loss: 1.3119\n",
      "\n",
      "Batch 620/992 ━━━━━━━━━━━━━━━━━━━━ 17:34:12\n",
      "Accuracy: 0.5175 - Loss: 1.3111\n",
      "\n",
      "Batch 621/992 ━━━━━━━━━━━━━━━━━━━━ 17:34:26\n",
      "Accuracy: 0.5177 - Loss: 1.3103\n",
      "\n",
      "Batch 622/992 ━━━━━━━━━━━━━━━━━━━━ 17:34:40\n",
      "Accuracy: 0.5183 - Loss: 1.3088\n",
      "\n",
      "Batch 623/992 ━━━━━━━━━━━━━━━━━━━━ 17:34:55\n",
      "Accuracy: 0.5187 - Loss: 1.3082\n",
      "\n",
      "Batch 624/992 ━━━━━━━━━━━━━━━━━━━━ 17:35:09\n",
      "Accuracy: 0.5190 - Loss: 1.3083\n",
      "\n",
      "Batch 625/992 ━━━━━━━━━━━━━━━━━━━━ 17:35:25\n",
      "Accuracy: 0.5194 - Loss: 1.3079\n",
      "\n",
      "Batch 626/992 ━━━━━━━━━━━━━━━━━━━━ 17:35:36\n",
      "Accuracy: 0.5198 - Loss: 1.3069\n",
      "\n",
      "Batch 627/992 ━━━━━━━━━━━━━━━━━━━━ 17:35:46\n",
      "Accuracy: 0.5201 - Loss: 1.3057\n",
      "\n",
      "Batch 628/992 ━━━━━━━━━━━━━━━━━━━━ 17:35:58\n",
      "Accuracy: 0.5205 - Loss: 1.3044\n",
      "\n",
      "Batch 629/992 ━━━━━━━━━━━━━━━━━━━━ 17:36:10\n",
      "Accuracy: 0.5211 - Loss: 1.3029\n",
      "\n",
      "Batch 630/992 ━━━━━━━━━━━━━━━━━━━━ 17:36:20\n",
      "Accuracy: 0.5214 - Loss: 1.3020\n",
      "\n",
      "Batch 631/992 ━━━━━━━━━━━━━━━━━━━━ 17:36:32\n",
      "Accuracy: 0.5214 - Loss: 1.3019\n",
      "\n",
      "Batch 632/992 ━━━━━━━━━━━━━━━━━━━━ 17:36:42\n",
      "Accuracy: 0.5214 - Loss: 1.3014\n",
      "\n",
      "Batch 633/992 ━━━━━━━━━━━━━━━━━━━━ 17:36:53\n",
      "Accuracy: 0.5213 - Loss: 1.3009\n",
      "\n",
      "Batch 634/992 ━━━━━━━━━━━━━━━━━━━━ 17:37:04\n",
      "Accuracy: 0.5215 - Loss: 1.2999\n",
      "\n",
      "Batch 635/992 ━━━━━━━━━━━━━━━━━━━━ 17:37:15\n",
      "Accuracy: 0.5220 - Loss: 1.2985\n",
      "\n",
      "Batch 636/992 ━━━━━━━━━━━━━━━━━━━━ 17:37:27\n",
      "Accuracy: 0.5224 - Loss: 1.2978\n",
      "\n",
      "Batch 637/992 ━━━━━━━━━━━━━━━━━━━━ 17:37:39\n",
      "Accuracy: 0.5226 - Loss: 1.2972\n",
      "\n",
      "Batch 638/992 ━━━━━━━━━━━━━━━━━━━━ 17:37:50\n",
      "Accuracy: 0.5229 - Loss: 1.2963\n",
      "\n",
      "Batch 639/992 ━━━━━━━━━━━━━━━━━━━━ 17:38:02\n",
      "Accuracy: 0.5229 - Loss: 1.2963\n",
      "\n",
      "Batch 640/992 ━━━━━━━━━━━━━━━━━━━━ 17:38:13\n",
      "Accuracy: 0.5229 - Loss: 1.2967\n",
      "\n",
      "Batch 641/992 ━━━━━━━━━━━━━━━━━━━━ 17:38:24\n",
      "Accuracy: 0.5228 - Loss: 1.2980\n",
      "\n",
      "Batch 642/992 ━━━━━━━━━━━━━━━━━━━━ 17:38:36\n",
      "Accuracy: 0.5232 - Loss: 1.2975\n",
      "\n",
      "Batch 643/992 ━━━━━━━━━━━━━━━━━━━━ 17:38:48\n",
      "Accuracy: 0.5229 - Loss: 1.2975\n",
      "\n",
      "Batch 644/992 ━━━━━━━━━━━━━━━━━━━━ 17:38:59\n",
      "Accuracy: 0.5231 - Loss: 1.2974\n",
      "\n",
      "Batch 645/992 ━━━━━━━━━━━━━━━━━━━━ 17:39:10\n",
      "Accuracy: 0.5233 - Loss: 1.2964\n",
      "\n",
      "Batch 646/992 ━━━━━━━━━━━━━━━━━━━━ 17:39:20\n",
      "Accuracy: 0.5236 - Loss: 1.2954\n",
      "\n",
      "Batch 647/992 ━━━━━━━━━━━━━━━━━━━━ 17:39:31\n",
      "Accuracy: 0.5238 - Loss: 1.2951\n",
      "\n",
      "Batch 648/992 ━━━━━━━━━━━━━━━━━━━━ 17:39:41\n",
      "Accuracy: 0.5241 - Loss: 1.2941\n",
      "\n",
      "Batch 649/992 ━━━━━━━━━━━━━━━━━━━━ 17:39:52\n",
      "Accuracy: 0.5247 - Loss: 1.2932\n",
      "\n",
      "Batch 650/992 ━━━━━━━━━━━━━━━━━━━━ 17:40:03\n",
      "Accuracy: 0.5246 - Loss: 1.2929\n",
      "\n",
      "Batch 651/992 ━━━━━━━━━━━━━━━━━━━━ 17:40:13\n",
      "Accuracy: 0.5248 - Loss: 1.2920\n",
      "\n",
      "Batch 652/992 ━━━━━━━━━━━━━━━━━━━━ 17:40:24\n",
      "Accuracy: 0.5251 - Loss: 1.2909\n",
      "\n",
      "Batch 653/992 ━━━━━━━━━━━━━━━━━━━━ 17:40:35\n",
      "Accuracy: 0.5255 - Loss: 1.2900\n",
      "\n",
      "Batch 654/992 ━━━━━━━━━━━━━━━━━━━━ 17:40:45\n",
      "Accuracy: 0.5254 - Loss: 1.2896\n",
      "\n",
      "Batch 655/992 ━━━━━━━━━━━━━━━━━━━━ 17:40:56\n",
      "Accuracy: 0.5256 - Loss: 1.2897\n",
      "\n",
      "Batch 656/992 ━━━━━━━━━━━━━━━━━━━━ 17:41:06\n",
      "Accuracy: 0.5257 - Loss: 1.2894\n",
      "\n",
      "Batch 657/992 ━━━━━━━━━━━━━━━━━━━━ 17:41:17\n",
      "Accuracy: 0.5259 - Loss: 1.2886\n",
      "\n",
      "Batch 658/992 ━━━━━━━━━━━━━━━━━━━━ 17:41:27\n",
      "Accuracy: 0.5264 - Loss: 1.2873\n",
      "\n",
      "Batch 659/992 ━━━━━━━━━━━━━━━━━━━━ 17:41:38\n",
      "Accuracy: 0.5267 - Loss: 1.2868\n",
      "\n",
      "Batch 660/992 ━━━━━━━━━━━━━━━━━━━━ 17:41:49\n",
      "Accuracy: 0.5269 - Loss: 1.2864\n",
      "\n",
      "Batch 661/992 ━━━━━━━━━━━━━━━━━━━━ 17:41:59\n",
      "Accuracy: 0.5272 - Loss: 1.2858\n",
      "\n",
      "Batch 662/992 ━━━━━━━━━━━━━━━━━━━━ 17:42:10\n",
      "Accuracy: 0.5279 - Loss: 1.2847\n",
      "\n",
      "Batch 663/992 ━━━━━━━━━━━━━━━━━━━━ 17:42:20\n",
      "Accuracy: 0.5285 - Loss: 1.2837\n",
      "\n",
      "Batch 664/992 ━━━━━━━━━━━━━━━━━━━━ 17:42:31\n",
      "Accuracy: 0.5290 - Loss: 1.2826\n",
      "\n",
      "Batch 665/992 ━━━━━━━━━━━━━━━━━━━━ 17:42:42\n",
      "Accuracy: 0.5295 - Loss: 1.2813\n",
      "\n",
      "Batch 666/992 ━━━━━━━━━━━━━━━━━━━━ 17:42:53\n",
      "Accuracy: 0.5298 - Loss: 1.2808\n",
      "\n",
      "Batch 667/992 ━━━━━━━━━━━━━━━━━━━━ 17:43:03\n",
      "Accuracy: 0.5300 - Loss: 1.2800\n",
      "\n",
      "Batch 668/992 ━━━━━━━━━━━━━━━━━━━━ 17:43:14\n",
      "Accuracy: 0.5305 - Loss: 1.2790\n",
      "\n",
      "Batch 669/992 ━━━━━━━━━━━━━━━━━━━━ 17:43:24\n",
      "Accuracy: 0.5308 - Loss: 1.2785\n",
      "\n",
      "Batch 670/992 ━━━━━━━━━━━━━━━━━━━━ 17:43:35\n",
      "Accuracy: 0.5310 - Loss: 1.2783\n",
      "\n",
      "Batch 671/992 ━━━━━━━━━━━━━━━━━━━━ 17:43:45\n",
      "Accuracy: 0.5313 - Loss: 1.2773\n",
      "\n",
      "Batch 672/992 ━━━━━━━━━━━━━━━━━━━━ 17:43:56\n",
      "Accuracy: 0.5318 - Loss: 1.2760\n",
      "\n",
      "Batch 673/992 ━━━━━━━━━━━━━━━━━━━━ 17:44:07\n",
      "Accuracy: 0.5319 - Loss: 1.2758\n",
      "\n",
      "Batch 674/992 ━━━━━━━━━━━━━━━━━━━━ 17:44:17\n",
      "Accuracy: 0.5323 - Loss: 1.2750\n",
      "\n",
      "Batch 675/992 ━━━━━━━━━━━━━━━━━━━━ 17:44:28\n",
      "Accuracy: 0.5322 - Loss: 1.2745\n",
      "\n",
      "Batch 676/992 ━━━━━━━━━━━━━━━━━━━━ 17:44:39\n",
      "Accuracy: 0.5327 - Loss: 1.2733\n",
      "\n",
      "Batch 677/992 ━━━━━━━━━━━━━━━━━━━━ 17:44:49\n",
      "Accuracy: 0.5329 - Loss: 1.2729\n",
      "\n",
      "Batch 678/992 ━━━━━━━━━━━━━━━━━━━━ 17:45:00\n",
      "Accuracy: 0.5330 - Loss: 1.2728\n",
      "\n",
      "Batch 679/992 ━━━━━━━━━━━━━━━━━━━━ 17:45:11\n",
      "Accuracy: 0.5333 - Loss: 1.2726\n",
      "\n",
      "Batch 680/992 ━━━━━━━━━━━━━━━━━━━━ 17:45:21\n",
      "Accuracy: 0.5338 - Loss: 1.2719\n",
      "\n",
      "Batch 681/992 ━━━━━━━━━━━━━━━━━━━━ 17:45:32\n",
      "Accuracy: 0.5340 - Loss: 1.2716\n",
      "\n",
      "Batch 682/992 ━━━━━━━━━━━━━━━━━━━━ 17:45:42\n",
      "Accuracy: 0.5343 - Loss: 1.2708\n",
      "\n",
      "Batch 683/992 ━━━━━━━━━━━━━━━━━━━━ 17:45:53\n",
      "Accuracy: 0.5350 - Loss: 1.2696\n",
      "\n",
      "Batch 684/992 ━━━━━━━━━━━━━━━━━━━━ 17:46:04\n",
      "Accuracy: 0.5353 - Loss: 1.2688\n",
      "\n",
      "Batch 685/992 ━━━━━━━━━━━━━━━━━━━━ 17:46:14\n",
      "Accuracy: 0.5356 - Loss: 1.2684\n",
      "\n",
      "Batch 686/992 ━━━━━━━━━━━━━━━━━━━━ 17:46:25\n",
      "Accuracy: 0.5353 - Loss: 1.2688\n",
      "\n",
      "Batch 687/992 ━━━━━━━━━━━━━━━━━━━━ 17:46:36\n",
      "Accuracy: 0.5353 - Loss: 1.2686\n",
      "\n",
      "Batch 688/992 ━━━━━━━━━━━━━━━━━━━━ 17:46:46\n",
      "Accuracy: 0.5358 - Loss: 1.2673\n",
      "\n",
      "Batch 689/992 ━━━━━━━━━━━━━━━━━━━━ 17:46:57\n",
      "Accuracy: 0.5363 - Loss: 1.2661\n",
      "\n",
      "Batch 690/992 ━━━━━━━━━━━━━━━━━━━━ 17:47:08\n",
      "Accuracy: 0.5364 - Loss: 1.2658\n",
      "\n",
      "Batch 691/992 ━━━━━━━━━━━━━━━━━━━━ 17:47:18\n",
      "Accuracy: 0.5365 - Loss: 1.2653\n",
      "\n",
      "Batch 692/992 ━━━━━━━━━━━━━━━━━━━━ 17:47:29\n",
      "Accuracy: 0.5370 - Loss: 1.2644\n",
      "\n",
      "Batch 693/992 ━━━━━━━━━━━━━━━━━━━━ 17:47:39\n",
      "Accuracy: 0.5370 - Loss: 1.2637\n",
      "\n",
      "Batch 694/992 ━━━━━━━━━━━━━━━━━━━━ 17:47:50\n",
      "Accuracy: 0.5371 - Loss: 1.2634\n",
      "\n",
      "Batch 695/992 ━━━━━━━━━━━━━━━━━━━━ 17:48:00\n",
      "Accuracy: 0.5378 - Loss: 1.2620\n",
      "\n",
      "Batch 696/992 ━━━━━━━━━━━━━━━━━━━━ 17:48:11\n",
      "Accuracy: 0.5379 - Loss: 1.2620\n",
      "\n",
      "Batch 697/992 ━━━━━━━━━━━━━━━━━━━━ 17:48:21\n",
      "Accuracy: 0.5380 - Loss: 1.2620\n",
      "\n",
      "Batch 698/992 ━━━━━━━━━━━━━━━━━━━━ 17:48:32\n",
      "Accuracy: 0.5383 - Loss: 1.2619\n",
      "\n",
      "Batch 699/992 ━━━━━━━━━━━━━━━━━━━━ 17:48:43\n",
      "Accuracy: 0.5383 - Loss: 1.2612\n",
      "\n",
      "Batch 700/992 ━━━━━━━━━━━━━━━━━━━━ 17:48:54\n",
      "Accuracy: 0.5387 - Loss: 1.2603\n",
      "\n",
      "Batch 701/992 ━━━━━━━━━━━━━━━━━━━━ 17:49:04\n",
      "Accuracy: 0.5389 - Loss: 1.2598\n",
      "\n",
      "Batch 702/992 ━━━━━━━━━━━━━━━━━━━━ 17:49:15\n",
      "Accuracy: 0.5385 - Loss: 1.2597\n",
      "\n",
      "Batch 703/992 ━━━━━━━━━━━━━━━━━━━━ 17:49:25\n",
      "Accuracy: 0.5389 - Loss: 1.2585\n",
      "\n",
      "Batch 704/992 ━━━━━━━━━━━━━━━━━━━━ 17:49:36\n",
      "Accuracy: 0.5392 - Loss: 1.2573\n",
      "\n",
      "Batch 705/992 ━━━━━━━━━━━━━━━━━━━━ 17:49:47\n",
      "Accuracy: 0.5394 - Loss: 1.2572\n",
      "\n",
      "Batch 706/992 ━━━━━━━━━━━━━━━━━━━━ 17:49:57\n",
      "Accuracy: 0.5391 - Loss: 1.2578\n",
      "\n",
      "Batch 707/992 ━━━━━━━━━━━━━━━━━━━━ 17:50:08\n",
      "Accuracy: 0.5398 - Loss: 1.2564\n",
      "\n",
      "Batch 708/992 ━━━━━━━━━━━━━━━━━━━━ 17:50:18\n",
      "Accuracy: 0.5401 - Loss: 1.2558\n",
      "\n",
      "Batch 709/992 ━━━━━━━━━━━━━━━━━━━━ 17:50:29\n",
      "Accuracy: 0.5406 - Loss: 1.2555\n",
      "\n",
      "Batch 710/992 ━━━━━━━━━━━━━━━━━━━━ 17:50:40\n",
      "Accuracy: 0.5408 - Loss: 1.2546\n",
      "\n",
      "Batch 711/992 ━━━━━━━━━━━━━━━━━━━━ 17:50:51\n",
      "Accuracy: 0.5408 - Loss: 1.2543\n",
      "\n",
      "Batch 712/992 ━━━━━━━━━━━━━━━━━━━━ 17:51:01\n",
      "Accuracy: 0.5411 - Loss: 1.2534\n",
      "\n",
      "Batch 713/992 ━━━━━━━━━━━━━━━━━━━━ 17:51:12\n",
      "Accuracy: 0.5412 - Loss: 1.2543\n",
      "\n",
      "Batch 714/992 ━━━━━━━━━━━━━━━━━━━━ 17:51:23\n",
      "Accuracy: 0.5408 - Loss: 1.2547\n",
      "\n",
      "Batch 715/992 ━━━━━━━━━━━━━━━━━━━━ 17:51:33\n",
      "Accuracy: 0.5413 - Loss: 1.2541\n",
      "\n",
      "Batch 716/992 ━━━━━━━━━━━━━━━━━━━━ 17:51:44\n",
      "Accuracy: 0.5414 - Loss: 1.2537\n",
      "\n",
      "Batch 717/992 ━━━━━━━━━━━━━━━━━━━━ 17:51:54\n",
      "Accuracy: 0.5415 - Loss: 1.2539\n",
      "\n",
      "Batch 718/992 ━━━━━━━━━━━━━━━━━━━━ 17:52:05\n",
      "Accuracy: 0.5414 - Loss: 1.2538\n",
      "\n",
      "Batch 719/992 ━━━━━━━━━━━━━━━━━━━━ 17:52:15\n",
      "Accuracy: 0.5417 - Loss: 1.2528\n",
      "\n",
      "Batch 720/992 ━━━━━━━━━━━━━━━━━━━━ 17:52:26\n",
      "Accuracy: 0.5418 - Loss: 1.2523\n",
      "\n",
      "Batch 721/992 ━━━━━━━━━━━━━━━━━━━━ 17:52:37\n",
      "Accuracy: 0.5416 - Loss: 1.2523\n",
      "\n",
      "Batch 722/992 ━━━━━━━━━━━━━━━━━━━━ 17:52:48\n",
      "Accuracy: 0.5419 - Loss: 1.2520\n",
      "\n",
      "Batch 723/992 ━━━━━━━━━━━━━━━━━━━━ 17:52:58\n",
      "Accuracy: 0.5420 - Loss: 1.2517\n",
      "\n",
      "Batch 724/992 ━━━━━━━━━━━━━━━━━━━━ 17:53:09\n",
      "Accuracy: 0.5416 - Loss: 1.2518\n",
      "\n",
      "Batch 725/992 ━━━━━━━━━━━━━━━━━━━━ 17:53:19\n",
      "Accuracy: 0.5417 - Loss: 1.2510\n",
      "\n",
      "Batch 726/992 ━━━━━━━━━━━━━━━━━━━━ 17:53:30\n",
      "Accuracy: 0.5422 - Loss: 1.2503\n",
      "\n",
      "Batch 727/992 ━━━━━━━━━━━━━━━━━━━━ 17:53:40\n",
      "Accuracy: 0.5428 - Loss: 1.2488\n",
      "\n",
      "Batch 728/992 ━━━━━━━━━━━━━━━━━━━━ 17:53:51\n",
      "Accuracy: 0.5429 - Loss: 1.2483\n",
      "\n",
      "Batch 729/992 ━━━━━━━━━━━━━━━━━━━━ 17:54:01\n",
      "Accuracy: 0.5432 - Loss: 1.2480\n",
      "\n",
      "Batch 730/992 ━━━━━━━━━━━━━━━━━━━━ 17:54:12\n",
      "Accuracy: 0.5428 - Loss: 1.2483\n",
      "\n",
      "Batch 731/992 ━━━━━━━━━━━━━━━━━━━━ 17:54:22\n",
      "Accuracy: 0.5429 - Loss: 1.2482\n",
      "\n",
      "Batch 732/992 ━━━━━━━━━━━━━━━━━━━━ 17:54:33\n",
      "Accuracy: 0.5430 - Loss: 1.2477\n",
      "\n",
      "Batch 733/992 ━━━━━━━━━━━━━━━━━━━━ 17:54:44\n",
      "Accuracy: 0.5430 - Loss: 1.2481\n",
      "\n",
      "Batch 734/992 ━━━━━━━━━━━━━━━━━━━━ 17:54:54\n",
      "Accuracy: 0.5433 - Loss: 1.2478\n",
      "\n",
      "Batch 735/992 ━━━━━━━━━━━━━━━━━━━━ 17:55:05\n",
      "Accuracy: 0.5437 - Loss: 1.2470\n",
      "\n",
      "Batch 736/992 ━━━━━━━━━━━━━━━━━━━━ 17:55:15\n",
      "Accuracy: 0.5440 - Loss: 1.2464\n",
      "\n",
      "Batch 737/992 ━━━━━━━━━━━━━━━━━━━━ 17:55:26\n",
      "Accuracy: 0.5439 - Loss: 1.2465\n",
      "\n",
      "Batch 738/992 ━━━━━━━━━━━━━━━━━━━━ 17:55:37\n",
      "Accuracy: 0.5437 - Loss: 1.2468\n",
      "\n",
      "Batch 739/992 ━━━━━━━━━━━━━━━━━━━━ 17:55:47\n",
      "Accuracy: 0.5438 - Loss: 1.2464\n",
      "\n",
      "Batch 740/992 ━━━━━━━━━━━━━━━━━━━━ 17:55:58\n",
      "Accuracy: 0.5443 - Loss: 1.2459\n",
      "\n",
      "Batch 741/992 ━━━━━━━━━━━━━━━━━━━━ 17:56:08\n",
      "Accuracy: 0.5444 - Loss: 1.2455\n",
      "\n",
      "Batch 742/992 ━━━━━━━━━━━━━━━━━━━━ 17:56:19\n",
      "Accuracy: 0.5446 - Loss: 1.2446\n",
      "\n",
      "Batch 743/992 ━━━━━━━━━━━━━━━━━━━━ 17:56:30\n",
      "Accuracy: 0.5446 - Loss: 1.2444\n",
      "\n",
      "Batch 744/992 ━━━━━━━━━━━━━━━━━━━━ 17:56:40\n",
      "Accuracy: 0.5450 - Loss: 1.2437\n",
      "\n",
      "Batch 745/992 ━━━━━━━━━━━━━━━━━━━━ 17:56:51\n",
      "Accuracy: 0.5451 - Loss: 1.2433\n",
      "\n",
      "Batch 746/992 ━━━━━━━━━━━━━━━━━━━━ 17:57:02\n",
      "Accuracy: 0.5452 - Loss: 1.2431\n",
      "\n",
      "Batch 747/992 ━━━━━━━━━━━━━━━━━━━━ 17:57:12\n",
      "Accuracy: 0.5457 - Loss: 1.2420\n",
      "\n",
      "Batch 748/992 ━━━━━━━━━━━━━━━━━━━━ 17:57:23\n",
      "Accuracy: 0.5460 - Loss: 1.2413\n",
      "\n",
      "Batch 749/992 ━━━━━━━━━━━━━━━━━━━━ 17:57:33\n",
      "Accuracy: 0.5461 - Loss: 1.2409\n",
      "\n",
      "Batch 750/992 ━━━━━━━━━━━━━━━━━━━━ 17:57:44\n",
      "Accuracy: 0.5462 - Loss: 1.2402\n",
      "\n",
      "Batch 751/992 ━━━━━━━━━━━━━━━━━━━━ 17:57:55\n",
      "Accuracy: 0.5466 - Loss: 1.2394\n",
      "\n",
      "Batch 752/992 ━━━━━━━━━━━━━━━━━━━━ 17:58:06\n",
      "Accuracy: 0.5469 - Loss: 1.2392\n",
      "\n",
      "Batch 753/992 ━━━━━━━━━━━━━━━━━━━━ 17:58:17\n",
      "Accuracy: 0.5473 - Loss: 1.2381\n",
      "\n",
      "Batch 754/992 ━━━━━━━━━━━━━━━━━━━━ 17:58:30\n",
      "Accuracy: 0.5476 - Loss: 1.2371\n",
      "\n",
      "Batch 755/992 ━━━━━━━━━━━━━━━━━━━━ 17:58:42\n",
      "Accuracy: 0.5475 - Loss: 1.2368\n",
      "\n",
      "Batch 756/992 ━━━━━━━━━━━━━━━━━━━━ 17:58:54\n",
      "Accuracy: 0.5481 - Loss: 1.2356\n",
      "\n",
      "Batch 757/992 ━━━━━━━━━━━━━━━━━━━━ 17:59:05\n",
      "Accuracy: 0.5484 - Loss: 1.2347\n",
      "\n",
      "Batch 758/992 ━━━━━━━━━━━━━━━━━━━━ 17:59:16\n",
      "Accuracy: 0.5486 - Loss: 1.2340\n",
      "\n",
      "Batch 759/992 ━━━━━━━━━━━━━━━━━━━━ 17:59:27\n",
      "Accuracy: 0.5491 - Loss: 1.2330\n",
      "\n",
      "Batch 760/992 ━━━━━━━━━━━━━━━━━━━━ 17:59:38\n",
      "Accuracy: 0.5488 - Loss: 1.2337\n",
      "\n",
      "Batch 761/992 ━━━━━━━━━━━━━━━━━━━━ 17:59:49\n",
      "Accuracy: 0.5489 - Loss: 1.2334\n",
      "\n",
      "Batch 762/992 ━━━━━━━━━━━━━━━━━━━━ 18:00:01\n",
      "Accuracy: 0.5494 - Loss: 1.2326\n",
      "\n",
      "Batch 763/992 ━━━━━━━━━━━━━━━━━━━━ 18:00:12\n",
      "Accuracy: 0.5495 - Loss: 1.2325\n",
      "\n",
      "Batch 764/992 ━━━━━━━━━━━━━━━━━━━━ 18:00:22\n",
      "Accuracy: 0.5499 - Loss: 1.2318\n",
      "\n",
      "Batch 765/992 ━━━━━━━━━━━━━━━━━━━━ 18:00:34\n",
      "Accuracy: 0.5503 - Loss: 1.2307\n",
      "\n",
      "Batch 766/992 ━━━━━━━━━━━━━━━━━━━━ 18:00:45\n",
      "Accuracy: 0.5501 - Loss: 1.2313\n",
      "\n",
      "Batch 767/992 ━━━━━━━━━━━━━━━━━━━━ 18:00:57\n",
      "Accuracy: 0.5505 - Loss: 1.2306\n",
      "\n",
      "Batch 768/992 ━━━━━━━━━━━━━━━━━━━━ 18:01:08\n",
      "Accuracy: 0.5501 - Loss: 1.2314\n",
      "\n",
      "Batch 769/992 ━━━━━━━━━━━━━━━━━━━━ 18:01:19\n",
      "Accuracy: 0.5504 - Loss: 1.2308\n",
      "\n",
      "Batch 770/992 ━━━━━━━━━━━━━━━━━━━━ 18:01:30\n",
      "Accuracy: 0.5502 - Loss: 1.2307\n",
      "\n",
      "Batch 771/992 ━━━━━━━━━━━━━━━━━━━━ 18:01:42\n",
      "Accuracy: 0.5503 - Loss: 1.2306\n",
      "\n",
      "Batch 772/992 ━━━━━━━━━━━━━━━━━━━━ 18:01:53\n",
      "Accuracy: 0.5507 - Loss: 1.2296\n",
      "\n",
      "Batch 773/992 ━━━━━━━━━━━━━━━━━━━━ 18:02:06\n",
      "Accuracy: 0.5513 - Loss: 1.2287\n",
      "\n",
      "Batch 774/992 ━━━━━━━━━━━━━━━━━━━━ 18:02:23\n",
      "Accuracy: 0.5510 - Loss: 1.2289\n",
      "\n",
      "Batch 775/992 ━━━━━━━━━━━━━━━━━━━━ 18:02:38\n",
      "Accuracy: 0.5515 - Loss: 1.2281\n",
      "\n",
      "Batch 776/992 ━━━━━━━━━━━━━━━━━━━━ 18:02:51\n",
      "Accuracy: 0.5515 - Loss: 1.2280\n",
      "\n",
      "Batch 777/992 ━━━━━━━━━━━━━━━━━━━━ 18:03:03\n",
      "Accuracy: 0.5518 - Loss: 1.2270\n",
      "\n",
      "Batch 778/992 ━━━━━━━━━━━━━━━━━━━━ 18:03:14\n",
      "Accuracy: 0.5524 - Loss: 1.2258\n",
      "\n",
      "Batch 779/992 ━━━━━━━━━━━━━━━━━━━━ 18:03:25\n",
      "Accuracy: 0.5525 - Loss: 1.2255\n",
      "\n",
      "Batch 780/992 ━━━━━━━━━━━━━━━━━━━━ 18:03:36\n",
      "Accuracy: 0.5529 - Loss: 1.2247\n",
      "\n",
      "Batch 781/992 ━━━━━━━━━━━━━━━━━━━━ 18:03:49\n",
      "Accuracy: 0.5533 - Loss: 1.2239\n",
      "\n",
      "Batch 782/992 ━━━━━━━━━━━━━━━━━━━━ 18:04:02\n",
      "Accuracy: 0.5537 - Loss: 1.2232\n",
      "\n",
      "Batch 783/992 ━━━━━━━━━━━━━━━━━━━━ 18:04:14\n",
      "Accuracy: 0.5540 - Loss: 1.2228\n",
      "\n",
      "Batch 784/992 ━━━━━━━━━━━━━━━━━━━━ 18:04:26\n",
      "Accuracy: 0.5542 - Loss: 1.2221\n",
      "\n",
      "Batch 785/992 ━━━━━━━━━━━━━━━━━━━━ 18:04:39\n",
      "Accuracy: 0.5546 - Loss: 1.2213\n",
      "\n",
      "Batch 786/992 ━━━━━━━━━━━━━━━━━━━━ 18:04:51\n",
      "Accuracy: 0.5550 - Loss: 1.2208\n",
      "\n",
      "Batch 787/992 ━━━━━━━━━━━━━━━━━━━━ 18:05:07\n",
      "Accuracy: 0.5554 - Loss: 1.2203\n",
      "\n",
      "Batch 788/992 ━━━━━━━━━━━━━━━━━━━━ 18:05:22\n",
      "Accuracy: 0.5554 - Loss: 1.2204\n",
      "\n",
      "Batch 789/992 ━━━━━━━━━━━━━━━━━━━━ 18:05:34\n",
      "Accuracy: 0.5556 - Loss: 1.2198\n",
      "\n",
      "Batch 790/992 ━━━━━━━━━━━━━━━━━━━━ 18:05:47\n",
      "Accuracy: 0.5559 - Loss: 1.2192\n",
      "\n",
      "Batch 791/992 ━━━━━━━━━━━━━━━━━━━━ 18:05:59\n",
      "Accuracy: 0.5559 - Loss: 1.2188\n",
      "\n",
      "Batch 792/992 ━━━━━━━━━━━━━━━━━━━━ 18:06:10\n",
      "Accuracy: 0.5559 - Loss: 1.2183\n",
      "\n",
      "Batch 793/992 ━━━━━━━━━━━━━━━━━━━━ 18:06:23\n",
      "Accuracy: 0.5563 - Loss: 1.2173\n",
      "\n",
      "Batch 794/992 ━━━━━━━━━━━━━━━━━━━━ 18:06:35\n",
      "Accuracy: 0.5565 - Loss: 1.2165\n",
      "\n",
      "Batch 795/992 ━━━━━━━━━━━━━━━━━━━━ 18:06:49\n",
      "Accuracy: 0.5564 - Loss: 1.2168\n",
      "\n",
      "Batch 796/992 ━━━━━━━━━━━━━━━━━━━━ 18:07:00\n",
      "Accuracy: 0.5567 - Loss: 1.2164\n",
      "\n",
      "Batch 797/992 ━━━━━━━━━━━━━━━━━━━━ 18:07:11\n",
      "Accuracy: 0.5566 - Loss: 1.2167\n",
      "\n",
      "Batch 798/992 ━━━━━━━━━━━━━━━━━━━━ 18:07:21\n",
      "Accuracy: 0.5572 - Loss: 1.2155\n",
      "\n",
      "Batch 799/992 ━━━━━━━━━━━━━━━━━━━━ 18:07:34\n",
      "Accuracy: 0.5573 - Loss: 1.2150\n",
      "\n",
      "Batch 800/992 ━━━━━━━━━━━━━━━━━━━━ 18:07:48\n",
      "Accuracy: 0.5575 - Loss: 1.2148\n",
      "\n",
      "Batch 801/992 ━━━━━━━━━━━━━━━━━━━━ 18:08:00\n",
      "Accuracy: 0.5581 - Loss: 1.2135\n",
      "\n",
      "Batch 802/992 ━━━━━━━━━━━━━━━━━━━━ 18:08:11\n",
      "Accuracy: 0.5583 - Loss: 1.2131\n",
      "\n",
      "Batch 803/992 ━━━━━━━━━━━━━━━━━━━━ 18:08:23\n",
      "Accuracy: 0.5585 - Loss: 1.2126\n",
      "\n",
      "Batch 804/992 ━━━━━━━━━━━━━━━━━━━━ 18:08:37\n",
      "Accuracy: 0.5589 - Loss: 1.2116\n",
      "\n",
      "Batch 805/992 ━━━━━━━━━━━━━━━━━━━━ 18:08:52\n",
      "Accuracy: 0.5590 - Loss: 1.2114\n",
      "\n",
      "Batch 806/992 ━━━━━━━━━━━━━━━━━━━━ 18:09:04\n",
      "Accuracy: 0.5594 - Loss: 1.2109\n",
      "\n",
      "Batch 807/992 ━━━━━━━━━━━━━━━━━━━━ 18:09:18\n",
      "Accuracy: 0.5599 - Loss: 1.2098\n",
      "\n",
      "Batch 808/992 ━━━━━━━━━━━━━━━━━━━━ 18:09:31\n",
      "Accuracy: 0.5602 - Loss: 1.2090\n",
      "\n",
      "Batch 809/992 ━━━━━━━━━━━━━━━━━━━━ 18:09:43\n",
      "Accuracy: 0.5604 - Loss: 1.2084\n",
      "\n",
      "Batch 810/992 ━━━━━━━━━━━━━━━━━━━━ 18:09:56\n",
      "Accuracy: 0.5606 - Loss: 1.2079\n",
      "\n",
      "Batch 811/992 ━━━━━━━━━━━━━━━━━━━━ 18:10:09\n",
      "Accuracy: 0.5610 - Loss: 1.2072\n",
      "\n",
      "Batch 812/992 ━━━━━━━━━━━━━━━━━━━━ 18:10:21\n",
      "Accuracy: 0.5614 - Loss: 1.2064\n",
      "\n",
      "Batch 813/992 ━━━━━━━━━━━━━━━━━━━━ 18:10:35\n",
      "Accuracy: 0.5615 - Loss: 1.2060\n",
      "\n",
      "Batch 814/992 ━━━━━━━━━━━━━━━━━━━━ 18:10:49\n",
      "Accuracy: 0.5617 - Loss: 1.2054\n",
      "\n",
      "Batch 815/992 ━━━━━━━━━━━━━━━━━━━━ 18:11:00\n",
      "Accuracy: 0.5621 - Loss: 1.2048\n",
      "\n",
      "Batch 816/992 ━━━━━━━━━━━━━━━━━━━━ 18:11:11\n",
      "Accuracy: 0.5622 - Loss: 1.2044\n",
      "\n",
      "Batch 817/992 ━━━━━━━━━━━━━━━━━━━━ 18:11:23\n",
      "Accuracy: 0.5623 - Loss: 1.2041\n",
      "\n",
      "Batch 818/992 ━━━━━━━━━━━━━━━━━━━━ 18:11:33\n",
      "Accuracy: 0.5625 - Loss: 1.2036\n",
      "\n",
      "Batch 819/992 ━━━━━━━━━━━━━━━━━━━━ 18:11:44\n",
      "Accuracy: 0.5627 - Loss: 1.2033\n",
      "\n",
      "Batch 820/992 ━━━━━━━━━━━━━━━━━━━━ 18:11:54\n",
      "Accuracy: 0.5630 - Loss: 1.2027\n",
      "\n",
      "Batch 821/992 ━━━━━━━━━━━━━━━━━━━━ 18:12:05\n",
      "Accuracy: 0.5630 - Loss: 1.2023\n",
      "\n",
      "Batch 822/992 ━━━━━━━━━━━━━━━━━━━━ 18:12:17\n",
      "Accuracy: 0.5634 - Loss: 1.2014\n",
      "\n",
      "Batch 823/992 ━━━━━━━━━━━━━━━━━━━━ 18:12:29\n",
      "Accuracy: 0.5635 - Loss: 1.2014\n",
      "\n",
      "Batch 824/992 ━━━━━━━━━━━━━━━━━━━━ 18:12:40\n",
      "Accuracy: 0.5639 - Loss: 1.2003\n",
      "\n",
      "Batch 825/992 ━━━━━━━━━━━━━━━━━━━━ 18:12:52\n",
      "Accuracy: 0.5641 - Loss: 1.1997\n",
      "\n",
      "Batch 826/992 ━━━━━━━━━━━━━━━━━━━━ 18:13:03\n",
      "Accuracy: 0.5643 - Loss: 1.1992\n",
      "\n",
      "Batch 827/992 ━━━━━━━━━━━━━━━━━━━━ 18:13:14\n",
      "Accuracy: 0.5647 - Loss: 1.1986\n",
      "\n",
      "Batch 828/992 ━━━━━━━━━━━━━━━━━━━━ 18:13:25\n",
      "Accuracy: 0.5652 - Loss: 1.1975\n",
      "\n",
      "Batch 829/992 ━━━━━━━━━━━━━━━━━━━━ 18:13:36\n",
      "Accuracy: 0.5653 - Loss: 1.1976\n",
      "\n",
      "Batch 830/992 ━━━━━━━━━━━━━━━━━━━━ 18:13:47\n",
      "Accuracy: 0.5657 - Loss: 1.1969\n",
      "\n",
      "Batch 831/992 ━━━━━━━━━━━━━━━━━━━━ 18:13:58\n",
      "Accuracy: 0.5662 - Loss: 1.1956\n",
      "\n",
      "Batch 832/992 ━━━━━━━━━━━━━━━━━━━━ 18:14:09\n",
      "Accuracy: 0.5666 - Loss: 1.1945\n",
      "\n",
      "Batch 833/992 ━━━━━━━━━━━━━━━━━━━━ 18:14:20\n",
      "Accuracy: 0.5666 - Loss: 1.1946\n",
      "\n",
      "Batch 834/992 ━━━━━━━━━━━━━━━━━━━━ 18:14:32\n",
      "Accuracy: 0.5670 - Loss: 1.1938\n",
      "\n",
      "Batch 835/992 ━━━━━━━━━━━━━━━━━━━━ 18:14:43\n",
      "Accuracy: 0.5672 - Loss: 1.1930\n",
      "\n",
      "Batch 836/992 ━━━━━━━━━━━━━━━━━━━━ 18:14:56\n",
      "Accuracy: 0.5676 - Loss: 1.1919\n",
      "\n",
      "Batch 837/992 ━━━━━━━━━━━━━━━━━━━━ 18:15:06\n",
      "Accuracy: 0.5677 - Loss: 1.1914\n",
      "\n",
      "Batch 838/992 ━━━━━━━━━━━━━━━━━━━━ 18:15:18\n",
      "Accuracy: 0.5676 - Loss: 1.1912\n",
      "\n",
      "Batch 839/992 ━━━━━━━━━━━━━━━━━━━━ 18:15:29\n",
      "Accuracy: 0.5679 - Loss: 1.1904\n",
      "\n",
      "Batch 840/992 ━━━━━━━━━━━━━━━━━━━━ 18:15:40\n",
      "Accuracy: 0.5682 - Loss: 1.1897\n",
      "\n",
      "Batch 841/992 ━━━━━━━━━━━━━━━━━━━━ 18:15:52\n",
      "Accuracy: 0.5682 - Loss: 1.1891\n",
      "\n",
      "Batch 842/992 ━━━━━━━━━━━━━━━━━━━━ 18:16:03\n",
      "Accuracy: 0.5686 - Loss: 1.1881\n",
      "\n",
      "Batch 843/992 ━━━━━━━━━━━━━━━━━━━━ 18:16:14\n",
      "Accuracy: 0.5690 - Loss: 1.1869\n",
      "\n",
      "Batch 844/992 ━━━━━━━━━━━━━━━━━━━━ 18:16:25\n",
      "Accuracy: 0.5693 - Loss: 1.1860\n",
      "\n",
      "Batch 845/992 ━━━━━━━━━━━━━━━━━━━━ 18:16:36\n",
      "Accuracy: 0.5695 - Loss: 1.1854\n",
      "\n",
      "Batch 846/992 ━━━━━━━━━━━━━━━━━━━━ 18:16:48\n",
      "Accuracy: 0.5697 - Loss: 1.1850\n",
      "\n",
      "Batch 847/992 ━━━━━━━━━━━━━━━━━━━━ 18:17:00\n",
      "Accuracy: 0.5701 - Loss: 1.1840\n",
      "\n",
      "Batch 848/992 ━━━━━━━━━━━━━━━━━━━━ 18:17:11\n",
      "Accuracy: 0.5703 - Loss: 1.1837\n",
      "\n",
      "Batch 849/992 ━━━━━━━━━━━━━━━━━━━━ 18:17:22\n",
      "Accuracy: 0.5705 - Loss: 1.1828\n",
      "\n",
      "Batch 850/992 ━━━━━━━━━━━━━━━━━━━━ 18:17:33\n",
      "Accuracy: 0.5709 - Loss: 1.1819\n",
      "\n",
      "Batch 851/992 ━━━━━━━━━━━━━━━━━━━━ 18:17:45\n",
      "Accuracy: 0.5709 - Loss: 1.1814\n",
      "\n",
      "Batch 852/992 ━━━━━━━━━━━━━━━━━━━━ 18:17:55\n",
      "Accuracy: 0.5712 - Loss: 1.1810\n",
      "\n",
      "Batch 853/992 ━━━━━━━━━━━━━━━━━━━━ 18:18:07\n",
      "Accuracy: 0.5715 - Loss: 1.1804\n",
      "\n",
      "Batch 854/992 ━━━━━━━━━━━━━━━━━━━━ 18:18:18\n",
      "Accuracy: 0.5719 - Loss: 1.1795\n",
      "\n",
      "Batch 855/992 ━━━━━━━━━━━━━━━━━━━━ 18:18:28\n",
      "Accuracy: 0.5722 - Loss: 1.1787\n",
      "\n",
      "Batch 856/992 ━━━━━━━━━━━━━━━━━━━━ 18:18:39\n",
      "Accuracy: 0.5726 - Loss: 1.1779\n",
      "\n",
      "Batch 857/992 ━━━━━━━━━━━━━━━━━━━━ 18:18:50\n",
      "Accuracy: 0.5726 - Loss: 1.1780\n",
      "\n",
      "Batch 858/992 ━━━━━━━━━━━━━━━━━━━━ 18:19:00\n",
      "Accuracy: 0.5728 - Loss: 1.1772\n",
      "\n",
      "Batch 859/992 ━━━━━━━━━━━━━━━━━━━━ 18:19:11\n",
      "Accuracy: 0.5731 - Loss: 1.1768\n",
      "\n",
      "Batch 860/992 ━━━━━━━━━━━━━━━━━━━━ 18:19:22\n",
      "Accuracy: 0.5730 - Loss: 1.1768\n",
      "\n",
      "Batch 861/992 ━━━━━━━━━━━━━━━━━━━━ 18:19:33\n",
      "Accuracy: 0.5729 - Loss: 1.1766\n",
      "\n",
      "Batch 862/992 ━━━━━━━━━━━━━━━━━━━━ 18:19:44\n",
      "Accuracy: 0.5734 - Loss: 1.1757\n",
      "\n",
      "Batch 863/992 ━━━━━━━━━━━━━━━━━━━━ 18:19:55\n",
      "Accuracy: 0.5734 - Loss: 1.1754\n",
      "\n",
      "Batch 864/992 ━━━━━━━━━━━━━━━━━━━━ 18:20:05\n",
      "Accuracy: 0.5738 - Loss: 1.1747\n",
      "\n",
      "Batch 865/992 ━━━━━━━━━━━━━━━━━━━━ 18:20:16\n",
      "Accuracy: 0.5741 - Loss: 1.1737\n",
      "\n",
      "Batch 866/992 ━━━━━━━━━━━━━━━━━━━━ 18:20:27\n",
      "Accuracy: 0.5739 - Loss: 1.1741\n",
      "\n",
      "Batch 867/992 ━━━━━━━━━━━━━━━━━━━━ 18:20:40\n",
      "Accuracy: 0.5741 - Loss: 1.1741\n",
      "\n",
      "Batch 868/992 ━━━━━━━━━━━━━━━━━━━━ 18:20:51\n",
      "Accuracy: 0.5743 - Loss: 1.1732\n",
      "\n",
      "Batch 869/992 ━━━━━━━━━━━━━━━━━━━━ 18:21:01\n",
      "Accuracy: 0.5745 - Loss: 1.1726\n",
      "\n",
      "Batch 870/992 ━━━━━━━━━━━━━━━━━━━━ 18:21:12\n",
      "Accuracy: 0.5747 - Loss: 1.1722\n",
      "\n",
      "Batch 871/992 ━━━━━━━━━━━━━━━━━━━━ 18:21:23\n",
      "Accuracy: 0.5751 - Loss: 1.1713\n",
      "\n",
      "Batch 872/992 ━━━━━━━━━━━━━━━━━━━━ 18:21:34\n",
      "Accuracy: 0.5753 - Loss: 1.1707\n",
      "\n",
      "Batch 873/992 ━━━━━━━━━━━━━━━━━━━━ 18:21:44\n",
      "Accuracy: 0.5755 - Loss: 1.1702\n",
      "\n",
      "Batch 874/992 ━━━━━━━━━━━━━━━━━━━━ 18:21:55\n",
      "Accuracy: 0.5759 - Loss: 1.1691\n",
      "\n",
      "Batch 875/992 ━━━━━━━━━━━━━━━━━━━━ 18:22:05\n",
      "Accuracy: 0.5761 - Loss: 1.1686\n",
      "\n",
      "Batch 876/992 ━━━━━━━━━━━━━━━━━━━━ 18:22:16\n",
      "Accuracy: 0.5761 - Loss: 1.1690\n",
      "\n",
      "Batch 877/992 ━━━━━━━━━━━━━━━━━━━━ 18:22:27\n",
      "Accuracy: 0.5760 - Loss: 1.1691\n",
      "\n",
      "Batch 878/992 ━━━━━━━━━━━━━━━━━━━━ 18:22:38\n",
      "Accuracy: 0.5762 - Loss: 1.1686\n",
      "\n",
      "Batch 879/992 ━━━━━━━━━━━━━━━━━━━━ 18:22:50\n",
      "Accuracy: 0.5761 - Loss: 1.1688\n",
      "\n",
      "Batch 880/992 ━━━━━━━━━━━━━━━━━━━━ 18:23:02\n",
      "Accuracy: 0.5763 - Loss: 1.1685\n",
      "\n",
      "Batch 881/992 ━━━━━━━━━━━━━━━━━━━━ 18:23:14\n",
      "Accuracy: 0.5765 - Loss: 1.1678\n",
      "\n",
      "Batch 882/992 ━━━━━━━━━━━━━━━━━━━━ 18:23:26\n",
      "Accuracy: 0.5765 - Loss: 1.1676\n",
      "\n",
      "Batch 883/992 ━━━━━━━━━━━━━━━━━━━━ 18:23:37\n",
      "Accuracy: 0.5763 - Loss: 1.1679\n",
      "\n",
      "Batch 884/992 ━━━━━━━━━━━━━━━━━━━━ 18:23:48\n",
      "Accuracy: 0.5764 - Loss: 1.1674\n",
      "\n",
      "Batch 885/992 ━━━━━━━━━━━━━━━━━━━━ 18:23:59\n",
      "Accuracy: 0.5760 - Loss: 1.1684\n",
      "\n",
      "Batch 886/992 ━━━━━━━━━━━━━━━━━━━━ 18:24:11\n",
      "Accuracy: 0.5762 - Loss: 1.1679\n",
      "\n",
      "Batch 887/992 ━━━━━━━━━━━━━━━━━━━━ 18:24:22\n",
      "Accuracy: 0.5761 - Loss: 1.1679\n",
      "\n",
      "Batch 888/992 ━━━━━━━━━━━━━━━━━━━━ 18:24:33\n",
      "Accuracy: 0.5763 - Loss: 1.1674\n",
      "\n",
      "Batch 889/992 ━━━━━━━━━━━━━━━━━━━━ 18:24:44\n",
      "Accuracy: 0.5765 - Loss: 1.1667\n",
      "\n",
      "Batch 890/992 ━━━━━━━━━━━━━━━━━━━━ 18:24:55\n",
      "Accuracy: 0.5764 - Loss: 1.1667\n",
      "\n",
      "Batch 891/992 ━━━━━━━━━━━━━━━━━━━━ 18:25:06\n",
      "Accuracy: 0.5763 - Loss: 1.1664\n",
      "\n",
      "Batch 892/992 ━━━━━━━━━━━━━━━━━━━━ 18:25:16\n",
      "Accuracy: 0.5765 - Loss: 1.1657\n",
      "\n",
      "Batch 893/992 ━━━━━━━━━━━━━━━━━━━━ 18:25:27\n",
      "Accuracy: 0.5767 - Loss: 1.1651\n",
      "\n",
      "Batch 894/992 ━━━━━━━━━━━━━━━━━━━━ 18:25:37\n",
      "Accuracy: 0.5769 - Loss: 1.1648\n",
      "\n",
      "Batch 895/992 ━━━━━━━━━━━━━━━━━━━━ 18:25:48\n",
      "Accuracy: 0.5771 - Loss: 1.1642\n",
      "\n",
      "Batch 896/992 ━━━━━━━━━━━━━━━━━━━━ 18:25:58\n",
      "Accuracy: 0.5773 - Loss: 1.1641\n",
      "\n",
      "Batch 897/992 ━━━━━━━━━━━━━━━━━━━━ 18:26:09\n",
      "Accuracy: 0.5773 - Loss: 1.1637\n",
      "\n",
      "Batch 898/992 ━━━━━━━━━━━━━━━━━━━━ 18:26:20\n",
      "Accuracy: 0.5774 - Loss: 1.1634\n",
      "\n",
      "Batch 899/992 ━━━━━━━━━━━━━━━━━━━━ 18:26:32\n",
      "Accuracy: 0.5776 - Loss: 1.1627\n",
      "\n",
      "Batch 900/992 ━━━━━━━━━━━━━━━━━━━━ 18:26:44\n",
      "Accuracy: 0.5778 - Loss: 1.1624\n",
      "\n",
      "Batch 901/992 ━━━━━━━━━━━━━━━━━━━━ 18:26:56\n",
      "Accuracy: 0.5780 - Loss: 1.1619\n",
      "\n",
      "Batch 902/992 ━━━━━━━━━━━━━━━━━━━━ 18:27:08\n",
      "Accuracy: 0.5777 - Loss: 1.1620\n",
      "\n",
      "Batch 903/992 ━━━━━━━━━━━━━━━━━━━━ 18:27:19\n",
      "Accuracy: 0.5781 - Loss: 1.1613\n",
      "\n",
      "Batch 904/992 ━━━━━━━━━━━━━━━━━━━━ 18:27:30\n",
      "Accuracy: 0.5784 - Loss: 1.1605\n",
      "\n",
      "Batch 905/992 ━━━━━━━━━━━━━━━━━━━━ 18:27:42\n",
      "Accuracy: 0.5787 - Loss: 1.1599\n",
      "\n",
      "Batch 906/992 ━━━━━━━━━━━━━━━━━━━━ 18:27:53\n",
      "Accuracy: 0.5789 - Loss: 1.1592\n",
      "\n",
      "Batch 907/992 ━━━━━━━━━━━━━━━━━━━━ 18:28:04\n",
      "Accuracy: 0.5791 - Loss: 1.1588\n",
      "\n",
      "Batch 908/992 ━━━━━━━━━━━━━━━━━━━━ 18:28:16\n",
      "Accuracy: 0.5790 - Loss: 1.1590\n",
      "\n",
      "Batch 909/992 ━━━━━━━━━━━━━━━━━━━━ 18:28:27\n",
      "Accuracy: 0.5791 - Loss: 1.1587\n",
      "\n",
      "Batch 910/992 ━━━━━━━━━━━━━━━━━━━━ 18:28:39\n",
      "Accuracy: 0.5793 - Loss: 1.1582\n",
      "\n",
      "Batch 911/992 ━━━━━━━━━━━━━━━━━━━━ 18:28:50\n",
      "Accuracy: 0.5793 - Loss: 1.1580\n",
      "\n",
      "Batch 912/992 ━━━━━━━━━━━━━━━━━━━━ 18:29:03\n",
      "Accuracy: 0.5796 - Loss: 1.1579\n",
      "\n",
      "Batch 913/992 ━━━━━━━━━━━━━━━━━━━━ 18:29:15\n",
      "Accuracy: 0.5798 - Loss: 1.1575\n",
      "\n",
      "Batch 914/992 ━━━━━━━━━━━━━━━━━━━━ 18:29:25\n",
      "Accuracy: 0.5800 - Loss: 1.1569\n",
      "\n",
      "Batch 915/992 ━━━━━━━━━━━━━━━━━━━━ 18:29:36\n",
      "Accuracy: 0.5803 - Loss: 1.1566\n",
      "\n",
      "Batch 916/992 ━━━━━━━━━━━━━━━━━━━━ 18:29:47\n",
      "Accuracy: 0.5805 - Loss: 1.1564\n",
      "\n",
      "Batch 917/992 ━━━━━━━━━━━━━━━━━━━━ 18:29:58\n",
      "Accuracy: 0.5807 - Loss: 1.1557\n",
      "\n",
      "Batch 918/992 ━━━━━━━━━━━━━━━━━━━━ 18:30:09\n",
      "Accuracy: 0.5807 - Loss: 1.1561\n",
      "\n",
      "Batch 919/992 ━━━━━━━━━━━━━━━━━━━━ 18:30:20\n",
      "Accuracy: 0.5809 - Loss: 1.1555\n",
      "\n",
      "Batch 920/992 ━━━━━━━━━━━━━━━━━━━━ 18:30:31\n",
      "Accuracy: 0.5813 - Loss: 1.1548\n",
      "\n",
      "Batch 921/992 ━━━━━━━━━━━━━━━━━━━━ 18:30:42\n",
      "Accuracy: 0.5814 - Loss: 1.1545\n",
      "\n",
      "Batch 922/992 ━━━━━━━━━━━━━━━━━━━━ 18:30:53\n",
      "Accuracy: 0.5816 - Loss: 1.1540\n",
      "\n",
      "Batch 923/992 ━━━━━━━━━━━━━━━━━━━━ 18:31:05\n",
      "Accuracy: 0.5818 - Loss: 1.1539\n",
      "\n",
      "Batch 924/992 ━━━━━━━━━━━━━━━━━━━━ 18:31:15\n",
      "Accuracy: 0.5821 - Loss: 1.1531\n",
      "\n",
      "Batch 925/992 ━━━━━━━━━━━━━━━━━━━━ 18:31:27\n",
      "Accuracy: 0.5819 - Loss: 1.1532\n",
      "\n",
      "Batch 926/992 ━━━━━━━━━━━━━━━━━━━━ 18:31:38\n",
      "Accuracy: 0.5819 - Loss: 1.1527\n",
      "\n",
      "Batch 927/992 ━━━━━━━━━━━━━━━━━━━━ 18:31:49\n",
      "Accuracy: 0.5821 - Loss: 1.1519\n",
      "\n",
      "Batch 928/992 ━━━━━━━━━━━━━━━━━━━━ 18:32:01\n",
      "Accuracy: 0.5819 - Loss: 1.1521\n",
      "\n",
      "Batch 929/992 ━━━━━━━━━━━━━━━━━━━━ 18:32:12\n",
      "Accuracy: 0.5819 - Loss: 1.1520\n",
      "\n",
      "Batch 930/992 ━━━━━━━━━━━━━━━━━━━━ 18:32:23\n",
      "Accuracy: 0.5817 - Loss: 1.1522\n",
      "\n",
      "Batch 931/992 ━━━━━━━━━━━━━━━━━━━━ 18:32:34\n",
      "Accuracy: 0.5815 - Loss: 1.1524\n",
      "\n",
      "Batch 932/992 ━━━━━━━━━━━━━━━━━━━━ 18:32:45\n",
      "Accuracy: 0.5815 - Loss: 1.1520\n",
      "\n",
      "Batch 933/992 ━━━━━━━━━━━━━━━━━━━━ 18:32:55\n",
      "Accuracy: 0.5815 - Loss: 1.1521\n",
      "\n",
      "Batch 934/992 ━━━━━━━━━━━━━━━━━━━━ 18:33:07\n",
      "Accuracy: 0.5818 - Loss: 1.1512\n",
      "\n",
      "Batch 935/992 ━━━━━━━━━━━━━━━━━━━━ 18:33:17\n",
      "Accuracy: 0.5821 - Loss: 1.1505\n",
      "\n",
      "Batch 936/992 ━━━━━━━━━━━━━━━━━━━━ 18:33:28\n",
      "Accuracy: 0.5824 - Loss: 1.1497\n",
      "\n",
      "Batch 937/992 ━━━━━━━━━━━━━━━━━━━━ 18:33:39\n",
      "Accuracy: 0.5826 - Loss: 1.1493\n",
      "\n",
      "Batch 938/992 ━━━━━━━━━━━━━━━━━━━━ 18:33:51\n",
      "Accuracy: 0.5825 - Loss: 1.1492\n",
      "\n",
      "Batch 939/992 ━━━━━━━━━━━━━━━━━━━━ 18:34:02\n",
      "Accuracy: 0.5823 - Loss: 1.1494\n",
      "\n",
      "Batch 940/992 ━━━━━━━━━━━━━━━━━━━━ 18:34:14\n",
      "Accuracy: 0.5823 - Loss: 1.1489\n",
      "\n",
      "Batch 941/992 ━━━━━━━━━━━━━━━━━━━━ 18:34:26\n",
      "Accuracy: 0.5824 - Loss: 1.1491\n",
      "\n",
      "Batch 942/992 ━━━━━━━━━━━━━━━━━━━━ 18:34:37\n",
      "Accuracy: 0.5825 - Loss: 1.1486\n",
      "\n",
      "Batch 943/992 ━━━━━━━━━━━━━━━━━━━━ 18:34:49\n",
      "Accuracy: 0.5826 - Loss: 1.1483\n",
      "\n",
      "Batch 944/992 ━━━━━━━━━━━━━━━━━━━━ 18:35:01\n",
      "Accuracy: 0.5830 - Loss: 1.1474\n",
      "\n",
      "Batch 945/992 ━━━━━━━━━━━━━━━━━━━━ 18:35:12\n",
      "Accuracy: 0.5831 - Loss: 1.1471\n",
      "\n",
      "Batch 946/992 ━━━━━━━━━━━━━━━━━━━━ 18:35:24\n",
      "Accuracy: 0.5830 - Loss: 1.1469\n",
      "\n",
      "Batch 947/992 ━━━━━━━━━━━━━━━━━━━━ 18:35:36\n",
      "Accuracy: 0.5829 - Loss: 1.1469\n",
      "\n",
      "Batch 948/992 ━━━━━━━━━━━━━━━━━━━━ 18:35:47\n",
      "Accuracy: 0.5831 - Loss: 1.1465\n",
      "\n",
      "Batch 949/992 ━━━━━━━━━━━━━━━━━━━━ 18:35:59\n",
      "Accuracy: 0.5834 - Loss: 1.1456\n",
      "\n",
      "Batch 950/992 ━━━━━━━━━━━━━━━━━━━━ 18:36:11\n",
      "Accuracy: 0.5837 - Loss: 1.1449\n",
      "\n",
      "Batch 951/992 ━━━━━━━━━━━━━━━━━━━━ 18:36:23\n",
      "Accuracy: 0.5839 - Loss: 1.1447\n",
      "\n",
      "Batch 952/992 ━━━━━━━━━━━━━━━━━━━━ 18:36:34\n",
      "Accuracy: 0.5839 - Loss: 1.1442\n",
      "\n",
      "Batch 953/992 ━━━━━━━━━━━━━━━━━━━━ 18:36:46\n",
      "Accuracy: 0.5841 - Loss: 1.1435\n",
      "\n",
      "Batch 954/992 ━━━━━━━━━━━━━━━━━━━━ 18:36:58\n",
      "Accuracy: 0.5841 - Loss: 1.1434\n",
      "\n",
      "Batch 955/992 ━━━━━━━━━━━━━━━━━━━━ 18:37:10\n",
      "Accuracy: 0.5843 - Loss: 1.1432\n",
      "\n",
      "Batch 956/992 ━━━━━━━━━━━━━━━━━━━━ 18:37:23\n",
      "Accuracy: 0.5846 - Loss: 1.1426\n",
      "\n",
      "Batch 957/992 ━━━━━━━━━━━━━━━━━━━━ 18:37:36\n",
      "Accuracy: 0.5848 - Loss: 1.1421\n",
      "\n",
      "Batch 958/992 ━━━━━━━━━━━━━━━━━━━━ 18:37:48\n",
      "Accuracy: 0.5851 - Loss: 1.1418\n",
      "\n",
      "Batch 959/992 ━━━━━━━━━━━━━━━━━━━━ 18:38:00\n",
      "Accuracy: 0.5855 - Loss: 1.1412\n",
      "\n",
      "Batch 960/992 ━━━━━━━━━━━━━━━━━━━━ 18:38:11\n",
      "Accuracy: 0.5859 - Loss: 1.1402\n",
      "\n",
      "Batch 961/992 ━━━━━━━━━━━━━━━━━━━━ 18:38:23\n",
      "Accuracy: 0.5860 - Loss: 1.1401\n",
      "\n",
      "Batch 962/992 ━━━━━━━━━━━━━━━━━━━━ 18:38:34\n",
      "Accuracy: 0.5863 - Loss: 1.1394\n",
      "\n",
      "Batch 963/992 ━━━━━━━━━━━━━━━━━━━━ 18:38:47\n",
      "Accuracy: 0.5863 - Loss: 1.1391\n",
      "\n",
      "Batch 964/992 ━━━━━━━━━━━━━━━━━━━━ 18:39:00\n",
      "Accuracy: 0.5867 - Loss: 1.1382\n",
      "\n",
      "Batch 965/992 ━━━━━━━━━━━━━━━━━━━━ 18:39:13\n",
      "Accuracy: 0.5869 - Loss: 1.1374\n",
      "\n",
      "Batch 966/992 ━━━━━━━━━━━━━━━━━━━━ 18:39:28\n",
      "Accuracy: 0.5867 - Loss: 1.1374\n",
      "\n",
      "Batch 967/992 ━━━━━━━━━━━━━━━━━━━━ 18:39:40\n",
      "Accuracy: 0.5869 - Loss: 1.1368\n",
      "\n",
      "Batch 968/992 ━━━━━━━━━━━━━━━━━━━━ 18:39:51\n",
      "Accuracy: 0.5866 - Loss: 1.1371\n",
      "\n",
      "Batch 969/992 ━━━━━━━━━━━━━━━━━━━━ 18:40:03\n",
      "Accuracy: 0.5869 - Loss: 1.1366\n",
      "\n",
      "Batch 970/992 ━━━━━━━━━━━━━━━━━━━━ 18:40:16\n",
      "Accuracy: 0.5871 - Loss: 1.1363\n",
      "\n",
      "Batch 971/992 ━━━━━━━━━━━━━━━━━━━━ 18:40:27\n",
      "Accuracy: 0.5872 - Loss: 1.1362\n",
      "\n",
      "Batch 972/992 ━━━━━━━━━━━━━━━━━━━━ 18:40:38\n",
      "Accuracy: 0.5873 - Loss: 1.1358\n",
      "\n",
      "Batch 973/992 ━━━━━━━━━━━━━━━━━━━━ 18:40:50\n",
      "Accuracy: 0.5875 - Loss: 1.1356\n",
      "\n",
      "Batch 974/992 ━━━━━━━━━━━━━━━━━━━━ 18:41:02\n",
      "Accuracy: 0.5875 - Loss: 1.1352\n",
      "\n",
      "Batch 975/992 ━━━━━━━━━━━━━━━━━━━━ 18:41:14\n",
      "Accuracy: 0.5879 - Loss: 1.1344\n",
      "\n",
      "Batch 976/992 ━━━━━━━━━━━━━━━━━━━━ 18:41:24\n",
      "Accuracy: 0.5884 - Loss: 1.1335\n",
      "\n",
      "Batch 977/992 ━━━━━━━━━━━━━━━━━━━━ 18:41:35\n",
      "Accuracy: 0.5885 - Loss: 1.1332\n",
      "\n",
      "Batch 978/992 ━━━━━━━━━━━━━━━━━━━━ 18:41:47\n",
      "Accuracy: 0.5888 - Loss: 1.1325\n",
      "\n",
      "Batch 979/992 ━━━━━━━━━━━━━━━━━━━━ 18:41:58\n",
      "Accuracy: 0.5891 - Loss: 1.1321\n",
      "\n",
      "Batch 980/992 ━━━━━━━━━━━━━━━━━━━━ 18:42:09\n",
      "Accuracy: 0.5892 - Loss: 1.1316\n",
      "\n",
      "Batch 981/992 ━━━━━━━━━━━━━━━━━━━━ 18:42:19\n",
      "Accuracy: 0.5894 - Loss: 1.1312\n",
      "\n",
      "Batch 982/992 ━━━━━━━━━━━━━━━━━━━━ 18:42:31\n",
      "Accuracy: 0.5897 - Loss: 1.1306\n",
      "\n",
      "Batch 983/992 ━━━━━━━━━━━━━━━━━━━━ 18:42:41\n",
      "Accuracy: 0.5900 - Loss: 1.1298\n",
      "\n",
      "Batch 984/992 ━━━━━━━━━━━━━━━━━━━━ 18:42:52\n",
      "Accuracy: 0.5902 - Loss: 1.1292\n",
      "\n",
      "Batch 985/992 ━━━━━━━━━━━━━━━━━━━━ 18:43:03\n",
      "Accuracy: 0.5902 - Loss: 1.1291\n",
      "\n",
      "Batch 986/992 ━━━━━━━━━━━━━━━━━━━━ 18:43:15\n",
      "Accuracy: 0.5906 - Loss: 1.1284\n",
      "\n",
      "Batch 987/992 ━━━━━━━━━━━━━━━━━━━━ 18:43:26\n",
      "Accuracy: 0.5909 - Loss: 1.1279\n",
      "\n",
      "Batch 988/992 ━━━━━━━━━━━━━━━━━━━━ 18:43:37\n",
      "Accuracy: 0.5910 - Loss: 1.1279\n",
      "\n",
      "Batch 989/992 ━━━━━━━━━━━━━━━━━━━━ 18:43:47\n",
      "Accuracy: 0.5910 - Loss: 1.1276\n",
      "\n",
      "Batch 990/992 ━━━━━━━━━━━━━━━━━━━━ 18:43:58\n",
      "Accuracy: 0.5913 - Loss: 1.1271\n",
      "\n",
      "Batch 991/992 ━━━━━━━━━━━━━━━━━━━━ 18:44:08\n",
      "Accuracy: 0.5913 - Loss: 1.1269\n",
      "\n",
      "Batch 992/992 ━━━━━━━━━━━━━━━━━━━━ 18:44:19\n",
      "Accuracy: 0.5912 - Loss: 1.1269\n",
      "\n",
      "\n",
      "Epoch 2/10\n",
      "Batch 1/992 ━━━━━━━━━━━━━━━━━━━━ 19:02:39\n",
      "Accuracy: 0.6250 - Loss: 0.9053\n",
      "\n",
      "Batch 2/992 ━━━━━━━━━━━━━━━━━━━━ 19:02:50\n",
      "Accuracy: 0.6250 - Loss: 1.0244\n",
      "\n",
      "Batch 3/992 ━━━━━━━━━━━━━━━━━━━━ 19:03:02\n",
      "Accuracy: 0.7500 - Loss: 0.7809\n",
      "\n",
      "Batch 4/992 ━━━━━━━━━━━━━━━━━━━━ 19:03:14\n",
      "Accuracy: 0.7812 - Loss: 0.6972\n",
      "\n",
      "Batch 5/992 ━━━━━━━━━━━━━━━━━━━━ 19:03:25\n",
      "Accuracy: 0.8000 - Loss: 0.6676\n",
      "\n",
      "Batch 6/992 ━━━━━━━━━━━━━━━━━━━━ 19:03:37\n",
      "Accuracy: 0.7708 - Loss: 0.6844\n",
      "\n",
      "Batch 7/992 ━━━━━━━━━━━━━━━━━━━━ 19:03:49\n",
      "Accuracy: 0.7321 - Loss: 0.8130\n",
      "\n",
      "Batch 8/992 ━━━━━━━━━━━━━━━━━━━━ 19:04:00\n",
      "Accuracy: 0.7656 - Loss: 0.7438\n",
      "\n",
      "Batch 9/992 ━━━━━━━━━━━━━━━━━━━━ 19:04:10\n",
      "Accuracy: 0.7778 - Loss: 0.6993\n",
      "\n",
      "Batch 10/992 ━━━━━━━━━━━━━━━━━━━━ 19:04:21\n",
      "Accuracy: 0.7875 - Loss: 0.6746\n",
      "\n",
      "Batch 11/992 ━━━━━━━━━━━━━━━━━━━━ 19:04:31\n",
      "Accuracy: 0.7841 - Loss: 0.6662\n",
      "\n",
      "Batch 12/992 ━━━━━━━━━━━━━━━━━━━━ 19:04:41\n",
      "Accuracy: 0.7604 - Loss: 0.7122\n",
      "\n",
      "Batch 13/992 ━━━━━━━━━━━━━━━━━━━━ 19:04:51\n",
      "Accuracy: 0.7596 - Loss: 0.7046\n",
      "\n",
      "Batch 14/992 ━━━━━━━━━━━━━━━━━━━━ 19:05:02\n",
      "Accuracy: 0.7679 - Loss: 0.6716\n",
      "\n",
      "Batch 15/992 ━━━━━━━━━━━━━━━━━━━━ 19:05:12\n",
      "Accuracy: 0.7500 - Loss: 0.6906\n",
      "\n",
      "Batch 16/992 ━━━━━━━━━━━━━━━━━━━━ 19:05:23\n",
      "Accuracy: 0.7422 - Loss: 0.7306\n",
      "\n",
      "Batch 17/992 ━━━━━━━━━━━━━━━━━━━━ 19:05:34\n",
      "Accuracy: 0.7426 - Loss: 0.7269\n",
      "\n",
      "Batch 18/992 ━━━━━━━━━━━━━━━━━━━━ 19:05:45\n",
      "Accuracy: 0.7431 - Loss: 0.7109\n",
      "\n",
      "Batch 19/992 ━━━━━━━━━━━━━━━━━━━━ 19:05:56\n",
      "Accuracy: 0.7434 - Loss: 0.7257\n",
      "\n",
      "Batch 20/992 ━━━━━━━━━━━━━━━━━━━━ 19:06:07\n",
      "Accuracy: 0.7437 - Loss: 0.7225\n",
      "\n",
      "Batch 21/992 ━━━━━━━━━━━━━━━━━━━━ 19:06:20\n",
      "Accuracy: 0.7440 - Loss: 0.7290\n",
      "\n",
      "Batch 22/992 ━━━━━━━━━━━━━━━━━━━━ 19:06:30\n",
      "Accuracy: 0.7330 - Loss: 0.7624\n",
      "\n",
      "Batch 23/992 ━━━━━━━━━━━━━━━━━━━━ 19:06:41\n",
      "Accuracy: 0.7391 - Loss: 0.7594\n",
      "\n",
      "Batch 24/992 ━━━━━━━━━━━━━━━━━━━━ 19:06:53\n",
      "Accuracy: 0.7396 - Loss: 0.7518\n",
      "\n",
      "Batch 25/992 ━━━━━━━━━━━━━━━━━━━━ 19:07:04\n",
      "Accuracy: 0.7400 - Loss: 0.7448\n",
      "\n",
      "Batch 26/992 ━━━━━━━━━━━━━━━━━━━━ 19:07:15\n",
      "Accuracy: 0.7500 - Loss: 0.7225\n",
      "\n",
      "Batch 27/992 ━━━━━━━━━━━━━━━━━━━━ 19:07:26\n",
      "Accuracy: 0.7500 - Loss: 0.7263\n",
      "\n",
      "Batch 28/992 ━━━━━━━━━━━━━━━━━━━━ 19:07:37\n",
      "Accuracy: 0.7589 - Loss: 0.7126\n",
      "\n",
      "Batch 29/992 ━━━━━━━━━━━━━━━━━━━━ 19:07:49\n",
      "Accuracy: 0.7672 - Loss: 0.6969\n",
      "\n",
      "Batch 30/992 ━━━━━━━━━━━━━━━━━━━━ 19:07:59\n",
      "Accuracy: 0.7625 - Loss: 0.6956\n",
      "\n",
      "Batch 31/992 ━━━━━━━━━━━━━━━━━━━━ 19:08:12\n",
      "Accuracy: 0.7702 - Loss: 0.6836\n",
      "\n",
      "Batch 32/992 ━━━━━━━━━━━━━━━━━━━━ 19:08:23\n",
      "Accuracy: 0.7734 - Loss: 0.6755\n",
      "\n",
      "Batch 33/992 ━━━━━━━━━━━━━━━━━━━━ 19:08:34\n",
      "Accuracy: 0.7689 - Loss: 0.6792\n",
      "\n",
      "Batch 34/992 ━━━━━━━━━━━━━━━━━━━━ 19:08:45\n",
      "Accuracy: 0.7684 - Loss: 0.6878\n",
      "\n",
      "Batch 35/992 ━━━━━━━━━━━━━━━━━━━━ 19:08:56\n",
      "Accuracy: 0.7714 - Loss: 0.6831\n",
      "\n",
      "Batch 36/992 ━━━━━━━━━━━━━━━━━━━━ 19:09:08\n",
      "Accuracy: 0.7639 - Loss: 0.6881\n",
      "\n",
      "Batch 37/992 ━━━━━━━━━━━━━━━━━━━━ 19:09:21\n",
      "Accuracy: 0.7703 - Loss: 0.6741\n",
      "\n",
      "Batch 38/992 ━━━━━━━━━━━━━━━━━━━━ 19:09:31\n",
      "Accuracy: 0.7697 - Loss: 0.6712\n",
      "\n",
      "Batch 39/992 ━━━━━━━━━━━━━━━━━━━━ 19:09:42\n",
      "Accuracy: 0.7660 - Loss: 0.6700\n",
      "\n",
      "Batch 40/992 ━━━━━━━━━━━━━━━━━━━━ 19:09:52\n",
      "Accuracy: 0.7688 - Loss: 0.6628\n",
      "\n",
      "Batch 41/992 ━━━━━━━━━━━━━━━━━━━━ 19:10:03\n",
      "Accuracy: 0.7683 - Loss: 0.6583\n",
      "\n",
      "Batch 42/992 ━━━━━━━━━━━━━━━━━━━━ 19:10:14\n",
      "Accuracy: 0.7708 - Loss: 0.6497\n",
      "\n",
      "Batch 43/992 ━━━━━━━━━━━━━━━━━━━━ 19:10:24\n",
      "Accuracy: 0.7762 - Loss: 0.6354\n",
      "\n",
      "Batch 44/992 ━━━━━━━━━━━━━━━━━━━━ 19:10:35\n",
      "Accuracy: 0.7727 - Loss: 0.6410\n",
      "\n",
      "Batch 45/992 ━━━━━━━━━━━━━━━━━━━━ 19:10:47\n",
      "Accuracy: 0.7694 - Loss: 0.6471\n",
      "\n",
      "Batch 46/992 ━━━━━━━━━━━━━━━━━━━━ 19:11:00\n",
      "Accuracy: 0.7717 - Loss: 0.6418\n",
      "\n",
      "Batch 47/992 ━━━━━━━━━━━━━━━━━━━━ 19:11:13\n",
      "Accuracy: 0.7766 - Loss: 0.6374\n",
      "\n",
      "Batch 48/992 ━━━━━━━━━━━━━━━━━━━━ 19:11:24\n",
      "Accuracy: 0.7786 - Loss: 0.6328\n",
      "\n",
      "Batch 49/992 ━━━━━━━━━━━━━━━━━━━━ 19:11:34\n",
      "Accuracy: 0.7755 - Loss: 0.6342\n",
      "\n",
      "Batch 50/992 ━━━━━━━━━━━━━━━━━━━━ 19:11:44\n",
      "Accuracy: 0.7775 - Loss: 0.6313\n",
      "\n",
      "Batch 51/992 ━━━━━━━━━━━━━━━━━━━━ 19:11:55\n",
      "Accuracy: 0.7721 - Loss: 0.6378\n",
      "\n",
      "Batch 52/992 ━━━━━━━━━━━━━━━━━━━━ 19:12:08\n",
      "Accuracy: 0.7692 - Loss: 0.6426\n",
      "\n",
      "Batch 53/992 ━━━━━━━━━━━━━━━━━━━━ 19:12:19\n",
      "Accuracy: 0.7689 - Loss: 0.6400\n",
      "\n",
      "Batch 54/992 ━━━━━━━━━━━━━━━━━━━━ 19:12:31\n",
      "Accuracy: 0.7685 - Loss: 0.6427\n",
      "\n",
      "Batch 55/992 ━━━━━━━━━━━━━━━━━━━━ 19:12:43\n",
      "Accuracy: 0.7636 - Loss: 0.6526\n",
      "\n",
      "Batch 56/992 ━━━━━━━━━━━━━━━━━━━━ 19:12:54\n",
      "Accuracy: 0.7589 - Loss: 0.6581\n",
      "\n",
      "Batch 57/992 ━━━━━━━━━━━━━━━━━━━━ 19:13:05\n",
      "Accuracy: 0.7544 - Loss: 0.6679\n",
      "\n",
      "Batch 58/992 ━━━━━━━━━━━━━━━━━━━━ 19:13:19\n",
      "Accuracy: 0.7543 - Loss: 0.6655\n",
      "\n",
      "Batch 59/992 ━━━━━━━━━━━━━━━━━━━━ 19:13:29\n",
      "Accuracy: 0.7564 - Loss: 0.6618\n",
      "\n",
      "Batch 60/992 ━━━━━━━━━━━━━━━━━━━━ 19:13:39\n",
      "Accuracy: 0.7563 - Loss: 0.6712\n",
      "\n",
      "Batch 61/992 ━━━━━━━━━━━━━━━━━━━━ 19:13:49\n",
      "Accuracy: 0.7561 - Loss: 0.6713\n",
      "\n",
      "Batch 62/992 ━━━━━━━━━━━━━━━━━━━━ 19:13:59\n",
      "Accuracy: 0.7560 - Loss: 0.6718\n",
      "\n",
      "Batch 63/992 ━━━━━━━━━━━━━━━━━━━━ 19:14:09\n",
      "Accuracy: 0.7579 - Loss: 0.6716\n",
      "\n",
      "Batch 64/992 ━━━━━━━━━━━━━━━━━━━━ 19:14:20\n",
      "Accuracy: 0.7539 - Loss: 0.6807\n",
      "\n",
      "Batch 65/992 ━━━━━━━━━━━━━━━━━━━━ 19:14:30\n",
      "Accuracy: 0.7519 - Loss: 0.6804\n",
      "\n",
      "Batch 66/992 ━━━━━━━━━━━━━━━━━━━━ 19:14:40\n",
      "Accuracy: 0.7538 - Loss: 0.6775\n",
      "\n",
      "Batch 67/992 ━━━━━━━━━━━━━━━━━━━━ 19:14:50\n",
      "Accuracy: 0.7537 - Loss: 0.6790\n",
      "\n",
      "Batch 68/992 ━━━━━━━━━━━━━━━━━━━━ 19:15:00\n",
      "Accuracy: 0.7537 - Loss: 0.6790\n",
      "\n",
      "Batch 69/992 ━━━━━━━━━━━━━━━━━━━━ 19:15:11\n",
      "Accuracy: 0.7536 - Loss: 0.6802\n",
      "\n",
      "Batch 70/992 ━━━━━━━━━━━━━━━━━━━━ 19:15:21\n",
      "Accuracy: 0.7518 - Loss: 0.6826\n",
      "\n",
      "Batch 71/992 ━━━━━━━━━━━━━━━━━━━━ 19:15:31\n",
      "Accuracy: 0.7535 - Loss: 0.6827\n",
      "\n",
      "Batch 72/992 ━━━━━━━━━━━━━━━━━━━━ 19:15:41\n",
      "Accuracy: 0.7535 - Loss: 0.6860\n",
      "\n",
      "Batch 73/992 ━━━━━━━━━━━━━━━━━━━━ 19:15:51\n",
      "Accuracy: 0.7551 - Loss: 0.6839\n",
      "\n",
      "Batch 74/992 ━━━━━━━━━━━━━━━━━━━━ 19:16:01\n",
      "Accuracy: 0.7534 - Loss: 0.6876\n",
      "\n",
      "Batch 75/992 ━━━━━━━━━━━━━━━━━━━━ 19:16:11\n",
      "Accuracy: 0.7533 - Loss: 0.6869\n",
      "\n",
      "Batch 76/992 ━━━━━━━━━━━━━━━━━━━━ 19:16:21\n",
      "Accuracy: 0.7549 - Loss: 0.6825\n",
      "\n",
      "Batch 77/992 ━━━━━━━━━━━━━━━━━━━━ 19:16:31\n",
      "Accuracy: 0.7532 - Loss: 0.6879\n",
      "\n",
      "Batch 78/992 ━━━━━━━━━━━━━━━━━━━━ 19:16:41\n",
      "Accuracy: 0.7532 - Loss: 0.6927\n",
      "\n",
      "Batch 79/992 ━━━━━━━━━━━━━━━━━━━━ 19:16:51\n",
      "Accuracy: 0.7484 - Loss: 0.6991\n",
      "\n",
      "Batch 80/992 ━━━━━━━━━━━━━━━━━━━━ 19:17:02\n",
      "Accuracy: 0.7484 - Loss: 0.6964\n",
      "\n",
      "Batch 81/992 ━━━━━━━━━━━━━━━━━━━━ 19:17:12\n",
      "Accuracy: 0.7515 - Loss: 0.6904\n",
      "\n",
      "Batch 82/992 ━━━━━━━━━━━━━━━━━━━━ 19:17:22\n",
      "Accuracy: 0.7500 - Loss: 0.6911\n",
      "\n",
      "Batch 83/992 ━━━━━━━━━━━━━━━━━━━━ 19:17:32\n",
      "Accuracy: 0.7500 - Loss: 0.6904\n",
      "\n",
      "Batch 84/992 ━━━━━━━━━━━━━━━━━━━━ 19:17:42\n",
      "Accuracy: 0.7515 - Loss: 0.6886\n",
      "\n",
      "Batch 85/992 ━━━━━━━━━━━━━━━━━━━━ 19:17:53\n",
      "Accuracy: 0.7485 - Loss: 0.6998\n",
      "\n",
      "Batch 86/992 ━━━━━━━━━━━━━━━━━━━━ 19:18:03\n",
      "Accuracy: 0.7471 - Loss: 0.7002\n",
      "\n",
      "Batch 87/992 ━━━━━━━━━━━━━━━━━━━━ 19:18:13\n",
      "Accuracy: 0.7471 - Loss: 0.6967\n",
      "\n",
      "Batch 88/992 ━━━━━━━━━━━━━━━━━━━━ 19:18:23\n",
      "Accuracy: 0.7486 - Loss: 0.6943\n",
      "\n",
      "Batch 89/992 ━━━━━━━━━━━━━━━━━━━━ 19:18:33\n",
      "Accuracy: 0.7458 - Loss: 0.6951\n",
      "\n",
      "Batch 90/992 ━━━━━━━━━━━━━━━━━━━━ 19:18:43\n",
      "Accuracy: 0.7444 - Loss: 0.6974\n",
      "\n",
      "Batch 91/992 ━━━━━━━━━━━━━━━━━━━━ 19:18:53\n",
      "Accuracy: 0.7445 - Loss: 0.6944\n",
      "\n",
      "Batch 92/992 ━━━━━━━━━━━━━━━━━━━━ 19:19:03\n",
      "Accuracy: 0.7446 - Loss: 0.6940\n",
      "\n",
      "Batch 93/992 ━━━━━━━━━━━━━━━━━━━━ 19:19:14\n",
      "Accuracy: 0.7460 - Loss: 0.6904\n",
      "\n",
      "Batch 94/992 ━━━━━━━━━━━━━━━━━━━━ 19:19:24\n",
      "Accuracy: 0.7473 - Loss: 0.6889\n",
      "\n",
      "Batch 95/992 ━━━━━━━━━━━━━━━━━━━━ 19:19:34\n",
      "Accuracy: 0.7461 - Loss: 0.6925\n",
      "\n",
      "Batch 96/992 ━━━━━━━━━━━━━━━━━━━━ 19:19:44\n",
      "Accuracy: 0.7448 - Loss: 0.6955\n",
      "\n",
      "Batch 97/992 ━━━━━━━━━━━━━━━━━━━━ 19:19:55\n",
      "Accuracy: 0.7448 - Loss: 0.6949\n",
      "\n",
      "Batch 98/992 ━━━━━━━━━━━━━━━━━━━━ 19:20:05\n",
      "Accuracy: 0.7462 - Loss: 0.6910\n",
      "\n",
      "Batch 99/992 ━━━━━━━━━━━━━━━━━━━━ 19:20:15\n",
      "Accuracy: 0.7462 - Loss: 0.6907\n",
      "\n",
      "Batch 100/992 ━━━━━━━━━━━━━━━━━━━━ 19:20:25\n",
      "Accuracy: 0.7437 - Loss: 0.6935\n",
      "\n",
      "Batch 101/992 ━━━━━━━━━━━━━━━━━━━━ 19:20:35\n",
      "Accuracy: 0.7438 - Loss: 0.6937\n",
      "\n",
      "Batch 102/992 ━━━━━━━━━━━━━━━━━━━━ 19:20:45\n",
      "Accuracy: 0.7439 - Loss: 0.6944\n",
      "\n",
      "Batch 103/992 ━━━━━━━━━━━━━━━━━━━━ 19:20:55\n",
      "Accuracy: 0.7439 - Loss: 0.6957\n",
      "\n",
      "Batch 104/992 ━━━━━━━━━━━━━━━━━━━━ 19:21:05\n",
      "Accuracy: 0.7452 - Loss: 0.6935\n",
      "\n",
      "Batch 105/992 ━━━━━━━━━━━━━━━━━━━━ 19:21:16\n",
      "Accuracy: 0.7440 - Loss: 0.6978\n",
      "\n",
      "Batch 106/992 ━━━━━━━━━━━━━━━━━━━━ 19:21:26\n",
      "Accuracy: 0.7441 - Loss: 0.6999\n",
      "\n",
      "Batch 107/992 ━━━━━━━━━━━━━━━━━━━━ 19:21:36\n",
      "Accuracy: 0.7442 - Loss: 0.6997\n",
      "\n",
      "Batch 108/992 ━━━━━━━━━━━━━━━━━━━━ 19:21:46\n",
      "Accuracy: 0.7454 - Loss: 0.6981\n",
      "\n",
      "Batch 109/992 ━━━━━━━━━━━━━━━━━━━━ 19:21:56\n",
      "Accuracy: 0.7477 - Loss: 0.6940\n",
      "\n",
      "Batch 110/992 ━━━━━━━━━━━━━━━━━━━━ 19:22:06\n",
      "Accuracy: 0.7466 - Loss: 0.6995\n",
      "\n",
      "Batch 111/992 ━━━━━━━━━━━━━━━━━━━━ 19:22:16\n",
      "Accuracy: 0.7489 - Loss: 0.6948\n",
      "\n",
      "Batch 112/992 ━━━━━━━━━━━━━━━━━━━━ 19:22:26\n",
      "Accuracy: 0.7467 - Loss: 0.6984\n",
      "\n",
      "Batch 113/992 ━━━━━━━━━━━━━━━━━━━━ 19:22:37\n",
      "Accuracy: 0.7467 - Loss: 0.6973\n",
      "\n",
      "Batch 114/992 ━━━━━━━━━━━━━━━━━━━━ 19:22:47\n",
      "Accuracy: 0.7456 - Loss: 0.6977\n",
      "\n",
      "Batch 115/992 ━━━━━━━━━━━━━━━━━━━━ 19:22:57\n",
      "Accuracy: 0.7457 - Loss: 0.6973\n",
      "\n",
      "Batch 116/992 ━━━━━━━━━━━━━━━━━━━━ 19:23:07\n",
      "Accuracy: 0.7446 - Loss: 0.6990\n",
      "\n",
      "Batch 117/992 ━━━━━━━━━━━━━━━━━━━━ 19:23:17\n",
      "Accuracy: 0.7436 - Loss: 0.7033\n",
      "\n",
      "Batch 118/992 ━━━━━━━━━━━━━━━━━━━━ 19:23:28\n",
      "Accuracy: 0.7436 - Loss: 0.7012\n",
      "\n",
      "Batch 119/992 ━━━━━━━━━━━━━━━━━━━━ 19:23:38\n",
      "Accuracy: 0.7458 - Loss: 0.6967\n",
      "\n",
      "Batch 120/992 ━━━━━━━━━━━━━━━━━━━━ 19:23:48\n",
      "Accuracy: 0.7458 - Loss: 0.6971\n",
      "\n",
      "Batch 121/992 ━━━━━━━━━━━━━━━━━━━━ 19:23:58\n",
      "Accuracy: 0.7479 - Loss: 0.6937\n",
      "\n",
      "Batch 122/992 ━━━━━━━━━━━━━━━━━━━━ 19:24:08\n",
      "Accuracy: 0.7490 - Loss: 0.6915\n",
      "\n",
      "Batch 123/992 ━━━━━━━━━━━━━━━━━━━━ 19:24:18\n",
      "Accuracy: 0.7490 - Loss: 0.6908\n",
      "\n",
      "Batch 124/992 ━━━━━━━━━━━━━━━━━━━━ 19:24:28\n",
      "Accuracy: 0.7480 - Loss: 0.6913\n",
      "\n",
      "Batch 125/992 ━━━━━━━━━━━━━━━━━━━━ 19:24:38\n",
      "Accuracy: 0.7490 - Loss: 0.6893\n",
      "\n",
      "Batch 126/992 ━━━━━━━━━━━━━━━━━━━━ 19:24:48\n",
      "Accuracy: 0.7490 - Loss: 0.6904\n",
      "\n",
      "Batch 127/992 ━━━━━━━━━━━━━━━━━━━━ 19:24:58\n",
      "Accuracy: 0.7510 - Loss: 0.6861\n",
      "\n",
      "Batch 128/992 ━━━━━━━━━━━━━━━━━━━━ 19:25:08\n",
      "Accuracy: 0.7510 - Loss: 0.6856\n",
      "\n",
      "Batch 129/992 ━━━━━━━━━━━━━━━━━━━━ 19:25:19\n",
      "Accuracy: 0.7510 - Loss: 0.6861\n",
      "\n",
      "Batch 130/992 ━━━━━━━━━━━━━━━━━━━━ 19:25:29\n",
      "Accuracy: 0.7519 - Loss: 0.6842\n",
      "\n",
      "Batch 131/992 ━━━━━━━━━━━━━━━━━━━━ 19:25:39\n",
      "Accuracy: 0.7519 - Loss: 0.6859\n",
      "\n",
      "Batch 132/992 ━━━━━━━━━━━━━━━━━━━━ 19:25:49\n",
      "Accuracy: 0.7538 - Loss: 0.6823\n",
      "\n",
      "Batch 133/992 ━━━━━━━━━━━━━━━━━━━━ 19:25:59\n",
      "Accuracy: 0.7538 - Loss: 0.6812\n",
      "\n",
      "Batch 134/992 ━━━━━━━━━━━━━━━━━━━━ 19:26:10\n",
      "Accuracy: 0.7537 - Loss: 0.6805\n",
      "\n",
      "Batch 135/992 ━━━━━━━━━━━━━━━━━━━━ 19:26:20\n",
      "Accuracy: 0.7546 - Loss: 0.6781\n",
      "\n",
      "Batch 136/992 ━━━━━━━━━━━━━━━━━━━━ 19:26:30\n",
      "Accuracy: 0.7546 - Loss: 0.6790\n",
      "\n",
      "Batch 137/992 ━━━━━━━━━━━━━━━━━━━━ 19:26:40\n",
      "Accuracy: 0.7546 - Loss: 0.6820\n",
      "\n",
      "Batch 138/992 ━━━━━━━━━━━━━━━━━━━━ 19:26:50\n",
      "Accuracy: 0.7545 - Loss: 0.6799\n",
      "\n",
      "Batch 139/992 ━━━━━━━━━━━━━━━━━━━━ 19:27:00\n",
      "Accuracy: 0.7545 - Loss: 0.6820\n",
      "\n",
      "Batch 140/992 ━━━━━━━━━━━━━━━━━━━━ 19:27:10\n",
      "Accuracy: 0.7545 - Loss: 0.6824\n",
      "\n",
      "Batch 141/992 ━━━━━━━━━━━━━━━━━━━━ 19:27:20\n",
      "Accuracy: 0.7553 - Loss: 0.6796\n",
      "\n",
      "Batch 142/992 ━━━━━━━━━━━━━━━━━━━━ 19:27:31\n",
      "Accuracy: 0.7570 - Loss: 0.6762\n",
      "\n",
      "Batch 143/992 ━━━━━━━━━━━━━━━━━━━━ 19:27:41\n",
      "Accuracy: 0.7570 - Loss: 0.6755\n",
      "\n",
      "Batch 144/992 ━━━━━━━━━━━━━━━━━━━━ 19:27:51\n",
      "Accuracy: 0.7578 - Loss: 0.6735\n",
      "\n",
      "Batch 145/992 ━━━━━━━━━━━━━━━━━━━━ 19:28:01\n",
      "Accuracy: 0.7569 - Loss: 0.6732\n",
      "\n",
      "Batch 146/992 ━━━━━━━━━━━━━━━━━━━━ 19:28:11\n",
      "Accuracy: 0.7568 - Loss: 0.6727\n",
      "\n",
      "Batch 147/992 ━━━━━━━━━━━━━━━━━━━━ 19:28:21\n",
      "Accuracy: 0.7560 - Loss: 0.6737\n",
      "\n",
      "Batch 148/992 ━━━━━━━━━━━━━━━━━━━━ 19:28:31\n",
      "Accuracy: 0.7559 - Loss: 0.6744\n",
      "\n",
      "Batch 149/992 ━━━━━━━━━━━━━━━━━━━━ 19:28:41\n",
      "Accuracy: 0.7559 - Loss: 0.6733\n",
      "\n",
      "Batch 150/992 ━━━━━━━━━━━━━━━━━━━━ 19:28:51\n",
      "Accuracy: 0.7558 - Loss: 0.6733\n",
      "\n",
      "Batch 151/992 ━━━━━━━━━━━━━━━━━━━━ 19:29:01\n",
      "Accuracy: 0.7566 - Loss: 0.6728\n",
      "\n",
      "Batch 152/992 ━━━━━━━━━━━━━━━━━━━━ 19:29:11\n",
      "Accuracy: 0.7574 - Loss: 0.6725\n",
      "\n",
      "Batch 153/992 ━━━━━━━━━━━━━━━━━━━━ 19:29:22\n",
      "Accuracy: 0.7557 - Loss: 0.6752\n",
      "\n",
      "Batch 154/992 ━━━━━━━━━━━━━━━━━━━━ 19:29:32\n",
      "Accuracy: 0.7573 - Loss: 0.6715\n",
      "\n",
      "Batch 155/992 ━━━━━━━━━━━━━━━━━━━━ 19:29:42\n",
      "Accuracy: 0.7581 - Loss: 0.6705\n",
      "\n",
      "Batch 156/992 ━━━━━━━━━━━━━━━━━━━━ 19:29:52\n",
      "Accuracy: 0.7572 - Loss: 0.6707\n",
      "\n",
      "Batch 157/992 ━━━━━━━━━━━━━━━━━━━━ 19:30:02\n",
      "Accuracy: 0.7580 - Loss: 0.6684\n",
      "\n",
      "Batch 158/992 ━━━━━━━━━━━━━━━━━━━━ 19:30:12\n",
      "Accuracy: 0.7579 - Loss: 0.6702\n",
      "\n",
      "Batch 159/992 ━━━━━━━━━━━━━━━━━━━━ 19:30:22\n",
      "Accuracy: 0.7594 - Loss: 0.6677\n",
      "\n",
      "Batch 160/992 ━━━━━━━━━━━━━━━━━━━━ 19:30:32\n",
      "Accuracy: 0.7594 - Loss: 0.6686\n",
      "\n",
      "Batch 161/992 ━━━━━━━━━━━━━━━━━━━━ 19:30:42\n",
      "Accuracy: 0.7601 - Loss: 0.6678\n",
      "\n",
      "Batch 162/992 ━━━━━━━━━━━━━━━━━━━━ 19:30:53\n",
      "Accuracy: 0.7593 - Loss: 0.6687\n",
      "\n",
      "Batch 163/992 ━━━━━━━━━━━━━━━━━━━━ 19:31:03\n",
      "Accuracy: 0.7592 - Loss: 0.6690\n",
      "\n",
      "Batch 164/992 ━━━━━━━━━━━━━━━━━━━━ 19:31:13\n",
      "Accuracy: 0.7599 - Loss: 0.6682\n",
      "\n",
      "Batch 165/992 ━━━━━━━━━━━━━━━━━━━━ 19:31:23\n",
      "Accuracy: 0.7606 - Loss: 0.6667\n",
      "\n",
      "Batch 166/992 ━━━━━━━━━━━━━━━━━━━━ 19:31:33\n",
      "Accuracy: 0.7613 - Loss: 0.6653\n",
      "\n",
      "Batch 167/992 ━━━━━━━━━━━━━━━━━━━━ 19:31:43\n",
      "Accuracy: 0.7620 - Loss: 0.6645\n",
      "\n",
      "Batch 168/992 ━━━━━━━━━━━━━━━━━━━━ 19:31:53\n",
      "Accuracy: 0.7612 - Loss: 0.6650\n",
      "\n",
      "Batch 169/992 ━━━━━━━━━━━━━━━━━━━━ 19:32:04\n",
      "Accuracy: 0.7618 - Loss: 0.6644\n",
      "\n",
      "Batch 170/992 ━━━━━━━━━━━━━━━━━━━━ 19:32:14\n",
      "Accuracy: 0.7632 - Loss: 0.6618\n",
      "\n",
      "Batch 171/992 ━━━━━━━━━━━━━━━━━━━━ 19:32:24\n",
      "Accuracy: 0.7624 - Loss: 0.6630\n",
      "\n",
      "Batch 172/992 ━━━━━━━━━━━━━━━━━━━━ 19:32:34\n",
      "Accuracy: 0.7638 - Loss: 0.6610\n",
      "\n",
      "Batch 173/992 ━━━━━━━━━━━━━━━━━━━━ 19:32:44\n",
      "Accuracy: 0.7645 - Loss: 0.6599\n",
      "\n",
      "Batch 174/992 ━━━━━━━━━━━━━━━━━━━━ 19:32:54\n",
      "Accuracy: 0.7644 - Loss: 0.6618\n",
      "\n",
      "Batch 175/992 ━━━━━━━━━━━━━━━━━━━━ 19:33:04\n",
      "Accuracy: 0.7643 - Loss: 0.6624\n",
      "\n",
      "Batch 176/992 ━━━━━━━━━━━━━━━━━━━━ 19:33:14\n",
      "Accuracy: 0.7642 - Loss: 0.6629\n",
      "\n",
      "Batch 177/992 ━━━━━━━━━━━━━━━━━━━━ 19:33:24\n",
      "Accuracy: 0.7641 - Loss: 0.6611\n",
      "\n",
      "Batch 178/992 ━━━━━━━━━━━━━━━━━━━━ 19:33:35\n",
      "Accuracy: 0.7647 - Loss: 0.6597\n",
      "\n",
      "Batch 179/992 ━━━━━━━━━━━━━━━━━━━━ 19:33:45\n",
      "Accuracy: 0.7633 - Loss: 0.6627\n",
      "\n",
      "Batch 180/992 ━━━━━━━━━━━━━━━━━━━━ 19:33:55\n",
      "Accuracy: 0.7639 - Loss: 0.6615\n",
      "\n",
      "Batch 181/992 ━━━━━━━━━━━━━━━━━━━━ 19:34:05\n",
      "Accuracy: 0.7645 - Loss: 0.6598\n",
      "\n",
      "Batch 182/992 ━━━━━━━━━━━━━━━━━━━━ 19:34:16\n",
      "Accuracy: 0.7644 - Loss: 0.6609\n",
      "\n",
      "Batch 183/992 ━━━━━━━━━━━━━━━━━━━━ 19:34:26\n",
      "Accuracy: 0.7650 - Loss: 0.6594\n",
      "\n",
      "Batch 184/992 ━━━━━━━━━━━━━━━━━━━━ 19:34:36\n",
      "Accuracy: 0.7656 - Loss: 0.6578\n",
      "\n",
      "Batch 185/992 ━━━━━━━━━━━━━━━━━━━━ 19:34:46\n",
      "Accuracy: 0.7662 - Loss: 0.6572\n",
      "\n",
      "Batch 186/992 ━━━━━━━━━━━━━━━━━━━━ 19:34:56\n",
      "Accuracy: 0.7661 - Loss: 0.6559\n",
      "\n",
      "Batch 187/992 ━━━━━━━━━━━━━━━━━━━━ 19:35:06\n",
      "Accuracy: 0.7667 - Loss: 0.6551\n",
      "\n",
      "Batch 188/992 ━━━━━━━━━━━━━━━━━━━━ 19:35:16\n",
      "Accuracy: 0.7680 - Loss: 0.6526\n",
      "\n",
      "Batch 189/992 ━━━━━━━━━━━━━━━━━━━━ 19:35:27\n",
      "Accuracy: 0.7672 - Loss: 0.6539\n",
      "\n",
      "Batch 190/992 ━━━━━━━━━━━━━━━━━━━━ 19:35:37\n",
      "Accuracy: 0.7664 - Loss: 0.6539\n",
      "\n",
      "Batch 191/992 ━━━━━━━━━━━━━━━━━━━━ 19:35:47\n",
      "Accuracy: 0.7670 - Loss: 0.6523\n",
      "\n",
      "Batch 192/992 ━━━━━━━━━━━━━━━━━━━━ 19:35:57\n",
      "Accuracy: 0.7669 - Loss: 0.6522\n",
      "\n",
      "Batch 193/992 ━━━━━━━━━━━━━━━━━━━━ 19:36:07\n",
      "Accuracy: 0.7668 - Loss: 0.6519\n",
      "\n",
      "Batch 194/992 ━━━━━━━━━━━━━━━━━━━━ 19:36:17\n",
      "Accuracy: 0.7668 - Loss: 0.6516\n",
      "\n",
      "Batch 195/992 ━━━━━━━━━━━━━━━━━━━━ 19:36:27\n",
      "Accuracy: 0.7673 - Loss: 0.6504\n",
      "\n",
      "Batch 196/992 ━━━━━━━━━━━━━━━━━━━━ 19:36:37\n",
      "Accuracy: 0.7672 - Loss: 0.6500\n",
      "\n",
      "Batch 197/992 ━━━━━━━━━━━━━━━━━━━━ 19:36:48\n",
      "Accuracy: 0.7678 - Loss: 0.6480\n",
      "\n",
      "Batch 198/992 ━━━━━━━━━━━━━━━━━━━━ 19:36:58\n",
      "Accuracy: 0.7677 - Loss: 0.6500\n",
      "\n",
      "Batch 199/992 ━━━━━━━━━━━━━━━━━━━━ 19:37:08\n",
      "Accuracy: 0.7676 - Loss: 0.6487\n",
      "\n",
      "Batch 200/992 ━━━━━━━━━━━━━━━━━━━━ 19:37:18\n",
      "Accuracy: 0.7681 - Loss: 0.6478\n",
      "\n",
      "Batch 201/992 ━━━━━━━━━━━━━━━━━━━━ 19:37:28\n",
      "Accuracy: 0.7674 - Loss: 0.6492\n",
      "\n",
      "Batch 202/992 ━━━━━━━━━━━━━━━━━━━━ 19:37:39\n",
      "Accuracy: 0.7673 - Loss: 0.6493\n",
      "\n",
      "Batch 203/992 ━━━━━━━━━━━━━━━━━━━━ 19:37:49\n",
      "Accuracy: 0.7672 - Loss: 0.6488\n",
      "\n",
      "Batch 204/992 ━━━━━━━━━━━━━━━━━━━━ 19:37:59\n",
      "Accuracy: 0.7684 - Loss: 0.6464\n",
      "\n",
      "Batch 205/992 ━━━━━━━━━━━━━━━━━━━━ 19:38:09\n",
      "Accuracy: 0.7683 - Loss: 0.6469\n",
      "\n",
      "Batch 206/992 ━━━━━━━━━━━━━━━━━━━━ 19:38:19\n",
      "Accuracy: 0.7682 - Loss: 0.6463\n",
      "\n",
      "Batch 207/992 ━━━━━━━━━━━━━━━━━━━━ 19:38:29\n",
      "Accuracy: 0.7681 - Loss: 0.6458\n",
      "\n",
      "Batch 208/992 ━━━━━━━━━━━━━━━━━━━━ 19:38:39\n",
      "Accuracy: 0.7674 - Loss: 0.6454\n",
      "\n",
      "Batch 209/992 ━━━━━━━━━━━━━━━━━━━━ 19:38:49\n",
      "Accuracy: 0.7661 - Loss: 0.6483\n",
      "\n",
      "Batch 210/992 ━━━━━━━━━━━━━━━━━━━━ 19:39:00\n",
      "Accuracy: 0.7661 - Loss: 0.6485\n",
      "\n",
      "Batch 211/992 ━━━━━━━━━━━━━━━━━━━━ 19:39:10\n",
      "Accuracy: 0.7672 - Loss: 0.6459\n",
      "\n",
      "Batch 212/992 ━━━━━━━━━━━━━━━━━━━━ 19:39:20\n",
      "Accuracy: 0.7677 - Loss: 0.6443\n",
      "\n",
      "Batch 213/992 ━━━━━━━━━━━━━━━━━━━━ 19:39:30\n",
      "Accuracy: 0.7676 - Loss: 0.6436\n",
      "\n",
      "Batch 214/992 ━━━━━━━━━━━━━━━━━━━━ 19:39:41\n",
      "Accuracy: 0.7681 - Loss: 0.6422\n",
      "\n",
      "Batch 215/992 ━━━━━━━━━━━━━━━━━━━━ 19:39:51\n",
      "Accuracy: 0.7692 - Loss: 0.6409\n",
      "\n",
      "Batch 216/992 ━━━━━━━━━━━━━━━━━━━━ 19:40:01\n",
      "Accuracy: 0.7691 - Loss: 0.6438\n",
      "\n",
      "Batch 217/992 ━━━━━━━━━━━━━━━━━━━━ 19:40:11\n",
      "Accuracy: 0.7702 - Loss: 0.6419\n",
      "\n",
      "Batch 218/992 ━━━━━━━━━━━━━━━━━━━━ 19:40:21\n",
      "Accuracy: 0.7701 - Loss: 0.6425\n",
      "\n",
      "Batch 219/992 ━━━━━━━━━━━━━━━━━━━━ 19:40:31\n",
      "Accuracy: 0.7700 - Loss: 0.6427\n",
      "\n",
      "Batch 220/992 ━━━━━━━━━━━━━━━━━━━━ 19:40:41\n",
      "Accuracy: 0.7710 - Loss: 0.6404\n",
      "\n",
      "Batch 221/992 ━━━━━━━━━━━━━━━━━━━━ 19:40:51\n",
      "Accuracy: 0.7709 - Loss: 0.6396\n",
      "\n",
      "Batch 222/992 ━━━━━━━━━━━━━━━━━━━━ 19:41:01\n",
      "Accuracy: 0.7720 - Loss: 0.6383\n",
      "\n",
      "Batch 223/992 ━━━━━━━━━━━━━━━━━━━━ 19:41:11\n",
      "Accuracy: 0.7719 - Loss: 0.6396\n",
      "\n",
      "Batch 224/992 ━━━━━━━━━━━━━━━━━━━━ 19:41:21\n",
      "Accuracy: 0.7729 - Loss: 0.6380\n",
      "\n",
      "Batch 225/992 ━━━━━━━━━━━━━━━━━━━━ 19:41:32\n",
      "Accuracy: 0.7733 - Loss: 0.6382\n",
      "\n",
      "Batch 226/992 ━━━━━━━━━━━━━━━━━━━━ 19:41:42\n",
      "Accuracy: 0.7738 - Loss: 0.6376\n",
      "\n",
      "Batch 227/992 ━━━━━━━━━━━━━━━━━━━━ 19:41:52\n",
      "Accuracy: 0.7748 - Loss: 0.6361\n",
      "\n",
      "Batch 228/992 ━━━━━━━━━━━━━━━━━━━━ 19:42:02\n",
      "Accuracy: 0.7758 - Loss: 0.6339\n",
      "\n",
      "Batch 229/992 ━━━━━━━━━━━━━━━━━━━━ 19:42:12\n",
      "Accuracy: 0.7767 - Loss: 0.6321\n",
      "\n",
      "Batch 230/992 ━━━━━━━━━━━━━━━━━━━━ 19:42:22\n",
      "Accuracy: 0.7766 - Loss: 0.6340\n",
      "\n",
      "Batch 231/992 ━━━━━━━━━━━━━━━━━━━━ 19:42:33\n",
      "Accuracy: 0.7771 - Loss: 0.6326\n",
      "\n",
      "Batch 232/992 ━━━━━━━━━━━━━━━━━━━━ 19:42:43\n",
      "Accuracy: 0.7775 - Loss: 0.6314\n",
      "\n",
      "Batch 233/992 ━━━━━━━━━━━━━━━━━━━━ 19:42:53\n",
      "Accuracy: 0.7784 - Loss: 0.6291\n",
      "\n",
      "Batch 234/992 ━━━━━━━━━━━━━━━━━━━━ 19:43:03\n",
      "Accuracy: 0.7772 - Loss: 0.6321\n",
      "\n",
      "Batch 235/992 ━━━━━━━━━━━━━━━━━━━━ 19:43:13\n",
      "Accuracy: 0.7777 - Loss: 0.6315\n",
      "\n",
      "Batch 236/992 ━━━━━━━━━━━━━━━━━━━━ 19:43:23\n",
      "Accuracy: 0.7786 - Loss: 0.6300\n",
      "\n",
      "Batch 237/992 ━━━━━━━━━━━━━━━━━━━━ 19:43:33\n",
      "Accuracy: 0.7795 - Loss: 0.6277\n",
      "\n",
      "Batch 238/992 ━━━━━━━━━━━━━━━━━━━━ 19:43:44\n",
      "Accuracy: 0.7789 - Loss: 0.6280\n",
      "\n",
      "Batch 239/992 ━━━━━━━━━━━━━━━━━━━━ 19:43:54\n",
      "Accuracy: 0.7798 - Loss: 0.6258\n",
      "\n",
      "Batch 240/992 ━━━━━━━━━━━━━━━━━━━━ 19:44:04\n",
      "Accuracy: 0.7792 - Loss: 0.6265\n",
      "\n",
      "Batch 241/992 ━━━━━━━━━━━━━━━━━━━━ 19:44:14\n",
      "Accuracy: 0.7801 - Loss: 0.6248\n",
      "\n",
      "Batch 242/992 ━━━━━━━━━━━━━━━━━━━━ 19:44:24\n",
      "Accuracy: 0.7805 - Loss: 0.6244\n",
      "\n",
      "Batch 243/992 ━━━━━━━━━━━━━━━━━━━━ 19:44:34\n",
      "Accuracy: 0.7809 - Loss: 0.6231\n",
      "\n",
      "Batch 244/992 ━━━━━━━━━━━━━━━━━━━━ 19:44:44\n",
      "Accuracy: 0.7812 - Loss: 0.6217\n",
      "\n",
      "Batch 245/992 ━━━━━━━━━━━━━━━━━━━━ 19:44:54\n",
      "Accuracy: 0.7811 - Loss: 0.6215\n",
      "\n",
      "Batch 246/992 ━━━━━━━━━━━━━━━━━━━━ 19:45:04\n",
      "Accuracy: 0.7815 - Loss: 0.6199\n",
      "\n",
      "Batch 247/992 ━━━━━━━━━━━━━━━━━━━━ 19:45:14\n",
      "Accuracy: 0.7819 - Loss: 0.6187\n",
      "\n",
      "Batch 248/992 ━━━━━━━━━━━━━━━━━━━━ 19:45:25\n",
      "Accuracy: 0.7828 - Loss: 0.6172\n",
      "\n",
      "Batch 249/992 ━━━━━━━━━━━━━━━━━━━━ 19:45:35\n",
      "Accuracy: 0.7826 - Loss: 0.6162\n",
      "\n",
      "Batch 250/992 ━━━━━━━━━━━━━━━━━━━━ 19:45:45\n",
      "Accuracy: 0.7825 - Loss: 0.6149\n",
      "\n",
      "Batch 251/992 ━━━━━━━━━━━━━━━━━━━━ 19:45:55\n",
      "Accuracy: 0.7824 - Loss: 0.6155\n",
      "\n",
      "Batch 252/992 ━━━━━━━━━━━━━━━━━━━━ 19:46:06\n",
      "Accuracy: 0.7822 - Loss: 0.6145\n",
      "\n",
      "Batch 253/992 ━━━━━━━━━━━━━━━━━━━━ 19:46:16\n",
      "Accuracy: 0.7826 - Loss: 0.6146\n",
      "\n",
      "Batch 254/992 ━━━━━━━━━━━━━━━━━━━━ 19:46:26\n",
      "Accuracy: 0.7830 - Loss: 0.6138\n",
      "\n",
      "Batch 255/992 ━━━━━━━━━━━━━━━━━━━━ 19:46:36\n",
      "Accuracy: 0.7833 - Loss: 0.6129\n",
      "\n",
      "Batch 256/992 ━━━━━━━━━━━━━━━━━━━━ 19:46:46\n",
      "Accuracy: 0.7837 - Loss: 0.6117\n",
      "\n",
      "Batch 257/992 ━━━━━━━━━━━━━━━━━━━━ 19:46:56\n",
      "Accuracy: 0.7821 - Loss: 0.6166\n",
      "\n",
      "Batch 258/992 ━━━━━━━━━━━━━━━━━━━━ 19:47:06\n",
      "Accuracy: 0.7820 - Loss: 0.6173\n",
      "\n",
      "Batch 259/992 ━━━━━━━━━━━━━━━━━━━━ 19:47:16\n",
      "Accuracy: 0.7823 - Loss: 0.6171\n",
      "\n",
      "Batch 260/992 ━━━━━━━━━━━━━━━━━━━━ 19:47:26\n",
      "Accuracy: 0.7832 - Loss: 0.6157\n",
      "\n",
      "Batch 261/992 ━━━━━━━━━━━━━━━━━━━━ 19:47:37\n",
      "Accuracy: 0.7830 - Loss: 0.6154\n",
      "\n",
      "Batch 262/992 ━━━━━━━━━━━━━━━━━━━━ 19:47:50\n",
      "Accuracy: 0.7829 - Loss: 0.6152\n",
      "\n",
      "Batch 263/992 ━━━━━━━━━━━━━━━━━━━━ 19:48:00\n",
      "Accuracy: 0.7828 - Loss: 0.6150\n",
      "\n",
      "Batch 264/992 ━━━━━━━━━━━━━━━━━━━━ 19:48:11\n",
      "Accuracy: 0.7831 - Loss: 0.6142\n",
      "\n",
      "Batch 265/992 ━━━━━━━━━━━━━━━━━━━━ 19:48:22\n",
      "Accuracy: 0.7840 - Loss: 0.6126\n",
      "\n",
      "Batch 266/992 ━━━━━━━━━━━━━━━━━━━━ 19:48:36\n",
      "Accuracy: 0.7838 - Loss: 0.6140\n",
      "\n",
      "Batch 267/992 ━━━━━━━━━━━━━━━━━━━━ 19:48:48\n",
      "Accuracy: 0.7842 - Loss: 0.6130\n",
      "\n",
      "Batch 268/992 ━━━━━━━━━━━━━━━━━━━━ 19:48:58\n",
      "Accuracy: 0.7845 - Loss: 0.6124\n",
      "\n",
      "Batch 269/992 ━━━━━━━━━━━━━━━━━━━━ 19:49:13\n",
      "Accuracy: 0.7839 - Loss: 0.6132\n",
      "\n",
      "Batch 270/992 ━━━━━━━━━━━━━━━━━━━━ 19:49:26\n",
      "Accuracy: 0.7847 - Loss: 0.6116\n",
      "\n",
      "Batch 271/992 ━━━━━━━━━━━━━━━━━━━━ 19:49:38\n",
      "Accuracy: 0.7841 - Loss: 0.6130\n",
      "\n",
      "Batch 272/992 ━━━━━━━━━━━━━━━━━━━━ 19:49:50\n",
      "Accuracy: 0.7840 - Loss: 0.6132\n",
      "\n",
      "Batch 273/992 ━━━━━━━━━━━━━━━━━━━━ 19:50:02\n",
      "Accuracy: 0.7843 - Loss: 0.6129\n",
      "\n",
      "Batch 274/992 ━━━━━━━━━━━━━━━━━━━━ 19:50:16\n",
      "Accuracy: 0.7847 - Loss: 0.6119\n",
      "\n",
      "Batch 275/992 ━━━━━━━━━━━━━━━━━━━━ 19:50:30\n",
      "Accuracy: 0.7845 - Loss: 0.6113\n",
      "\n",
      "Batch 276/992 ━━━━━━━━━━━━━━━━━━━━ 19:50:42\n",
      "Accuracy: 0.7849 - Loss: 0.6115\n",
      "\n",
      "Batch 277/992 ━━━━━━━━━━━━━━━━━━━━ 19:50:52\n",
      "Accuracy: 0.7847 - Loss: 0.6126\n",
      "\n",
      "Batch 278/992 ━━━━━━━━━━━━━━━━━━━━ 19:51:03\n",
      "Accuracy: 0.7851 - Loss: 0.6116\n",
      "\n",
      "Batch 279/992 ━━━━━━━━━━━━━━━━━━━━ 19:51:13\n",
      "Accuracy: 0.7854 - Loss: 0.6106\n",
      "\n",
      "Batch 280/992 ━━━━━━━━━━━━━━━━━━━━ 19:51:26\n",
      "Accuracy: 0.7857 - Loss: 0.6101\n",
      "\n",
      "Batch 281/992 ━━━━━━━━━━━━━━━━━━━━ 19:51:37\n",
      "Accuracy: 0.7865 - Loss: 0.6085\n",
      "\n",
      "Batch 282/992 ━━━━━━━━━━━━━━━━━━━━ 19:51:49\n",
      "Accuracy: 0.7868 - Loss: 0.6084\n",
      "\n",
      "Batch 283/992 ━━━━━━━━━━━━━━━━━━━━ 19:52:01\n",
      "Accuracy: 0.7867 - Loss: 0.6090\n",
      "\n",
      "Batch 284/992 ━━━━━━━━━━━━━━━━━━━━ 19:52:13\n",
      "Accuracy: 0.7874 - Loss: 0.6072\n",
      "\n",
      "Batch 285/992 ━━━━━━━━━━━━━━━━━━━━ 19:52:26\n",
      "Accuracy: 0.7877 - Loss: 0.6062\n",
      "\n",
      "Batch 286/992 ━━━━━━━━━━━━━━━━━━━━ 19:52:37\n",
      "Accuracy: 0.7885 - Loss: 0.6049\n",
      "\n",
      "Batch 287/992 ━━━━━━━━━━━━━━━━━━━━ 19:52:48\n",
      "Accuracy: 0.7883 - Loss: 0.6049\n",
      "\n",
      "Batch 288/992 ━━━━━━━━━━━━━━━━━━━━ 19:53:00\n",
      "Accuracy: 0.7886 - Loss: 0.6053\n",
      "\n",
      "Batch 289/992 ━━━━━━━━━━━━━━━━━━━━ 19:53:10\n",
      "Accuracy: 0.7885 - Loss: 0.6051\n",
      "\n",
      "Batch 290/992 ━━━━━━━━━━━━━━━━━━━━ 19:53:20\n",
      "Accuracy: 0.7888 - Loss: 0.6039\n",
      "\n",
      "Batch 291/992 ━━━━━━━━━━━━━━━━━━━━ 19:53:32\n",
      "Accuracy: 0.7891 - Loss: 0.6037\n",
      "\n",
      "Batch 292/992 ━━━━━━━━━━━━━━━━━━━━ 19:53:44\n",
      "Accuracy: 0.7890 - Loss: 0.6030\n",
      "\n",
      "Batch 293/992 ━━━━━━━━━━━━━━━━━━━━ 19:53:54\n",
      "Accuracy: 0.7897 - Loss: 0.6013\n",
      "\n",
      "Batch 294/992 ━━━━━━━━━━━━━━━━━━━━ 19:54:05\n",
      "Accuracy: 0.7900 - Loss: 0.6009\n",
      "\n",
      "Batch 295/992 ━━━━━━━━━━━━━━━━━━━━ 19:54:17\n",
      "Accuracy: 0.7903 - Loss: 0.6007\n",
      "\n",
      "Batch 296/992 ━━━━━━━━━━━━━━━━━━━━ 19:54:29\n",
      "Accuracy: 0.7905 - Loss: 0.6006\n",
      "\n",
      "Batch 297/992 ━━━━━━━━━━━━━━━━━━━━ 19:54:40\n",
      "Accuracy: 0.7908 - Loss: 0.5996\n",
      "\n",
      "Batch 298/992 ━━━━━━━━━━━━━━━━━━━━ 19:54:52\n",
      "Accuracy: 0.7907 - Loss: 0.5989\n",
      "\n",
      "Batch 299/992 ━━━━━━━━━━━━━━━━━━━━ 19:55:04\n",
      "Accuracy: 0.7901 - Loss: 0.5994\n",
      "\n",
      "Batch 300/992 ━━━━━━━━━━━━━━━━━━━━ 19:55:17\n",
      "Accuracy: 0.7900 - Loss: 0.6011\n",
      "\n",
      "Batch 301/992 ━━━━━━━━━━━━━━━━━━━━ 19:55:31\n",
      "Accuracy: 0.7886 - Loss: 0.6020\n",
      "\n",
      "Batch 302/992 ━━━━━━━━━━━━━━━━━━━━ 19:55:43\n",
      "Accuracy: 0.7877 - Loss: 0.6045\n",
      "\n",
      "Batch 303/992 ━━━━━━━━━━━━━━━━━━━━ 19:55:54\n",
      "Accuracy: 0.7880 - Loss: 0.6043\n",
      "\n",
      "Batch 304/992 ━━━━━━━━━━━━━━━━━━━━ 19:56:05\n",
      "Accuracy: 0.7870 - Loss: 0.6070\n",
      "\n",
      "Batch 305/992 ━━━━━━━━━━━━━━━━━━━━ 19:56:16\n",
      "Accuracy: 0.7869 - Loss: 0.6074\n",
      "\n",
      "Batch 306/992 ━━━━━━━━━━━━━━━━━━━━ 19:56:30\n",
      "Accuracy: 0.7872 - Loss: 0.6067\n",
      "\n",
      "Batch 307/992 ━━━━━━━━━━━━━━━━━━━━ 19:56:41\n",
      "Accuracy: 0.7866 - Loss: 0.6076\n",
      "\n",
      "Batch 308/992 ━━━━━━━━━━━━━━━━━━━━ 19:56:52\n",
      "Accuracy: 0.7865 - Loss: 0.6076\n",
      "\n",
      "Batch 309/992 ━━━━━━━━━━━━━━━━━━━━ 19:57:05\n",
      "Accuracy: 0.7864 - Loss: 0.6075\n",
      "\n",
      "Batch 310/992 ━━━━━━━━━━━━━━━━━━━━ 19:57:17\n",
      "Accuracy: 0.7867 - Loss: 0.6078\n",
      "\n",
      "Batch 311/992 ━━━━━━━━━━━━━━━━━━━━ 19:57:29\n",
      "Accuracy: 0.7866 - Loss: 0.6085\n",
      "\n",
      "Batch 312/992 ━━━━━━━━━━━━━━━━━━━━ 19:57:41\n",
      "Accuracy: 0.7857 - Loss: 0.6095\n",
      "\n",
      "Batch 313/992 ━━━━━━━━━━━━━━━━━━━━ 19:57:54\n",
      "Accuracy: 0.7855 - Loss: 0.6095\n",
      "\n",
      "Batch 314/992 ━━━━━━━━━━━━━━━━━━━━ 19:58:06\n",
      "Accuracy: 0.7846 - Loss: 0.6108\n",
      "\n",
      "Batch 315/992 ━━━━━━━━━━━━━━━━━━━━ 19:58:17\n",
      "Accuracy: 0.7837 - Loss: 0.6129\n",
      "\n",
      "Batch 316/992 ━━━━━━━━━━━━━━━━━━━━ 19:58:29\n",
      "Accuracy: 0.7832 - Loss: 0.6160\n",
      "\n",
      "Batch 317/992 ━━━━━━━━━━━━━━━━━━━━ 19:58:40\n",
      "Accuracy: 0.7835 - Loss: 0.6152\n",
      "\n",
      "Batch 318/992 ━━━━━━━━━━━━━━━━━━━━ 19:58:50\n",
      "Accuracy: 0.7834 - Loss: 0.6149\n",
      "\n",
      "Batch 319/992 ━━━━━━━━━━━━━━━━━━━━ 19:59:01\n",
      "Accuracy: 0.7837 - Loss: 0.6142\n",
      "\n",
      "Batch 320/992 ━━━━━━━━━━━━━━━━━━━━ 19:59:12\n",
      "Accuracy: 0.7840 - Loss: 0.6133\n",
      "\n",
      "Batch 321/992 ━━━━━━━━━━━━━━━━━━━━ 19:59:22\n",
      "Accuracy: 0.7843 - Loss: 0.6128\n",
      "\n",
      "Batch 322/992 ━━━━━━━━━━━━━━━━━━━━ 19:59:33\n",
      "Accuracy: 0.7845 - Loss: 0.6130\n",
      "\n",
      "Batch 323/992 ━━━━━━━━━━━━━━━━━━━━ 19:59:44\n",
      "Accuracy: 0.7829 - Loss: 0.6167\n",
      "\n",
      "Batch 324/992 ━━━━━━━━━━━━━━━━━━━━ 19:59:55\n",
      "Accuracy: 0.7832 - Loss: 0.6161\n",
      "\n",
      "Batch 325/992 ━━━━━━━━━━━━━━━━━━━━ 20:00:06\n",
      "Accuracy: 0.7835 - Loss: 0.6153\n",
      "\n",
      "Batch 326/992 ━━━━━━━━━━━━━━━━━━━━ 20:00:16\n",
      "Accuracy: 0.7837 - Loss: 0.6148\n",
      "\n",
      "Batch 327/992 ━━━━━━━━━━━━━━━━━━━━ 20:00:26\n",
      "Accuracy: 0.7844 - Loss: 0.6137\n",
      "\n",
      "Batch 328/992 ━━━━━━━━━━━━━━━━━━━━ 20:00:36\n",
      "Accuracy: 0.7832 - Loss: 0.6155\n",
      "\n",
      "Batch 329/992 ━━━━━━━━━━━━━━━━━━━━ 20:00:47\n",
      "Accuracy: 0.7834 - Loss: 0.6150\n",
      "\n",
      "Batch 330/992 ━━━━━━━━━━━━━━━━━━━━ 20:00:58\n",
      "Accuracy: 0.7841 - Loss: 0.6139\n",
      "\n",
      "Batch 331/992 ━━━━━━━━━━━━━━━━━━━━ 20:01:09\n",
      "Accuracy: 0.7836 - Loss: 0.6150\n",
      "\n",
      "Batch 332/992 ━━━━━━━━━━━━━━━━━━━━ 20:01:20\n",
      "Accuracy: 0.7835 - Loss: 0.6146\n",
      "\n",
      "Batch 333/992 ━━━━━━━━━━━━━━━━━━━━ 20:01:33\n",
      "Accuracy: 0.7830 - Loss: 0.6161\n",
      "\n",
      "Batch 334/992 ━━━━━━━━━━━━━━━━━━━━ 20:01:45\n",
      "Accuracy: 0.7826 - Loss: 0.6165\n",
      "\n",
      "Batch 335/992 ━━━━━━━━━━━━━━━━━━━━ 20:01:56\n",
      "Accuracy: 0.7817 - Loss: 0.6186\n",
      "\n",
      "Batch 336/992 ━━━━━━━━━━━━━━━━━━━━ 20:02:06\n",
      "Accuracy: 0.7816 - Loss: 0.6184\n",
      "\n",
      "Batch 337/992 ━━━━━━━━━━━━━━━━━━━━ 20:02:17\n",
      "Accuracy: 0.7812 - Loss: 0.6199\n",
      "\n",
      "Batch 338/992 ━━━━━━━━━━━━━━━━━━━━ 20:02:27\n",
      "Accuracy: 0.7818 - Loss: 0.6186\n",
      "\n",
      "Batch 339/992 ━━━━━━━━━━━━━━━━━━━━ 20:02:38\n",
      "Accuracy: 0.7817 - Loss: 0.6182\n",
      "\n",
      "Batch 340/992 ━━━━━━━━━━━━━━━━━━━━ 20:02:48\n",
      "Accuracy: 0.7824 - Loss: 0.6169\n",
      "\n",
      "Batch 341/992 ━━━━━━━━━━━━━━━━━━━━ 20:02:59\n",
      "Accuracy: 0.7830 - Loss: 0.6154\n",
      "\n",
      "Batch 342/992 ━━━━━━━━━━━━━━━━━━━━ 20:03:10\n",
      "Accuracy: 0.7833 - Loss: 0.6149\n",
      "\n",
      "Batch 343/992 ━━━━━━━━━━━━━━━━━━━━ 20:03:21\n",
      "Accuracy: 0.7828 - Loss: 0.6158\n",
      "\n",
      "Batch 344/992 ━━━━━━━━━━━━━━━━━━━━ 20:03:32\n",
      "Accuracy: 0.7827 - Loss: 0.6158\n",
      "\n",
      "Batch 345/992 ━━━━━━━━━━━━━━━━━━━━ 20:03:42\n",
      "Accuracy: 0.7826 - Loss: 0.6170\n",
      "\n",
      "Batch 346/992 ━━━━━━━━━━━━━━━━━━━━ 20:03:53\n",
      "Accuracy: 0.7818 - Loss: 0.6180\n",
      "\n",
      "Batch 347/992 ━━━━━━━━━━━━━━━━━━━━ 20:04:03\n",
      "Accuracy: 0.7821 - Loss: 0.6199\n",
      "\n",
      "Batch 348/992 ━━━━━━━━━━━━━━━━━━━━ 20:04:13\n",
      "Accuracy: 0.7823 - Loss: 0.6199\n",
      "\n",
      "Batch 349/992 ━━━━━━━━━━━━━━━━━━━━ 20:04:24\n",
      "Accuracy: 0.7830 - Loss: 0.6189\n",
      "\n",
      "Batch 350/992 ━━━━━━━━━━━━━━━━━━━━ 20:04:34\n",
      "Accuracy: 0.7829 - Loss: 0.6198\n",
      "\n",
      "Batch 351/992 ━━━━━━━━━━━━━━━━━━━━ 20:04:44\n",
      "Accuracy: 0.7831 - Loss: 0.6187\n",
      "\n",
      "Batch 352/992 ━━━━━━━━━━━━━━━━━━━━ 20:04:55\n",
      "Accuracy: 0.7834 - Loss: 0.6182\n",
      "\n",
      "Batch 353/992 ━━━━━━━━━━━━━━━━━━━━ 20:05:05\n",
      "Accuracy: 0.7829 - Loss: 0.6187\n",
      "\n",
      "Batch 354/992 ━━━━━━━━━━━━━━━━━━━━ 20:05:16\n",
      "Accuracy: 0.7832 - Loss: 0.6177\n",
      "\n",
      "Batch 355/992 ━━━━━━━━━━━━━━━━━━━━ 20:05:26\n",
      "Accuracy: 0.7827 - Loss: 0.6178\n",
      "\n",
      "Batch 356/992 ━━━━━━━━━━━━━━━━━━━━ 20:05:37\n",
      "Accuracy: 0.7830 - Loss: 0.6178\n",
      "\n",
      "Batch 357/992 ━━━━━━━━━━━━━━━━━━━━ 20:05:48\n",
      "Accuracy: 0.7829 - Loss: 0.6174\n",
      "\n",
      "Batch 358/992 ━━━━━━━━━━━━━━━━━━━━ 20:05:58\n",
      "Accuracy: 0.7832 - Loss: 0.6162\n",
      "\n",
      "Batch 359/992 ━━━━━━━━━━━━━━━━━━━━ 20:06:08\n",
      "Accuracy: 0.7831 - Loss: 0.6168\n",
      "\n",
      "Batch 360/992 ━━━━━━━━━━━━━━━━━━━━ 20:06:19\n",
      "Accuracy: 0.7826 - Loss: 0.6178\n",
      "\n",
      "Batch 361/992 ━━━━━━━━━━━━━━━━━━━━ 20:06:30\n",
      "Accuracy: 0.7822 - Loss: 0.6194\n",
      "\n",
      "Batch 362/992 ━━━━━━━━━━━━━━━━━━━━ 20:06:41\n",
      "Accuracy: 0.7821 - Loss: 0.6195\n",
      "\n",
      "Batch 363/992 ━━━━━━━━━━━━━━━━━━━━ 20:06:51\n",
      "Accuracy: 0.7820 - Loss: 0.6187\n",
      "\n",
      "Batch 364/992 ━━━━━━━━━━━━━━━━━━━━ 20:07:02\n",
      "Accuracy: 0.7819 - Loss: 0.6191\n",
      "\n",
      "Batch 365/992 ━━━━━━━━━━━━━━━━━━━━ 20:07:13\n",
      "Accuracy: 0.7822 - Loss: 0.6184\n",
      "\n",
      "Batch 366/992 ━━━━━━━━━━━━━━━━━━━━ 20:07:24\n",
      "Accuracy: 0.7824 - Loss: 0.6183\n",
      "\n",
      "Batch 367/992 ━━━━━━━━━━━━━━━━━━━━ 20:07:35\n",
      "Accuracy: 0.7824 - Loss: 0.6178\n",
      "\n",
      "Batch 368/992 ━━━━━━━━━━━━━━━━━━━━ 20:07:45\n",
      "Accuracy: 0.7823 - Loss: 0.6178\n",
      "\n",
      "Batch 369/992 ━━━━━━━━━━━━━━━━━━━━ 20:07:57\n",
      "Accuracy: 0.7822 - Loss: 0.6176\n",
      "\n",
      "Batch 370/992 ━━━━━━━━━━━━━━━━━━━━ 20:08:08\n",
      "Accuracy: 0.7824 - Loss: 0.6169\n",
      "\n",
      "Batch 371/992 ━━━━━━━━━━━━━━━━━━━━ 20:08:18\n",
      "Accuracy: 0.7817 - Loss: 0.6172\n",
      "\n",
      "Batch 372/992 ━━━━━━━━━━━━━━━━━━━━ 20:08:29\n",
      "Accuracy: 0.7819 - Loss: 0.6174\n",
      "\n",
      "Batch 373/992 ━━━━━━━━━━━━━━━━━━━━ 20:08:39\n",
      "Accuracy: 0.7818 - Loss: 0.6173\n",
      "\n",
      "Batch 374/992 ━━━━━━━━━━━━━━━━━━━━ 20:08:50\n",
      "Accuracy: 0.7814 - Loss: 0.6189\n",
      "\n",
      "Batch 375/992 ━━━━━━━━━━━━━━━━━━━━ 20:09:00\n",
      "Accuracy: 0.7817 - Loss: 0.6184\n",
      "\n",
      "Batch 376/992 ━━━━━━━━━━━━━━━━━━━━ 20:09:11\n",
      "Accuracy: 0.7816 - Loss: 0.6181\n",
      "\n",
      "Batch 377/992 ━━━━━━━━━━━━━━━━━━━━ 20:09:22\n",
      "Accuracy: 0.7818 - Loss: 0.6177\n",
      "\n",
      "Batch 378/992 ━━━━━━━━━━━━━━━━━━━━ 20:09:34\n",
      "Accuracy: 0.7821 - Loss: 0.6171\n",
      "\n",
      "Batch 379/992 ━━━━━━━━━━━━━━━━━━━━ 20:09:45\n",
      "Accuracy: 0.7820 - Loss: 0.6175\n",
      "\n",
      "Batch 380/992 ━━━━━━━━━━━━━━━━━━━━ 20:09:55\n",
      "Accuracy: 0.7826 - Loss: 0.6167\n",
      "\n",
      "Batch 381/992 ━━━━━━━━━━━━━━━━━━━━ 20:10:06\n",
      "Accuracy: 0.7825 - Loss: 0.6178\n",
      "\n",
      "Batch 382/992 ━━━━━━━━━━━━━━━━━━━━ 20:10:17\n",
      "Accuracy: 0.7827 - Loss: 0.6175\n",
      "\n",
      "Batch 383/992 ━━━━━━━━━━━━━━━━━━━━ 20:10:29\n",
      "Accuracy: 0.7830 - Loss: 0.6169\n",
      "\n",
      "Batch 384/992 ━━━━━━━━━━━━━━━━━━━━ 20:10:41\n",
      "Accuracy: 0.7829 - Loss: 0.6170\n",
      "\n",
      "Batch 385/992 ━━━━━━━━━━━━━━━━━━━━ 20:10:52\n",
      "Accuracy: 0.7818 - Loss: 0.6181\n",
      "\n",
      "Batch 386/992 ━━━━━━━━━━━━━━━━━━━━ 20:11:03\n",
      "Accuracy: 0.7817 - Loss: 0.6182\n",
      "\n",
      "Batch 387/992 ━━━━━━━━━━━━━━━━━━━━ 20:11:14\n",
      "Accuracy: 0.7820 - Loss: 0.6176\n",
      "\n",
      "Batch 388/992 ━━━━━━━━━━━━━━━━━━━━ 20:11:24\n",
      "Accuracy: 0.7822 - Loss: 0.6170\n",
      "\n",
      "Batch 389/992 ━━━━━━━━━━━━━━━━━━━━ 20:11:36\n",
      "Accuracy: 0.7821 - Loss: 0.6161\n",
      "\n",
      "Batch 390/992 ━━━━━━━━━━━━━━━━━━━━ 20:11:47\n",
      "Accuracy: 0.7824 - Loss: 0.6150\n",
      "\n",
      "Batch 391/992 ━━━━━━━━━━━━━━━━━━━━ 20:11:57\n",
      "Accuracy: 0.7823 - Loss: 0.6153\n",
      "\n",
      "Batch 392/992 ━━━━━━━━━━━━━━━━━━━━ 20:12:08\n",
      "Accuracy: 0.7822 - Loss: 0.6147\n",
      "\n",
      "Batch 393/992 ━━━━━━━━━━━━━━━━━━━━ 20:12:22\n",
      "Accuracy: 0.7824 - Loss: 0.6144\n",
      "\n",
      "Batch 394/992 ━━━━━━━━━━━━━━━━━━━━ 20:12:36\n",
      "Accuracy: 0.7827 - Loss: 0.6142\n",
      "\n",
      "Batch 395/992 ━━━━━━━━━━━━━━━━━━━━ 20:12:51\n",
      "Accuracy: 0.7829 - Loss: 0.6139\n",
      "\n",
      "Batch 396/992 ━━━━━━━━━━━━━━━━━━━━ 20:13:05\n",
      "Accuracy: 0.7828 - Loss: 0.6148\n",
      "\n",
      "Batch 397/992 ━━━━━━━━━━━━━━━━━━━━ 20:13:19\n",
      "Accuracy: 0.7824 - Loss: 0.6148\n",
      "\n",
      "Batch 398/992 ━━━━━━━━━━━━━━━━━━━━ 20:13:34\n",
      "Accuracy: 0.7823 - Loss: 0.6145\n",
      "\n",
      "Batch 399/992 ━━━━━━━━━━━━━━━━━━━━ 20:13:51\n",
      "Accuracy: 0.7826 - Loss: 0.6141\n",
      "\n",
      "Batch 400/992 ━━━━━━━━━━━━━━━━━━━━ 20:14:02\n",
      "Accuracy: 0.7822 - Loss: 0.6145\n",
      "\n",
      "Batch 401/992 ━━━━━━━━━━━━━━━━━━━━ 20:14:14\n",
      "Accuracy: 0.7827 - Loss: 0.6138\n",
      "\n",
      "Batch 402/992 ━━━━━━━━━━━━━━━━━━━━ 20:14:25\n",
      "Accuracy: 0.7833 - Loss: 0.6126\n",
      "\n",
      "Batch 403/992 ━━━━━━━━━━━━━━━━━━━━ 20:14:36\n",
      "Accuracy: 0.7838 - Loss: 0.6116\n",
      "\n",
      "Batch 404/992 ━━━━━━━━━━━━━━━━━━━━ 20:14:47\n",
      "Accuracy: 0.7837 - Loss: 0.6118\n",
      "\n",
      "Batch 405/992 ━━━━━━━━━━━━━━━━━━━━ 20:14:58\n",
      "Accuracy: 0.7840 - Loss: 0.6114\n",
      "\n",
      "Batch 406/992 ━━━━━━━━━━━━━━━━━━━━ 20:15:09\n",
      "Accuracy: 0.7836 - Loss: 0.6112\n",
      "\n",
      "Batch 407/992 ━━━━━━━━━━━━━━━━━━━━ 20:15:20\n",
      "Accuracy: 0.7835 - Loss: 0.6107\n",
      "\n",
      "Batch 408/992 ━━━━━━━━━━━━━━━━━━━━ 20:15:31\n",
      "Accuracy: 0.7837 - Loss: 0.6100\n",
      "\n",
      "Batch 409/992 ━━━━━━━━━━━━━━━━━━━━ 20:15:42\n",
      "Accuracy: 0.7836 - Loss: 0.6101\n",
      "\n",
      "Batch 410/992 ━━━━━━━━━━━━━━━━━━━━ 20:15:53\n",
      "Accuracy: 0.7838 - Loss: 0.6099\n",
      "\n",
      "Batch 411/992 ━━━━━━━━━━━━━━━━━━━━ 20:16:04\n",
      "Accuracy: 0.7841 - Loss: 0.6094\n",
      "\n",
      "Batch 412/992 ━━━━━━━━━━━━━━━━━━━━ 20:16:16\n",
      "Accuracy: 0.7843 - Loss: 0.6086\n",
      "\n",
      "Batch 413/992 ━━━━━━━━━━━━━━━━━━━━ 20:16:26\n",
      "Accuracy: 0.7842 - Loss: 0.6094\n",
      "\n",
      "Batch 414/992 ━━━━━━━━━━━━━━━━━━━━ 20:16:38\n",
      "Accuracy: 0.7841 - Loss: 0.6099\n",
      "\n",
      "Batch 415/992 ━━━━━━━━━━━━━━━━━━━━ 20:16:49\n",
      "Accuracy: 0.7843 - Loss: 0.6091\n",
      "\n",
      "Batch 416/992 ━━━━━━━━━━━━━━━━━━━━ 20:17:03\n",
      "Accuracy: 0.7840 - Loss: 0.6095\n",
      "\n",
      "Batch 417/992 ━━━━━━━━━━━━━━━━━━━━ 20:17:18\n",
      "Accuracy: 0.7839 - Loss: 0.6097\n",
      "\n",
      "Batch 418/992 ━━━━━━━━━━━━━━━━━━━━ 20:17:29\n",
      "Accuracy: 0.7841 - Loss: 0.6094\n",
      "\n",
      "Batch 419/992 ━━━━━━━━━━━━━━━━━━━━ 20:17:40\n",
      "Accuracy: 0.7843 - Loss: 0.6093\n",
      "\n",
      "Batch 420/992 ━━━━━━━━━━━━━━━━━━━━ 20:17:52\n",
      "Accuracy: 0.7842 - Loss: 0.6099\n",
      "\n",
      "Batch 421/992 ━━━━━━━━━━━━━━━━━━━━ 20:18:04\n",
      "Accuracy: 0.7844 - Loss: 0.6098\n",
      "\n",
      "Batch 422/992 ━━━━━━━━━━━━━━━━━━━━ 20:18:15\n",
      "Accuracy: 0.7844 - Loss: 0.6097\n",
      "\n",
      "Batch 423/992 ━━━━━━━━━━━━━━━━━━━━ 20:18:26\n",
      "Accuracy: 0.7840 - Loss: 0.6113\n",
      "\n",
      "Batch 424/992 ━━━━━━━━━━━━━━━━━━━━ 20:18:37\n",
      "Accuracy: 0.7839 - Loss: 0.6109\n",
      "\n",
      "Batch 425/992 ━━━━━━━━━━━━━━━━━━━━ 20:18:48\n",
      "Accuracy: 0.7835 - Loss: 0.6111\n",
      "\n",
      "Batch 426/992 ━━━━━━━━━━━━━━━━━━━━ 20:19:00\n",
      "Accuracy: 0.7832 - Loss: 0.6122\n",
      "\n",
      "Batch 427/992 ━━━━━━━━━━━━━━━━━━━━ 20:19:10\n",
      "Accuracy: 0.7831 - Loss: 0.6121\n",
      "\n",
      "Batch 428/992 ━━━━━━━━━━━━━━━━━━━━ 20:19:21\n",
      "Accuracy: 0.7833 - Loss: 0.6115\n",
      "\n",
      "Batch 429/992 ━━━━━━━━━━━━━━━━━━━━ 20:19:32\n",
      "Accuracy: 0.7835 - Loss: 0.6108\n",
      "\n",
      "Batch 430/992 ━━━━━━━━━━━━━━━━━━━━ 20:19:42\n",
      "Accuracy: 0.7834 - Loss: 0.6111\n",
      "\n",
      "Batch 431/992 ━━━━━━━━━━━━━━━━━━━━ 20:19:53\n",
      "Accuracy: 0.7836 - Loss: 0.6107\n",
      "\n",
      "Batch 432/992 ━━━━━━━━━━━━━━━━━━━━ 20:20:04\n",
      "Accuracy: 0.7841 - Loss: 0.6097\n",
      "\n",
      "Batch 433/992 ━━━━━━━━━━━━━━━━━━━━ 20:20:16\n",
      "Accuracy: 0.7846 - Loss: 0.6092\n",
      "\n",
      "Batch 434/992 ━━━━━━━━━━━━━━━━━━━━ 20:20:28\n",
      "Accuracy: 0.7849 - Loss: 0.6087\n",
      "\n",
      "Batch 435/992 ━━━━━━━━━━━━━━━━━━━━ 20:20:40\n",
      "Accuracy: 0.7848 - Loss: 0.6095\n",
      "\n",
      "Batch 436/992 ━━━━━━━━━━━━━━━━━━━━ 20:20:51\n",
      "Accuracy: 0.7850 - Loss: 0.6097\n",
      "\n",
      "Batch 437/992 ━━━━━━━━━━━━━━━━━━━━ 20:21:01\n",
      "Accuracy: 0.7852 - Loss: 0.6095\n",
      "\n",
      "Batch 438/992 ━━━━━━━━━━━━━━━━━━━━ 20:21:12\n",
      "Accuracy: 0.7845 - Loss: 0.6105\n",
      "\n",
      "Batch 439/992 ━━━━━━━━━━━━━━━━━━━━ 20:21:22\n",
      "Accuracy: 0.7850 - Loss: 0.6097\n",
      "\n",
      "Batch 440/992 ━━━━━━━━━━━━━━━━━━━━ 20:21:34\n",
      "Accuracy: 0.7849 - Loss: 0.6096\n",
      "\n",
      "Batch 441/992 ━━━━━━━━━━━━━━━━━━━━ 20:21:45\n",
      "Accuracy: 0.7851 - Loss: 0.6093\n",
      "\n",
      "Batch 442/992 ━━━━━━━━━━━━━━━━━━━━ 20:21:56\n",
      "Accuracy: 0.7854 - Loss: 0.6087\n",
      "\n",
      "Batch 443/992 ━━━━━━━━━━━━━━━━━━━━ 20:22:06\n",
      "Accuracy: 0.7850 - Loss: 0.6092\n",
      "\n",
      "Batch 444/992 ━━━━━━━━━━━━━━━━━━━━ 20:22:16\n",
      "Accuracy: 0.7852 - Loss: 0.6094\n",
      "\n",
      "Batch 445/992 ━━━━━━━━━━━━━━━━━━━━ 20:22:27\n",
      "Accuracy: 0.7854 - Loss: 0.6098\n",
      "\n",
      "Batch 446/992 ━━━━━━━━━━━━━━━━━━━━ 20:22:37\n",
      "Accuracy: 0.7853 - Loss: 0.6095\n",
      "\n",
      "Batch 447/992 ━━━━━━━━━━━━━━━━━━━━ 20:22:48\n",
      "Accuracy: 0.7850 - Loss: 0.6106\n",
      "\n",
      "Batch 448/992 ━━━━━━━━━━━━━━━━━━━━ 20:22:58\n",
      "Accuracy: 0.7846 - Loss: 0.6110\n",
      "\n",
      "Batch 449/992 ━━━━━━━━━━━━━━━━━━━━ 20:23:09\n",
      "Accuracy: 0.7845 - Loss: 0.6115\n",
      "\n",
      "Batch 450/992 ━━━━━━━━━━━━━━━━━━━━ 20:23:20\n",
      "Accuracy: 0.7844 - Loss: 0.6122\n",
      "\n",
      "Batch 451/992 ━━━━━━━━━━━━━━━━━━━━ 20:23:30\n",
      "Accuracy: 0.7838 - Loss: 0.6131\n",
      "\n",
      "Batch 452/992 ━━━━━━━━━━━━━━━━━━━━ 20:23:40\n",
      "Accuracy: 0.7840 - Loss: 0.6135\n",
      "\n",
      "Batch 453/992 ━━━━━━━━━━━━━━━━━━━━ 20:23:51\n",
      "Accuracy: 0.7845 - Loss: 0.6126\n",
      "\n",
      "Batch 454/992 ━━━━━━━━━━━━━━━━━━━━ 20:24:03\n",
      "Accuracy: 0.7844 - Loss: 0.6125\n",
      "\n",
      "Batch 455/992 ━━━━━━━━━━━━━━━━━━━━ 20:24:15\n",
      "Accuracy: 0.7846 - Loss: 0.6122\n",
      "\n",
      "Batch 456/992 ━━━━━━━━━━━━━━━━━━━━ 20:24:25\n",
      "Accuracy: 0.7851 - Loss: 0.6117\n",
      "\n",
      "Batch 457/992 ━━━━━━━━━━━━━━━━━━━━ 20:24:36\n",
      "Accuracy: 0.7847 - Loss: 0.6124\n",
      "\n",
      "Batch 458/992 ━━━━━━━━━━━━━━━━━━━━ 20:24:46\n",
      "Accuracy: 0.7847 - Loss: 0.6119\n",
      "\n",
      "Batch 459/992 ━━━━━━━━━━━━━━━━━━━━ 20:24:57\n",
      "Accuracy: 0.7843 - Loss: 0.6132\n",
      "\n",
      "Batch 460/992 ━━━━━━━━━━━━━━━━━━━━ 20:25:07\n",
      "Accuracy: 0.7845 - Loss: 0.6128\n",
      "\n",
      "Batch 461/992 ━━━━━━━━━━━━━━━━━━━━ 20:25:18\n",
      "Accuracy: 0.7844 - Loss: 0.6134\n",
      "\n",
      "Batch 462/992 ━━━━━━━━━━━━━━━━━━━━ 20:25:28\n",
      "Accuracy: 0.7846 - Loss: 0.6130\n",
      "\n",
      "Batch 463/992 ━━━━━━━━━━━━━━━━━━━━ 20:25:38\n",
      "Accuracy: 0.7848 - Loss: 0.6124\n",
      "\n",
      "Batch 464/992 ━━━━━━━━━━━━━━━━━━━━ 20:25:49\n",
      "Accuracy: 0.7850 - Loss: 0.6121\n",
      "\n",
      "Batch 465/992 ━━━━━━━━━━━━━━━━━━━━ 20:25:59\n",
      "Accuracy: 0.7844 - Loss: 0.6147\n",
      "\n",
      "Batch 466/992 ━━━━━━━━━━━━━━━━━━━━ 20:26:10\n",
      "Accuracy: 0.7843 - Loss: 0.6155\n",
      "\n",
      "Batch 467/992 ━━━━━━━━━━━━━━━━━━━━ 20:26:21\n",
      "Accuracy: 0.7848 - Loss: 0.6146\n",
      "\n",
      "Batch 468/992 ━━━━━━━━━━━━━━━━━━━━ 20:26:32\n",
      "Accuracy: 0.7850 - Loss: 0.6139\n",
      "\n",
      "Batch 469/992 ━━━━━━━━━━━━━━━━━━━━ 20:26:43\n",
      "Accuracy: 0.7846 - Loss: 0.6141\n",
      "\n",
      "Batch 470/992 ━━━━━━━━━━━━━━━━━━━━ 20:26:53\n",
      "Accuracy: 0.7848 - Loss: 0.6137\n",
      "\n",
      "Batch 471/992 ━━━━━━━━━━━━━━━━━━━━ 20:27:04\n",
      "Accuracy: 0.7848 - Loss: 0.6137\n",
      "\n",
      "Batch 472/992 ━━━━━━━━━━━━━━━━━━━━ 20:27:14\n",
      "Accuracy: 0.7847 - Loss: 0.6134\n",
      "\n",
      "Batch 473/992 ━━━━━━━━━━━━━━━━━━━━ 20:27:25\n",
      "Accuracy: 0.7846 - Loss: 0.6133\n",
      "\n",
      "Batch 474/992 ━━━━━━━━━━━━━━━━━━━━ 20:27:36\n",
      "Accuracy: 0.7848 - Loss: 0.6127\n",
      "\n",
      "Batch 475/992 ━━━━━━━━━━━━━━━━━━━━ 20:27:46\n",
      "Accuracy: 0.7853 - Loss: 0.6119\n",
      "\n",
      "Batch 476/992 ━━━━━━━━━━━━━━━━━━━━ 20:27:57\n",
      "Accuracy: 0.7855 - Loss: 0.6117\n",
      "\n",
      "Batch 477/992 ━━━━━━━━━━━━━━━━━━━━ 20:28:08\n",
      "Accuracy: 0.7859 - Loss: 0.6106\n",
      "\n",
      "Batch 478/992 ━━━━━━━━━━━━━━━━━━━━ 20:28:18\n",
      "Accuracy: 0.7856 - Loss: 0.6115\n",
      "\n",
      "Batch 479/992 ━━━━━━━━━━━━━━━━━━━━ 20:28:29\n",
      "Accuracy: 0.7858 - Loss: 0.6115\n",
      "\n",
      "Batch 480/992 ━━━━━━━━━━━━━━━━━━━━ 20:28:39\n",
      "Accuracy: 0.7854 - Loss: 0.6125\n",
      "\n",
      "Batch 481/992 ━━━━━━━━━━━━━━━━━━━━ 20:28:50\n",
      "Accuracy: 0.7859 - Loss: 0.6117\n",
      "\n",
      "Batch 482/992 ━━━━━━━━━━━━━━━━━━━━ 20:29:01\n",
      "Accuracy: 0.7860 - Loss: 0.6114\n",
      "\n",
      "Batch 483/992 ━━━━━━━━━━━━━━━━━━━━ 20:29:11\n",
      "Accuracy: 0.7857 - Loss: 0.6121\n",
      "\n",
      "Batch 484/992 ━━━━━━━━━━━━━━━━━━━━ 20:29:22\n",
      "Accuracy: 0.7862 - Loss: 0.6116\n",
      "\n",
      "Batch 485/992 ━━━━━━━━━━━━━━━━━━━━ 20:29:33\n",
      "Accuracy: 0.7866 - Loss: 0.6109\n",
      "\n",
      "Batch 486/992 ━━━━━━━━━━━━━━━━━━━━ 20:29:44\n",
      "Accuracy: 0.7865 - Loss: 0.6110\n",
      "\n",
      "Batch 487/992 ━━━━━━━━━━━━━━━━━━━━ 20:29:54\n",
      "Accuracy: 0.7864 - Loss: 0.6108\n",
      "\n",
      "Batch 488/992 ━━━━━━━━━━━━━━━━━━━━ 20:30:05\n",
      "Accuracy: 0.7869 - Loss: 0.6097\n",
      "\n",
      "Batch 489/992 ━━━━━━━━━━━━━━━━━━━━ 20:30:16\n",
      "Accuracy: 0.7868 - Loss: 0.6096\n",
      "\n",
      "Batch 490/992 ━━━━━━━━━━━━━━━━━━━━ 20:30:26\n",
      "Accuracy: 0.7867 - Loss: 0.6096\n",
      "\n",
      "Batch 491/992 ━━━━━━━━━━━━━━━━━━━━ 20:30:36\n",
      "Accuracy: 0.7864 - Loss: 0.6099\n",
      "\n",
      "Batch 492/992 ━━━━━━━━━━━━━━━━━━━━ 20:30:47\n",
      "Accuracy: 0.7866 - Loss: 0.6094\n",
      "\n",
      "Batch 493/992 ━━━━━━━━━━━━━━━━━━━━ 20:30:57\n",
      "Accuracy: 0.7868 - Loss: 0.6089\n",
      "\n",
      "Batch 494/992 ━━━━━━━━━━━━━━━━━━━━ 20:31:08\n",
      "Accuracy: 0.7872 - Loss: 0.6077\n",
      "\n",
      "Batch 495/992 ━━━━━━━━━━━━━━━━━━━━ 20:31:19\n",
      "Accuracy: 0.7876 - Loss: 0.6070\n",
      "\n",
      "Batch 496/992 ━━━━━━━━━━━━━━━━━━━━ 20:31:29\n",
      "Accuracy: 0.7876 - Loss: 0.6067\n",
      "\n",
      "Batch 497/992 ━━━━━━━━━━━━━━━━━━━━ 20:31:40\n",
      "Accuracy: 0.7875 - Loss: 0.6065\n",
      "\n",
      "Batch 498/992 ━━━━━━━━━━━━━━━━━━━━ 20:31:50\n",
      "Accuracy: 0.7877 - Loss: 0.6063\n",
      "\n",
      "Batch 499/992 ━━━━━━━━━━━━━━━━━━━━ 20:32:01\n",
      "Accuracy: 0.7873 - Loss: 0.6068\n",
      "\n",
      "Batch 500/992 ━━━━━━━━━━━━━━━━━━━━ 20:32:11\n",
      "Accuracy: 0.7875 - Loss: 0.6063\n",
      "\n",
      "Batch 501/992 ━━━━━━━━━━━━━━━━━━━━ 20:32:22\n",
      "Accuracy: 0.7874 - Loss: 0.6062\n",
      "\n",
      "Batch 502/992 ━━━━━━━━━━━━━━━━━━━━ 20:32:32\n",
      "Accuracy: 0.7878 - Loss: 0.6055\n",
      "\n",
      "Batch 503/992 ━━━━━━━━━━━━━━━━━━━━ 20:32:43\n",
      "Accuracy: 0.7880 - Loss: 0.6049\n",
      "\n",
      "Batch 504/992 ━━━━━━━━━━━━━━━━━━━━ 20:32:54\n",
      "Accuracy: 0.7882 - Loss: 0.6043\n",
      "\n",
      "Batch 505/992 ━━━━━━━━━━━━━━━━━━━━ 20:33:04\n",
      "Accuracy: 0.7884 - Loss: 0.6041\n",
      "\n",
      "Batch 506/992 ━━━━━━━━━━━━━━━━━━━━ 20:33:15\n",
      "Accuracy: 0.7888 - Loss: 0.6031\n",
      "\n",
      "Batch 507/992 ━━━━━━━━━━━━━━━━━━━━ 20:33:26\n",
      "Accuracy: 0.7887 - Loss: 0.6029\n",
      "\n",
      "Batch 508/992 ━━━━━━━━━━━━━━━━━━━━ 20:33:37\n",
      "Accuracy: 0.7886 - Loss: 0.6027\n",
      "\n",
      "Batch 509/992 ━━━━━━━━━━━━━━━━━━━━ 20:33:47\n",
      "Accuracy: 0.7883 - Loss: 0.6028\n",
      "\n",
      "Batch 510/992 ━━━━━━━━━━━━━━━━━━━━ 20:33:58\n",
      "Accuracy: 0.7885 - Loss: 0.6021\n",
      "\n",
      "Batch 511/992 ━━━━━━━━━━━━━━━━━━━━ 20:34:08\n",
      "Accuracy: 0.7882 - Loss: 0.6022\n",
      "\n",
      "Batch 512/992 ━━━━━━━━━━━━━━━━━━━━ 20:34:19\n",
      "Accuracy: 0.7881 - Loss: 0.6028\n",
      "\n",
      "Batch 513/992 ━━━━━━━━━━━━━━━━━━━━ 20:34:30\n",
      "Accuracy: 0.7883 - Loss: 0.6020\n",
      "\n",
      "Batch 514/992 ━━━━━━━━━━━━━━━━━━━━ 20:34:40\n",
      "Accuracy: 0.7884 - Loss: 0.6013\n",
      "\n",
      "Batch 515/992 ━━━━━━━━━━━━━━━━━━━━ 20:34:51\n",
      "Accuracy: 0.7883 - Loss: 0.6007\n",
      "\n",
      "Batch 516/992 ━━━━━━━━━━━━━━━━━━━━ 20:35:02\n",
      "Accuracy: 0.7885 - Loss: 0.6000\n",
      "\n",
      "Batch 517/992 ━━━━━━━━━━━━━━━━━━━━ 20:35:13\n",
      "Accuracy: 0.7887 - Loss: 0.5996\n",
      "\n",
      "Batch 518/992 ━━━━━━━━━━━━━━━━━━━━ 20:35:23\n",
      "Accuracy: 0.7886 - Loss: 0.5994\n",
      "\n",
      "Batch 519/992 ━━━━━━━━━━━━━━━━━━━━ 20:35:33\n",
      "Accuracy: 0.7885 - Loss: 0.5991\n",
      "\n",
      "Batch 520/992 ━━━━━━━━━━━━━━━━━━━━ 20:35:44\n",
      "Accuracy: 0.7887 - Loss: 0.5984\n",
      "\n",
      "Batch 521/992 ━━━━━━━━━━━━━━━━━━━━ 20:35:55\n",
      "Accuracy: 0.7886 - Loss: 0.5986\n",
      "\n",
      "Batch 522/992 ━━━━━━━━━━━━━━━━━━━━ 20:36:06\n",
      "Accuracy: 0.7886 - Loss: 0.5986\n",
      "\n",
      "Batch 523/992 ━━━━━━━━━━━━━━━━━━━━ 20:36:17\n",
      "Accuracy: 0.7890 - Loss: 0.5979\n",
      "\n",
      "Batch 524/992 ━━━━━━━━━━━━━━━━━━━━ 20:36:27\n",
      "Accuracy: 0.7889 - Loss: 0.5982\n",
      "\n",
      "Batch 525/992 ━━━━━━━━━━━━━━━━━━━━ 20:36:38\n",
      "Accuracy: 0.7886 - Loss: 0.5986\n",
      "\n",
      "Batch 526/992 ━━━━━━━━━━━━━━━━━━━━ 20:36:48\n",
      "Accuracy: 0.7887 - Loss: 0.5985\n",
      "\n",
      "Batch 527/992 ━━━━━━━━━━━━━━━━━━━━ 20:36:58\n",
      "Accuracy: 0.7891 - Loss: 0.5977\n",
      "\n",
      "Batch 528/992 ━━━━━━━━━━━━━━━━━━━━ 20:37:09\n",
      "Accuracy: 0.7895 - Loss: 0.5969\n",
      "\n",
      "Batch 529/992 ━━━━━━━━━━━━━━━━━━━━ 20:37:19\n",
      "Accuracy: 0.7895 - Loss: 0.5975\n",
      "\n",
      "Batch 530/992 ━━━━━━━━━━━━━━━━━━━━ 20:37:29\n",
      "Accuracy: 0.7896 - Loss: 0.5968\n",
      "\n",
      "Batch 531/992 ━━━━━━━━━━━━━━━━━━━━ 20:37:39\n",
      "Accuracy: 0.7895 - Loss: 0.5965\n",
      "\n",
      "Batch 532/992 ━━━━━━━━━━━━━━━━━━━━ 20:37:49\n",
      "Accuracy: 0.7895 - Loss: 0.5964\n",
      "\n",
      "Batch 533/992 ━━━━━━━━━━━━━━━━━━━━ 20:37:59\n",
      "Accuracy: 0.7899 - Loss: 0.5954\n",
      "\n",
      "Batch 534/992 ━━━━━━━━━━━━━━━━━━━━ 20:38:10\n",
      "Accuracy: 0.7893 - Loss: 0.5976\n",
      "\n",
      "Batch 535/992 ━━━━━━━━━━━━━━━━━━━━ 20:38:20\n",
      "Accuracy: 0.7893 - Loss: 0.5978\n",
      "\n",
      "Batch 536/992 ━━━━━━━━━━━━━━━━━━━━ 20:38:31\n",
      "Accuracy: 0.7892 - Loss: 0.5976\n",
      "\n",
      "Batch 537/992 ━━━━━━━━━━━━━━━━━━━━ 20:38:41\n",
      "Accuracy: 0.7893 - Loss: 0.5972\n",
      "\n",
      "Batch 538/992 ━━━━━━━━━━━━━━━━━━━━ 20:38:52\n",
      "Accuracy: 0.7890 - Loss: 0.5978\n",
      "\n",
      "Batch 539/992 ━━━━━━━━━━━━━━━━━━━━ 20:39:02\n",
      "Accuracy: 0.7892 - Loss: 0.5972\n",
      "\n",
      "Batch 540/992 ━━━━━━━━━━━━━━━━━━━━ 20:39:12\n",
      "Accuracy: 0.7889 - Loss: 0.5982\n",
      "\n",
      "Batch 541/992 ━━━━━━━━━━━━━━━━━━━━ 20:39:23\n",
      "Accuracy: 0.7888 - Loss: 0.5983\n",
      "\n",
      "Batch 542/992 ━━━━━━━━━━━━━━━━━━━━ 20:39:33\n",
      "Accuracy: 0.7887 - Loss: 0.5982\n",
      "\n",
      "Batch 543/992 ━━━━━━━━━━━━━━━━━━━━ 20:39:44\n",
      "Accuracy: 0.7889 - Loss: 0.5981\n",
      "\n",
      "Batch 544/992 ━━━━━━━━━━━━━━━━━━━━ 20:39:55\n",
      "Accuracy: 0.7891 - Loss: 0.5975\n",
      "\n",
      "Batch 545/992 ━━━━━━━━━━━━━━━━━━━━ 20:40:05\n",
      "Accuracy: 0.7892 - Loss: 0.5972\n",
      "\n",
      "Batch 546/992 ━━━━━━━━━━━━━━━━━━━━ 20:40:16\n",
      "Accuracy: 0.7891 - Loss: 0.5977\n",
      "\n",
      "Batch 547/992 ━━━━━━━━━━━━━━━━━━━━ 20:40:26\n",
      "Accuracy: 0.7893 - Loss: 0.5971\n",
      "\n",
      "Batch 548/992 ━━━━━━━━━━━━━━━━━━━━ 20:40:38\n",
      "Accuracy: 0.7892 - Loss: 0.5971\n",
      "\n",
      "Batch 549/992 ━━━━━━━━━━━━━━━━━━━━ 20:40:51\n",
      "Accuracy: 0.7896 - Loss: 0.5964\n",
      "\n",
      "Batch 550/992 ━━━━━━━━━━━━━━━━━━━━ 20:41:02\n",
      "Accuracy: 0.7900 - Loss: 0.5957\n",
      "\n",
      "Batch 551/992 ━━━━━━━━━━━━━━━━━━━━ 20:41:13\n",
      "Accuracy: 0.7899 - Loss: 0.5958\n",
      "\n",
      "Batch 552/992 ━━━━━━━━━━━━━━━━━━━━ 20:41:24\n",
      "Accuracy: 0.7903 - Loss: 0.5953\n",
      "\n",
      "Batch 553/992 ━━━━━━━━━━━━━━━━━━━━ 20:41:35\n",
      "Accuracy: 0.7898 - Loss: 0.5969\n",
      "\n",
      "Batch 554/992 ━━━━━━━━━━━━━━━━━━━━ 20:41:46\n",
      "Accuracy: 0.7897 - Loss: 0.5973\n",
      "\n",
      "Batch 555/992 ━━━━━━━━━━━━━━━━━━━━ 20:41:57\n",
      "Accuracy: 0.7899 - Loss: 0.5971\n",
      "\n",
      "Batch 556/992 ━━━━━━━━━━━━━━━━━━━━ 20:42:09\n",
      "Accuracy: 0.7900 - Loss: 0.5965\n",
      "\n",
      "Batch 557/992 ━━━━━━━━━━━━━━━━━━━━ 20:42:22\n",
      "Accuracy: 0.7902 - Loss: 0.5964\n",
      "\n",
      "Batch 558/992 ━━━━━━━━━━━━━━━━━━━━ 20:42:35\n",
      "Accuracy: 0.7905 - Loss: 0.5954\n",
      "\n",
      "Batch 559/992 ━━━━━━━━━━━━━━━━━━━━ 20:42:46\n",
      "Accuracy: 0.7905 - Loss: 0.5952\n",
      "\n",
      "Batch 560/992 ━━━━━━━━━━━━━━━━━━━━ 20:42:57\n",
      "Accuracy: 0.7904 - Loss: 0.5952\n",
      "\n",
      "Batch 561/992 ━━━━━━━━━━━━━━━━━━━━ 20:43:08\n",
      "Accuracy: 0.7901 - Loss: 0.5955\n",
      "\n",
      "Batch 562/992 ━━━━━━━━━━━━━━━━━━━━ 20:43:18\n",
      "Accuracy: 0.7903 - Loss: 0.5951\n",
      "\n",
      "Batch 563/992 ━━━━━━━━━━━━━━━━━━━━ 20:43:29\n",
      "Accuracy: 0.7902 - Loss: 0.5951\n",
      "\n",
      "Batch 564/992 ━━━━━━━━━━━━━━━━━━━━ 20:43:40\n",
      "Accuracy: 0.7901 - Loss: 0.5949\n",
      "\n",
      "Batch 565/992 ━━━━━━━━━━━━━━━━━━━━ 20:43:50\n",
      "Accuracy: 0.7903 - Loss: 0.5947\n",
      "\n",
      "Batch 566/992 ━━━━━━━━━━━━━━━━━━━━ 20:44:01\n",
      "Accuracy: 0.7904 - Loss: 0.5943\n",
      "\n",
      "Batch 567/992 ━━━━━━━━━━━━━━━━━━━━ 20:44:13\n",
      "Accuracy: 0.7903 - Loss: 0.5945\n",
      "\n",
      "Batch 568/992 ━━━━━━━━━━━━━━━━━━━━ 20:44:24\n",
      "Accuracy: 0.7901 - Loss: 0.5946\n",
      "\n",
      "Batch 569/992 ━━━━━━━━━━━━━━━━━━━━ 20:44:35\n",
      "Accuracy: 0.7895 - Loss: 0.5954\n",
      "\n",
      "Batch 570/992 ━━━━━━━━━━━━━━━━━━━━ 20:44:47\n",
      "Accuracy: 0.7895 - Loss: 0.5956\n",
      "\n",
      "Batch 571/992 ━━━━━━━━━━━━━━━━━━━━ 20:45:00\n",
      "Accuracy: 0.7896 - Loss: 0.5953\n",
      "\n",
      "Batch 572/992 ━━━━━━━━━━━━━━━━━━━━ 20:45:12\n",
      "Accuracy: 0.7896 - Loss: 0.5949\n",
      "\n",
      "Batch 573/992 ━━━━━━━━━━━━━━━━━━━━ 20:45:22\n",
      "Accuracy: 0.7895 - Loss: 0.5948\n",
      "\n",
      "Batch 574/992 ━━━━━━━━━━━━━━━━━━━━ 20:45:35\n",
      "Accuracy: 0.7892 - Loss: 0.5949\n",
      "\n",
      "Batch 575/992 ━━━━━━━━━━━━━━━━━━━━ 20:45:47\n",
      "Accuracy: 0.7893 - Loss: 0.5955\n",
      "\n",
      "Batch 576/992 ━━━━━━━━━━━━━━━━━━━━ 20:45:58\n",
      "Accuracy: 0.7895 - Loss: 0.5956\n",
      "\n",
      "Batch 577/992 ━━━━━━━━━━━━━━━━━━━━ 20:46:09\n",
      "Accuracy: 0.7892 - Loss: 0.5962\n",
      "\n",
      "Batch 578/992 ━━━━━━━━━━━━━━━━━━━━ 20:46:21\n",
      "Accuracy: 0.7891 - Loss: 0.5965\n",
      "\n",
      "Batch 579/992 ━━━━━━━━━━━━━━━━━━━━ 20:46:33\n",
      "Accuracy: 0.7891 - Loss: 0.5963\n",
      "\n",
      "Batch 580/992 ━━━━━━━━━━━━━━━━━━━━ 20:46:44\n",
      "Accuracy: 0.7890 - Loss: 0.5964\n",
      "\n",
      "Batch 581/992 ━━━━━━━━━━━━━━━━━━━━ 20:46:57\n",
      "Accuracy: 0.7892 - Loss: 0.5960\n",
      "\n",
      "Batch 582/992 ━━━━━━━━━━━━━━━━━━━━ 20:47:08\n",
      "Accuracy: 0.7891 - Loss: 0.5958\n",
      "\n",
      "Batch 583/992 ━━━━━━━━━━━━━━━━━━━━ 20:47:20\n",
      "Accuracy: 0.7895 - Loss: 0.5951\n",
      "\n",
      "Batch 584/992 ━━━━━━━━━━━━━━━━━━━━ 20:47:32\n",
      "Accuracy: 0.7896 - Loss: 0.5952\n",
      "\n",
      "Batch 585/992 ━━━━━━━━━━━━━━━━━━━━ 20:47:43\n",
      "Accuracy: 0.7891 - Loss: 0.5961\n",
      "\n",
      "Batch 586/992 ━━━━━━━━━━━━━━━━━━━━ 20:47:54\n",
      "Accuracy: 0.7890 - Loss: 0.5964\n",
      "\n",
      "Batch 587/992 ━━━━━━━━━━━━━━━━━━━━ 20:48:06\n",
      "Accuracy: 0.7888 - Loss: 0.5964\n",
      "\n",
      "Batch 588/992 ━━━━━━━━━━━━━━━━━━━━ 20:48:17\n",
      "Accuracy: 0.7891 - Loss: 0.5957\n",
      "\n",
      "Batch 589/992 ━━━━━━━━━━━━━━━━━━━━ 20:48:31\n",
      "Accuracy: 0.7890 - Loss: 0.5958\n",
      "\n",
      "Batch 590/992 ━━━━━━━━━━━━━━━━━━━━ 20:48:42\n",
      "Accuracy: 0.7888 - Loss: 0.5966\n",
      "\n",
      "Batch 591/992 ━━━━━━━━━━━━━━━━━━━━ 20:48:54\n",
      "Accuracy: 0.7891 - Loss: 0.5958\n",
      "\n",
      "Batch 592/992 ━━━━━━━━━━━━━━━━━━━━ 20:49:05\n",
      "Accuracy: 0.7893 - Loss: 0.5952\n",
      "\n",
      "Batch 593/992 ━━━━━━━━━━━━━━━━━━━━ 20:49:16\n",
      "Accuracy: 0.7892 - Loss: 0.5954\n",
      "\n",
      "Batch 594/992 ━━━━━━━━━━━━━━━━━━━━ 20:49:26\n",
      "Accuracy: 0.7891 - Loss: 0.5956\n",
      "\n",
      "Batch 595/992 ━━━━━━━━━━━━━━━━━━━━ 20:49:37\n",
      "Accuracy: 0.7893 - Loss: 0.5957\n",
      "\n",
      "Batch 596/992 ━━━━━━━━━━━━━━━━━━━━ 20:49:47\n",
      "Accuracy: 0.7892 - Loss: 0.5954\n",
      "\n",
      "Batch 597/992 ━━━━━━━━━━━━━━━━━━━━ 20:49:57\n",
      "Accuracy: 0.7894 - Loss: 0.5953\n",
      "\n",
      "Batch 598/992 ━━━━━━━━━━━━━━━━━━━━ 20:50:07\n",
      "Accuracy: 0.7891 - Loss: 0.5955\n",
      "\n",
      "Batch 599/992 ━━━━━━━━━━━━━━━━━━━━ 20:50:18\n",
      "Accuracy: 0.7892 - Loss: 0.5952\n",
      "\n",
      "Batch 600/992 ━━━━━━━━━━━━━━━━━━━━ 20:50:28\n",
      "Accuracy: 0.7892 - Loss: 0.5957\n",
      "\n",
      "Batch 601/992 ━━━━━━━━━━━━━━━━━━━━ 20:50:38\n",
      "Accuracy: 0.7893 - Loss: 0.5949\n",
      "\n",
      "Batch 602/992 ━━━━━━━━━━━━━━━━━━━━ 20:50:49\n",
      "Accuracy: 0.7888 - Loss: 0.5953\n",
      "\n",
      "Batch 603/992 ━━━━━━━━━━━━━━━━━━━━ 20:50:59\n",
      "Accuracy: 0.7892 - Loss: 0.5946\n",
      "\n",
      "Batch 604/992 ━━━━━━━━━━━━━━━━━━━━ 20:51:09\n",
      "Accuracy: 0.7891 - Loss: 0.5942\n",
      "\n",
      "Batch 605/992 ━━━━━━━━━━━━━━━━━━━━ 20:51:20\n",
      "Accuracy: 0.7890 - Loss: 0.5939\n",
      "\n",
      "Batch 606/992 ━━━━━━━━━━━━━━━━━━━━ 20:51:30\n",
      "Accuracy: 0.7890 - Loss: 0.5941\n",
      "\n",
      "Batch 607/992 ━━━━━━━━━━━━━━━━━━━━ 20:51:40\n",
      "Accuracy: 0.7889 - Loss: 0.5941\n",
      "\n",
      "Batch 608/992 ━━━━━━━━━━━━━━━━━━━━ 20:51:50\n",
      "Accuracy: 0.7893 - Loss: 0.5934\n",
      "\n",
      "Batch 609/992 ━━━━━━━━━━━━━━━━━━━━ 20:52:01\n",
      "Accuracy: 0.7890 - Loss: 0.5935\n",
      "\n",
      "Batch 610/992 ━━━━━━━━━━━━━━━━━━━━ 20:52:12\n",
      "Accuracy: 0.7889 - Loss: 0.5933\n",
      "\n",
      "Batch 611/992 ━━━━━━━━━━━━━━━━━━━━ 20:52:22\n",
      "Accuracy: 0.7893 - Loss: 0.5925\n",
      "\n",
      "Batch 612/992 ━━━━━━━━━━━━━━━━━━━━ 20:52:32\n",
      "Accuracy: 0.7894 - Loss: 0.5922\n",
      "\n",
      "Batch 613/992 ━━━━━━━━━━━━━━━━━━━━ 20:52:43\n",
      "Accuracy: 0.7894 - Loss: 0.5923\n",
      "\n",
      "Batch 614/992 ━━━━━━━━━━━━━━━━━━━━ 20:52:53\n",
      "Accuracy: 0.7891 - Loss: 0.5924\n",
      "\n",
      "Batch 615/992 ━━━━━━━━━━━━━━━━━━━━ 20:53:03\n",
      "Accuracy: 0.7888 - Loss: 0.5924\n",
      "\n",
      "Batch 616/992 ━━━━━━━━━━━━━━━━━━━━ 20:53:13\n",
      "Accuracy: 0.7888 - Loss: 0.5923\n",
      "\n",
      "Batch 617/992 ━━━━━━━━━━━━━━━━━━━━ 20:53:24\n",
      "Accuracy: 0.7887 - Loss: 0.5924\n",
      "\n",
      "Batch 618/992 ━━━━━━━━━━━━━━━━━━━━ 20:53:34\n",
      "Accuracy: 0.7890 - Loss: 0.5918\n",
      "\n",
      "Batch 619/992 ━━━━━━━━━━━━━━━━━━━━ 20:53:46\n",
      "Accuracy: 0.7892 - Loss: 0.5912\n",
      "\n",
      "Batch 620/992 ━━━━━━━━━━━━━━━━━━━━ 20:53:57\n",
      "Accuracy: 0.7895 - Loss: 0.5906\n",
      "\n",
      "Batch 621/992 ━━━━━━━━━━━━━━━━━━━━ 20:54:08\n",
      "Accuracy: 0.7899 - Loss: 0.5898\n",
      "\n",
      "Batch 622/992 ━━━━━━━━━━━━━━━━━━━━ 20:54:21\n",
      "Accuracy: 0.7902 - Loss: 0.5891\n",
      "\n",
      "Batch 623/992 ━━━━━━━━━━━━━━━━━━━━ 20:54:32\n",
      "Accuracy: 0.7905 - Loss: 0.5885\n",
      "\n",
      "Batch 624/992 ━━━━━━━━━━━━━━━━━━━━ 20:54:43\n",
      "Accuracy: 0.7903 - Loss: 0.5890\n",
      "\n",
      "Batch 625/992 ━━━━━━━━━━━━━━━━━━━━ 20:54:54\n",
      "Accuracy: 0.7906 - Loss: 0.5883\n",
      "\n",
      "Batch 626/992 ━━━━━━━━━━━━━━━━━━━━ 20:55:06\n",
      "Accuracy: 0.7905 - Loss: 0.5886\n",
      "\n",
      "Batch 627/992 ━━━━━━━━━━━━━━━━━━━━ 20:55:17\n",
      "Accuracy: 0.7905 - Loss: 0.5882\n",
      "\n",
      "Batch 628/992 ━━━━━━━━━━━━━━━━━━━━ 20:55:27\n",
      "Accuracy: 0.7908 - Loss: 0.5876\n",
      "\n",
      "Batch 629/992 ━━━━━━━━━━━━━━━━━━━━ 20:55:38\n",
      "Accuracy: 0.7909 - Loss: 0.5874\n",
      "\n",
      "Batch 630/992 ━━━━━━━━━━━━━━━━━━━━ 20:55:48\n",
      "Accuracy: 0.7909 - Loss: 0.5875\n",
      "\n",
      "Batch 631/992 ━━━━━━━━━━━━━━━━━━━━ 20:55:59\n",
      "Accuracy: 0.7908 - Loss: 0.5880\n",
      "\n",
      "Batch 632/992 ━━━━━━━━━━━━━━━━━━━━ 20:56:10\n",
      "Accuracy: 0.7911 - Loss: 0.5874\n",
      "\n",
      "Batch 633/992 ━━━━━━━━━━━━━━━━━━━━ 20:56:20\n",
      "Accuracy: 0.7913 - Loss: 0.5872\n",
      "\n",
      "Batch 634/992 ━━━━━━━━━━━━━━━━━━━━ 20:56:31\n",
      "Accuracy: 0.7912 - Loss: 0.5873\n",
      "\n",
      "Batch 635/992 ━━━━━━━━━━━━━━━━━━━━ 20:56:42\n",
      "Accuracy: 0.7915 - Loss: 0.5867\n",
      "\n",
      "Batch 636/992 ━━━━━━━━━━━━━━━━━━━━ 20:56:53\n",
      "Accuracy: 0.7915 - Loss: 0.5873\n",
      "\n",
      "Batch 637/992 ━━━━━━━━━━━━━━━━━━━━ 20:57:04\n",
      "Accuracy: 0.7916 - Loss: 0.5869\n",
      "\n",
      "Batch 638/992 ━━━━━━━━━━━━━━━━━━━━ 20:57:15\n",
      "Accuracy: 0.7917 - Loss: 0.5864\n",
      "\n",
      "Batch 639/992 ━━━━━━━━━━━━━━━━━━━━ 20:57:25\n",
      "Accuracy: 0.7919 - Loss: 0.5859\n",
      "\n",
      "Batch 640/992 ━━━━━━━━━━━━━━━━━━━━ 20:57:38\n",
      "Accuracy: 0.7922 - Loss: 0.5852\n",
      "\n",
      "Batch 641/992 ━━━━━━━━━━━━━━━━━━━━ 20:57:51\n",
      "Accuracy: 0.7923 - Loss: 0.5849\n",
      "\n",
      "Batch 642/992 ━━━━━━━━━━━━━━━━━━━━ 20:58:01\n",
      "Accuracy: 0.7926 - Loss: 0.5846\n",
      "\n",
      "Batch 643/992 ━━━━━━━━━━━━━━━━━━━━ 20:58:13\n",
      "Accuracy: 0.7926 - Loss: 0.5847\n",
      "\n",
      "Batch 644/992 ━━━━━━━━━━━━━━━━━━━━ 20:58:23\n",
      "Accuracy: 0.7925 - Loss: 0.5844\n",
      "\n",
      "Batch 645/992 ━━━━━━━━━━━━━━━━━━━━ 20:58:34\n",
      "Accuracy: 0.7928 - Loss: 0.5840\n",
      "\n",
      "Batch 646/992 ━━━━━━━━━━━━━━━━━━━━ 20:58:45\n",
      "Accuracy: 0.7926 - Loss: 0.5844\n",
      "\n",
      "Batch 647/992 ━━━━━━━━━━━━━━━━━━━━ 20:58:56\n",
      "Accuracy: 0.7929 - Loss: 0.5835\n",
      "\n",
      "Batch 648/992 ━━━━━━━━━━━━━━━━━━━━ 20:59:06\n",
      "Accuracy: 0.7928 - Loss: 0.5833\n",
      "\n",
      "Batch 649/992 ━━━━━━━━━━━━━━━━━━━━ 20:59:17\n",
      "Accuracy: 0.7930 - Loss: 0.5831\n",
      "\n",
      "Batch 650/992 ━━━━━━━━━━━━━━━━━━━━ 20:59:28\n",
      "Accuracy: 0.7931 - Loss: 0.5827\n",
      "\n",
      "Batch 651/992 ━━━━━━━━━━━━━━━━━━━━ 20:59:38\n",
      "Accuracy: 0.7932 - Loss: 0.5823\n",
      "\n",
      "Batch 652/992 ━━━━━━━━━━━━━━━━━━━━ 20:59:49\n",
      "Accuracy: 0.7931 - Loss: 0.5826\n",
      "\n",
      "Batch 653/992 ━━━━━━━━━━━━━━━━━━━━ 20:59:59\n",
      "Accuracy: 0.7935 - Loss: 0.5820\n",
      "\n",
      "Batch 654/992 ━━━━━━━━━━━━━━━━━━━━ 21:00:10\n",
      "Accuracy: 0.7936 - Loss: 0.5817\n",
      "\n",
      "Batch 655/992 ━━━━━━━━━━━━━━━━━━━━ 21:00:21\n",
      "Accuracy: 0.7937 - Loss: 0.5811\n",
      "\n",
      "Batch 656/992 ━━━━━━━━━━━━━━━━━━━━ 21:00:32\n",
      "Accuracy: 0.7938 - Loss: 0.5809\n",
      "\n",
      "Batch 657/992 ━━━━━━━━━━━━━━━━━━━━ 21:00:45\n",
      "Accuracy: 0.7938 - Loss: 0.5812\n",
      "\n",
      "Batch 658/992 ━━━━━━━━━━━━━━━━━━━━ 21:00:55\n",
      "Accuracy: 0.7939 - Loss: 0.5808\n",
      "\n",
      "Batch 659/992 ━━━━━━━━━━━━━━━━━━━━ 21:01:07\n",
      "Accuracy: 0.7940 - Loss: 0.5804\n",
      "\n",
      "Batch 660/992 ━━━━━━━━━━━━━━━━━━━━ 21:01:18\n",
      "Accuracy: 0.7941 - Loss: 0.5801\n",
      "\n",
      "Batch 661/992 ━━━━━━━━━━━━━━━━━━━━ 21:01:29\n",
      "Accuracy: 0.7943 - Loss: 0.5796\n",
      "\n",
      "Batch 662/992 ━━━━━━━━━━━━━━━━━━━━ 21:01:39\n",
      "Accuracy: 0.7940 - Loss: 0.5804\n",
      "\n",
      "Batch 663/992 ━━━━━━━━━━━━━━━━━━━━ 21:01:50\n",
      "Accuracy: 0.7941 - Loss: 0.5804\n",
      "\n",
      "Batch 664/992 ━━━━━━━━━━━━━━━━━━━━ 21:02:01\n",
      "Accuracy: 0.7941 - Loss: 0.5805\n",
      "\n",
      "Batch 665/992 ━━━━━━━━━━━━━━━━━━━━ 21:02:12\n",
      "Accuracy: 0.7944 - Loss: 0.5797\n",
      "\n",
      "Batch 666/992 ━━━━━━━━━━━━━━━━━━━━ 21:02:23\n",
      "Accuracy: 0.7945 - Loss: 0.5796\n",
      "\n",
      "Batch 667/992 ━━━━━━━━━━━━━━━━━━━━ 21:02:33\n",
      "Accuracy: 0.7940 - Loss: 0.5804\n",
      "\n",
      "Batch 668/992 ━━━━━━━━━━━━━━━━━━━━ 21:02:44\n",
      "Accuracy: 0.7942 - Loss: 0.5803\n",
      "\n",
      "Batch 669/992 ━━━━━━━━━━━━━━━━━━━━ 21:02:55\n",
      "Accuracy: 0.7941 - Loss: 0.5806\n",
      "\n",
      "Batch 670/992 ━━━━━━━━━━━━━━━━━━━━ 21:03:05\n",
      "Accuracy: 0.7940 - Loss: 0.5806\n",
      "\n",
      "Batch 671/992 ━━━━━━━━━━━━━━━━━━━━ 21:03:16\n",
      "Accuracy: 0.7940 - Loss: 0.5808\n",
      "\n",
      "Batch 672/992 ━━━━━━━━━━━━━━━━━━━━ 21:03:27\n",
      "Accuracy: 0.7941 - Loss: 0.5803\n",
      "\n",
      "Batch 673/992 ━━━━━━━━━━━━━━━━━━━━ 21:03:37\n",
      "Accuracy: 0.7944 - Loss: 0.5799\n",
      "\n",
      "Batch 674/992 ━━━━━━━━━━━━━━━━━━━━ 21:03:48\n",
      "Accuracy: 0.7945 - Loss: 0.5797\n",
      "\n",
      "Batch 675/992 ━━━━━━━━━━━━━━━━━━━━ 21:03:59\n",
      "Accuracy: 0.7944 - Loss: 0.5795\n",
      "\n",
      "Batch 676/992 ━━━━━━━━━━━━━━━━━━━━ 21:04:09\n",
      "Accuracy: 0.7946 - Loss: 0.5793\n",
      "\n",
      "Batch 677/992 ━━━━━━━━━━━━━━━━━━━━ 21:04:20\n",
      "Accuracy: 0.7947 - Loss: 0.5792\n",
      "\n",
      "Batch 678/992 ━━━━━━━━━━━━━━━━━━━━ 21:04:31\n",
      "Accuracy: 0.7946 - Loss: 0.5799\n",
      "\n",
      "Batch 679/992 ━━━━━━━━━━━━━━━━━━━━ 21:04:41\n",
      "Accuracy: 0.7946 - Loss: 0.5798\n",
      "\n",
      "Batch 680/992 ━━━━━━━━━━━━━━━━━━━━ 21:04:52\n",
      "Accuracy: 0.7943 - Loss: 0.5799\n",
      "\n",
      "Batch 681/992 ━━━━━━━━━━━━━━━━━━━━ 21:05:05\n",
      "Accuracy: 0.7942 - Loss: 0.5800\n",
      "\n",
      "Batch 682/992 ━━━━━━━━━━━━━━━━━━━━ 21:05:16\n",
      "Accuracy: 0.7944 - Loss: 0.5794\n",
      "\n",
      "Batch 683/992 ━━━━━━━━━━━━━━━━━━━━ 21:05:26\n",
      "Accuracy: 0.7945 - Loss: 0.5794\n",
      "\n",
      "Batch 684/992 ━━━━━━━━━━━━━━━━━━━━ 21:05:37\n",
      "Accuracy: 0.7940 - Loss: 0.5799\n",
      "\n",
      "Batch 685/992 ━━━━━━━━━━━━━━━━━━━━ 21:05:48\n",
      "Accuracy: 0.7940 - Loss: 0.5797\n",
      "\n",
      "Batch 686/992 ━━━━━━━━━━━━━━━━━━━━ 21:05:59\n",
      "Accuracy: 0.7937 - Loss: 0.5800\n",
      "\n",
      "Batch 687/992 ━━━━━━━━━━━━━━━━━━━━ 21:06:10\n",
      "Accuracy: 0.7937 - Loss: 0.5800\n",
      "\n",
      "Batch 688/992 ━━━━━━━━━━━━━━━━━━━━ 21:06:20\n",
      "Accuracy: 0.7938 - Loss: 0.5796\n",
      "\n",
      "Batch 689/992 ━━━━━━━━━━━━━━━━━━━━ 21:06:31\n",
      "Accuracy: 0.7937 - Loss: 0.5797\n",
      "\n",
      "Batch 690/992 ━━━━━━━━━━━━━━━━━━━━ 21:06:42\n",
      "Accuracy: 0.7940 - Loss: 0.5792\n",
      "\n",
      "Batch 691/992 ━━━━━━━━━━━━━━━━━━━━ 21:06:52\n",
      "Accuracy: 0.7940 - Loss: 0.5793\n",
      "\n",
      "Batch 692/992 ━━━━━━━━━━━━━━━━━━━━ 21:07:03\n",
      "Accuracy: 0.7941 - Loss: 0.5790\n",
      "\n",
      "Batch 693/992 ━━━━━━━━━━━━━━━━━━━━ 21:07:13\n",
      "Accuracy: 0.7940 - Loss: 0.5790\n",
      "\n",
      "Batch 694/992 ━━━━━━━━━━━━━━━━━━━━ 21:07:24\n",
      "Accuracy: 0.7939 - Loss: 0.5793\n",
      "\n",
      "Batch 695/992 ━━━━━━━━━━━━━━━━━━━━ 21:07:35\n",
      "Accuracy: 0.7937 - Loss: 0.5793\n",
      "\n",
      "Batch 696/992 ━━━━━━━━━━━━━━━━━━━━ 21:07:46\n",
      "Accuracy: 0.7940 - Loss: 0.5787\n",
      "\n",
      "Batch 697/992 ━━━━━━━━━━━━━━━━━━━━ 21:07:57\n",
      "Accuracy: 0.7939 - Loss: 0.5787\n",
      "\n",
      "Batch 698/992 ━━━━━━━━━━━━━━━━━━━━ 21:08:08\n",
      "Accuracy: 0.7939 - Loss: 0.5789\n",
      "\n",
      "Batch 699/992 ━━━━━━━━━━━━━━━━━━━━ 21:08:19\n",
      "Accuracy: 0.7942 - Loss: 0.5784\n",
      "\n",
      "Batch 700/992 ━━━━━━━━━━━━━━━━━━━━ 21:08:30\n",
      "Accuracy: 0.7939 - Loss: 0.5787\n",
      "\n",
      "Batch 701/992 ━━━━━━━━━━━━━━━━━━━━ 21:08:41\n",
      "Accuracy: 0.7939 - Loss: 0.5790\n",
      "\n",
      "Batch 702/992 ━━━━━━━━━━━━━━━━━━━━ 21:08:52\n",
      "Accuracy: 0.7942 - Loss: 0.5785\n",
      "\n",
      "Batch 703/992 ━━━━━━━━━━━━━━━━━━━━ 21:09:02\n",
      "Accuracy: 0.7939 - Loss: 0.5788\n",
      "\n",
      "Batch 704/992 ━━━━━━━━━━━━━━━━━━━━ 21:09:13\n",
      "Accuracy: 0.7942 - Loss: 0.5782\n",
      "\n",
      "Batch 705/992 ━━━━━━━━━━━━━━━━━━━━ 21:09:25\n",
      "Accuracy: 0.7945 - Loss: 0.5777\n",
      "\n",
      "Batch 706/992 ━━━━━━━━━━━━━━━━━━━━ 21:09:36\n",
      "Accuracy: 0.7948 - Loss: 0.5771\n",
      "\n",
      "Batch 707/992 ━━━━━━━━━━━━━━━━━━━━ 21:09:46\n",
      "Accuracy: 0.7947 - Loss: 0.5774\n",
      "\n",
      "Batch 708/992 ━━━━━━━━━━━━━━━━━━━━ 21:09:57\n",
      "Accuracy: 0.7950 - Loss: 0.5770\n",
      "\n",
      "Batch 709/992 ━━━━━━━━━━━━━━━━━━━━ 21:10:09\n",
      "Accuracy: 0.7948 - Loss: 0.5773\n",
      "\n",
      "Batch 710/992 ━━━━━━━━━━━━━━━━━━━━ 21:10:20\n",
      "Accuracy: 0.7951 - Loss: 0.5766\n",
      "\n",
      "Batch 711/992 ━━━━━━━━━━━━━━━━━━━━ 21:10:31\n",
      "Accuracy: 0.7952 - Loss: 0.5763\n",
      "\n",
      "Batch 712/992 ━━━━━━━━━━━━━━━━━━━━ 21:10:42\n",
      "Accuracy: 0.7953 - Loss: 0.5761\n",
      "\n",
      "Batch 713/992 ━━━━━━━━━━━━━━━━━━━━ 21:10:54\n",
      "Accuracy: 0.7954 - Loss: 0.5757\n",
      "\n",
      "Batch 714/992 ━━━━━━━━━━━━━━━━━━━━ 21:11:06\n",
      "Accuracy: 0.7957 - Loss: 0.5751\n",
      "\n",
      "Batch 715/992 ━━━━━━━━━━━━━━━━━━━━ 21:11:16\n",
      "Accuracy: 0.7958 - Loss: 0.5748\n",
      "\n",
      "Batch 716/992 ━━━━━━━━━━━━━━━━━━━━ 21:11:28\n",
      "Accuracy: 0.7959 - Loss: 0.5748\n",
      "\n",
      "Batch 717/992 ━━━━━━━━━━━━━━━━━━━━ 21:11:40\n",
      "Accuracy: 0.7962 - Loss: 0.5741\n",
      "\n",
      "Batch 718/992 ━━━━━━━━━━━━━━━━━━━━ 21:11:52\n",
      "Accuracy: 0.7963 - Loss: 0.5737\n",
      "\n",
      "Batch 719/992 ━━━━━━━━━━━━━━━━━━━━ 21:12:03\n",
      "Accuracy: 0.7964 - Loss: 0.5738\n",
      "\n",
      "Batch 720/992 ━━━━━━━━━━━━━━━━━━━━ 21:12:14\n",
      "Accuracy: 0.7965 - Loss: 0.5736\n",
      "\n",
      "Batch 721/992 ━━━━━━━━━━━━━━━━━━━━ 21:12:25\n",
      "Accuracy: 0.7965 - Loss: 0.5736\n",
      "\n",
      "Batch 722/992 ━━━━━━━━━━━━━━━━━━━━ 21:12:35\n",
      "Accuracy: 0.7966 - Loss: 0.5735\n",
      "\n",
      "Batch 723/992 ━━━━━━━━━━━━━━━━━━━━ 21:12:46\n",
      "Accuracy: 0.7963 - Loss: 0.5735\n",
      "\n",
      "Batch 724/992 ━━━━━━━━━━━━━━━━━━━━ 21:12:57\n",
      "Accuracy: 0.7964 - Loss: 0.5733\n",
      "\n",
      "Batch 725/992 ━━━━━━━━━━━━━━━━━━━━ 21:13:07\n",
      "Accuracy: 0.7962 - Loss: 0.5739\n",
      "\n",
      "Batch 726/992 ━━━━━━━━━━━━━━━━━━━━ 21:13:18\n",
      "Accuracy: 0.7961 - Loss: 0.5745\n",
      "\n",
      "Batch 727/992 ━━━━━━━━━━━━━━━━━━━━ 21:13:28\n",
      "Accuracy: 0.7963 - Loss: 0.5741\n",
      "\n",
      "Batch 728/992 ━━━━━━━━━━━━━━━━━━━━ 21:13:39\n",
      "Accuracy: 0.7964 - Loss: 0.5737\n",
      "\n",
      "Batch 729/992 ━━━━━━━━━━━━━━━━━━━━ 21:13:49\n",
      "Accuracy: 0.7961 - Loss: 0.5743\n",
      "\n",
      "Batch 730/992 ━━━━━━━━━━━━━━━━━━━━ 21:14:00\n",
      "Accuracy: 0.7957 - Loss: 0.5751\n",
      "\n",
      "Batch 731/992 ━━━━━━━━━━━━━━━━━━━━ 21:14:10\n",
      "Accuracy: 0.7950 - Loss: 0.5767\n",
      "\n",
      "Batch 732/992 ━━━━━━━━━━━━━━━━━━━━ 21:14:21\n",
      "Accuracy: 0.7949 - Loss: 0.5776\n",
      "\n",
      "Batch 733/992 ━━━━━━━━━━━━━━━━━━━━ 21:14:31\n",
      "Accuracy: 0.7948 - Loss: 0.5778\n",
      "\n",
      "Batch 734/992 ━━━━━━━━━━━━━━━━━━━━ 21:14:42\n",
      "Accuracy: 0.7950 - Loss: 0.5775\n",
      "\n",
      "Batch 735/992 ━━━━━━━━━━━━━━━━━━━━ 21:14:52\n",
      "Accuracy: 0.7952 - Loss: 0.5768\n",
      "\n",
      "Batch 736/992 ━━━━━━━━━━━━━━━━━━━━ 21:15:03\n",
      "Accuracy: 0.7955 - Loss: 0.5761\n",
      "\n",
      "Batch 737/992 ━━━━━━━━━━━━━━━━━━━━ 21:15:13\n",
      "Accuracy: 0.7955 - Loss: 0.5761\n",
      "\n",
      "Batch 738/992 ━━━━━━━━━━━━━━━━━━━━ 21:15:24\n",
      "Accuracy: 0.7954 - Loss: 0.5765\n",
      "\n",
      "Batch 739/992 ━━━━━━━━━━━━━━━━━━━━ 21:15:34\n",
      "Accuracy: 0.7953 - Loss: 0.5764\n",
      "\n",
      "Batch 740/992 ━━━━━━━━━━━━━━━━━━━━ 21:15:45\n",
      "Accuracy: 0.7954 - Loss: 0.5763\n",
      "\n",
      "Batch 741/992 ━━━━━━━━━━━━━━━━━━━━ 21:15:55\n",
      "Accuracy: 0.7955 - Loss: 0.5759\n",
      "\n",
      "Batch 742/992 ━━━━━━━━━━━━━━━━━━━━ 21:16:06\n",
      "Accuracy: 0.7955 - Loss: 0.5763\n",
      "\n",
      "Batch 743/992 ━━━━━━━━━━━━━━━━━━━━ 21:16:17\n",
      "Accuracy: 0.7954 - Loss: 0.5765\n",
      "\n",
      "Batch 744/992 ━━━━━━━━━━━━━━━━━━━━ 21:16:27\n",
      "Accuracy: 0.7955 - Loss: 0.5765\n",
      "\n",
      "Batch 745/992 ━━━━━━━━━━━━━━━━━━━━ 21:16:38\n",
      "Accuracy: 0.7955 - Loss: 0.5763\n",
      "\n",
      "Batch 746/992 ━━━━━━━━━━━━━━━━━━━━ 21:16:49\n",
      "Accuracy: 0.7954 - Loss: 0.5763\n",
      "\n",
      "Batch 747/992 ━━━━━━━━━━━━━━━━━━━━ 21:17:00\n",
      "Accuracy: 0.7953 - Loss: 0.5764\n",
      "\n",
      "Batch 748/992 ━━━━━━━━━━━━━━━━━━━━ 21:17:10\n",
      "Accuracy: 0.7953 - Loss: 0.5764\n",
      "\n",
      "Batch 749/992 ━━━━━━━━━━━━━━━━━━━━ 21:17:21\n",
      "Accuracy: 0.7956 - Loss: 0.5758\n",
      "\n",
      "Batch 750/992 ━━━━━━━━━━━━━━━━━━━━ 21:17:32\n",
      "Accuracy: 0.7957 - Loss: 0.5760\n",
      "\n",
      "Batch 751/992 ━━━━━━━━━━━━━━━━━━━━ 21:17:42\n",
      "Accuracy: 0.7958 - Loss: 0.5758\n",
      "\n",
      "Batch 752/992 ━━━━━━━━━━━━━━━━━━━━ 21:17:53\n",
      "Accuracy: 0.7960 - Loss: 0.5753\n",
      "\n",
      "Batch 753/992 ━━━━━━━━━━━━━━━━━━━━ 21:18:03\n",
      "Accuracy: 0.7961 - Loss: 0.5750\n",
      "\n",
      "Batch 754/992 ━━━━━━━━━━━━━━━━━━━━ 21:18:14\n",
      "Accuracy: 0.7964 - Loss: 0.5744\n",
      "\n",
      "Batch 755/992 ━━━━━━━━━━━━━━━━━━━━ 21:18:24\n",
      "Accuracy: 0.7965 - Loss: 0.5740\n",
      "\n",
      "Batch 756/992 ━━━━━━━━━━━━━━━━━━━━ 21:18:35\n",
      "Accuracy: 0.7968 - Loss: 0.5733\n",
      "\n",
      "Batch 757/992 ━━━━━━━━━━━━━━━━━━━━ 21:18:46\n",
      "Accuracy: 0.7967 - Loss: 0.5734\n",
      "\n",
      "Batch 758/992 ━━━━━━━━━━━━━━━━━━━━ 21:18:57\n",
      "Accuracy: 0.7967 - Loss: 0.5745\n",
      "\n",
      "Batch 759/992 ━━━━━━━━━━━━━━━━━━━━ 21:19:11\n",
      "Accuracy: 0.7969 - Loss: 0.5740\n",
      "\n",
      "Batch 760/992 ━━━━━━━━━━━━━━━━━━━━ 21:19:22\n",
      "Accuracy: 0.7970 - Loss: 0.5738\n",
      "\n",
      "Batch 761/992 ━━━━━━━━━━━━━━━━━━━━ 21:19:33\n",
      "Accuracy: 0.7966 - Loss: 0.5744\n",
      "\n",
      "Batch 762/992 ━━━━━━━━━━━━━━━━━━━━ 21:19:44\n",
      "Accuracy: 0.7966 - Loss: 0.5748\n",
      "\n",
      "Batch 763/992 ━━━━━━━━━━━━━━━━━━━━ 21:19:55\n",
      "Accuracy: 0.7967 - Loss: 0.5745\n",
      "\n",
      "Batch 764/992 ━━━━━━━━━━━━━━━━━━━━ 21:20:05\n",
      "Accuracy: 0.7965 - Loss: 0.5756\n",
      "\n",
      "Batch 765/992 ━━━━━━━━━━━━━━━━━━━━ 21:20:16\n",
      "Accuracy: 0.7966 - Loss: 0.5753\n",
      "\n",
      "Batch 766/992 ━━━━━━━━━━━━━━━━━━━━ 21:20:27\n",
      "Accuracy: 0.7965 - Loss: 0.5755\n",
      "\n",
      "Batch 767/992 ━━━━━━━━━━━━━━━━━━━━ 21:20:38\n",
      "Accuracy: 0.7963 - Loss: 0.5760\n",
      "\n",
      "Batch 768/992 ━━━━━━━━━━━━━━━━━━━━ 21:20:49\n",
      "Accuracy: 0.7964 - Loss: 0.5758\n",
      "\n",
      "Batch 769/992 ━━━━━━━━━━━━━━━━━━━━ 21:21:01\n",
      "Accuracy: 0.7967 - Loss: 0.5756\n",
      "\n",
      "Batch 770/992 ━━━━━━━━━━━━━━━━━━━━ 21:21:13\n",
      "Accuracy: 0.7969 - Loss: 0.5752\n",
      "\n",
      "Batch 771/992 ━━━━━━━━━━━━━━━━━━━━ 21:21:24\n",
      "Accuracy: 0.7970 - Loss: 0.5747\n",
      "\n",
      "Batch 772/992 ━━━━━━━━━━━━━━━━━━━━ 21:21:35\n",
      "Accuracy: 0.7971 - Loss: 0.5747\n",
      "\n",
      "Batch 773/992 ━━━━━━━━━━━━━━━━━━━━ 21:21:46\n",
      "Accuracy: 0.7974 - Loss: 0.5742\n",
      "\n",
      "Batch 774/992 ━━━━━━━━━━━━━━━━━━━━ 21:21:57\n",
      "Accuracy: 0.7975 - Loss: 0.5742\n",
      "\n",
      "Batch 775/992 ━━━━━━━━━━━━━━━━━━━━ 21:22:08\n",
      "Accuracy: 0.7976 - Loss: 0.5740\n",
      "\n",
      "Batch 776/992 ━━━━━━━━━━━━━━━━━━━━ 21:22:19\n",
      "Accuracy: 0.7975 - Loss: 0.5740\n",
      "\n",
      "Batch 777/992 ━━━━━━━━━━━━━━━━━━━━ 21:22:29\n",
      "Accuracy: 0.7978 - Loss: 0.5736\n",
      "\n",
      "Batch 778/992 ━━━━━━━━━━━━━━━━━━━━ 21:22:41\n",
      "Accuracy: 0.7974 - Loss: 0.5745\n",
      "\n",
      "Batch 779/992 ━━━━━━━━━━━━━━━━━━━━ 21:22:53\n",
      "Accuracy: 0.7975 - Loss: 0.5746\n",
      "\n",
      "Batch 780/992 ━━━━━━━━━━━━━━━━━━━━ 21:23:04\n",
      "Accuracy: 0.7974 - Loss: 0.5748\n",
      "\n",
      "Batch 781/992 ━━━━━━━━━━━━━━━━━━━━ 21:23:15\n",
      "Accuracy: 0.7974 - Loss: 0.5750\n",
      "\n",
      "Batch 782/992 ━━━━━━━━━━━━━━━━━━━━ 21:23:26\n",
      "Accuracy: 0.7972 - Loss: 0.5754\n",
      "\n",
      "Batch 783/992 ━━━━━━━━━━━━━━━━━━━━ 21:23:36\n",
      "Accuracy: 0.7969 - Loss: 0.5755\n",
      "\n",
      "Batch 784/992 ━━━━━━━━━━━━━━━━━━━━ 21:23:47\n",
      "Accuracy: 0.7970 - Loss: 0.5753\n",
      "\n",
      "Batch 785/992 ━━━━━━━━━━━━━━━━━━━━ 21:23:58\n",
      "Accuracy: 0.7967 - Loss: 0.5766\n",
      "\n",
      "Batch 786/992 ━━━━━━━━━━━━━━━━━━━━ 21:24:08\n",
      "Accuracy: 0.7969 - Loss: 0.5763\n",
      "\n",
      "Batch 787/992 ━━━━━━━━━━━━━━━━━━━━ 21:24:19\n",
      "Accuracy: 0.7969 - Loss: 0.5761\n",
      "\n",
      "Batch 788/992 ━━━━━━━━━━━━━━━━━━━━ 21:24:29\n",
      "Accuracy: 0.7970 - Loss: 0.5757\n",
      "\n",
      "Batch 789/992 ━━━━━━━━━━━━━━━━━━━━ 21:24:40\n",
      "Accuracy: 0.7967 - Loss: 0.5762\n",
      "\n",
      "Batch 790/992 ━━━━━━━━━━━━━━━━━━━━ 21:24:50\n",
      "Accuracy: 0.7968 - Loss: 0.5759\n",
      "\n",
      "Batch 791/992 ━━━━━━━━━━━━━━━━━━━━ 21:25:01\n",
      "Accuracy: 0.7968 - Loss: 0.5758\n",
      "\n",
      "Batch 792/992 ━━━━━━━━━━━━━━━━━━━━ 21:25:11\n",
      "Accuracy: 0.7967 - Loss: 0.5762\n",
      "\n",
      "Batch 793/992 ━━━━━━━━━━━━━━━━━━━━ 21:25:22\n",
      "Accuracy: 0.7967 - Loss: 0.5767\n",
      "\n",
      "Batch 794/992 ━━━━━━━━━━━━━━━━━━━━ 21:25:33\n",
      "Accuracy: 0.7966 - Loss: 0.5769\n",
      "\n",
      "Batch 795/992 ━━━━━━━━━━━━━━━━━━━━ 21:25:43\n",
      "Accuracy: 0.7967 - Loss: 0.5768\n",
      "\n",
      "Batch 796/992 ━━━━━━━━━━━━━━━━━━━━ 21:25:54\n",
      "Accuracy: 0.7970 - Loss: 0.5763\n",
      "\n",
      "Batch 797/992 ━━━━━━━━━━━━━━━━━━━━ 21:26:05\n",
      "Accuracy: 0.7969 - Loss: 0.5762\n",
      "\n",
      "Batch 798/992 ━━━━━━━━━━━━━━━━━━━━ 21:26:16\n",
      "Accuracy: 0.7968 - Loss: 0.5762\n",
      "\n",
      "Batch 799/992 ━━━━━━━━━━━━━━━━━━━━ 21:26:28\n",
      "Accuracy: 0.7971 - Loss: 0.5760\n",
      "\n",
      "Batch 800/992 ━━━━━━━━━━━━━━━━━━━━ 21:26:41\n",
      "Accuracy: 0.7973 - Loss: 0.5754\n",
      "\n",
      "Batch 801/992 ━━━━━━━━━━━━━━━━━━━━ 21:26:53\n",
      "Accuracy: 0.7973 - Loss: 0.5756\n",
      "\n",
      "Batch 802/992 ━━━━━━━━━━━━━━━━━━━━ 21:27:03\n",
      "Accuracy: 0.7974 - Loss: 0.5752\n",
      "\n",
      "Batch 803/992 ━━━━━━━━━━━━━━━━━━━━ 21:27:14\n",
      "Accuracy: 0.7972 - Loss: 0.5754\n",
      "\n",
      "Batch 804/992 ━━━━━━━━━━━━━━━━━━━━ 21:27:25\n",
      "Accuracy: 0.7971 - Loss: 0.5757\n",
      "\n",
      "Batch 805/992 ━━━━━━━━━━━━━━━━━━━━ 21:27:35\n",
      "Accuracy: 0.7972 - Loss: 0.5757\n",
      "\n",
      "Batch 806/992 ━━━━━━━━━━━━━━━━━━━━ 21:27:46\n",
      "Accuracy: 0.7973 - Loss: 0.5761\n",
      "\n",
      "Batch 807/992 ━━━━━━━━━━━━━━━━━━━━ 21:27:56\n",
      "Accuracy: 0.7972 - Loss: 0.5764\n",
      "\n",
      "Batch 808/992 ━━━━━━━━━━━━━━━━━━━━ 21:28:07\n",
      "Accuracy: 0.7975 - Loss: 0.5758\n",
      "\n",
      "Batch 809/992 ━━━━━━━━━━━━━━━━━━━━ 21:28:17\n",
      "Accuracy: 0.7976 - Loss: 0.5754\n",
      "\n",
      "Batch 810/992 ━━━━━━━━━━━━━━━━━━━━ 21:28:28\n",
      "Accuracy: 0.7975 - Loss: 0.5754\n",
      "\n",
      "Batch 811/992 ━━━━━━━━━━━━━━━━━━━━ 21:28:39\n",
      "Accuracy: 0.7973 - Loss: 0.5757\n",
      "\n",
      "Batch 812/992 ━━━━━━━━━━━━━━━━━━━━ 21:28:49\n",
      "Accuracy: 0.7974 - Loss: 0.5756\n",
      "\n",
      "Batch 813/992 ━━━━━━━━━━━━━━━━━━━━ 21:28:59\n",
      "Accuracy: 0.7970 - Loss: 0.5759\n",
      "\n",
      "Batch 814/992 ━━━━━━━━━━━━━━━━━━━━ 21:29:10\n",
      "Accuracy: 0.7971 - Loss: 0.5758\n",
      "\n",
      "Batch 815/992 ━━━━━━━━━━━━━━━━━━━━ 21:29:21\n",
      "Accuracy: 0.7974 - Loss: 0.5753\n",
      "\n",
      "Batch 816/992 ━━━━━━━━━━━━━━━━━━━━ 21:29:31\n",
      "Accuracy: 0.7976 - Loss: 0.5748\n",
      "\n",
      "Batch 817/992 ━━━━━━━━━━━━━━━━━━━━ 21:29:42\n",
      "Accuracy: 0.7973 - Loss: 0.5754\n",
      "\n",
      "Batch 818/992 ━━━━━━━━━━━━━━━━━━━━ 21:29:52\n",
      "Accuracy: 0.7972 - Loss: 0.5753\n",
      "\n",
      "Batch 819/992 ━━━━━━━━━━━━━━━━━━━━ 21:30:03\n",
      "Accuracy: 0.7975 - Loss: 0.5747\n",
      "\n",
      "Batch 820/992 ━━━━━━━━━━━━━━━━━━━━ 21:30:14\n",
      "Accuracy: 0.7974 - Loss: 0.5748\n",
      "\n",
      "Batch 821/992 ━━━━━━━━━━━━━━━━━━━━ 21:30:24\n",
      "Accuracy: 0.7975 - Loss: 0.5749\n",
      "\n",
      "Batch 822/992 ━━━━━━━━━━━━━━━━━━━━ 21:30:35\n",
      "Accuracy: 0.7974 - Loss: 0.5752\n",
      "\n",
      "Batch 823/992 ━━━━━━━━━━━━━━━━━━━━ 21:30:45\n",
      "Accuracy: 0.7974 - Loss: 0.5755\n",
      "\n",
      "Batch 824/992 ━━━━━━━━━━━━━━━━━━━━ 21:30:55\n",
      "Accuracy: 0.7973 - Loss: 0.5760\n",
      "\n",
      "Batch 825/992 ━━━━━━━━━━━━━━━━━━━━ 21:31:06\n",
      "Accuracy: 0.7973 - Loss: 0.5758\n",
      "\n",
      "Batch 826/992 ━━━━━━━━━━━━━━━━━━━━ 21:31:17\n",
      "Accuracy: 0.7975 - Loss: 0.5753\n",
      "\n",
      "Batch 827/992 ━━━━━━━━━━━━━━━━━━━━ 21:31:27\n",
      "Accuracy: 0.7975 - Loss: 0.5752\n",
      "\n",
      "Batch 828/992 ━━━━━━━━━━━━━━━━━━━━ 21:31:38\n",
      "Accuracy: 0.7977 - Loss: 0.5746\n",
      "\n",
      "Batch 829/992 ━━━━━━━━━━━━━━━━━━━━ 21:31:49\n",
      "Accuracy: 0.7978 - Loss: 0.5741\n",
      "\n",
      "Batch 830/992 ━━━━━━━━━━━━━━━━━━━━ 21:31:59\n",
      "Accuracy: 0.7977 - Loss: 0.5741\n",
      "\n",
      "Batch 831/992 ━━━━━━━━━━━━━━━━━━━━ 21:32:10\n",
      "Accuracy: 0.7978 - Loss: 0.5741\n",
      "\n",
      "Batch 832/992 ━━━━━━━━━━━━━━━━━━━━ 21:32:21\n",
      "Accuracy: 0.7981 - Loss: 0.5737\n",
      "\n",
      "Batch 833/992 ━━━━━━━━━━━━━━━━━━━━ 21:32:32\n",
      "Accuracy: 0.7980 - Loss: 0.5739\n",
      "\n",
      "Batch 834/992 ━━━━━━━━━━━━━━━━━━━━ 21:32:43\n",
      "Accuracy: 0.7981 - Loss: 0.5737\n",
      "\n",
      "Batch 835/992 ━━━━━━━━━━━━━━━━━━━━ 21:32:53\n",
      "Accuracy: 0.7981 - Loss: 0.5737\n",
      "\n",
      "Batch 836/992 ━━━━━━━━━━━━━━━━━━━━ 21:33:05\n",
      "Accuracy: 0.7983 - Loss: 0.5732\n",
      "\n",
      "Batch 837/992 ━━━━━━━━━━━━━━━━━━━━ 21:33:15\n",
      "Accuracy: 0.7984 - Loss: 0.5730\n",
      "\n",
      "Batch 838/992 ━━━━━━━━━━━━━━━━━━━━ 21:33:26\n",
      "Accuracy: 0.7982 - Loss: 0.5728\n",
      "\n",
      "Batch 839/992 ━━━━━━━━━━━━━━━━━━━━ 21:33:37\n",
      "Accuracy: 0.7981 - Loss: 0.5729\n",
      "\n",
      "Batch 840/992 ━━━━━━━━━━━━━━━━━━━━ 21:33:48\n",
      "Accuracy: 0.7981 - Loss: 0.5728\n",
      "\n",
      "Batch 841/992 ━━━━━━━━━━━━━━━━━━━━ 21:33:59\n",
      "Accuracy: 0.7982 - Loss: 0.5726\n",
      "\n",
      "Batch 842/992 ━━━━━━━━━━━━━━━━━━━━ 21:34:12\n",
      "Accuracy: 0.7982 - Loss: 0.5723\n",
      "\n",
      "Batch 843/992 ━━━━━━━━━━━━━━━━━━━━ 21:34:23\n",
      "Accuracy: 0.7985 - Loss: 0.5717\n",
      "\n",
      "Batch 844/992 ━━━━━━━━━━━━━━━━━━━━ 21:34:34\n",
      "Accuracy: 0.7984 - Loss: 0.5721\n",
      "\n",
      "Batch 845/992 ━━━━━━━━━━━━━━━━━━━━ 21:34:45\n",
      "Accuracy: 0.7984 - Loss: 0.5723\n",
      "\n",
      "Batch 846/992 ━━━━━━━━━━━━━━━━━━━━ 21:34:55\n",
      "Accuracy: 0.7983 - Loss: 0.5722\n",
      "\n",
      "Batch 847/992 ━━━━━━━━━━━━━━━━━━━━ 21:35:06\n",
      "Accuracy: 0.7984 - Loss: 0.5719\n",
      "\n",
      "Batch 848/992 ━━━━━━━━━━━━━━━━━━━━ 21:35:16\n",
      "Accuracy: 0.7986 - Loss: 0.5713\n",
      "\n",
      "Batch 849/992 ━━━━━━━━━━━━━━━━━━━━ 21:35:27\n",
      "Accuracy: 0.7986 - Loss: 0.5715\n",
      "\n",
      "Batch 850/992 ━━━━━━━━━━━━━━━━━━━━ 21:35:38\n",
      "Accuracy: 0.7984 - Loss: 0.5714\n",
      "\n",
      "Batch 851/992 ━━━━━━━━━━━━━━━━━━━━ 21:35:48\n",
      "Accuracy: 0.7986 - Loss: 0.5709\n",
      "\n",
      "Batch 852/992 ━━━━━━━━━━━━━━━━━━━━ 21:35:59\n",
      "Accuracy: 0.7984 - Loss: 0.5709\n",
      "\n",
      "Batch 853/992 ━━━━━━━━━━━━━━━━━━━━ 21:36:09\n",
      "Accuracy: 0.7985 - Loss: 0.5706\n",
      "\n",
      "Batch 854/992 ━━━━━━━━━━━━━━━━━━━━ 21:36:20\n",
      "Accuracy: 0.7986 - Loss: 0.5705\n",
      "\n",
      "Batch 855/992 ━━━━━━━━━━━━━━━━━━━━ 21:36:30\n",
      "Accuracy: 0.7987 - Loss: 0.5702\n",
      "\n",
      "Batch 856/992 ━━━━━━━━━━━━━━━━━━━━ 21:36:41\n",
      "Accuracy: 0.7988 - Loss: 0.5700\n",
      "\n",
      "Batch 857/992 ━━━━━━━━━━━━━━━━━━━━ 21:36:51\n",
      "Accuracy: 0.7990 - Loss: 0.5695\n",
      "\n",
      "Batch 858/992 ━━━━━━━━━━━━━━━━━━━━ 21:37:02\n",
      "Accuracy: 0.7990 - Loss: 0.5699\n",
      "\n",
      "Batch 859/992 ━━━━━━━━━━━━━━━━━━━━ 21:37:13\n",
      "Accuracy: 0.7989 - Loss: 0.5698\n",
      "\n",
      "Batch 860/992 ━━━━━━━━━━━━━━━━━━━━ 21:37:24\n",
      "Accuracy: 0.7990 - Loss: 0.5695\n",
      "\n",
      "Batch 861/992 ━━━━━━━━━━━━━━━━━━━━ 21:37:35\n",
      "Accuracy: 0.7991 - Loss: 0.5694\n",
      "\n",
      "Batch 862/992 ━━━━━━━━━━━━━━━━━━━━ 21:37:45\n",
      "Accuracy: 0.7990 - Loss: 0.5694\n",
      "\n",
      "Batch 863/992 ━━━━━━━━━━━━━━━━━━━━ 21:37:56\n",
      "Accuracy: 0.7992 - Loss: 0.5689\n",
      "\n",
      "Batch 864/992 ━━━━━━━━━━━━━━━━━━━━ 21:38:09\n",
      "Accuracy: 0.7992 - Loss: 0.5692\n",
      "\n",
      "Batch 865/992 ━━━━━━━━━━━━━━━━━━━━ 21:38:19\n",
      "Accuracy: 0.7991 - Loss: 0.5693\n",
      "\n",
      "Batch 866/992 ━━━━━━━━━━━━━━━━━━━━ 21:38:30\n",
      "Accuracy: 0.7989 - Loss: 0.5693\n",
      "\n",
      "Batch 867/992 ━━━━━━━━━━━━━━━━━━━━ 21:38:40\n",
      "Accuracy: 0.7990 - Loss: 0.5691\n",
      "\n",
      "Batch 868/992 ━━━━━━━━━━━━━━━━━━━━ 21:38:50\n",
      "Accuracy: 0.7988 - Loss: 0.5694\n",
      "\n",
      "Batch 869/992 ━━━━━━━━━━━━━━━━━━━━ 21:39:00\n",
      "Accuracy: 0.7988 - Loss: 0.5695\n",
      "\n",
      "Batch 870/992 ━━━━━━━━━━━━━━━━━━━━ 21:39:10\n",
      "Accuracy: 0.7987 - Loss: 0.5694\n",
      "\n",
      "Batch 871/992 ━━━━━━━━━━━━━━━━━━━━ 21:39:21\n",
      "Accuracy: 0.7988 - Loss: 0.5690\n",
      "\n",
      "Batch 872/992 ━━━━━━━━━━━━━━━━━━━━ 21:39:31\n",
      "Accuracy: 0.7987 - Loss: 0.5693\n",
      "\n",
      "Batch 873/992 ━━━━━━━━━━━━━━━━━━━━ 21:39:41\n",
      "Accuracy: 0.7987 - Loss: 0.5693\n",
      "\n",
      "Batch 874/992 ━━━━━━━━━━━━━━━━━━━━ 21:39:51\n",
      "Accuracy: 0.7986 - Loss: 0.5693\n",
      "\n",
      "Batch 875/992 ━━━━━━━━━━━━━━━━━━━━ 21:40:02\n",
      "Accuracy: 0.7987 - Loss: 0.5691\n",
      "\n",
      "Batch 876/992 ━━━━━━━━━━━━━━━━━━━━ 21:40:12\n",
      "Accuracy: 0.7988 - Loss: 0.5688\n",
      "\n",
      "Batch 877/992 ━━━━━━━━━━━━━━━━━━━━ 21:40:22\n",
      "Accuracy: 0.7987 - Loss: 0.5690\n",
      "\n",
      "Batch 878/992 ━━━━━━━━━━━━━━━━━━━━ 21:40:32\n",
      "Accuracy: 0.7988 - Loss: 0.5689\n",
      "\n",
      "Batch 879/992 ━━━━━━━━━━━━━━━━━━━━ 21:40:42\n",
      "Accuracy: 0.7989 - Loss: 0.5687\n",
      "\n",
      "Batch 880/992 ━━━━━━━━━━━━━━━━━━━━ 21:40:52\n",
      "Accuracy: 0.7990 - Loss: 0.5686\n",
      "\n",
      "Batch 881/992 ━━━━━━━━━━━━━━━━━━━━ 21:41:02\n",
      "Accuracy: 0.7990 - Loss: 0.5691\n",
      "\n",
      "Batch 882/992 ━━━━━━━━━━━━━━━━━━━━ 21:41:13\n",
      "Accuracy: 0.7992 - Loss: 0.5687\n",
      "\n",
      "Batch 883/992 ━━━━━━━━━━━━━━━━━━━━ 21:41:23\n",
      "Accuracy: 0.7990 - Loss: 0.5688\n",
      "\n",
      "Batch 884/992 ━━━━━━━━━━━━━━━━━━━━ 21:41:34\n",
      "Accuracy: 0.7991 - Loss: 0.5690\n",
      "\n",
      "Batch 885/992 ━━━━━━━━━━━━━━━━━━━━ 21:41:45\n",
      "Accuracy: 0.7990 - Loss: 0.5688\n",
      "\n",
      "Batch 886/992 ━━━━━━━━━━━━━━━━━━━━ 21:41:57\n",
      "Accuracy: 0.7990 - Loss: 0.5688\n",
      "\n",
      "Batch 887/992 ━━━━━━━━━━━━━━━━━━━━ 21:42:07\n",
      "Accuracy: 0.7988 - Loss: 0.5692\n",
      "\n",
      "Batch 888/992 ━━━━━━━━━━━━━━━━━━━━ 21:42:20\n",
      "Accuracy: 0.7988 - Loss: 0.5690\n",
      "\n",
      "Batch 889/992 ━━━━━━━━━━━━━━━━━━━━ 21:42:32\n",
      "Accuracy: 0.7989 - Loss: 0.5689\n",
      "\n",
      "Batch 890/992 ━━━━━━━━━━━━━━━━━━━━ 21:42:48\n",
      "Accuracy: 0.7987 - Loss: 0.5694\n",
      "\n",
      "Batch 891/992 ━━━━━━━━━━━━━━━━━━━━ 21:43:02\n",
      "Accuracy: 0.7984 - Loss: 0.5703\n",
      "\n",
      "Batch 892/992 ━━━━━━━━━━━━━━━━━━━━ 21:43:12\n",
      "Accuracy: 0.7985 - Loss: 0.5701\n",
      "\n",
      "Batch 893/992 ━━━━━━━━━━━━━━━━━━━━ 21:43:23\n",
      "Accuracy: 0.7986 - Loss: 0.5699\n",
      "\n",
      "Batch 894/992 ━━━━━━━━━━━━━━━━━━━━ 21:43:34\n",
      "Accuracy: 0.7987 - Loss: 0.5698\n",
      "\n",
      "Batch 895/992 ━━━━━━━━━━━━━━━━━━━━ 21:43:46\n",
      "Accuracy: 0.7986 - Loss: 0.5695\n",
      "\n",
      "Batch 896/992 ━━━━━━━━━━━━━━━━━━━━ 21:43:56\n",
      "Accuracy: 0.7987 - Loss: 0.5695\n",
      "\n",
      "Batch 897/992 ━━━━━━━━━━━━━━━━━━━━ 21:44:07\n",
      "Accuracy: 0.7988 - Loss: 0.5691\n",
      "\n",
      "Batch 898/992 ━━━━━━━━━━━━━━━━━━━━ 21:44:17\n",
      "Accuracy: 0.7986 - Loss: 0.5692\n",
      "\n",
      "Batch 899/992 ━━━━━━━━━━━━━━━━━━━━ 21:44:28\n",
      "Accuracy: 0.7984 - Loss: 0.5695\n",
      "\n",
      "Batch 900/992 ━━━━━━━━━━━━━━━━━━━━ 21:44:38\n",
      "Accuracy: 0.7983 - Loss: 0.5695\n",
      "\n",
      "Batch 901/992 ━━━━━━━━━━━━━━━━━━━━ 21:44:49\n",
      "Accuracy: 0.7983 - Loss: 0.5693\n",
      "\n",
      "Batch 902/992 ━━━━━━━━━━━━━━━━━━━━ 21:45:00\n",
      "Accuracy: 0.7984 - Loss: 0.5689\n",
      "\n",
      "Batch 903/992 ━━━━━━━━━━━━━━━━━━━━ 21:45:14\n",
      "Accuracy: 0.7982 - Loss: 0.5692\n",
      "\n",
      "Batch 904/992 ━━━━━━━━━━━━━━━━━━━━ 21:45:28\n",
      "Accuracy: 0.7983 - Loss: 0.5690\n",
      "\n",
      "Batch 905/992 ━━━━━━━━━━━━━━━━━━━━ 21:45:41\n",
      "Accuracy: 0.7983 - Loss: 0.5687\n",
      "\n",
      "Batch 906/992 ━━━━━━━━━━━━━━━━━━━━ 21:45:56\n",
      "Accuracy: 0.7984 - Loss: 0.5686\n",
      "\n",
      "Batch 907/992 ━━━━━━━━━━━━━━━━━━━━ 21:46:11\n",
      "Accuracy: 0.7984 - Loss: 0.5689\n",
      "\n",
      "Batch 908/992 ━━━━━━━━━━━━━━━━━━━━ 21:46:25\n",
      "Accuracy: 0.7985 - Loss: 0.5685\n",
      "\n",
      "Batch 909/992 ━━━━━━━━━━━━━━━━━━━━ 21:46:36\n",
      "Accuracy: 0.7984 - Loss: 0.5686\n",
      "\n",
      "Batch 910/992 ━━━━━━━━━━━━━━━━━━━━ 21:46:49\n",
      "Accuracy: 0.7985 - Loss: 0.5684\n",
      "\n",
      "Batch 911/992 ━━━━━━━━━━━━━━━━━━━━ 21:47:00\n",
      "Accuracy: 0.7986 - Loss: 0.5681\n",
      "\n",
      "Batch 912/992 ━━━━━━━━━━━━━━━━━━━━ 21:47:14\n",
      "Accuracy: 0.7987 - Loss: 0.5679\n",
      "\n",
      "Batch 913/992 ━━━━━━━━━━━━━━━━━━━━ 21:47:28\n",
      "Accuracy: 0.7983 - Loss: 0.5681\n",
      "\n",
      "Batch 914/992 ━━━━━━━━━━━━━━━━━━━━ 21:47:41\n",
      "Accuracy: 0.7984 - Loss: 0.5679\n",
      "\n",
      "Batch 915/992 ━━━━━━━━━━━━━━━━━━━━ 21:47:55\n",
      "Accuracy: 0.7981 - Loss: 0.5682\n",
      "\n",
      "Batch 916/992 ━━━━━━━━━━━━━━━━━━━━ 21:48:10\n",
      "Accuracy: 0.7983 - Loss: 0.5677\n",
      "\n",
      "Batch 917/992 ━━━━━━━━━━━━━━━━━━━━ 21:48:24\n",
      "Accuracy: 0.7985 - Loss: 0.5673\n",
      "\n",
      "Batch 918/992 ━━━━━━━━━━━━━━━━━━━━ 21:48:37\n",
      "Accuracy: 0.7986 - Loss: 0.5671\n",
      "\n",
      "Batch 919/992 ━━━━━━━━━━━━━━━━━━━━ 21:48:48\n",
      "Accuracy: 0.7983 - Loss: 0.5687\n",
      "\n",
      "Batch 920/992 ━━━━━━━━━━━━━━━━━━━━ 21:49:01\n",
      "Accuracy: 0.7981 - Loss: 0.5688\n",
      "\n",
      "Batch 921/992 ━━━━━━━━━━━━━━━━━━━━ 21:49:14\n",
      "Accuracy: 0.7982 - Loss: 0.5690\n",
      "\n",
      "Batch 922/992 ━━━━━━━━━━━━━━━━━━━━ 21:49:28\n",
      "Accuracy: 0.7981 - Loss: 0.5691\n",
      "\n",
      "Batch 923/992 ━━━━━━━━━━━━━━━━━━━━ 21:49:41\n",
      "Accuracy: 0.7981 - Loss: 0.5694\n",
      "\n",
      "Batch 924/992 ━━━━━━━━━━━━━━━━━━━━ 21:49:55\n",
      "Accuracy: 0.7980 - Loss: 0.5692\n",
      "\n",
      "Batch 925/992 ━━━━━━━━━━━━━━━━━━━━ 21:50:10\n",
      "Accuracy: 0.7976 - Loss: 0.5694\n",
      "\n",
      "Batch 926/992 ━━━━━━━━━━━━━━━━━━━━ 21:50:20\n",
      "Accuracy: 0.7978 - Loss: 0.5690\n",
      "\n",
      "Batch 927/992 ━━━━━━━━━━━━━━━━━━━━ 21:50:30\n",
      "Accuracy: 0.7976 - Loss: 0.5691\n",
      "\n",
      "Batch 928/992 ━━━━━━━━━━━━━━━━━━━━ 21:50:45\n",
      "Accuracy: 0.7973 - Loss: 0.5694\n",
      "\n",
      "Batch 929/992 ━━━━━━━━━━━━━━━━━━━━ 21:50:58\n",
      "Accuracy: 0.7974 - Loss: 0.5694\n",
      "\n",
      "Batch 930/992 ━━━━━━━━━━━━━━━━━━━━ 21:51:10\n",
      "Accuracy: 0.7973 - Loss: 0.5696\n",
      "\n",
      "Batch 931/992 ━━━━━━━━━━━━━━━━━━━━ 21:51:20\n",
      "Accuracy: 0.7975 - Loss: 0.5694\n",
      "\n",
      "Batch 932/992 ━━━━━━━━━━━━━━━━━━━━ 21:51:32\n",
      "Accuracy: 0.7976 - Loss: 0.5697\n",
      "\n",
      "Batch 933/992 ━━━━━━━━━━━━━━━━━━━━ 21:51:45\n",
      "Accuracy: 0.7976 - Loss: 0.5696\n",
      "\n",
      "Batch 934/992 ━━━━━━━━━━━━━━━━━━━━ 21:51:57\n",
      "Accuracy: 0.7976 - Loss: 0.5693\n",
      "\n",
      "Batch 935/992 ━━━━━━━━━━━━━━━━━━━━ 21:52:08\n",
      "Accuracy: 0.7979 - Loss: 0.5688\n",
      "\n",
      "Batch 936/992 ━━━━━━━━━━━━━━━━━━━━ 21:52:19\n",
      "Accuracy: 0.7977 - Loss: 0.5691\n",
      "\n",
      "Batch 937/992 ━━━━━━━━━━━━━━━━━━━━ 21:52:29\n",
      "Accuracy: 0.7978 - Loss: 0.5689\n",
      "\n",
      "Batch 938/992 ━━━━━━━━━━━━━━━━━━━━ 21:52:39\n",
      "Accuracy: 0.7977 - Loss: 0.5688\n",
      "\n",
      "Batch 939/992 ━━━━━━━━━━━━━━━━━━━━ 21:52:49\n",
      "Accuracy: 0.7979 - Loss: 0.5686\n",
      "\n",
      "Batch 940/992 ━━━━━━━━━━━━━━━━━━━━ 21:53:00\n",
      "Accuracy: 0.7979 - Loss: 0.5687\n",
      "\n",
      "Batch 941/992 ━━━━━━━━━━━━━━━━━━━━ 21:53:10\n",
      "Accuracy: 0.7980 - Loss: 0.5684\n",
      "\n",
      "Batch 942/992 ━━━━━━━━━━━━━━━━━━━━ 21:53:20\n",
      "Accuracy: 0.7982 - Loss: 0.5681\n",
      "\n",
      "Batch 943/992 ━━━━━━━━━━━━━━━━━━━━ 21:53:33\n",
      "Accuracy: 0.7984 - Loss: 0.5679\n",
      "\n",
      "Batch 944/992 ━━━━━━━━━━━━━━━━━━━━ 21:53:43\n",
      "Accuracy: 0.7983 - Loss: 0.5679\n",
      "\n",
      "Batch 945/992 ━━━━━━━━━━━━━━━━━━━━ 21:53:53\n",
      "Accuracy: 0.7984 - Loss: 0.5677\n",
      "\n",
      "Batch 946/992 ━━━━━━━━━━━━━━━━━━━━ 21:54:07\n",
      "Accuracy: 0.7986 - Loss: 0.5674\n",
      "\n",
      "Batch 947/992 ━━━━━━━━━━━━━━━━━━━━ 21:54:21\n",
      "Accuracy: 0.7987 - Loss: 0.5674\n",
      "\n",
      "Batch 948/992 ━━━━━━━━━━━━━━━━━━━━ 21:54:34\n",
      "Accuracy: 0.7989 - Loss: 0.5670\n",
      "\n",
      "Batch 949/992 ━━━━━━━━━━━━━━━━━━━━ 21:54:46\n",
      "Accuracy: 0.7987 - Loss: 0.5672\n",
      "\n",
      "Batch 950/992 ━━━━━━━━━━━━━━━━━━━━ 21:54:56\n",
      "Accuracy: 0.7988 - Loss: 0.5670\n",
      "\n",
      "Batch 951/992 ━━━━━━━━━━━━━━━━━━━━ 21:55:06\n",
      "Accuracy: 0.7988 - Loss: 0.5671\n",
      "\n",
      "Batch 952/992 ━━━━━━━━━━━━━━━━━━━━ 21:55:16\n",
      "Accuracy: 0.7990 - Loss: 0.5667\n",
      "\n",
      "Batch 953/992 ━━━━━━━━━━━━━━━━━━━━ 21:55:26\n",
      "Accuracy: 0.7991 - Loss: 0.5666\n",
      "\n",
      "Batch 954/992 ━━━━━━━━━━━━━━━━━━━━ 21:55:40\n",
      "Accuracy: 0.7991 - Loss: 0.5663\n",
      "\n",
      "Batch 955/992 ━━━━━━━━━━━━━━━━━━━━ 21:55:55\n",
      "Accuracy: 0.7992 - Loss: 0.5661\n",
      "\n",
      "Batch 956/992 ━━━━━━━━━━━━━━━━━━━━ 21:56:09\n",
      "Accuracy: 0.7992 - Loss: 0.5663\n",
      "\n",
      "Batch 957/992 ━━━━━━━━━━━━━━━━━━━━ 21:56:20\n",
      "Accuracy: 0.7992 - Loss: 0.5660\n",
      "\n",
      "Batch 958/992 ━━━━━━━━━━━━━━━━━━━━ 21:56:30\n",
      "Accuracy: 0.7993 - Loss: 0.5660\n",
      "\n",
      "Batch 959/992 ━━━━━━━━━━━━━━━━━━━━ 21:56:40\n",
      "Accuracy: 0.7994 - Loss: 0.5658\n",
      "\n",
      "Batch 960/992 ━━━━━━━━━━━━━━━━━━━━ 21:56:50\n",
      "Accuracy: 0.7996 - Loss: 0.5653\n",
      "\n",
      "Batch 961/992 ━━━━━━━━━━━━━━━━━━━━ 21:57:00\n",
      "Accuracy: 0.7996 - Loss: 0.5656\n",
      "\n",
      "Batch 962/992 ━━━━━━━━━━━━━━━━━━━━ 21:57:11\n",
      "Accuracy: 0.7998 - Loss: 0.5651\n",
      "\n",
      "Batch 963/992 ━━━━━━━━━━━━━━━━━━━━ 21:57:21\n",
      "Accuracy: 0.8000 - Loss: 0.5649\n",
      "\n",
      "Batch 964/992 ━━━━━━━━━━━━━━━━━━━━ 21:57:31\n",
      "Accuracy: 0.7998 - Loss: 0.5655\n",
      "\n",
      "Batch 965/992 ━━━━━━━━━━━━━━━━━━━━ 21:57:41\n",
      "Accuracy: 0.7999 - Loss: 0.5652\n",
      "\n",
      "Batch 966/992 ━━━━━━━━━━━━━━━━━━━━ 21:57:51\n",
      "Accuracy: 0.7998 - Loss: 0.5651\n",
      "\n",
      "Batch 967/992 ━━━━━━━━━━━━━━━━━━━━ 21:58:02\n",
      "Accuracy: 0.8000 - Loss: 0.5646\n",
      "\n",
      "Batch 968/992 ━━━━━━━━━━━━━━━━━━━━ 21:58:12\n",
      "Accuracy: 0.8002 - Loss: 0.5643\n",
      "\n",
      "Batch 969/992 ━━━━━━━━━━━━━━━━━━━━ 21:58:23\n",
      "Accuracy: 0.7999 - Loss: 0.5649\n",
      "\n",
      "Batch 970/992 ━━━━━━━━━━━━━━━━━━━━ 21:58:33\n",
      "Accuracy: 0.8000 - Loss: 0.5647\n",
      "\n",
      "Batch 971/992 ━━━━━━━━━━━━━━━━━━━━ 21:58:43\n",
      "Accuracy: 0.7998 - Loss: 0.5649\n",
      "\n",
      "Batch 972/992 ━━━━━━━━━━━━━━━━━━━━ 21:58:53\n",
      "Accuracy: 0.8000 - Loss: 0.5644\n",
      "\n",
      "Batch 973/992 ━━━━━━━━━━━━━━━━━━━━ 21:59:03\n",
      "Accuracy: 0.8000 - Loss: 0.5644\n",
      "\n",
      "Batch 974/992 ━━━━━━━━━━━━━━━━━━━━ 21:59:13\n",
      "Accuracy: 0.8001 - Loss: 0.5643\n",
      "\n",
      "Batch 975/992 ━━━━━━━━━━━━━━━━━━━━ 21:59:24\n",
      "Accuracy: 0.8001 - Loss: 0.5641\n",
      "\n",
      "Batch 976/992 ━━━━━━━━━━━━━━━━━━━━ 21:59:34\n",
      "Accuracy: 0.8001 - Loss: 0.5644\n",
      "\n",
      "Batch 977/992 ━━━━━━━━━━━━━━━━━━━━ 21:59:44\n",
      "Accuracy: 0.8003 - Loss: 0.5640\n",
      "\n",
      "Batch 978/992 ━━━━━━━━━━━━━━━━━━━━ 21:59:54\n",
      "Accuracy: 0.8002 - Loss: 0.5641\n",
      "\n",
      "Batch 979/992 ━━━━━━━━━━━━━━━━━━━━ 22:00:04\n",
      "Accuracy: 0.8003 - Loss: 0.5638\n",
      "\n",
      "Batch 980/992 ━━━━━━━━━━━━━━━━━━━━ 22:00:14\n",
      "Accuracy: 0.8003 - Loss: 0.5638\n",
      "\n",
      "Batch 981/992 ━━━━━━━━━━━━━━━━━━━━ 22:00:25\n",
      "Accuracy: 0.8003 - Loss: 0.5636\n",
      "\n",
      "Batch 982/992 ━━━━━━━━━━━━━━━━━━━━ 22:00:35\n",
      "Accuracy: 0.8003 - Loss: 0.5639\n",
      "\n",
      "Batch 983/992 ━━━━━━━━━━━━━━━━━━━━ 22:00:45\n",
      "Accuracy: 0.8001 - Loss: 0.5645\n",
      "\n",
      "Batch 984/992 ━━━━━━━━━━━━━━━━━━━━ 22:00:55\n",
      "Accuracy: 0.8002 - Loss: 0.5643\n",
      "\n",
      "Batch 985/992 ━━━━━━━━━━━━━━━━━━━━ 22:01:05\n",
      "Accuracy: 0.8004 - Loss: 0.5638\n",
      "\n",
      "Batch 986/992 ━━━━━━━━━━━━━━━━━━━━ 22:01:16\n",
      "Accuracy: 0.8005 - Loss: 0.5635\n",
      "\n",
      "Batch 987/992 ━━━━━━━━━━━━━━━━━━━━ 22:01:26\n",
      "Accuracy: 0.8003 - Loss: 0.5636\n",
      "\n",
      "Batch 988/992 ━━━━━━━━━━━━━━━━━━━━ 22:01:36\n",
      "Accuracy: 0.8001 - Loss: 0.5638\n",
      "\n",
      "Batch 989/992 ━━━━━━━━━━━━━━━━━━━━ 22:01:47\n",
      "Accuracy: 0.8001 - Loss: 0.5646\n",
      "\n",
      "Batch 990/992 ━━━━━━━━━━━━━━━━━━━━ 22:01:57\n",
      "Accuracy: 0.8001 - Loss: 0.5644\n",
      "\n",
      "Batch 991/992 ━━━━━━━━━━━━━━━━━━━━ 22:02:08\n",
      "Accuracy: 0.8002 - Loss: 0.5642\n",
      "\n",
      "Batch 992/992 ━━━━━━━━━━━━━━━━━━━━ 22:02:18\n",
      "Accuracy: 0.8004 - Loss: 0.5640\n",
      "\n",
      "\n",
      "Epoch 3/10\n",
      "Batch 1/992 ━━━━━━━━━━━━━━━━━━━━ 22:19:35\n",
      "Accuracy: 0.8750 - Loss: 0.2350\n",
      "\n",
      "Batch 2/992 ━━━━━━━━━━━━━━━━━━━━ 22:19:46\n",
      "Accuracy: 0.8750 - Loss: 0.3407\n",
      "\n",
      "Batch 3/992 ━━━━━━━━━━━━━━━━━━━━ 22:19:56\n",
      "Accuracy: 0.8333 - Loss: 0.5596\n",
      "\n",
      "Batch 4/992 ━━━━━━━━━━━━━━━━━━━━ 22:20:07\n",
      "Accuracy: 0.8438 - Loss: 0.5029\n",
      "\n",
      "Batch 5/992 ━━━━━━━━━━━━━━━━━━━━ 22:20:18\n",
      "Accuracy: 0.8500 - Loss: 0.4689\n",
      "\n",
      "Batch 6/992 ━━━━━━━━━━━━━━━━━━━━ 22:20:28\n",
      "Accuracy: 0.8750 - Loss: 0.3987\n",
      "\n",
      "Batch 7/992 ━━━━━━━━━━━━━━━━━━━━ 22:20:39\n",
      "Accuracy: 0.8750 - Loss: 0.4175\n",
      "\n",
      "Batch 8/992 ━━━━━━━━━━━━━━━━━━━━ 22:20:50\n",
      "Accuracy: 0.8906 - Loss: 0.3954\n",
      "\n",
      "Batch 9/992 ━━━━━━━━━━━━━━━━━━━━ 22:21:01\n",
      "Accuracy: 0.8889 - Loss: 0.3779\n",
      "\n",
      "Batch 10/992 ━━━━━━━━━━━━━━━━━━━━ 22:21:12\n",
      "Accuracy: 0.9000 - Loss: 0.3476\n",
      "\n",
      "Batch 11/992 ━━━━━━━━━━━━━━━━━━━━ 22:21:23\n",
      "Accuracy: 0.8864 - Loss: 0.3638\n",
      "\n",
      "Batch 12/992 ━━━━━━━━━━━━━━━━━━━━ 22:21:37\n",
      "Accuracy: 0.8958 - Loss: 0.3483\n",
      "\n",
      "Batch 13/992 ━━━━━━━━━━━━━━━━━━━━ 22:21:51\n",
      "Accuracy: 0.9038 - Loss: 0.3282\n",
      "\n",
      "Batch 14/992 ━━━━━━━━━━━━━━━━━━━━ 22:22:05\n",
      "Accuracy: 0.8929 - Loss: 0.3562\n",
      "\n",
      "Batch 15/992 ━━━━━━━━━━━━━━━━━━━━ 22:22:16\n",
      "Accuracy: 0.8917 - Loss: 0.3549\n",
      "\n",
      "Batch 16/992 ━━━━━━━━━━━━━━━━━━━━ 22:22:26\n",
      "Accuracy: 0.8828 - Loss: 0.3758\n",
      "\n",
      "Batch 17/992 ━━━━━━━━━━━━━━━━━━━━ 22:22:38\n",
      "Accuracy: 0.8897 - Loss: 0.3628\n",
      "\n",
      "Batch 18/992 ━━━━━━━━━━━━━━━━━━━━ 22:22:48\n",
      "Accuracy: 0.8889 - Loss: 0.3698\n",
      "\n",
      "Batch 19/992 ━━━━━━━━━━━━━━━━━━━━ 22:22:59\n",
      "Accuracy: 0.8882 - Loss: 0.3688\n",
      "\n",
      "Batch 20/992 ━━━━━━━━━━━━━━━━━━━━ 22:23:13\n",
      "Accuracy: 0.8875 - Loss: 0.3749\n",
      "\n",
      "Batch 21/992 ━━━━━━━━━━━━━━━━━━━━ 22:23:25\n",
      "Accuracy: 0.8810 - Loss: 0.3839\n",
      "\n",
      "Batch 22/992 ━━━━━━━━━━━━━━━━━━━━ 22:23:38\n",
      "Accuracy: 0.8864 - Loss: 0.3781\n",
      "\n",
      "Batch 23/992 ━━━━━━━━━━━━━━━━━━━━ 22:23:49\n",
      "Accuracy: 0.8859 - Loss: 0.3851\n",
      "\n",
      "Batch 24/992 ━━━━━━━━━━━━━━━━━━━━ 22:23:59\n",
      "Accuracy: 0.8802 - Loss: 0.4147\n",
      "\n",
      "Batch 25/992 ━━━━━━━━━━━━━━━━━━━━ 22:24:09\n",
      "Accuracy: 0.8800 - Loss: 0.4171\n",
      "\n",
      "Batch 26/992 ━━━━━━━━━━━━━━━━━━━━ 22:24:21\n",
      "Accuracy: 0.8846 - Loss: 0.4068\n",
      "\n",
      "Batch 27/992 ━━━━━━━━━━━━━━━━━━━━ 22:24:33\n",
      "Accuracy: 0.8843 - Loss: 0.4039\n",
      "\n",
      "Batch 28/992 ━━━━━━━━━━━━━━━━━━━━ 22:24:43\n",
      "Accuracy: 0.8750 - Loss: 0.4171\n",
      "\n",
      "Batch 29/992 ━━━━━━━━━━━━━━━━━━━━ 22:24:54\n",
      "Accuracy: 0.8793 - Loss: 0.4081\n",
      "\n",
      "Batch 30/992 ━━━━━━━━━━━━━━━━━━━━ 22:25:04\n",
      "Accuracy: 0.8708 - Loss: 0.4147\n",
      "\n",
      "Batch 31/992 ━━━━━━━━━━━━━━━━━━━━ 22:25:15\n",
      "Accuracy: 0.8750 - Loss: 0.4053\n",
      "\n",
      "Batch 32/992 ━━━━━━━━━━━━━━━━━━━━ 22:25:28\n",
      "Accuracy: 0.8711 - Loss: 0.4089\n",
      "\n",
      "Batch 33/992 ━━━━━━━━━━━━━━━━━━━━ 22:25:41\n",
      "Accuracy: 0.8712 - Loss: 0.4032\n",
      "\n",
      "Batch 34/992 ━━━━━━━━━━━━━━━━━━━━ 22:25:51\n",
      "Accuracy: 0.8676 - Loss: 0.4095\n",
      "\n",
      "Batch 35/992 ━━━━━━━━━━━━━━━━━━━━ 22:26:02\n",
      "Accuracy: 0.8679 - Loss: 0.4063\n",
      "\n",
      "Batch 36/992 ━━━━━━━━━━━━━━━━━━━━ 22:26:15\n",
      "Accuracy: 0.8681 - Loss: 0.4020\n",
      "\n",
      "Batch 37/992 ━━━━━━━━━━━━━━━━━━━━ 22:26:27\n",
      "Accuracy: 0.8716 - Loss: 0.3947\n",
      "\n",
      "Batch 38/992 ━━━━━━━━━━━━━━━━━━━━ 22:26:40\n",
      "Accuracy: 0.8717 - Loss: 0.3970\n",
      "\n",
      "Batch 39/992 ━━━━━━━━━━━━━━━━━━━━ 22:26:52\n",
      "Accuracy: 0.8686 - Loss: 0.3958\n",
      "\n",
      "Batch 40/992 ━━━━━━━━━━━━━━━━━━━━ 22:27:05\n",
      "Accuracy: 0.8719 - Loss: 0.3900\n",
      "\n",
      "Batch 41/992 ━━━━━━━━━━━━━━━━━━━━ 22:27:17\n",
      "Accuracy: 0.8750 - Loss: 0.3822\n",
      "\n",
      "Batch 42/992 ━━━━━━━━━━━━━━━━━━━━ 22:27:27\n",
      "Accuracy: 0.8720 - Loss: 0.3885\n",
      "\n",
      "Batch 43/992 ━━━━━━━━━━━━━━━━━━━━ 22:27:39\n",
      "Accuracy: 0.8692 - Loss: 0.3913\n",
      "\n",
      "Batch 44/992 ━━━━━━━━━━━━━━━━━━━━ 22:27:49\n",
      "Accuracy: 0.8636 - Loss: 0.4061\n",
      "\n",
      "Batch 45/992 ━━━━━━━━━━━━━━━━━━━━ 22:28:00\n",
      "Accuracy: 0.8667 - Loss: 0.3999\n",
      "\n",
      "Batch 46/992 ━━━━━━━━━━━━━━━━━━━━ 22:28:10\n",
      "Accuracy: 0.8696 - Loss: 0.3944\n",
      "\n",
      "Batch 47/992 ━━━━━━━━━━━━━━━━━━━━ 22:28:20\n",
      "Accuracy: 0.8644 - Loss: 0.4022\n",
      "\n",
      "Batch 48/992 ━━━━━━━━━━━━━━━━━━━━ 22:28:32\n",
      "Accuracy: 0.8646 - Loss: 0.4003\n",
      "\n",
      "Batch 49/992 ━━━━━━━━━━━━━━━━━━━━ 22:28:43\n",
      "Accuracy: 0.8622 - Loss: 0.4006\n",
      "\n",
      "Batch 50/992 ━━━━━━━━━━━━━━━━━━━━ 22:28:55\n",
      "Accuracy: 0.8650 - Loss: 0.3934\n",
      "\n",
      "Batch 51/992 ━━━━━━━━━━━━━━━━━━━━ 22:29:06\n",
      "Accuracy: 0.8652 - Loss: 0.3918\n",
      "\n",
      "Batch 52/992 ━━━━━━━━━━━━━━━━━━━━ 22:29:17\n",
      "Accuracy: 0.8630 - Loss: 0.3991\n",
      "\n",
      "Batch 53/992 ━━━━━━━━━━━━━━━━━━━━ 22:29:30\n",
      "Accuracy: 0.8632 - Loss: 0.3975\n",
      "\n",
      "Batch 54/992 ━━━━━━━━━━━━━━━━━━━━ 22:29:43\n",
      "Accuracy: 0.8634 - Loss: 0.3973\n",
      "\n",
      "Batch 55/992 ━━━━━━━━━━━━━━━━━━━━ 22:29:53\n",
      "Accuracy: 0.8636 - Loss: 0.3962\n",
      "\n",
      "Batch 56/992 ━━━━━━━━━━━━━━━━━━━━ 22:30:04\n",
      "Accuracy: 0.8594 - Loss: 0.4023\n",
      "\n",
      "Batch 57/992 ━━━━━━━━━━━━━━━━━━━━ 22:30:14\n",
      "Accuracy: 0.8575 - Loss: 0.4022\n",
      "\n",
      "Batch 58/992 ━━━━━━━━━━━━━━━━━━━━ 22:30:25\n",
      "Accuracy: 0.8599 - Loss: 0.3998\n",
      "\n",
      "Batch 59/992 ━━━━━━━━━━━━━━━━━━━━ 22:30:37\n",
      "Accuracy: 0.8559 - Loss: 0.4041\n",
      "\n",
      "Batch 60/992 ━━━━━━━━━━━━━━━━━━━━ 22:30:47\n",
      "Accuracy: 0.8562 - Loss: 0.4057\n",
      "\n",
      "Batch 61/992 ━━━━━━━━━━━━━━━━━━━━ 22:30:57\n",
      "Accuracy: 0.8545 - Loss: 0.4065\n",
      "\n",
      "Batch 62/992 ━━━━━━━━━━━━━━━━━━━━ 22:31:08\n",
      "Accuracy: 0.8548 - Loss: 0.4036\n",
      "\n",
      "Batch 63/992 ━━━━━━━━━━━━━━━━━━━━ 22:31:21\n",
      "Accuracy: 0.8532 - Loss: 0.4065\n",
      "\n",
      "Batch 64/992 ━━━━━━━━━━━━━━━━━━━━ 22:31:34\n",
      "Accuracy: 0.8555 - Loss: 0.4039\n",
      "\n",
      "Batch 65/992 ━━━━━━━━━━━━━━━━━━━━ 22:31:46\n",
      "Accuracy: 0.8577 - Loss: 0.4014\n",
      "\n",
      "Batch 66/992 ━━━━━━━━━━━━━━━━━━━━ 22:32:00\n",
      "Accuracy: 0.8580 - Loss: 0.3995\n",
      "\n",
      "Batch 67/992 ━━━━━━━━━━━━━━━━━━━━ 22:32:12\n",
      "Accuracy: 0.8582 - Loss: 0.3994\n",
      "\n",
      "Batch 68/992 ━━━━━━━━━━━━━━━━━━━━ 22:32:23\n",
      "Accuracy: 0.8566 - Loss: 0.4015\n",
      "\n",
      "Batch 69/992 ━━━━━━━━━━━━━━━━━━━━ 22:32:35\n",
      "Accuracy: 0.8587 - Loss: 0.3976\n",
      "\n",
      "Batch 70/992 ━━━━━━━━━━━━━━━━━━━━ 22:32:48\n",
      "Accuracy: 0.8589 - Loss: 0.3963\n",
      "\n",
      "Batch 71/992 ━━━━━━━━━━━━━━━━━━━━ 22:33:03\n",
      "Accuracy: 0.8592 - Loss: 0.3942\n",
      "\n",
      "Batch 72/992 ━━━━━━━━━━━━━━━━━━━━ 22:33:18\n",
      "Accuracy: 0.8594 - Loss: 0.3919\n",
      "\n",
      "Batch 73/992 ━━━━━━━━━━━━━━━━━━━━ 22:33:33\n",
      "Accuracy: 0.8579 - Loss: 0.4005\n",
      "\n",
      "Batch 74/992 ━━━━━━━━━━━━━━━━━━━━ 22:33:48\n",
      "Accuracy: 0.8598 - Loss: 0.3967\n",
      "\n",
      "Batch 75/992 ━━━━━━━━━━━━━━━━━━━━ 22:34:03\n",
      "Accuracy: 0.8617 - Loss: 0.3924\n",
      "\n",
      "Batch 76/992 ━━━━━━━━━━━━━━━━━━━━ 22:34:14\n",
      "Accuracy: 0.8618 - Loss: 0.3941\n",
      "\n",
      "Batch 77/992 ━━━━━━━━━━━━━━━━━━━━ 22:34:26\n",
      "Accuracy: 0.8571 - Loss: 0.4005\n",
      "\n",
      "Batch 78/992 ━━━━━━━━━━━━━━━━━━━━ 22:34:40\n",
      "Accuracy: 0.8558 - Loss: 0.4079\n",
      "\n",
      "Batch 79/992 ━━━━━━━━━━━━━━━━━━━━ 22:34:52\n",
      "Accuracy: 0.8576 - Loss: 0.4051\n",
      "\n",
      "Batch 80/992 ━━━━━━━━━━━━━━━━━━━━ 22:35:03\n",
      "Accuracy: 0.8594 - Loss: 0.4008\n",
      "\n",
      "Batch 81/992 ━━━━━━━━━━━━━━━━━━━━ 22:35:18\n",
      "Accuracy: 0.8596 - Loss: 0.3974\n",
      "\n",
      "Batch 82/992 ━━━━━━━━━━━━━━━━━━━━ 22:35:33\n",
      "Accuracy: 0.8582 - Loss: 0.4022\n",
      "\n",
      "Batch 83/992 ━━━━━━━━━━━━━━━━━━━━ 22:35:48\n",
      "Accuracy: 0.8584 - Loss: 0.4001\n",
      "\n",
      "Batch 84/992 ━━━━━━━━━━━━━━━━━━━━ 22:36:02\n",
      "Accuracy: 0.8542 - Loss: 0.4068\n",
      "\n",
      "Batch 85/992 ━━━━━━━━━━━━━━━━━━━━ 22:36:12\n",
      "Accuracy: 0.8544 - Loss: 0.4070\n",
      "\n",
      "Batch 86/992 ━━━━━━━━━━━━━━━━━━━━ 22:36:26\n",
      "Accuracy: 0.8517 - Loss: 0.4167\n",
      "\n",
      "Batch 87/992 ━━━━━━━━━━━━━━━━━━━━ 22:36:37\n",
      "Accuracy: 0.8506 - Loss: 0.4195\n",
      "\n",
      "Batch 88/992 ━━━━━━━━━━━━━━━━━━━━ 22:36:48\n",
      "Accuracy: 0.8509 - Loss: 0.4177\n",
      "\n",
      "Batch 89/992 ━━━━━━━━━━━━━━━━━━━━ 22:36:58\n",
      "Accuracy: 0.8525 - Loss: 0.4153\n",
      "\n",
      "Batch 90/992 ━━━━━━━━━━━━━━━━━━━━ 22:37:10\n",
      "Accuracy: 0.8528 - Loss: 0.4169\n",
      "\n",
      "Batch 91/992 ━━━━━━━━━━━━━━━━━━━━ 22:37:21\n",
      "Accuracy: 0.8503 - Loss: 0.4234\n",
      "\n",
      "Batch 92/992 ━━━━━━━━━━━━━━━━━━━━ 22:37:35\n",
      "Accuracy: 0.8519 - Loss: 0.4199\n",
      "\n",
      "Batch 93/992 ━━━━━━━━━━━━━━━━━━━━ 22:37:49\n",
      "Accuracy: 0.8535 - Loss: 0.4163\n",
      "\n",
      "Batch 94/992 ━━━━━━━━━━━━━━━━━━━━ 22:38:01\n",
      "Accuracy: 0.8551 - Loss: 0.4155\n",
      "\n",
      "Batch 95/992 ━━━━━━━━━━━━━━━━━━━━ 22:38:12\n",
      "Accuracy: 0.8566 - Loss: 0.4120\n",
      "\n",
      "Batch 96/992 ━━━━━━━━━━━━━━━━━━━━ 22:38:23\n",
      "Accuracy: 0.8568 - Loss: 0.4130\n",
      "\n",
      "Batch 97/992 ━━━━━━━━━━━━━━━━━━━━ 22:38:34\n",
      "Accuracy: 0.8544 - Loss: 0.4193\n",
      "\n",
      "Batch 98/992 ━━━━━━━━━━━━━━━━━━━━ 22:38:45\n",
      "Accuracy: 0.8520 - Loss: 0.4214\n",
      "\n",
      "Batch 99/992 ━━━━━━━━━━━━━━━━━━━━ 22:38:56\n",
      "Accuracy: 0.8535 - Loss: 0.4179\n",
      "\n",
      "Batch 100/992 ━━━━━━━━━━━━━━━━━━━━ 22:39:08\n",
      "Accuracy: 0.8537 - Loss: 0.4174\n",
      "\n",
      "Batch 101/992 ━━━━━━━━━━━━━━━━━━━━ 22:39:22\n",
      "Accuracy: 0.8527 - Loss: 0.4218\n",
      "\n",
      "Batch 102/992 ━━━━━━━━━━━━━━━━━━━━ 22:39:35\n",
      "Accuracy: 0.8529 - Loss: 0.4202\n",
      "\n",
      "Batch 103/992 ━━━━━━━━━━━━━━━━━━━━ 22:39:48\n",
      "Accuracy: 0.8532 - Loss: 0.4178\n",
      "\n",
      "Batch 104/992 ━━━━━━━━━━━━━━━━━━━━ 22:40:00\n",
      "Accuracy: 0.8498 - Loss: 0.4235\n",
      "\n",
      "Batch 105/992 ━━━━━━━━━━━━━━━━━━━━ 22:40:13\n",
      "Accuracy: 0.8512 - Loss: 0.4219\n",
      "\n",
      "Batch 106/992 ━━━━━━━━━━━━━━━━━━━━ 22:40:26\n",
      "Accuracy: 0.8526 - Loss: 0.4207\n",
      "\n",
      "Batch 107/992 ━━━━━━━━━━━━━━━━━━━━ 22:40:37\n",
      "Accuracy: 0.8540 - Loss: 0.4182\n",
      "\n",
      "Batch 108/992 ━━━━━━━━━━━━━━━━━━━━ 22:40:47\n",
      "Accuracy: 0.8542 - Loss: 0.4168\n",
      "\n",
      "Batch 109/992 ━━━━━━━━━━━━━━━━━━━━ 22:40:58\n",
      "Accuracy: 0.8555 - Loss: 0.4141\n",
      "\n",
      "Batch 110/992 ━━━━━━━━━━━━━━━━━━━━ 22:41:08\n",
      "Accuracy: 0.8557 - Loss: 0.4139\n",
      "\n",
      "Batch 111/992 ━━━━━━━━━━━━━━━━━━━━ 22:41:22\n",
      "Accuracy: 0.8559 - Loss: 0.4122\n",
      "\n",
      "Batch 112/992 ━━━━━━━━━━━━━━━━━━━━ 22:41:35\n",
      "Accuracy: 0.8538 - Loss: 0.4138\n",
      "\n",
      "Batch 113/992 ━━━━━━━━━━━━━━━━━━━━ 22:41:50\n",
      "Accuracy: 0.8540 - Loss: 0.4130\n",
      "\n",
      "Batch 114/992 ━━━━━━━━━━━━━━━━━━━━ 22:42:03\n",
      "Accuracy: 0.8553 - Loss: 0.4102\n",
      "\n",
      "Batch 115/992 ━━━━━━━━━━━━━━━━━━━━ 22:42:17\n",
      "Accuracy: 0.8522 - Loss: 0.4132\n",
      "\n",
      "Batch 116/992 ━━━━━━━━━━━━━━━━━━━━ 22:42:31\n",
      "Accuracy: 0.8513 - Loss: 0.4142\n",
      "\n",
      "Batch 117/992 ━━━━━━━━━━━━━━━━━━━━ 22:42:45\n",
      "Accuracy: 0.8515 - Loss: 0.4134\n",
      "\n",
      "Batch 118/992 ━━━━━━━━━━━━━━━━━━━━ 22:42:58\n",
      "Accuracy: 0.8528 - Loss: 0.4115\n",
      "\n",
      "Batch 119/992 ━━━━━━━━━━━━━━━━━━━━ 22:43:11\n",
      "Accuracy: 0.8529 - Loss: 0.4113\n",
      "\n",
      "Batch 120/992 ━━━━━━━━━━━━━━━━━━━━ 22:43:25\n",
      "Accuracy: 0.8521 - Loss: 0.4110\n",
      "\n",
      "Batch 121/992 ━━━━━━━━━━━━━━━━━━━━ 22:43:39\n",
      "Accuracy: 0.8523 - Loss: 0.4096\n",
      "\n",
      "Batch 122/992 ━━━━━━━━━━━━━━━━━━━━ 22:43:53\n",
      "Accuracy: 0.8525 - Loss: 0.4080\n",
      "\n",
      "Batch 123/992 ━━━━━━━━━━━━━━━━━━━━ 22:44:07\n",
      "Accuracy: 0.8526 - Loss: 0.4081\n",
      "\n",
      "Batch 124/992 ━━━━━━━━━━━━━━━━━━━━ 22:44:21\n",
      "Accuracy: 0.8498 - Loss: 0.4127\n",
      "\n",
      "Batch 125/992 ━━━━━━━━━━━━━━━━━━━━ 22:44:33\n",
      "Accuracy: 0.8500 - Loss: 0.4119\n",
      "\n",
      "Batch 126/992 ━━━━━━━━━━━━━━━━━━━━ 22:44:44\n",
      "Accuracy: 0.8502 - Loss: 0.4132\n",
      "\n",
      "Batch 127/992 ━━━━━━━━━━━━━━━━━━━━ 22:44:56\n",
      "Accuracy: 0.8514 - Loss: 0.4109\n",
      "\n",
      "Batch 128/992 ━━━━━━━━━━━━━━━━━━━━ 22:45:08\n",
      "Accuracy: 0.8516 - Loss: 0.4118\n",
      "\n",
      "Batch 129/992 ━━━━━━━━━━━━━━━━━━━━ 22:45:21\n",
      "Accuracy: 0.8508 - Loss: 0.4112\n",
      "\n",
      "Batch 130/992 ━━━━━━━━━━━━━━━━━━━━ 22:45:36\n",
      "Accuracy: 0.8500 - Loss: 0.4119\n",
      "\n",
      "Batch 131/992 ━━━━━━━━━━━━━━━━━━━━ 22:45:49\n",
      "Accuracy: 0.8502 - Loss: 0.4132\n",
      "\n",
      "Batch 132/992 ━━━━━━━━━━━━━━━━━━━━ 22:46:01\n",
      "Accuracy: 0.8494 - Loss: 0.4131\n",
      "\n",
      "Batch 133/992 ━━━━━━━━━━━━━━━━━━━━ 22:46:12\n",
      "Accuracy: 0.8487 - Loss: 0.4149\n",
      "\n",
      "Batch 134/992 ━━━━━━━━━━━━━━━━━━━━ 22:46:23\n",
      "Accuracy: 0.8498 - Loss: 0.4140\n",
      "\n",
      "Batch 135/992 ━━━━━━━━━━━━━━━━━━━━ 22:46:34\n",
      "Accuracy: 0.8500 - Loss: 0.4128\n",
      "\n",
      "Batch 136/992 ━━━━━━━━━━━━━━━━━━━━ 22:46:48\n",
      "Accuracy: 0.8502 - Loss: 0.4125\n",
      "\n",
      "Batch 137/992 ━━━━━━━━━━━━━━━━━━━━ 22:47:02\n",
      "Accuracy: 0.8504 - Loss: 0.4117\n",
      "\n",
      "Batch 138/992 ━━━━━━━━━━━━━━━━━━━━ 22:47:15\n",
      "Accuracy: 0.8496 - Loss: 0.4141\n",
      "\n",
      "Batch 139/992 ━━━━━━━━━━━━━━━━━━━━ 22:47:30\n",
      "Accuracy: 0.8480 - Loss: 0.4161\n",
      "\n",
      "Batch 140/992 ━━━━━━━━━━━━━━━━━━━━ 22:47:42\n",
      "Accuracy: 0.8491 - Loss: 0.4142\n",
      "\n",
      "Batch 141/992 ━━━━━━━━━━━━━━━━━━━━ 22:47:53\n",
      "Accuracy: 0.8502 - Loss: 0.4120\n",
      "\n",
      "Batch 142/992 ━━━━━━━━━━━━━━━━━━━━ 22:48:03\n",
      "Accuracy: 0.8495 - Loss: 0.4121\n",
      "\n",
      "Batch 143/992 ━━━━━━━━━━━━━━━━━━━━ 22:48:15\n",
      "Accuracy: 0.8497 - Loss: 0.4118\n",
      "\n",
      "Batch 144/992 ━━━━━━━━━━━━━━━━━━━━ 22:48:26\n",
      "Accuracy: 0.8507 - Loss: 0.4104\n",
      "\n",
      "Batch 145/992 ━━━━━━━━━━━━━━━━━━━━ 22:48:37\n",
      "Accuracy: 0.8509 - Loss: 0.4098\n",
      "\n",
      "Batch 146/992 ━━━━━━━━━━━━━━━━━━━━ 22:48:48\n",
      "Accuracy: 0.8510 - Loss: 0.4126\n",
      "\n",
      "Batch 147/992 ━━━━━━━━━━━━━━━━━━━━ 22:48:59\n",
      "Accuracy: 0.8503 - Loss: 0.4135\n",
      "\n",
      "Batch 148/992 ━━━━━━━━━━━━━━━━━━━━ 22:49:11\n",
      "Accuracy: 0.8514 - Loss: 0.4126\n",
      "\n",
      "Batch 149/992 ━━━━━━━━━━━━━━━━━━━━ 22:49:22\n",
      "Accuracy: 0.8523 - Loss: 0.4107\n",
      "\n",
      "Batch 150/992 ━━━━━━━━━━━━━━━━━━━━ 22:49:34\n",
      "Accuracy: 0.8525 - Loss: 0.4096\n",
      "\n",
      "Batch 151/992 ━━━━━━━━━━━━━━━━━━━━ 22:49:48\n",
      "Accuracy: 0.8526 - Loss: 0.4096\n",
      "\n",
      "Batch 152/992 ━━━━━━━━━━━━━━━━━━━━ 22:50:01\n",
      "Accuracy: 0.8528 - Loss: 0.4089\n",
      "\n",
      "Batch 153/992 ━━━━━━━━━━━━━━━━━━━━ 22:50:12\n",
      "Accuracy: 0.8529 - Loss: 0.4089\n",
      "\n",
      "Batch 154/992 ━━━━━━━━━━━━━━━━━━━━ 22:50:25\n",
      "Accuracy: 0.8539 - Loss: 0.4068\n",
      "\n",
      "Batch 155/992 ━━━━━━━━━━━━━━━━━━━━ 22:50:40\n",
      "Accuracy: 0.8548 - Loss: 0.4061\n",
      "\n",
      "Batch 156/992 ━━━━━━━━━━━━━━━━━━━━ 22:50:52\n",
      "Accuracy: 0.8550 - Loss: 0.4054\n",
      "\n",
      "Batch 157/992 ━━━━━━━━━━━━━━━━━━━━ 22:51:04\n",
      "Accuracy: 0.8543 - Loss: 0.4066\n",
      "\n",
      "Batch 158/992 ━━━━━━━━━━━━━━━━━━━━ 22:51:15\n",
      "Accuracy: 0.8544 - Loss: 0.4082\n",
      "\n",
      "Batch 159/992 ━━━━━━━━━━━━━━━━━━━━ 22:51:26\n",
      "Accuracy: 0.8546 - Loss: 0.4078\n",
      "\n",
      "Batch 160/992 ━━━━━━━━━━━━━━━━━━━━ 22:51:39\n",
      "Accuracy: 0.8547 - Loss: 0.4085\n",
      "\n",
      "Batch 161/992 ━━━━━━━━━━━━━━━━━━━━ 22:51:52\n",
      "Accuracy: 0.8548 - Loss: 0.4075\n",
      "\n",
      "Batch 162/992 ━━━━━━━━━━━━━━━━━━━━ 22:52:04\n",
      "Accuracy: 0.8542 - Loss: 0.4076\n",
      "\n",
      "Batch 163/992 ━━━━━━━━━━━━━━━━━━━━ 22:52:15\n",
      "Accuracy: 0.8551 - Loss: 0.4057\n",
      "\n",
      "Batch 164/992 ━━━━━━━━━━━━━━━━━━━━ 22:52:26\n",
      "Accuracy: 0.8544 - Loss: 0.4059\n",
      "\n",
      "Batch 165/992 ━━━━━━━━━━━━━━━━━━━━ 22:52:38\n",
      "Accuracy: 0.8553 - Loss: 0.4044\n",
      "\n",
      "Batch 166/992 ━━━━━━━━━━━━━━━━━━━━ 22:52:49\n",
      "Accuracy: 0.8562 - Loss: 0.4035\n",
      "\n",
      "Batch 167/992 ━━━━━━━━━━━━━━━━━━━━ 22:52:59\n",
      "Accuracy: 0.8570 - Loss: 0.4017\n",
      "\n",
      "Batch 168/992 ━━━━━━━━━━━━━━━━━━━━ 22:53:10\n",
      "Accuracy: 0.8571 - Loss: 0.4020\n",
      "\n",
      "Batch 169/992 ━━━━━━━━━━━━━━━━━━━━ 22:53:20\n",
      "Accuracy: 0.8572 - Loss: 0.4009\n",
      "\n",
      "Batch 170/992 ━━━━━━━━━━━━━━━━━━━━ 22:53:31\n",
      "Accuracy: 0.8574 - Loss: 0.4004\n",
      "\n",
      "Batch 171/992 ━━━━━━━━━━━━━━━━━━━━ 22:53:44\n",
      "Accuracy: 0.8575 - Loss: 0.4005\n",
      "\n",
      "Batch 172/992 ━━━━━━━━━━━━━━━━━━━━ 22:53:55\n",
      "Accuracy: 0.8576 - Loss: 0.4011\n",
      "\n",
      "Batch 173/992 ━━━━━━━━━━━━━━━━━━━━ 22:54:06\n",
      "Accuracy: 0.8584 - Loss: 0.3999\n",
      "\n",
      "Batch 174/992 ━━━━━━━━━━━━━━━━━━━━ 22:54:17\n",
      "Accuracy: 0.8570 - Loss: 0.4009\n",
      "\n",
      "Batch 175/992 ━━━━━━━━━━━━━━━━━━━━ 22:54:27\n",
      "Accuracy: 0.8571 - Loss: 0.4011\n",
      "\n",
      "Batch 176/992 ━━━━━━━━━━━━━━━━━━━━ 22:54:37\n",
      "Accuracy: 0.8572 - Loss: 0.4022\n",
      "\n",
      "Batch 177/992 ━━━━━━━━━━━━━━━━━━━━ 22:54:47\n",
      "Accuracy: 0.8566 - Loss: 0.4016\n",
      "\n",
      "Batch 178/992 ━━━━━━━━━━━━━━━━━━━━ 22:54:58\n",
      "Accuracy: 0.8574 - Loss: 0.4011\n",
      "\n",
      "Batch 179/992 ━━━━━━━━━━━━━━━━━━━━ 22:55:08\n",
      "Accuracy: 0.8582 - Loss: 0.3991\n",
      "\n",
      "Batch 180/992 ━━━━━━━━━━━━━━━━━━━━ 22:55:18\n",
      "Accuracy: 0.8590 - Loss: 0.3972\n",
      "\n",
      "Batch 181/992 ━━━━━━━━━━━━━━━━━━━━ 22:55:29\n",
      "Accuracy: 0.8584 - Loss: 0.3984\n",
      "\n",
      "Batch 182/992 ━━━━━━━━━━━━━━━━━━━━ 22:55:40\n",
      "Accuracy: 0.8585 - Loss: 0.3988\n",
      "\n",
      "Batch 183/992 ━━━━━━━━━━━━━━━━━━━━ 22:55:51\n",
      "Accuracy: 0.8593 - Loss: 0.3969\n",
      "\n",
      "Batch 184/992 ━━━━━━━━━━━━━━━━━━━━ 22:56:02\n",
      "Accuracy: 0.8601 - Loss: 0.3952\n",
      "\n",
      "Batch 185/992 ━━━━━━━━━━━━━━━━━━━━ 22:56:13\n",
      "Accuracy: 0.8601 - Loss: 0.3949\n",
      "\n",
      "Batch 186/992 ━━━━━━━━━━━━━━━━━━━━ 22:56:23\n",
      "Accuracy: 0.8589 - Loss: 0.3965\n",
      "\n",
      "Batch 187/992 ━━━━━━━━━━━━━━━━━━━━ 22:56:33\n",
      "Accuracy: 0.8590 - Loss: 0.3968\n",
      "\n",
      "Batch 188/992 ━━━━━━━━━━━━━━━━━━━━ 22:56:44\n",
      "Accuracy: 0.8584 - Loss: 0.3994\n",
      "\n",
      "Batch 189/992 ━━━━━━━━━━━━━━━━━━━━ 22:56:54\n",
      "Accuracy: 0.8591 - Loss: 0.3978\n",
      "\n",
      "Batch 190/992 ━━━━━━━━━━━━━━━━━━━━ 22:57:04\n",
      "Accuracy: 0.8592 - Loss: 0.3970\n",
      "\n",
      "Batch 191/992 ━━━━━━━━━━━━━━━━━━━━ 22:57:15\n",
      "Accuracy: 0.8580 - Loss: 0.3997\n",
      "\n",
      "Batch 192/992 ━━━━━━━━━━━━━━━━━━━━ 22:57:27\n",
      "Accuracy: 0.8587 - Loss: 0.3988\n",
      "\n",
      "Batch 193/992 ━━━━━━━━━━━━━━━━━━━━ 22:57:39\n",
      "Accuracy: 0.8582 - Loss: 0.4012\n",
      "\n",
      "Batch 194/992 ━━━━━━━━━━━━━━━━━━━━ 22:57:51\n",
      "Accuracy: 0.8589 - Loss: 0.4005\n",
      "\n",
      "Batch 195/992 ━━━━━━━━━━━━━━━━━━━━ 22:58:01\n",
      "Accuracy: 0.8590 - Loss: 0.4006\n",
      "\n",
      "Batch 196/992 ━━━━━━━━━━━━━━━━━━━━ 22:58:13\n",
      "Accuracy: 0.8584 - Loss: 0.4014\n",
      "\n",
      "Batch 197/992 ━━━━━━━━━━━━━━━━━━━━ 22:58:25\n",
      "Accuracy: 0.8591 - Loss: 0.4005\n",
      "\n",
      "Batch 198/992 ━━━━━━━━━━━━━━━━━━━━ 22:58:35\n",
      "Accuracy: 0.8592 - Loss: 0.3995\n",
      "\n",
      "Batch 199/992 ━━━━━━━━━━━━━━━━━━━━ 22:58:45\n",
      "Accuracy: 0.8580 - Loss: 0.3994\n",
      "\n",
      "Batch 200/992 ━━━━━━━━━━━━━━━━━━━━ 22:58:55\n",
      "Accuracy: 0.8587 - Loss: 0.3981\n",
      "\n",
      "Batch 201/992 ━━━━━━━━━━━━━━━━━━━━ 22:59:05\n",
      "Accuracy: 0.8582 - Loss: 0.3978\n",
      "\n",
      "Batch 202/992 ━━━━━━━━━━━━━━━━━━━━ 22:59:17\n",
      "Accuracy: 0.8589 - Loss: 0.3970\n",
      "\n",
      "Batch 203/992 ━━━━━━━━━━━━━━━━━━━━ 22:59:28\n",
      "Accuracy: 0.8590 - Loss: 0.3974\n",
      "\n",
      "Batch 204/992 ━━━━━━━━━━━━━━━━━━━━ 22:59:41\n",
      "Accuracy: 0.8597 - Loss: 0.3961\n",
      "\n",
      "Batch 205/992 ━━━━━━━━━━━━━━━━━━━━ 22:59:51\n",
      "Accuracy: 0.8591 - Loss: 0.3991\n",
      "\n",
      "Batch 206/992 ━━━━━━━━━━━━━━━━━━━━ 23:00:02\n",
      "Accuracy: 0.8586 - Loss: 0.4013\n",
      "\n",
      "Batch 207/992 ━━━━━━━━━━━━━━━━━━━━ 23:00:12\n",
      "Accuracy: 0.8587 - Loss: 0.4012\n",
      "\n",
      "Batch 208/992 ━━━━━━━━━━━━━━━━━━━━ 23:00:23\n",
      "Accuracy: 0.8570 - Loss: 0.4040\n",
      "\n",
      "Batch 209/992 ━━━━━━━━━━━━━━━━━━━━ 23:00:34\n",
      "Accuracy: 0.8577 - Loss: 0.4032\n",
      "\n",
      "Batch 210/992 ━━━━━━━━━━━━━━━━━━━━ 23:00:45\n",
      "Accuracy: 0.8577 - Loss: 0.4027\n",
      "\n",
      "Batch 211/992 ━━━━━━━━━━━━━━━━━━━━ 23:00:57\n",
      "Accuracy: 0.8566 - Loss: 0.4043\n",
      "\n",
      "Batch 212/992 ━━━━━━━━━━━━━━━━━━━━ 23:01:08\n",
      "Accuracy: 0.8555 - Loss: 0.4073\n",
      "\n",
      "Batch 213/992 ━━━━━━━━━━━━━━━━━━━━ 23:01:19\n",
      "Accuracy: 0.8556 - Loss: 0.4066\n",
      "\n",
      "Batch 214/992 ━━━━━━━━━━━━━━━━━━━━ 23:01:30\n",
      "Accuracy: 0.8546 - Loss: 0.4067\n",
      "\n",
      "Batch 215/992 ━━━━━━━━━━━━━━━━━━━━ 23:01:41\n",
      "Accuracy: 0.8552 - Loss: 0.4057\n",
      "\n",
      "Batch 216/992 ━━━━━━━━━━━━━━━━━━━━ 23:01:52\n",
      "Accuracy: 0.8559 - Loss: 0.4045\n",
      "\n",
      "Batch 217/992 ━━━━━━━━━━━━━━━━━━━━ 23:02:03\n",
      "Accuracy: 0.8566 - Loss: 0.4033\n",
      "\n",
      "Batch 218/992 ━━━━━━━━━━━━━━━━━━━━ 23:02:14\n",
      "Accuracy: 0.8567 - Loss: 0.4040\n",
      "\n",
      "Batch 219/992 ━━━━━━━━━━━━━━━━━━━━ 23:02:24\n",
      "Accuracy: 0.8573 - Loss: 0.4029\n",
      "\n",
      "Batch 220/992 ━━━━━━━━━━━━━━━━━━━━ 23:02:36\n",
      "Accuracy: 0.8580 - Loss: 0.4013\n",
      "\n",
      "Batch 221/992 ━━━━━━━━━━━━━━━━━━━━ 23:02:48\n",
      "Accuracy: 0.8580 - Loss: 0.4005\n",
      "\n",
      "Batch 222/992 ━━━━━━━━━━━━━━━━━━━━ 23:02:59\n",
      "Accuracy: 0.8581 - Loss: 0.4005\n",
      "\n",
      "Batch 223/992 ━━━━━━━━━━━━━━━━━━━━ 23:03:11\n",
      "Accuracy: 0.8582 - Loss: 0.4005\n",
      "\n",
      "Batch 224/992 ━━━━━━━━━━━━━━━━━━━━ 23:03:22\n",
      "Accuracy: 0.8577 - Loss: 0.4016\n",
      "\n",
      "Batch 225/992 ━━━━━━━━━━━━━━━━━━━━ 23:03:33\n",
      "Accuracy: 0.8578 - Loss: 0.4008\n",
      "\n",
      "Batch 226/992 ━━━━━━━━━━━━━━━━━━━━ 23:03:44\n",
      "Accuracy: 0.8567 - Loss: 0.4028\n",
      "\n",
      "Batch 227/992 ━━━━━━━━━━━━━━━━━━━━ 23:03:54\n",
      "Accuracy: 0.8574 - Loss: 0.4011\n",
      "\n",
      "Batch 228/992 ━━━━━━━━━━━━━━━━━━━━ 23:04:04\n",
      "Accuracy: 0.8564 - Loss: 0.4028\n",
      "\n",
      "Batch 229/992 ━━━━━━━━━━━━━━━━━━━━ 23:04:14\n",
      "Accuracy: 0.8570 - Loss: 0.4018\n",
      "\n",
      "Batch 230/992 ━━━━━━━━━━━━━━━━━━━━ 23:04:25\n",
      "Accuracy: 0.8565 - Loss: 0.4025\n",
      "\n",
      "Batch 231/992 ━━━━━━━━━━━━━━━━━━━━ 23:04:36\n",
      "Accuracy: 0.8561 - Loss: 0.4038\n",
      "\n",
      "Batch 232/992 ━━━━━━━━━━━━━━━━━━━━ 23:04:48\n",
      "Accuracy: 0.8561 - Loss: 0.4057\n",
      "\n",
      "Batch 233/992 ━━━━━━━━━━━━━━━━━━━━ 23:04:59\n",
      "Accuracy: 0.8568 - Loss: 0.4046\n",
      "\n",
      "Batch 234/992 ━━━━━━━━━━━━━━━━━━━━ 23:05:10\n",
      "Accuracy: 0.8563 - Loss: 0.4057\n",
      "\n",
      "Batch 235/992 ━━━━━━━━━━━━━━━━━━━━ 23:05:21\n",
      "Accuracy: 0.8559 - Loss: 0.4073\n",
      "\n",
      "Batch 236/992 ━━━━━━━━━━━━━━━━━━━━ 23:05:32\n",
      "Accuracy: 0.8559 - Loss: 0.4073\n",
      "\n",
      "Batch 237/992 ━━━━━━━━━━━━━━━━━━━━ 23:05:43\n",
      "Accuracy: 0.8560 - Loss: 0.4072\n",
      "\n",
      "Batch 238/992 ━━━━━━━━━━━━━━━━━━━━ 23:05:53\n",
      "Accuracy: 0.8566 - Loss: 0.4068\n",
      "\n",
      "Batch 239/992 ━━━━━━━━━━━━━━━━━━━━ 23:06:04\n",
      "Accuracy: 0.8567 - Loss: 0.4063\n",
      "\n",
      "Batch 240/992 ━━━━━━━━━━━━━━━━━━━━ 23:06:14\n",
      "Accuracy: 0.8568 - Loss: 0.4062\n",
      "\n",
      "Batch 241/992 ━━━━━━━━━━━━━━━━━━━━ 23:06:25\n",
      "Accuracy: 0.8563 - Loss: 0.4069\n",
      "\n",
      "Batch 242/992 ━━━━━━━━━━━━━━━━━━━━ 23:06:37\n",
      "Accuracy: 0.8569 - Loss: 0.4063\n",
      "\n",
      "Batch 243/992 ━━━━━━━━━━━━━━━━━━━━ 23:06:49\n",
      "Accuracy: 0.8575 - Loss: 0.4057\n",
      "\n",
      "Batch 244/992 ━━━━━━━━━━━━━━━━━━━━ 23:07:01\n",
      "Accuracy: 0.8581 - Loss: 0.4044\n",
      "\n",
      "Batch 245/992 ━━━━━━━━━━━━━━━━━━━━ 23:07:12\n",
      "Accuracy: 0.8582 - Loss: 0.4052\n",
      "\n",
      "Batch 246/992 ━━━━━━━━━━━━━━━━━━━━ 23:07:23\n",
      "Accuracy: 0.8582 - Loss: 0.4048\n",
      "\n",
      "Batch 247/992 ━━━━━━━━━━━━━━━━━━━━ 23:07:34\n",
      "Accuracy: 0.8578 - Loss: 0.4053\n",
      "\n",
      "Batch 248/992 ━━━━━━━━━━━━━━━━━━━━ 23:07:45\n",
      "Accuracy: 0.8569 - Loss: 0.4067\n",
      "\n",
      "Batch 249/992 ━━━━━━━━━━━━━━━━━━━━ 23:07:55\n",
      "Accuracy: 0.8564 - Loss: 0.4065\n",
      "\n",
      "Batch 250/992 ━━━━━━━━━━━━━━━━━━━━ 23:08:06\n",
      "Accuracy: 0.8565 - Loss: 0.4059\n",
      "\n",
      "Batch 251/992 ━━━━━━━━━━━━━━━━━━━━ 23:08:17\n",
      "Accuracy: 0.8561 - Loss: 0.4069\n",
      "\n",
      "Batch 252/992 ━━━━━━━━━━━━━━━━━━━━ 23:08:28\n",
      "Accuracy: 0.8557 - Loss: 0.4074\n",
      "\n",
      "Batch 253/992 ━━━━━━━━━━━━━━━━━━━━ 23:08:40\n",
      "Accuracy: 0.8557 - Loss: 0.4077\n",
      "\n",
      "Batch 254/992 ━━━━━━━━━━━━━━━━━━━━ 23:08:53\n",
      "Accuracy: 0.8558 - Loss: 0.4074\n",
      "\n",
      "Batch 255/992 ━━━━━━━━━━━━━━━━━━━━ 23:09:04\n",
      "Accuracy: 0.8559 - Loss: 0.4071\n",
      "\n",
      "Batch 256/992 ━━━━━━━━━━━━━━━━━━━━ 23:09:17\n",
      "Accuracy: 0.8550 - Loss: 0.4080\n",
      "\n",
      "Batch 257/992 ━━━━━━━━━━━━━━━━━━━━ 23:09:28\n",
      "Accuracy: 0.8541 - Loss: 0.4099\n",
      "\n",
      "Batch 258/992 ━━━━━━━━━━━━━━━━━━━━ 23:09:41\n",
      "Accuracy: 0.8547 - Loss: 0.4088\n",
      "\n",
      "Batch 259/992 ━━━━━━━━━━━━━━━━━━━━ 23:09:52\n",
      "Accuracy: 0.8547 - Loss: 0.4085\n",
      "\n",
      "Batch 260/992 ━━━━━━━━━━━━━━━━━━━━ 23:10:03\n",
      "Accuracy: 0.8553 - Loss: 0.4071\n",
      "\n",
      "Batch 261/992 ━━━━━━━━━━━━━━━━━━━━ 23:10:14\n",
      "Accuracy: 0.8554 - Loss: 0.4074\n",
      "\n",
      "Batch 262/992 ━━━━━━━━━━━━━━━━━━━━ 23:10:26\n",
      "Accuracy: 0.8559 - Loss: 0.4068\n",
      "\n",
      "Batch 263/992 ━━━━━━━━━━━━━━━━━━━━ 23:10:39\n",
      "Accuracy: 0.8565 - Loss: 0.4058\n",
      "\n",
      "Batch 264/992 ━━━━━━━━━━━━━━━━━━━━ 23:10:50\n",
      "Accuracy: 0.8565 - Loss: 0.4052\n",
      "\n",
      "Batch 265/992 ━━━━━━━━━━━━━━━━━━━━ 23:11:01\n",
      "Accuracy: 0.8561 - Loss: 0.4055\n",
      "\n",
      "Batch 266/992 ━━━━━━━━━━━━━━━━━━━━ 23:11:12\n",
      "Accuracy: 0.8567 - Loss: 0.4041\n",
      "\n",
      "Batch 267/992 ━━━━━━━━━━━━━━━━━━━━ 23:11:22\n",
      "Accuracy: 0.8567 - Loss: 0.4036\n",
      "\n",
      "Batch 268/992 ━━━━━━━━━━━━━━━━━━━━ 23:11:34\n",
      "Accuracy: 0.8568 - Loss: 0.4028\n",
      "\n",
      "Batch 269/992 ━━━━━━━━━━━━━━━━━━━━ 23:11:47\n",
      "Accuracy: 0.8569 - Loss: 0.4020\n",
      "\n",
      "Batch 270/992 ━━━━━━━━━━━━━━━━━━━━ 23:11:58\n",
      "Accuracy: 0.8574 - Loss: 0.4010\n",
      "\n",
      "Batch 271/992 ━━━━━━━━━━━━━━━━━━━━ 23:12:09\n",
      "Accuracy: 0.8575 - Loss: 0.4017\n",
      "\n",
      "Batch 272/992 ━━━━━━━━━━━━━━━━━━━━ 23:12:20\n",
      "Accuracy: 0.8580 - Loss: 0.4007\n",
      "\n",
      "Batch 273/992 ━━━━━━━━━━━━━━━━━━━━ 23:12:31\n",
      "Accuracy: 0.8576 - Loss: 0.4010\n",
      "\n",
      "Batch 274/992 ━━━━━━━━━━━━━━━━━━━━ 23:12:43\n",
      "Accuracy: 0.8577 - Loss: 0.4011\n",
      "\n",
      "Batch 275/992 ━━━━━━━━━━━━━━━━━━━━ 23:12:54\n",
      "Accuracy: 0.8582 - Loss: 0.4000\n",
      "\n",
      "Batch 276/992 ━━━━━━━━━━━━━━━━━━━━ 23:13:05\n",
      "Accuracy: 0.8587 - Loss: 0.3996\n",
      "\n",
      "Batch 277/992 ━━━━━━━━━━━━━━━━━━━━ 23:13:16\n",
      "Accuracy: 0.8588 - Loss: 0.3992\n",
      "\n",
      "Batch 278/992 ━━━━━━━━━━━━━━━━━━━━ 23:13:27\n",
      "Accuracy: 0.8584 - Loss: 0.4002\n",
      "\n",
      "Batch 279/992 ━━━━━━━━━━━━━━━━━━━━ 23:13:39\n",
      "Accuracy: 0.8584 - Loss: 0.3997\n",
      "\n",
      "Batch 280/992 ━━━━━━━━━━━━━━━━━━━━ 23:13:51\n",
      "Accuracy: 0.8580 - Loss: 0.4001\n",
      "\n",
      "Batch 281/992 ━━━━━━━━━━━━━━━━━━━━ 23:14:02\n",
      "Accuracy: 0.8581 - Loss: 0.4004\n",
      "\n",
      "Batch 282/992 ━━━━━━━━━━━━━━━━━━━━ 23:14:13\n",
      "Accuracy: 0.8582 - Loss: 0.4004\n",
      "\n",
      "Batch 283/992 ━━━━━━━━━━━━━━━━━━━━ 23:14:25\n",
      "Accuracy: 0.8582 - Loss: 0.4005\n",
      "\n",
      "Batch 284/992 ━━━━━━━━━━━━━━━━━━━━ 23:14:35\n",
      "Accuracy: 0.8587 - Loss: 0.3993\n",
      "\n",
      "Batch 285/992 ━━━━━━━━━━━━━━━━━━━━ 23:14:46\n",
      "Accuracy: 0.8583 - Loss: 0.3996\n",
      "\n",
      "Batch 286/992 ━━━━━━━━━━━━━━━━━━━━ 23:14:56\n",
      "Accuracy: 0.8584 - Loss: 0.4001\n",
      "\n",
      "Batch 287/992 ━━━━━━━━━━━━━━━━━━━━ 23:15:07\n",
      "Accuracy: 0.8589 - Loss: 0.3991\n",
      "\n",
      "Batch 288/992 ━━━━━━━━━━━━━━━━━━━━ 23:15:18\n",
      "Accuracy: 0.8581 - Loss: 0.4013\n",
      "\n",
      "Batch 289/992 ━━━━━━━━━━━━━━━━━━━━ 23:15:30\n",
      "Accuracy: 0.8581 - Loss: 0.4010\n",
      "\n",
      "Batch 290/992 ━━━━━━━━━━━━━━━━━━━━ 23:15:42\n",
      "Accuracy: 0.8582 - Loss: 0.4021\n",
      "\n",
      "Batch 291/992 ━━━━━━━━━━━━━━━━━━━━ 23:15:54\n",
      "Accuracy: 0.8582 - Loss: 0.4031\n",
      "\n",
      "Batch 292/992 ━━━━━━━━━━━━━━━━━━━━ 23:16:05\n",
      "Accuracy: 0.8583 - Loss: 0.4039\n",
      "\n",
      "Batch 293/992 ━━━━━━━━━━━━━━━━━━━━ 23:16:18\n",
      "Accuracy: 0.8588 - Loss: 0.4028\n",
      "\n",
      "Batch 294/992 ━━━━━━━━━━━━━━━━━━━━ 23:16:29\n",
      "Accuracy: 0.8593 - Loss: 0.4019\n",
      "\n",
      "Batch 295/992 ━━━━━━━━━━━━━━━━━━━━ 23:16:41\n",
      "Accuracy: 0.8597 - Loss: 0.4012\n",
      "\n",
      "Batch 296/992 ━━━━━━━━━━━━━━━━━━━━ 23:16:53\n",
      "Accuracy: 0.8598 - Loss: 0.4011\n",
      "\n",
      "Batch 297/992 ━━━━━━━━━━━━━━━━━━━━ 23:17:04\n",
      "Accuracy: 0.8590 - Loss: 0.4020\n",
      "\n",
      "Batch 298/992 ━━━━━━━━━━━━━━━━━━━━ 23:17:15\n",
      "Accuracy: 0.8591 - Loss: 0.4014\n",
      "\n",
      "Batch 299/992 ━━━━━━━━━━━━━━━━━━━━ 23:17:25\n",
      "Accuracy: 0.8595 - Loss: 0.4009\n",
      "\n",
      "Batch 300/992 ━━━━━━━━━━━━━━━━━━━━ 23:17:36\n",
      "Accuracy: 0.8596 - Loss: 0.4002\n",
      "\n",
      "Batch 301/992 ━━━━━━━━━━━━━━━━━━━━ 23:17:47\n",
      "Accuracy: 0.8592 - Loss: 0.4010\n",
      "\n",
      "Batch 302/992 ━━━━━━━━━━━━━━━━━━━━ 23:17:58\n",
      "Accuracy: 0.8593 - Loss: 0.4010\n",
      "\n",
      "Batch 303/992 ━━━━━━━━━━━━━━━━━━━━ 23:18:09\n",
      "Accuracy: 0.8593 - Loss: 0.4008\n",
      "\n",
      "Batch 304/992 ━━━━━━━━━━━━━━━━━━━━ 23:18:20\n",
      "Accuracy: 0.8598 - Loss: 0.4003\n",
      "\n",
      "Batch 305/992 ━━━━━━━━━━━━━━━━━━━━ 23:18:33\n",
      "Accuracy: 0.8598 - Loss: 0.4002\n",
      "\n",
      "Batch 306/992 ━━━━━━━━━━━━━━━━━━━━ 23:18:45\n",
      "Accuracy: 0.8599 - Loss: 0.4001\n",
      "\n",
      "Batch 307/992 ━━━━━━━━━━━━━━━━━━━━ 23:18:58\n",
      "Accuracy: 0.8599 - Loss: 0.4011\n",
      "\n",
      "Batch 308/992 ━━━━━━━━━━━━━━━━━━━━ 23:19:09\n",
      "Accuracy: 0.8600 - Loss: 0.4029\n",
      "\n",
      "Batch 309/992 ━━━━━━━━━━━━━━━━━━━━ 23:19:21\n",
      "Accuracy: 0.8596 - Loss: 0.4030\n",
      "\n",
      "Batch 310/992 ━━━━━━━━━━━━━━━━━━━━ 23:19:32\n",
      "Accuracy: 0.8589 - Loss: 0.4050\n",
      "\n",
      "Batch 311/992 ━━━━━━━━━━━━━━━━━━━━ 23:19:44\n",
      "Accuracy: 0.8581 - Loss: 0.4057\n",
      "\n",
      "Batch 312/992 ━━━━━━━━━━━━━━━━━━━━ 23:19:55\n",
      "Accuracy: 0.8582 - Loss: 0.4059\n",
      "\n",
      "Batch 313/992 ━━━━━━━━━━━━━━━━━━━━ 23:20:08\n",
      "Accuracy: 0.8582 - Loss: 0.4062\n",
      "\n",
      "Batch 314/992 ━━━━━━━━━━━━━━━━━━━━ 23:20:20\n",
      "Accuracy: 0.8579 - Loss: 0.4070\n",
      "\n",
      "Batch 315/992 ━━━━━━━━━━━━━━━━━━━━ 23:20:30\n",
      "Accuracy: 0.8579 - Loss: 0.4072\n",
      "\n",
      "Batch 316/992 ━━━━━━━━━━━━━━━━━━━━ 23:20:41\n",
      "Accuracy: 0.8584 - Loss: 0.4069\n",
      "\n",
      "Batch 317/992 ━━━━━━━━━━━━━━━━━━━━ 23:20:52\n",
      "Accuracy: 0.8580 - Loss: 0.4073\n",
      "\n",
      "Batch 318/992 ━━━━━━━━━━━━━━━━━━━━ 23:21:04\n",
      "Accuracy: 0.8577 - Loss: 0.4079\n",
      "\n",
      "Batch 319/992 ━━━━━━━━━━━━━━━━━━━━ 23:21:15\n",
      "Accuracy: 0.8570 - Loss: 0.4083\n",
      "\n",
      "Batch 320/992 ━━━━━━━━━━━━━━━━━━━━ 23:21:25\n",
      "Accuracy: 0.8566 - Loss: 0.4089\n",
      "\n",
      "Batch 321/992 ━━━━━━━━━━━━━━━━━━━━ 23:21:36\n",
      "Accuracy: 0.8567 - Loss: 0.4089\n",
      "\n",
      "Batch 322/992 ━━━━━━━━━━━━━━━━━━━━ 23:21:47\n",
      "Accuracy: 0.8571 - Loss: 0.4079\n",
      "\n",
      "Batch 323/992 ━━━━━━━━━━━━━━━━━━━━ 23:21:59\n",
      "Accuracy: 0.8568 - Loss: 0.4090\n",
      "\n",
      "Batch 324/992 ━━━━━━━━━━━━━━━━━━━━ 23:22:12\n",
      "Accuracy: 0.8561 - Loss: 0.4099\n",
      "\n",
      "Batch 325/992 ━━━━━━━━━━━━━━━━━━━━ 23:22:25\n",
      "Accuracy: 0.8565 - Loss: 0.4090\n",
      "\n",
      "Batch 326/992 ━━━━━━━━━━━━━━━━━━━━ 23:22:36\n",
      "Accuracy: 0.8566 - Loss: 0.4087\n",
      "\n",
      "Batch 327/992 ━━━━━━━━━━━━━━━━━━━━ 23:22:50\n",
      "Accuracy: 0.8570 - Loss: 0.4081\n",
      "\n",
      "Batch 328/992 ━━━━━━━━━━━━━━━━━━━━ 23:23:04\n",
      "Accuracy: 0.8563 - Loss: 0.4096\n",
      "\n",
      "Batch 329/992 ━━━━━━━━━━━━━━━━━━━━ 23:23:17\n",
      "Accuracy: 0.8560 - Loss: 0.4104\n",
      "\n",
      "Batch 330/992 ━━━━━━━━━━━━━━━━━━━━ 23:23:29\n",
      "Accuracy: 0.8561 - Loss: 0.4106\n",
      "\n",
      "Batch 331/992 ━━━━━━━━━━━━━━━━━━━━ 23:23:41\n",
      "Accuracy: 0.8565 - Loss: 0.4099\n",
      "\n",
      "Batch 332/992 ━━━━━━━━━━━━━━━━━━━━ 23:23:54\n",
      "Accuracy: 0.8558 - Loss: 0.4123\n",
      "\n",
      "Batch 333/992 ━━━━━━━━━━━━━━━━━━━━ 23:24:06\n",
      "Accuracy: 0.8562 - Loss: 0.4115\n",
      "\n",
      "Batch 334/992 ━━━━━━━━━━━━━━━━━━━━ 23:24:17\n",
      "Accuracy: 0.8559 - Loss: 0.4114\n",
      "\n",
      "Batch 335/992 ━━━━━━━━━━━━━━━━━━━━ 23:24:28\n",
      "Accuracy: 0.8560 - Loss: 0.4109\n",
      "\n",
      "Batch 336/992 ━━━━━━━━━━━━━━━━━━━━ 23:24:39\n",
      "Accuracy: 0.8564 - Loss: 0.4106\n",
      "\n",
      "Batch 337/992 ━━━━━━━━━━━━━━━━━━━━ 23:24:51\n",
      "Accuracy: 0.8561 - Loss: 0.4112\n",
      "\n",
      "Batch 338/992 ━━━━━━━━━━━━━━━━━━━━ 23:25:04\n",
      "Accuracy: 0.8565 - Loss: 0.4103\n",
      "\n",
      "Batch 339/992 ━━━━━━━━━━━━━━━━━━━━ 23:25:17\n",
      "Accuracy: 0.8562 - Loss: 0.4105\n",
      "\n",
      "Batch 340/992 ━━━━━━━━━━━━━━━━━━━━ 23:25:28\n",
      "Accuracy: 0.8562 - Loss: 0.4105\n",
      "\n",
      "Batch 341/992 ━━━━━━━━━━━━━━━━━━━━ 23:25:40\n",
      "Accuracy: 0.8559 - Loss: 0.4114\n",
      "\n",
      "Batch 342/992 ━━━━━━━━━━━━━━━━━━━━ 23:25:52\n",
      "Accuracy: 0.8564 - Loss: 0.4105\n",
      "\n",
      "Batch 343/992 ━━━━━━━━━━━━━━━━━━━━ 23:26:04\n",
      "Accuracy: 0.8564 - Loss: 0.4101\n",
      "\n",
      "Batch 344/992 ━━━━━━━━━━━━━━━━━━━━ 23:26:16\n",
      "Accuracy: 0.8557 - Loss: 0.4123\n",
      "\n",
      "Batch 345/992 ━━━━━━━━━━━━━━━━━━━━ 23:26:29\n",
      "Accuracy: 0.8558 - Loss: 0.4120\n",
      "\n",
      "Batch 346/992 ━━━━━━━━━━━━━━━━━━━━ 23:26:41\n",
      "Accuracy: 0.8548 - Loss: 0.4128\n",
      "\n",
      "Batch 347/992 ━━━━━━━━━━━━━━━━━━━━ 23:26:53\n",
      "Accuracy: 0.8552 - Loss: 0.4120\n",
      "\n",
      "Batch 348/992 ━━━━━━━━━━━━━━━━━━━━ 23:27:07\n",
      "Accuracy: 0.8545 - Loss: 0.4133\n",
      "\n",
      "Batch 349/992 ━━━━━━━━━━━━━━━━━━━━ 23:27:21\n",
      "Accuracy: 0.8542 - Loss: 0.4145\n",
      "\n",
      "Batch 350/992 ━━━━━━━━━━━━━━━━━━━━ 23:27:33\n",
      "Accuracy: 0.8546 - Loss: 0.4135\n",
      "\n",
      "Batch 351/992 ━━━━━━━━━━━━━━━━━━━━ 23:27:52\n",
      "Accuracy: 0.8551 - Loss: 0.4127\n",
      "\n",
      "Batch 352/992 ━━━━━━━━━━━━━━━━━━━━ 23:28:05\n",
      "Accuracy: 0.8551 - Loss: 0.4120\n",
      "\n",
      "Batch 353/992 ━━━━━━━━━━━━━━━━━━━━ 23:28:16\n",
      "Accuracy: 0.8555 - Loss: 0.4113\n",
      "\n",
      "Batch 354/992 ━━━━━━━━━━━━━━━━━━━━ 23:28:28\n",
      "Accuracy: 0.8552 - Loss: 0.4113\n",
      "\n",
      "Batch 355/992 ━━━━━━━━━━━━━━━━━━━━ 23:28:39\n",
      "Accuracy: 0.8553 - Loss: 0.4110\n",
      "\n",
      "Batch 356/992 ━━━━━━━━━━━━━━━━━━━━ 23:28:50\n",
      "Accuracy: 0.8550 - Loss: 0.4109\n",
      "\n",
      "Batch 357/992 ━━━━━━━━━━━━━━━━━━━━ 23:29:01\n",
      "Accuracy: 0.8554 - Loss: 0.4104\n",
      "\n",
      "Batch 358/992 ━━━━━━━━━━━━━━━━━━━━ 23:29:11\n",
      "Accuracy: 0.8558 - Loss: 0.4096\n",
      "\n",
      "Batch 359/992 ━━━━━━━━━━━━━━━━━━━━ 23:29:23\n",
      "Accuracy: 0.8552 - Loss: 0.4102\n",
      "\n",
      "Batch 360/992 ━━━━━━━━━━━━━━━━━━━━ 23:29:34\n",
      "Accuracy: 0.8552 - Loss: 0.4097\n",
      "\n",
      "Batch 361/992 ━━━━━━━━━━━━━━━━━━━━ 23:29:45\n",
      "Accuracy: 0.8549 - Loss: 0.4098\n",
      "\n",
      "Batch 362/992 ━━━━━━━━━━━━━━━━━━━━ 23:29:56\n",
      "Accuracy: 0.8546 - Loss: 0.4099\n",
      "\n",
      "Batch 363/992 ━━━━━━━━━━━━━━━━━━━━ 23:30:08\n",
      "Accuracy: 0.8547 - Loss: 0.4102\n",
      "\n",
      "Batch 364/992 ━━━━━━━━━━━━━━━━━━━━ 23:30:21\n",
      "Accuracy: 0.8547 - Loss: 0.4113\n",
      "\n",
      "Batch 365/992 ━━━━━━━━━━━━━━━━━━━━ 23:30:31\n",
      "Accuracy: 0.8551 - Loss: 0.4104\n",
      "\n",
      "Batch 366/992 ━━━━━━━━━━━━━━━━━━━━ 23:30:42\n",
      "Accuracy: 0.8555 - Loss: 0.4096\n",
      "\n",
      "Batch 367/992 ━━━━━━━━━━━━━━━━━━━━ 23:30:54\n",
      "Accuracy: 0.8552 - Loss: 0.4111\n",
      "\n",
      "Batch 368/992 ━━━━━━━━━━━━━━━━━━━━ 23:31:06\n",
      "Accuracy: 0.8553 - Loss: 0.4107\n",
      "\n",
      "Batch 369/992 ━━━━━━━━━━━━━━━━━━━━ 23:31:17\n",
      "Accuracy: 0.8554 - Loss: 0.4108\n",
      "\n",
      "Batch 370/992 ━━━━━━━━━━━━━━━━━━━━ 23:31:29\n",
      "Accuracy: 0.8547 - Loss: 0.4119\n",
      "\n",
      "Batch 371/992 ━━━━━━━━━━━━━━━━━━━━ 23:31:40\n",
      "Accuracy: 0.8551 - Loss: 0.4115\n",
      "\n",
      "Batch 372/992 ━━━━━━━━━━━━━━━━━━━━ 23:31:51\n",
      "Accuracy: 0.8548 - Loss: 0.4118\n",
      "\n",
      "Batch 373/992 ━━━━━━━━━━━━━━━━━━━━ 23:32:02\n",
      "Accuracy: 0.8542 - Loss: 0.4119\n",
      "\n",
      "Batch 374/992 ━━━━━━━━━━━━━━━━━━━━ 23:32:12\n",
      "Accuracy: 0.8539 - Loss: 0.4131\n",
      "\n",
      "Batch 375/992 ━━━━━━━━━━━━━━━━━━━━ 23:32:23\n",
      "Accuracy: 0.8540 - Loss: 0.4129\n",
      "\n",
      "Batch 376/992 ━━━━━━━━━━━━━━━━━━━━ 23:32:34\n",
      "Accuracy: 0.8537 - Loss: 0.4127\n",
      "\n",
      "Batch 377/992 ━━━━━━━━━━━━━━━━━━━━ 23:32:45\n",
      "Accuracy: 0.8534 - Loss: 0.4129\n",
      "\n",
      "Batch 378/992 ━━━━━━━━━━━━━━━━━━━━ 23:32:55\n",
      "Accuracy: 0.8532 - Loss: 0.4136\n",
      "\n",
      "Batch 379/992 ━━━━━━━━━━━━━━━━━━━━ 23:33:05\n",
      "Accuracy: 0.8529 - Loss: 0.4144\n",
      "\n",
      "Batch 380/992 ━━━━━━━━━━━━━━━━━━━━ 23:33:16\n",
      "Accuracy: 0.8526 - Loss: 0.4143\n",
      "\n",
      "Batch 381/992 ━━━━━━━━━━━━━━━━━━━━ 23:33:27\n",
      "Accuracy: 0.8524 - Loss: 0.4147\n",
      "\n",
      "Batch 382/992 ━━━━━━━━━━━━━━━━━━━━ 23:33:37\n",
      "Accuracy: 0.8524 - Loss: 0.4144\n",
      "\n",
      "Batch 383/992 ━━━━━━━━━━━━━━━━━━━━ 23:33:48\n",
      "Accuracy: 0.8525 - Loss: 0.4146\n",
      "\n",
      "Batch 384/992 ━━━━━━━━━━━━━━━━━━━━ 23:34:00\n",
      "Accuracy: 0.8525 - Loss: 0.4167\n",
      "\n",
      "Batch 385/992 ━━━━━━━━━━━━━━━━━━━━ 23:34:10\n",
      "Accuracy: 0.8526 - Loss: 0.4167\n",
      "\n",
      "Batch 386/992 ━━━━━━━━━━━━━━━━━━━━ 23:34:21\n",
      "Accuracy: 0.8527 - Loss: 0.4166\n",
      "\n",
      "Batch 387/992 ━━━━━━━━━━━━━━━━━━━━ 23:34:32\n",
      "Accuracy: 0.8530 - Loss: 0.4157\n",
      "\n",
      "Batch 388/992 ━━━━━━━━━━━━━━━━━━━━ 23:34:43\n",
      "Accuracy: 0.8521 - Loss: 0.4171\n",
      "\n",
      "Batch 389/992 ━━━━━━━━━━━━━━━━━━━━ 23:34:54\n",
      "Accuracy: 0.8519 - Loss: 0.4173\n",
      "\n",
      "Batch 390/992 ━━━━━━━━━━━━━━━━━━━━ 23:35:05\n",
      "Accuracy: 0.8516 - Loss: 0.4173\n",
      "\n",
      "Batch 391/992 ━━━━━━━━━━━━━━━━━━━━ 23:35:15\n",
      "Accuracy: 0.8520 - Loss: 0.4166\n",
      "\n",
      "Batch 392/992 ━━━━━━━━━━━━━━━━━━━━ 23:35:26\n",
      "Accuracy: 0.8524 - Loss: 0.4158\n",
      "\n",
      "Batch 393/992 ━━━━━━━━━━━━━━━━━━━━ 23:35:37\n",
      "Accuracy: 0.8524 - Loss: 0.4154\n",
      "\n",
      "Batch 394/992 ━━━━━━━━━━━━━━━━━━━━ 23:35:48\n",
      "Accuracy: 0.8525 - Loss: 0.4160\n",
      "\n",
      "Batch 395/992 ━━━━━━━━━━━━━━━━━━━━ 23:35:59\n",
      "Accuracy: 0.8522 - Loss: 0.4165\n",
      "\n",
      "Batch 396/992 ━━━━━━━━━━━━━━━━━━━━ 23:36:09\n",
      "Accuracy: 0.8523 - Loss: 0.4166\n",
      "\n",
      "Batch 397/992 ━━━━━━━━━━━━━━━━━━━━ 23:36:20\n",
      "Accuracy: 0.8514 - Loss: 0.4182\n",
      "\n",
      "Batch 398/992 ━━━━━━━━━━━━━━━━━━━━ 23:36:31\n",
      "Accuracy: 0.8514 - Loss: 0.4180\n",
      "\n",
      "Batch 399/992 ━━━━━━━━━━━━━━━━━━━━ 23:36:42\n",
      "Accuracy: 0.8515 - Loss: 0.4175\n",
      "\n",
      "Batch 400/992 ━━━━━━━━━━━━━━━━━━━━ 23:36:53\n",
      "Accuracy: 0.8512 - Loss: 0.4188\n",
      "\n",
      "Batch 401/992 ━━━━━━━━━━━━━━━━━━━━ 23:37:03\n",
      "Accuracy: 0.8510 - Loss: 0.4188\n",
      "\n",
      "Batch 402/992 ━━━━━━━━━━━━━━━━━━━━ 23:37:14\n",
      "Accuracy: 0.8511 - Loss: 0.4184\n",
      "\n",
      "Batch 403/992 ━━━━━━━━━━━━━━━━━━━━ 23:37:24\n",
      "Accuracy: 0.8511 - Loss: 0.4184\n",
      "\n",
      "Batch 404/992 ━━━━━━━━━━━━━━━━━━━━ 23:37:35\n",
      "Accuracy: 0.8515 - Loss: 0.4177\n",
      "\n",
      "Batch 405/992 ━━━━━━━━━━━━━━━━━━━━ 23:37:45\n",
      "Accuracy: 0.8512 - Loss: 0.4178\n",
      "\n",
      "Batch 406/992 ━━━━━━━━━━━━━━━━━━━━ 23:37:57\n",
      "Accuracy: 0.8510 - Loss: 0.4175\n",
      "\n",
      "Batch 407/992 ━━━━━━━━━━━━━━━━━━━━ 23:38:09\n",
      "Accuracy: 0.8514 - Loss: 0.4169\n",
      "\n",
      "Batch 408/992 ━━━━━━━━━━━━━━━━━━━━ 23:38:21\n",
      "Accuracy: 0.8514 - Loss: 0.4167\n",
      "\n",
      "Batch 409/992 ━━━━━━━━━━━━━━━━━━━━ 23:38:33\n",
      "Accuracy: 0.8518 - Loss: 0.4161\n",
      "\n",
      "Batch 410/992 ━━━━━━━━━━━━━━━━━━━━ 23:38:44\n",
      "Accuracy: 0.8515 - Loss: 0.4159\n",
      "\n",
      "Batch 411/992 ━━━━━━━━━━━━━━━━━━━━ 23:38:55\n",
      "Accuracy: 0.8516 - Loss: 0.4158\n",
      "\n",
      "Batch 412/992 ━━━━━━━━━━━━━━━━━━━━ 23:39:06\n",
      "Accuracy: 0.8516 - Loss: 0.4155\n",
      "\n",
      "Batch 413/992 ━━━━━━━━━━━━━━━━━━━━ 23:39:18\n",
      "Accuracy: 0.8508 - Loss: 0.4173\n",
      "\n",
      "Batch 414/992 ━━━━━━━━━━━━━━━━━━━━ 23:39:30\n",
      "Accuracy: 0.8511 - Loss: 0.4163\n",
      "\n",
      "Batch 415/992 ━━━━━━━━━━━━━━━━━━━━ 23:39:41\n",
      "Accuracy: 0.8509 - Loss: 0.4161\n",
      "\n",
      "Batch 416/992 ━━━━━━━━━━━━━━━━━━━━ 23:39:52\n",
      "Accuracy: 0.8513 - Loss: 0.4155\n",
      "\n",
      "Batch 417/992 ━━━━━━━━━━━━━━━━━━━━ 23:40:04\n",
      "Accuracy: 0.8513 - Loss: 0.4151\n",
      "\n",
      "Batch 418/992 ━━━━━━━━━━━━━━━━━━━━ 23:40:16\n",
      "Accuracy: 0.8514 - Loss: 0.4147\n",
      "\n",
      "Batch 419/992 ━━━━━━━━━━━━━━━━━━━━ 23:40:27\n",
      "Accuracy: 0.8511 - Loss: 0.4148\n",
      "\n",
      "Batch 420/992 ━━━━━━━━━━━━━━━━━━━━ 23:40:37\n",
      "Accuracy: 0.8512 - Loss: 0.4149\n",
      "\n",
      "Batch 421/992 ━━━━━━━━━━━━━━━━━━━━ 23:40:48\n",
      "Accuracy: 0.8512 - Loss: 0.4157\n",
      "\n",
      "Batch 422/992 ━━━━━━━━━━━━━━━━━━━━ 23:41:00\n",
      "Accuracy: 0.8513 - Loss: 0.4153\n",
      "\n",
      "Batch 423/992 ━━━━━━━━━━━━━━━━━━━━ 23:41:11\n",
      "Accuracy: 0.8511 - Loss: 0.4156\n",
      "\n",
      "Batch 424/992 ━━━━━━━━━━━━━━━━━━━━ 23:41:23\n",
      "Accuracy: 0.8508 - Loss: 0.4163\n",
      "\n",
      "Batch 425/992 ━━━━━━━━━━━━━━━━━━━━ 23:41:34\n",
      "Accuracy: 0.8506 - Loss: 0.4169\n",
      "\n",
      "Batch 426/992 ━━━━━━━━━━━━━━━━━━━━ 23:41:45\n",
      "Accuracy: 0.8506 - Loss: 0.4166\n",
      "\n",
      "Batch 427/992 ━━━━━━━━━━━━━━━━━━━━ 23:41:56\n",
      "Accuracy: 0.8504 - Loss: 0.4173\n",
      "\n",
      "Batch 428/992 ━━━━━━━━━━━━━━━━━━━━ 23:42:08\n",
      "Accuracy: 0.8502 - Loss: 0.4175\n",
      "\n",
      "Batch 429/992 ━━━━━━━━━━━━━━━━━━━━ 23:42:18\n",
      "Accuracy: 0.8502 - Loss: 0.4178\n",
      "\n",
      "Batch 430/992 ━━━━━━━━━━━━━━━━━━━━ 23:42:29\n",
      "Accuracy: 0.8506 - Loss: 0.4171\n",
      "\n",
      "Batch 431/992 ━━━━━━━━━━━━━━━━━━━━ 23:42:40\n",
      "Accuracy: 0.8503 - Loss: 0.4169\n",
      "\n",
      "Batch 432/992 ━━━━━━━━━━━━━━━━━━━━ 23:42:51\n",
      "Accuracy: 0.8504 - Loss: 0.4167\n",
      "\n",
      "Batch 433/992 ━━━━━━━━━━━━━━━━━━━━ 23:43:02\n",
      "Accuracy: 0.8508 - Loss: 0.4161\n",
      "\n",
      "Batch 434/992 ━━━━━━━━━━━━━━━━━━━━ 23:43:13\n",
      "Accuracy: 0.8508 - Loss: 0.4160\n",
      "\n",
      "Batch 435/992 ━━━━━━━━━━━━━━━━━━━━ 23:43:25\n",
      "Accuracy: 0.8509 - Loss: 0.4162\n",
      "\n",
      "Batch 436/992 ━━━━━━━━━━━━━━━━━━━━ 23:43:36\n",
      "Accuracy: 0.8512 - Loss: 0.4157\n",
      "\n",
      "Batch 437/992 ━━━━━━━━━━━━━━━━━━━━ 23:43:47\n",
      "Accuracy: 0.8515 - Loss: 0.4150\n",
      "\n",
      "Batch 438/992 ━━━━━━━━━━━━━━━━━━━━ 23:43:58\n",
      "Accuracy: 0.8516 - Loss: 0.4151\n",
      "\n",
      "Batch 439/992 ━━━━━━━━━━━━━━━━━━━━ 23:44:10\n",
      "Accuracy: 0.8519 - Loss: 0.4146\n",
      "\n",
      "Batch 440/992 ━━━━━━━━━━━━━━━━━━━━ 23:44:21\n",
      "Accuracy: 0.8517 - Loss: 0.4153\n",
      "\n",
      "Batch 441/992 ━━━━━━━━━━━━━━━━━━━━ 23:44:32\n",
      "Accuracy: 0.8515 - Loss: 0.4159\n",
      "\n",
      "Batch 442/992 ━━━━━━━━━━━━━━━━━━━━ 23:44:43\n",
      "Accuracy: 0.8512 - Loss: 0.4160\n",
      "\n",
      "Batch 443/992 ━━━━━━━━━━━━━━━━━━━━ 23:44:54\n",
      "Accuracy: 0.8510 - Loss: 0.4168\n",
      "\n",
      "Batch 444/992 ━━━━━━━━━━━━━━━━━━━━ 23:45:05\n",
      "Accuracy: 0.8514 - Loss: 0.4159\n",
      "\n",
      "Batch 445/992 ━━━━━━━━━━━━━━━━━━━━ 23:45:16\n",
      "Accuracy: 0.8514 - Loss: 0.4153\n",
      "\n",
      "Batch 446/992 ━━━━━━━━━━━━━━━━━━━━ 23:45:27\n",
      "Accuracy: 0.8512 - Loss: 0.4159\n",
      "\n",
      "Batch 447/992 ━━━━━━━━━━━━━━━━━━━━ 23:45:39\n",
      "Accuracy: 0.8512 - Loss: 0.4159\n",
      "\n",
      "Batch 448/992 ━━━━━━━━━━━━━━━━━━━━ 23:45:51\n",
      "Accuracy: 0.8513 - Loss: 0.4157\n",
      "\n",
      "Batch 449/992 ━━━━━━━━━━━━━━━━━━━━ 23:46:02\n",
      "Accuracy: 0.8516 - Loss: 0.4149\n",
      "\n",
      "Batch 450/992 ━━━━━━━━━━━━━━━━━━━━ 23:46:13\n",
      "Accuracy: 0.8514 - Loss: 0.4154\n",
      "\n",
      "Batch 451/992 ━━━━━━━━━━━━━━━━━━━━ 23:46:24\n",
      "Accuracy: 0.8512 - Loss: 0.4155\n",
      "\n",
      "Batch 452/992 ━━━━━━━━━━━━━━━━━━━━ 23:46:34\n",
      "Accuracy: 0.8509 - Loss: 0.4154\n",
      "\n",
      "Batch 453/992 ━━━━━━━━━━━━━━━━━━━━ 23:46:45\n",
      "Accuracy: 0.8507 - Loss: 0.4167\n",
      "\n",
      "Batch 454/992 ━━━━━━━━━━━━━━━━━━━━ 23:46:56\n",
      "Accuracy: 0.8508 - Loss: 0.4167\n",
      "\n",
      "Batch 455/992 ━━━━━━━━━━━━━━━━━━━━ 23:47:07\n",
      "Accuracy: 0.8508 - Loss: 0.4170\n",
      "\n",
      "Batch 456/992 ━━━━━━━━━━━━━━━━━━━━ 23:47:17\n",
      "Accuracy: 0.8506 - Loss: 0.4176\n",
      "\n",
      "Batch 457/992 ━━━━━━━━━━━━━━━━━━━━ 23:47:27\n",
      "Accuracy: 0.8509 - Loss: 0.4169\n",
      "\n",
      "Batch 458/992 ━━━━━━━━━━━━━━━━━━━━ 23:47:38\n",
      "Accuracy: 0.8510 - Loss: 0.4175\n",
      "\n",
      "Batch 459/992 ━━━━━━━━━━━━━━━━━━━━ 23:47:49\n",
      "Accuracy: 0.8513 - Loss: 0.4169\n",
      "\n",
      "Batch 460/992 ━━━━━━━━━━━━━━━━━━━━ 23:48:01\n",
      "Accuracy: 0.8514 - Loss: 0.4168\n",
      "\n",
      "Batch 461/992 ━━━━━━━━━━━━━━━━━━━━ 23:48:12\n",
      "Accuracy: 0.8517 - Loss: 0.4161\n",
      "\n",
      "Batch 462/992 ━━━━━━━━━━━━━━━━━━━━ 23:48:22\n",
      "Accuracy: 0.8512 - Loss: 0.4177\n",
      "\n",
      "Batch 463/992 ━━━━━━━━━━━━━━━━━━━━ 23:48:33\n",
      "Accuracy: 0.8510 - Loss: 0.4186\n",
      "\n",
      "Batch 464/992 ━━━━━━━━━━━━━━━━━━━━ 23:48:43\n",
      "Accuracy: 0.8510 - Loss: 0.4187\n",
      "\n",
      "Batch 465/992 ━━━━━━━━━━━━━━━━━━━━ 23:48:54\n",
      "Accuracy: 0.8511 - Loss: 0.4186\n",
      "\n",
      "Batch 466/992 ━━━━━━━━━━━━━━━━━━━━ 23:49:05\n",
      "Accuracy: 0.8511 - Loss: 0.4192\n",
      "\n",
      "Batch 467/992 ━━━━━━━━━━━━━━━━━━━━ 23:49:15\n",
      "Accuracy: 0.8512 - Loss: 0.4191\n",
      "\n",
      "Batch 468/992 ━━━━━━━━━━━━━━━━━━━━ 23:49:25\n",
      "Accuracy: 0.8507 - Loss: 0.4200\n",
      "\n",
      "Batch 469/992 ━━━━━━━━━━━━━━━━━━━━ 23:49:35\n",
      "Accuracy: 0.8507 - Loss: 0.4202\n",
      "\n",
      "Batch 470/992 ━━━━━━━━━━━━━━━━━━━━ 23:49:46\n",
      "Accuracy: 0.8511 - Loss: 0.4195\n",
      "\n",
      "Batch 471/992 ━━━━━━━━━━━━━━━━━━━━ 23:49:57\n",
      "Accuracy: 0.8511 - Loss: 0.4194\n",
      "\n",
      "Batch 472/992 ━━━━━━━━━━━━━━━━━━━━ 23:50:09\n",
      "Accuracy: 0.8514 - Loss: 0.4186\n",
      "\n",
      "Batch 473/992 ━━━━━━━━━━━━━━━━━━━━ 23:50:20\n",
      "Accuracy: 0.8512 - Loss: 0.4191\n",
      "\n",
      "Batch 474/992 ━━━━━━━━━━━━━━━━━━━━ 23:50:31\n",
      "Accuracy: 0.8513 - Loss: 0.4190\n",
      "\n",
      "Batch 475/992 ━━━━━━━━━━━━━━━━━━━━ 23:50:43\n",
      "Accuracy: 0.8511 - Loss: 0.4201\n",
      "\n",
      "Batch 476/992 ━━━━━━━━━━━━━━━━━━━━ 23:50:57\n",
      "Accuracy: 0.8514 - Loss: 0.4194\n",
      "\n",
      "Batch 477/992 ━━━━━━━━━━━━━━━━━━━━ 23:51:09\n",
      "Accuracy: 0.8517 - Loss: 0.4189\n",
      "\n",
      "Batch 478/992 ━━━━━━━━━━━━━━━━━━━━ 23:51:21\n",
      "Accuracy: 0.8520 - Loss: 0.4183\n",
      "\n",
      "Batch 479/992 ━━━━━━━━━━━━━━━━━━━━ 23:51:35\n",
      "Accuracy: 0.8520 - Loss: 0.4183\n",
      "\n",
      "Batch 480/992 ━━━━━━━━━━━━━━━━━━━━ 23:51:49\n",
      "Accuracy: 0.8518 - Loss: 0.4189\n",
      "\n",
      "Batch 481/992 ━━━━━━━━━━━━━━━━━━━━ 23:52:04\n",
      "Accuracy: 0.8519 - Loss: 0.4189\n",
      "\n",
      "Batch 482/992 ━━━━━━━━━━━━━━━━━━━━ 23:52:14\n",
      "Accuracy: 0.8519 - Loss: 0.4187\n",
      "\n",
      "Batch 483/992 ━━━━━━━━━━━━━━━━━━━━ 23:52:27\n",
      "Accuracy: 0.8517 - Loss: 0.4192\n",
      "\n",
      "Batch 484/992 ━━━━━━━━━━━━━━━━━━━━ 23:52:39\n",
      "Accuracy: 0.8518 - Loss: 0.4189\n",
      "\n",
      "Batch 485/992 ━━━━━━━━━━━━━━━━━━━━ 23:52:51\n",
      "Accuracy: 0.8513 - Loss: 0.4198\n",
      "\n",
      "Batch 486/992 ━━━━━━━━━━━━━━━━━━━━ 23:53:02\n",
      "Accuracy: 0.8511 - Loss: 0.4196\n",
      "\n",
      "Batch 487/992 ━━━━━━━━━━━━━━━━━━━━ 23:53:14\n",
      "Accuracy: 0.8511 - Loss: 0.4193\n",
      "\n",
      "Batch 488/992 ━━━━━━━━━━━━━━━━━━━━ 23:53:27\n",
      "Accuracy: 0.8514 - Loss: 0.4188\n",
      "\n",
      "Batch 489/992 ━━━━━━━━━━━━━━━━━━━━ 23:53:38\n",
      "Accuracy: 0.8512 - Loss: 0.4203\n",
      "\n",
      "Batch 490/992 ━━━━━━━━━━━━━━━━━━━━ 23:53:49\n",
      "Accuracy: 0.8515 - Loss: 0.4196\n",
      "\n",
      "Batch 491/992 ━━━━━━━━━━━━━━━━━━━━ 23:54:00\n",
      "Accuracy: 0.8516 - Loss: 0.4195\n",
      "\n",
      "Batch 492/992 ━━━━━━━━━━━━━━━━━━━━ 23:54:12\n",
      "Accuracy: 0.8511 - Loss: 0.4202\n",
      "\n",
      "Batch 493/992 ━━━━━━━━━━━━━━━━━━━━ 23:54:26\n",
      "Accuracy: 0.8512 - Loss: 0.4197\n",
      "\n",
      "Batch 494/992 ━━━━━━━━━━━━━━━━━━━━ 23:54:36\n",
      "Accuracy: 0.8515 - Loss: 0.4194\n",
      "\n",
      "Batch 495/992 ━━━━━━━━━━━━━━━━━━━━ 23:54:46\n",
      "Accuracy: 0.8518 - Loss: 0.4189\n",
      "\n",
      "Batch 496/992 ━━━━━━━━━━━━━━━━━━━━ 23:54:57\n",
      "Accuracy: 0.8518 - Loss: 0.4197\n",
      "\n",
      "Batch 497/992 ━━━━━━━━━━━━━━━━━━━━ 23:55:07\n",
      "Accuracy: 0.8519 - Loss: 0.4192\n",
      "\n",
      "Batch 498/992 ━━━━━━━━━━━━━━━━━━━━ 23:55:17\n",
      "Accuracy: 0.8519 - Loss: 0.4200\n",
      "\n",
      "Batch 499/992 ━━━━━━━━━━━━━━━━━━━━ 23:55:28\n",
      "Accuracy: 0.8515 - Loss: 0.4214\n",
      "\n",
      "Batch 500/992 ━━━━━━━━━━━━━━━━━━━━ 23:55:38\n",
      "Accuracy: 0.8515 - Loss: 0.4212\n",
      "\n",
      "Batch 501/992 ━━━━━━━━━━━━━━━━━━━━ 23:55:48\n",
      "Accuracy: 0.8515 - Loss: 0.4209\n",
      "\n",
      "Batch 502/992 ━━━━━━━━━━━━━━━━━━━━ 23:55:59\n",
      "Accuracy: 0.8516 - Loss: 0.4208\n",
      "\n",
      "Batch 503/992 ━━━━━━━━━━━━━━━━━━━━ 23:56:09\n",
      "Accuracy: 0.8516 - Loss: 0.4210\n",
      "\n",
      "Batch 504/992 ━━━━━━━━━━━━━━━━━━━━ 23:56:20\n",
      "Accuracy: 0.8519 - Loss: 0.4204\n",
      "\n",
      "Batch 505/992 ━━━━━━━━━━━━━━━━━━━━ 23:56:30\n",
      "Accuracy: 0.8522 - Loss: 0.4197\n",
      "\n",
      "Batch 506/992 ━━━━━━━━━━━━━━━━━━━━ 23:56:40\n",
      "Accuracy: 0.8520 - Loss: 0.4200\n",
      "\n",
      "Batch 507/992 ━━━━━━━━━━━━━━━━━━━━ 23:56:51\n",
      "Accuracy: 0.8518 - Loss: 0.4204\n",
      "\n",
      "Batch 508/992 ━━━━━━━━━━━━━━━━━━━━ 23:57:01\n",
      "Accuracy: 0.8514 - Loss: 0.4214\n",
      "\n",
      "Batch 509/992 ━━━━━━━━━━━━━━━━━━━━ 23:57:11\n",
      "Accuracy: 0.8512 - Loss: 0.4218\n",
      "\n",
      "Batch 510/992 ━━━━━━━━━━━━━━━━━━━━ 23:57:22\n",
      "Accuracy: 0.8515 - Loss: 0.4213\n",
      "\n",
      "Batch 511/992 ━━━━━━━━━━━━━━━━━━━━ 23:57:32\n",
      "Accuracy: 0.8515 - Loss: 0.4213\n",
      "\n",
      "Batch 512/992 ━━━━━━━━━━━━━━━━━━━━ 23:57:42\n",
      "Accuracy: 0.8518 - Loss: 0.4208\n",
      "\n",
      "Batch 513/992 ━━━━━━━━━━━━━━━━━━━━ 23:57:53\n",
      "Accuracy: 0.8516 - Loss: 0.4217\n",
      "\n",
      "Batch 514/992 ━━━━━━━━━━━━━━━━━━━━ 23:58:04\n",
      "Accuracy: 0.8514 - Loss: 0.4221\n",
      "\n",
      "Batch 515/992 ━━━━━━━━━━━━━━━━━━━━ 23:58:14\n",
      "Accuracy: 0.8510 - Loss: 0.4224\n",
      "\n",
      "Batch 516/992 ━━━━━━━━━━━━━━━━━━━━ 23:58:24\n",
      "Accuracy: 0.8513 - Loss: 0.4218\n",
      "\n",
      "Batch 517/992 ━━━━━━━━━━━━━━━━━━━━ 23:58:35\n",
      "Accuracy: 0.8508 - Loss: 0.4233\n",
      "\n",
      "Batch 518/992 ━━━━━━━━━━━━━━━━━━━━ 23:58:45\n",
      "Accuracy: 0.8511 - Loss: 0.4228\n",
      "\n",
      "Batch 519/992 ━━━━━━━━━━━━━━━━━━━━ 23:58:55\n",
      "Accuracy: 0.8512 - Loss: 0.4233\n",
      "\n",
      "Batch 520/992 ━━━━━━━━━━━━━━━━━━━━ 23:59:05\n",
      "Accuracy: 0.8510 - Loss: 0.4234\n",
      "\n",
      "Batch 521/992 ━━━━━━━━━━━━━━━━━━━━ 23:59:15\n",
      "Accuracy: 0.8512 - Loss: 0.4230\n",
      "\n",
      "Batch 522/992 ━━━━━━━━━━━━━━━━━━━━ 23:59:25\n",
      "Accuracy: 0.8513 - Loss: 0.4227\n",
      "\n",
      "Batch 523/992 ━━━━━━━━━━━━━━━━━━━━ 23:59:35\n",
      "Accuracy: 0.8513 - Loss: 0.4226\n",
      "\n",
      "Batch 524/992 ━━━━━━━━━━━━━━━━━━━━ 23:59:46\n",
      "Accuracy: 0.8516 - Loss: 0.4223\n",
      "\n",
      "Batch 525/992 ━━━━━━━━━━━━━━━━━━━━ 23:59:56\n",
      "Accuracy: 0.8514 - Loss: 0.4226\n",
      "\n",
      "Batch 526/992 ━━━━━━━━━━━━━━━━━━━━ 00:00:06\n",
      "Accuracy: 0.8517 - Loss: 0.4221\n",
      "\n",
      "Batch 527/992 ━━━━━━━━━━━━━━━━━━━━ 00:00:16\n",
      "Accuracy: 0.8518 - Loss: 0.4221\n",
      "\n",
      "Batch 528/992 ━━━━━━━━━━━━━━━━━━━━ 00:00:26\n",
      "Accuracy: 0.8518 - Loss: 0.4227\n",
      "\n",
      "Batch 529/992 ━━━━━━━━━━━━━━━━━━━━ 00:00:36\n",
      "Accuracy: 0.8514 - Loss: 0.4233\n",
      "\n",
      "Batch 530/992 ━━━━━━━━━━━━━━━━━━━━ 00:00:47\n",
      "Accuracy: 0.8514 - Loss: 0.4236\n",
      "\n",
      "Batch 531/992 ━━━━━━━━━━━━━━━━━━━━ 00:00:57\n",
      "Accuracy: 0.8517 - Loss: 0.4232\n",
      "\n",
      "Batch 532/992 ━━━━━━━━━━━━━━━━━━━━ 00:01:08\n",
      "Accuracy: 0.8515 - Loss: 0.4231\n",
      "\n",
      "Batch 533/992 ━━━━━━━━━━━━━━━━━━━━ 00:01:18\n",
      "Accuracy: 0.8518 - Loss: 0.4226\n",
      "\n",
      "Batch 534/992 ━━━━━━━━━━━━━━━━━━━━ 00:01:28\n",
      "Accuracy: 0.8521 - Loss: 0.4221\n",
      "\n",
      "Batch 535/992 ━━━━━━━━━━━━━━━━━━━━ 00:01:38\n",
      "Accuracy: 0.8521 - Loss: 0.4222\n",
      "\n",
      "Batch 536/992 ━━━━━━━━━━━━━━━━━━━━ 00:01:49\n",
      "Accuracy: 0.8521 - Loss: 0.4220\n",
      "\n",
      "Batch 537/992 ━━━━━━━━━━━━━━━━━━━━ 00:01:59\n",
      "Accuracy: 0.8517 - Loss: 0.4234\n",
      "\n",
      "Batch 538/992 ━━━━━━━━━━━━━━━━━━━━ 00:02:09\n",
      "Accuracy: 0.8513 - Loss: 0.4241\n",
      "\n",
      "Batch 539/992 ━━━━━━━━━━━━━━━━━━━━ 00:02:23\n",
      "Accuracy: 0.8513 - Loss: 0.4241\n",
      "\n",
      "Batch 540/992 ━━━━━━━━━━━━━━━━━━━━ 00:02:35\n",
      "Accuracy: 0.8516 - Loss: 0.4236\n",
      "\n",
      "Batch 541/992 ━━━━━━━━━━━━━━━━━━━━ 00:02:45\n",
      "Accuracy: 0.8519 - Loss: 0.4230\n",
      "\n",
      "Batch 542/992 ━━━━━━━━━━━━━━━━━━━━ 00:02:56\n",
      "Accuracy: 0.8517 - Loss: 0.4229\n",
      "\n",
      "Batch 543/992 ━━━━━━━━━━━━━━━━━━━━ 00:03:07\n",
      "Accuracy: 0.8517 - Loss: 0.4226\n",
      "\n",
      "Batch 544/992 ━━━━━━━━━━━━━━━━━━━━ 00:03:18\n",
      "Accuracy: 0.8518 - Loss: 0.4223\n",
      "\n",
      "Batch 545/992 ━━━━━━━━━━━━━━━━━━━━ 00:03:28\n",
      "Accuracy: 0.8518 - Loss: 0.4220\n",
      "\n",
      "Batch 546/992 ━━━━━━━━━━━━━━━━━━━━ 00:03:39\n",
      "Accuracy: 0.8516 - Loss: 0.4224\n",
      "\n",
      "Batch 547/992 ━━━━━━━━━━━━━━━━━━━━ 00:03:49\n",
      "Accuracy: 0.8517 - Loss: 0.4222\n",
      "\n",
      "Batch 548/992 ━━━━━━━━━━━━━━━━━━━━ 00:04:00\n",
      "Accuracy: 0.8520 - Loss: 0.4218\n",
      "\n",
      "Batch 549/992 ━━━━━━━━━━━━━━━━━━━━ 00:04:11\n",
      "Accuracy: 0.8515 - Loss: 0.4225\n",
      "\n",
      "Batch 550/992 ━━━━━━━━━━━━━━━━━━━━ 00:04:22\n",
      "Accuracy: 0.8516 - Loss: 0.4224\n",
      "\n",
      "Batch 551/992 ━━━━━━━━━━━━━━━━━━━━ 00:04:32\n",
      "Accuracy: 0.8510 - Loss: 0.4232\n",
      "\n",
      "Batch 552/992 ━━━━━━━━━━━━━━━━━━━━ 00:04:43\n",
      "Accuracy: 0.8505 - Loss: 0.4236\n",
      "\n",
      "Batch 553/992 ━━━━━━━━━━━━━━━━━━━━ 00:04:53\n",
      "Accuracy: 0.8506 - Loss: 0.4235\n",
      "\n",
      "Batch 554/992 ━━━━━━━━━━━━━━━━━━━━ 00:05:04\n",
      "Accuracy: 0.8506 - Loss: 0.4233\n",
      "\n",
      "Batch 555/992 ━━━━━━━━━━━━━━━━━━━━ 00:05:15\n",
      "Accuracy: 0.8505 - Loss: 0.4234\n",
      "\n",
      "Batch 556/992 ━━━━━━━━━━━━━━━━━━━━ 00:05:26\n",
      "Accuracy: 0.8505 - Loss: 0.4233\n",
      "\n",
      "Batch 557/992 ━━━━━━━━━━━━━━━━━━━━ 00:05:37\n",
      "Accuracy: 0.8505 - Loss: 0.4233\n",
      "\n",
      "Batch 558/992 ━━━━━━━━━━━━━━━━━━━━ 00:05:48\n",
      "Accuracy: 0.8506 - Loss: 0.4230\n",
      "\n",
      "Batch 559/992 ━━━━━━━━━━━━━━━━━━━━ 00:05:58\n",
      "Accuracy: 0.8504 - Loss: 0.4242\n",
      "\n",
      "Batch 560/992 ━━━━━━━━━━━━━━━━━━━━ 00:06:09\n",
      "Accuracy: 0.8507 - Loss: 0.4236\n",
      "\n",
      "Batch 561/992 ━━━━━━━━━━━━━━━━━━━━ 00:06:20\n",
      "Accuracy: 0.8509 - Loss: 0.4231\n",
      "\n",
      "Batch 562/992 ━━━━━━━━━━━━━━━━━━━━ 00:06:30\n",
      "Accuracy: 0.8512 - Loss: 0.4228\n",
      "\n",
      "Batch 563/992 ━━━━━━━━━━━━━━━━━━━━ 00:06:41\n",
      "Accuracy: 0.8512 - Loss: 0.4228\n",
      "\n",
      "Batch 564/992 ━━━━━━━━━━━━━━━━━━━━ 00:06:51\n",
      "Accuracy: 0.8513 - Loss: 0.4229\n",
      "\n",
      "Batch 565/992 ━━━━━━━━━━━━━━━━━━━━ 00:07:03\n",
      "Accuracy: 0.8511 - Loss: 0.4231\n",
      "\n",
      "Batch 566/992 ━━━━━━━━━━━━━━━━━━━━ 00:07:13\n",
      "Accuracy: 0.8509 - Loss: 0.4234\n",
      "\n",
      "Batch 567/992 ━━━━━━━━━━━━━━━━━━━━ 00:07:25\n",
      "Accuracy: 0.8503 - Loss: 0.4246\n",
      "\n",
      "Batch 568/992 ━━━━━━━━━━━━━━━━━━━━ 00:07:36\n",
      "Accuracy: 0.8501 - Loss: 0.4252\n",
      "\n",
      "Batch 569/992 ━━━━━━━━━━━━━━━━━━━━ 00:07:46\n",
      "Accuracy: 0.8497 - Loss: 0.4260\n",
      "\n",
      "Batch 570/992 ━━━━━━━━━━━━━━━━━━━━ 00:07:57\n",
      "Accuracy: 0.8496 - Loss: 0.4261\n",
      "\n",
      "Batch 571/992 ━━━━━━━━━━━━━━━━━━━━ 00:08:08\n",
      "Accuracy: 0.8494 - Loss: 0.4267\n",
      "\n",
      "Batch 572/992 ━━━━━━━━━━━━━━━━━━━━ 00:08:19\n",
      "Accuracy: 0.8494 - Loss: 0.4266\n",
      "\n",
      "Batch 573/992 ━━━━━━━━━━━━━━━━━━━━ 00:08:30\n",
      "Accuracy: 0.8495 - Loss: 0.4265\n",
      "\n",
      "Batch 574/992 ━━━━━━━━━━━━━━━━━━━━ 00:08:40\n",
      "Accuracy: 0.8493 - Loss: 0.4272\n",
      "\n",
      "Batch 575/992 ━━━━━━━━━━━━━━━━━━━━ 00:08:51\n",
      "Accuracy: 0.8491 - Loss: 0.4274\n",
      "\n",
      "Batch 576/992 ━━━━━━━━━━━━━━━━━━━━ 00:09:02\n",
      "Accuracy: 0.8490 - Loss: 0.4274\n",
      "\n",
      "Batch 577/992 ━━━━━━━━━━━━━━━━━━━━ 00:09:12\n",
      "Accuracy: 0.8490 - Loss: 0.4278\n",
      "\n",
      "Batch 578/992 ━━━━━━━━━━━━━━━━━━━━ 00:09:23\n",
      "Accuracy: 0.8486 - Loss: 0.4282\n",
      "\n",
      "Batch 579/992 ━━━━━━━━━━━━━━━━━━━━ 00:09:34\n",
      "Accuracy: 0.8487 - Loss: 0.4281\n",
      "\n",
      "Batch 580/992 ━━━━━━━━━━━━━━━━━━━━ 00:09:45\n",
      "Accuracy: 0.8487 - Loss: 0.4279\n",
      "\n",
      "Batch 581/992 ━━━━━━━━━━━━━━━━━━━━ 00:09:55\n",
      "Accuracy: 0.8488 - Loss: 0.4276\n",
      "\n",
      "Batch 582/992 ━━━━━━━━━━━━━━━━━━━━ 00:10:06\n",
      "Accuracy: 0.8482 - Loss: 0.4283\n",
      "\n",
      "Batch 583/992 ━━━━━━━━━━━━━━━━━━━━ 00:10:16\n",
      "Accuracy: 0.8480 - Loss: 0.4291\n",
      "\n",
      "Batch 584/992 ━━━━━━━━━━━━━━━━━━━━ 00:10:27\n",
      "Accuracy: 0.8482 - Loss: 0.4286\n",
      "\n",
      "Batch 585/992 ━━━━━━━━━━━━━━━━━━━━ 00:10:38\n",
      "Accuracy: 0.8485 - Loss: 0.4283\n",
      "\n",
      "Batch 586/992 ━━━━━━━━━━━━━━━━━━━━ 00:10:49\n",
      "Accuracy: 0.8485 - Loss: 0.4283\n",
      "\n",
      "Batch 587/992 ━━━━━━━━━━━━━━━━━━━━ 00:10:59\n",
      "Accuracy: 0.8482 - Loss: 0.4286\n",
      "\n",
      "Batch 588/992 ━━━━━━━━━━━━━━━━━━━━ 00:11:13\n",
      "Accuracy: 0.8484 - Loss: 0.4281\n",
      "\n",
      "Batch 589/992 ━━━━━━━━━━━━━━━━━━━━ 00:11:23\n",
      "Accuracy: 0.8487 - Loss: 0.4276\n",
      "\n",
      "Batch 590/992 ━━━━━━━━━━━━━━━━━━━━ 00:11:34\n",
      "Accuracy: 0.8487 - Loss: 0.4280\n",
      "\n",
      "Batch 591/992 ━━━━━━━━━━━━━━━━━━━━ 00:11:45\n",
      "Accuracy: 0.8484 - Loss: 0.4283\n",
      "\n",
      "Batch 592/992 ━━━━━━━━━━━━━━━━━━━━ 00:11:56\n",
      "Accuracy: 0.8482 - Loss: 0.4289\n",
      "\n",
      "Batch 593/992 ━━━━━━━━━━━━━━━━━━━━ 00:12:07\n",
      "Accuracy: 0.8480 - Loss: 0.4295\n",
      "\n",
      "Batch 594/992 ━━━━━━━━━━━━━━━━━━━━ 00:12:18\n",
      "Accuracy: 0.8481 - Loss: 0.4297\n",
      "\n",
      "Batch 595/992 ━━━━━━━━━━━━━━━━━━━━ 00:12:28\n",
      "Accuracy: 0.8481 - Loss: 0.4297\n",
      "\n",
      "Batch 596/992 ━━━━━━━━━━━━━━━━━━━━ 00:12:39\n",
      "Accuracy: 0.8479 - Loss: 0.4301\n",
      "\n",
      "Batch 597/992 ━━━━━━━━━━━━━━━━━━━━ 00:12:50\n",
      "Accuracy: 0.8476 - Loss: 0.4309\n",
      "\n",
      "Batch 598/992 ━━━━━━━━━━━━━━━━━━━━ 00:13:02\n",
      "Accuracy: 0.8476 - Loss: 0.4308\n",
      "\n",
      "Batch 599/992 ━━━━━━━━━━━━━━━━━━━━ 00:13:14\n",
      "Accuracy: 0.8475 - Loss: 0.4307\n",
      "\n",
      "Batch 600/992 ━━━━━━━━━━━━━━━━━━━━ 00:13:25\n",
      "Accuracy: 0.8475 - Loss: 0.4305\n",
      "\n",
      "Batch 601/992 ━━━━━━━━━━━━━━━━━━━━ 00:13:36\n",
      "Accuracy: 0.8478 - Loss: 0.4303\n",
      "\n",
      "Batch 602/992 ━━━━━━━━━━━━━━━━━━━━ 00:13:47\n",
      "Accuracy: 0.8480 - Loss: 0.4300\n",
      "\n",
      "Batch 603/992 ━━━━━━━━━━━━━━━━━━━━ 00:13:59\n",
      "Accuracy: 0.8481 - Loss: 0.4298\n",
      "\n",
      "Batch 604/992 ━━━━━━━━━━━━━━━━━━━━ 00:14:10\n",
      "Accuracy: 0.8481 - Loss: 0.4299\n",
      "\n",
      "Batch 605/992 ━━━━━━━━━━━━━━━━━━━━ 00:14:21\n",
      "Accuracy: 0.8483 - Loss: 0.4294\n",
      "\n",
      "Batch 606/992 ━━━━━━━━━━━━━━━━━━━━ 00:14:32\n",
      "Accuracy: 0.8486 - Loss: 0.4289\n",
      "\n",
      "Batch 607/992 ━━━━━━━━━━━━━━━━━━━━ 00:14:43\n",
      "Accuracy: 0.8488 - Loss: 0.4285\n",
      "\n",
      "Batch 608/992 ━━━━━━━━━━━━━━━━━━━━ 00:14:53\n",
      "Accuracy: 0.8487 - Loss: 0.4288\n",
      "\n",
      "Batch 609/992 ━━━━━━━━━━━━━━━━━━━━ 00:15:04\n",
      "Accuracy: 0.8489 - Loss: 0.4285\n",
      "\n",
      "Batch 610/992 ━━━━━━━━━━━━━━━━━━━━ 00:15:15\n",
      "Accuracy: 0.8492 - Loss: 0.4279\n",
      "\n",
      "Batch 611/992 ━━━━━━━━━━━━━━━━━━━━ 00:15:25\n",
      "Accuracy: 0.8490 - Loss: 0.4279\n",
      "\n",
      "Batch 612/992 ━━━━━━━━━━━━━━━━━━━━ 00:15:36\n",
      "Accuracy: 0.8489 - Loss: 0.4285\n",
      "\n",
      "Batch 613/992 ━━━━━━━━━━━━━━━━━━━━ 00:15:47\n",
      "Accuracy: 0.8491 - Loss: 0.4280\n",
      "\n",
      "Batch 614/992 ━━━━━━━━━━━━━━━━━━━━ 00:15:58\n",
      "Accuracy: 0.8491 - Loss: 0.4284\n",
      "\n",
      "Batch 615/992 ━━━━━━━━━━━━━━━━━━━━ 00:16:08\n",
      "Accuracy: 0.8494 - Loss: 0.4280\n",
      "\n",
      "Batch 616/992 ━━━━━━━━━━━━━━━━━━━━ 00:16:19\n",
      "Accuracy: 0.8496 - Loss: 0.4275\n",
      "\n",
      "Batch 617/992 ━━━━━━━━━━━━━━━━━━━━ 00:16:29\n",
      "Accuracy: 0.8491 - Loss: 0.4282\n",
      "\n",
      "Batch 618/992 ━━━━━━━━━━━━━━━━━━━━ 00:16:40\n",
      "Accuracy: 0.8491 - Loss: 0.4288\n",
      "\n",
      "Batch 619/992 ━━━━━━━━━━━━━━━━━━━━ 00:16:51\n",
      "Accuracy: 0.8494 - Loss: 0.4281\n",
      "\n",
      "Batch 620/992 ━━━━━━━━━━━━━━━━━━━━ 00:17:02\n",
      "Accuracy: 0.8490 - Loss: 0.4289\n",
      "\n",
      "Batch 621/992 ━━━━━━━━━━━━━━━━━━━━ 00:17:15\n",
      "Accuracy: 0.8488 - Loss: 0.4289\n",
      "\n",
      "Batch 622/992 ━━━━━━━━━━━━━━━━━━━━ 00:17:26\n",
      "Accuracy: 0.8487 - Loss: 0.4293\n",
      "\n",
      "Batch 623/992 ━━━━━━━━━━━━━━━━━━━━ 00:17:37\n",
      "Accuracy: 0.8485 - Loss: 0.4293\n",
      "\n",
      "Batch 624/992 ━━━━━━━━━━━━━━━━━━━━ 00:17:48\n",
      "Accuracy: 0.8488 - Loss: 0.4288\n",
      "\n",
      "Batch 625/992 ━━━━━━━━━━━━━━━━━━━━ 00:17:59\n",
      "Accuracy: 0.8488 - Loss: 0.4286\n",
      "\n",
      "Batch 626/992 ━━━━━━━━━━━━━━━━━━━━ 00:18:10\n",
      "Accuracy: 0.8488 - Loss: 0.4284\n",
      "\n",
      "Batch 627/992 ━━━━━━━━━━━━━━━━━━━━ 00:18:20\n",
      "Accuracy: 0.8489 - Loss: 0.4283\n",
      "\n",
      "Batch 628/992 ━━━━━━━━━━━━━━━━━━━━ 00:18:31\n",
      "Accuracy: 0.8489 - Loss: 0.4283\n",
      "\n",
      "Batch 629/992 ━━━━━━━━━━━━━━━━━━━━ 00:18:42\n",
      "Accuracy: 0.8490 - Loss: 0.4284\n",
      "\n",
      "Batch 630/992 ━━━━━━━━━━━━━━━━━━━━ 00:18:52\n",
      "Accuracy: 0.8486 - Loss: 0.4292\n",
      "\n",
      "Batch 631/992 ━━━━━━━━━━━━━━━━━━━━ 00:19:03\n",
      "Accuracy: 0.8485 - Loss: 0.4294\n",
      "\n",
      "Batch 632/992 ━━━━━━━━━━━━━━━━━━━━ 00:19:13\n",
      "Accuracy: 0.8487 - Loss: 0.4290\n",
      "\n",
      "Batch 633/992 ━━━━━━━━━━━━━━━━━━━━ 00:19:24\n",
      "Accuracy: 0.8489 - Loss: 0.4285\n",
      "\n",
      "Batch 634/992 ━━━━━━━━━━━━━━━━━━━━ 00:19:34\n",
      "Accuracy: 0.8488 - Loss: 0.4291\n",
      "\n",
      "Batch 635/992 ━━━━━━━━━━━━━━━━━━━━ 00:19:45\n",
      "Accuracy: 0.8488 - Loss: 0.4290\n",
      "\n",
      "Batch 636/992 ━━━━━━━━━━━━━━━━━━━━ 00:19:56\n",
      "Accuracy: 0.8489 - Loss: 0.4288\n",
      "\n",
      "Batch 637/992 ━━━━━━━━━━━━━━━━━━━━ 00:20:07\n",
      "Accuracy: 0.8491 - Loss: 0.4283\n",
      "\n",
      "Batch 638/992 ━━━━━━━━━━━━━━━━━━━━ 00:20:17\n",
      "Accuracy: 0.8493 - Loss: 0.4279\n",
      "\n",
      "Batch 639/992 ━━━━━━━━━━━━━━━━━━━━ 00:20:28\n",
      "Accuracy: 0.8492 - Loss: 0.4285\n",
      "\n",
      "Batch 640/992 ━━━━━━━━━━━━━━━━━━━━ 00:20:38\n",
      "Accuracy: 0.8492 - Loss: 0.4282\n",
      "\n",
      "Batch 641/992 ━━━━━━━━━━━━━━━━━━━━ 00:20:49\n",
      "Accuracy: 0.8495 - Loss: 0.4277\n",
      "\n",
      "Batch 642/992 ━━━━━━━━━━━━━━━━━━━━ 00:20:59\n",
      "Accuracy: 0.8495 - Loss: 0.4275\n",
      "\n",
      "Batch 643/992 ━━━━━━━━━━━━━━━━━━━━ 00:21:12\n",
      "Accuracy: 0.8495 - Loss: 0.4273\n",
      "\n",
      "Batch 644/992 ━━━━━━━━━━━━━━━━━━━━ 00:21:23\n",
      "Accuracy: 0.8496 - Loss: 0.4273\n",
      "\n",
      "Batch 645/992 ━━━━━━━━━━━━━━━━━━━━ 00:21:33\n",
      "Accuracy: 0.8498 - Loss: 0.4268\n",
      "\n",
      "Batch 646/992 ━━━━━━━━━━━━━━━━━━━━ 00:21:43\n",
      "Accuracy: 0.8497 - Loss: 0.4268\n",
      "\n",
      "Batch 647/992 ━━━━━━━━━━━━━━━━━━━━ 00:21:54\n",
      "Accuracy: 0.8497 - Loss: 0.4271\n",
      "\n",
      "Batch 648/992 ━━━━━━━━━━━━━━━━━━━━ 00:22:04\n",
      "Accuracy: 0.8495 - Loss: 0.4280\n",
      "\n",
      "Batch 649/992 ━━━━━━━━━━━━━━━━━━━━ 00:22:15\n",
      "Accuracy: 0.8494 - Loss: 0.4282\n",
      "\n",
      "Batch 650/992 ━━━━━━━━━━━━━━━━━━━━ 00:22:25\n",
      "Accuracy: 0.8492 - Loss: 0.4285\n",
      "\n",
      "Batch 651/992 ━━━━━━━━━━━━━━━━━━━━ 00:22:35\n",
      "Accuracy: 0.8493 - Loss: 0.4283\n",
      "\n",
      "Batch 652/992 ━━━━━━━━━━━━━━━━━━━━ 00:22:46\n",
      "Accuracy: 0.8493 - Loss: 0.4284\n",
      "\n",
      "Batch 653/992 ━━━━━━━━━━━━━━━━━━━━ 00:22:58\n",
      "Accuracy: 0.8495 - Loss: 0.4279\n",
      "\n",
      "Batch 654/992 ━━━━━━━━━━━━━━━━━━━━ 00:23:08\n",
      "Accuracy: 0.8496 - Loss: 0.4277\n",
      "\n",
      "Batch 655/992 ━━━━━━━━━━━━━━━━━━━━ 00:23:19\n",
      "Accuracy: 0.8498 - Loss: 0.4273\n",
      "\n",
      "Batch 656/992 ━━━━━━━━━━━━━━━━━━━━ 00:23:30\n",
      "Accuracy: 0.8497 - Loss: 0.4279\n",
      "\n",
      "Batch 657/992 ━━━━━━━━━━━━━━━━━━━━ 00:23:41\n",
      "Accuracy: 0.8499 - Loss: 0.4277\n",
      "\n",
      "Batch 658/992 ━━━━━━━━━━━━━━━━━━━━ 00:23:54\n",
      "Accuracy: 0.8501 - Loss: 0.4273\n",
      "\n",
      "Batch 659/992 ━━━━━━━━━━━━━━━━━━━━ 00:24:05\n",
      "Accuracy: 0.8500 - Loss: 0.4281\n",
      "\n",
      "Batch 660/992 ━━━━━━━━━━━━━━━━━━━━ 00:24:15\n",
      "Accuracy: 0.8498 - Loss: 0.4293\n",
      "\n",
      "Batch 661/992 ━━━━━━━━━━━━━━━━━━━━ 00:24:26\n",
      "Accuracy: 0.8498 - Loss: 0.4291\n",
      "\n",
      "Batch 662/992 ━━━━━━━━━━━━━━━━━━━━ 00:24:36\n",
      "Accuracy: 0.8499 - Loss: 0.4291\n",
      "\n",
      "Batch 663/992 ━━━━━━━━━━━━━━━━━━━━ 00:24:46\n",
      "Accuracy: 0.8497 - Loss: 0.4295\n",
      "\n",
      "Batch 664/992 ━━━━━━━━━━━━━━━━━━━━ 00:24:57\n",
      "Accuracy: 0.8498 - Loss: 0.4297\n",
      "\n",
      "Batch 665/992 ━━━━━━━━━━━━━━━━━━━━ 00:25:08\n",
      "Accuracy: 0.8500 - Loss: 0.4292\n",
      "\n",
      "Batch 666/992 ━━━━━━━━━━━━━━━━━━━━ 00:25:18\n",
      "Accuracy: 0.8502 - Loss: 0.4289\n",
      "\n",
      "Batch 667/992 ━━━━━━━━━━━━━━━━━━━━ 00:25:29\n",
      "Accuracy: 0.8503 - Loss: 0.4293\n",
      "\n",
      "Batch 668/992 ━━━━━━━━━━━━━━━━━━━━ 00:25:39\n",
      "Accuracy: 0.8505 - Loss: 0.4288\n",
      "\n",
      "Batch 669/992 ━━━━━━━━━━━━━━━━━━━━ 00:25:50\n",
      "Accuracy: 0.8505 - Loss: 0.4290\n",
      "\n",
      "Batch 670/992 ━━━━━━━━━━━━━━━━━━━━ 00:26:01\n",
      "Accuracy: 0.8506 - Loss: 0.4294\n",
      "\n",
      "Batch 671/992 ━━━━━━━━━━━━━━━━━━━━ 00:26:11\n",
      "Accuracy: 0.8508 - Loss: 0.4290\n",
      "\n",
      "Batch 672/992 ━━━━━━━━━━━━━━━━━━━━ 00:26:22\n",
      "Accuracy: 0.8508 - Loss: 0.4288\n",
      "\n",
      "Batch 673/992 ━━━━━━━━━━━━━━━━━━━━ 00:26:33\n",
      "Accuracy: 0.8509 - Loss: 0.4285\n",
      "\n",
      "Batch 674/992 ━━━━━━━━━━━━━━━━━━━━ 00:26:43\n",
      "Accuracy: 0.8509 - Loss: 0.4281\n",
      "\n",
      "Batch 675/992 ━━━━━━━━━━━━━━━━━━━━ 00:26:54\n",
      "Accuracy: 0.8511 - Loss: 0.4277\n",
      "\n",
      "Batch 676/992 ━━━━━━━━━━━━━━━━━━━━ 00:27:05\n",
      "Accuracy: 0.8511 - Loss: 0.4277\n",
      "\n",
      "Batch 677/992 ━━━━━━━━━━━━━━━━━━━━ 00:27:15\n",
      "Accuracy: 0.8510 - Loss: 0.4277\n",
      "\n",
      "Batch 678/992 ━━━━━━━━━━━━━━━━━━━━ 00:27:26\n",
      "Accuracy: 0.8512 - Loss: 0.4272\n",
      "\n",
      "Batch 679/992 ━━━━━━━━━━━━━━━━━━━━ 00:27:37\n",
      "Accuracy: 0.8513 - Loss: 0.4271\n",
      "\n",
      "Batch 680/992 ━━━━━━━━━━━━━━━━━━━━ 00:27:47\n",
      "Accuracy: 0.8509 - Loss: 0.4276\n",
      "\n",
      "Batch 681/992 ━━━━━━━━━━━━━━━━━━━━ 00:27:57\n",
      "Accuracy: 0.8510 - Loss: 0.4279\n",
      "\n",
      "Batch 682/992 ━━━━━━━━━━━━━━━━━━━━ 00:28:08\n",
      "Accuracy: 0.8510 - Loss: 0.4278\n",
      "\n",
      "Batch 683/992 ━━━━━━━━━━━━━━━━━━━━ 00:28:20\n",
      "Accuracy: 0.8512 - Loss: 0.4272\n",
      "\n",
      "Batch 684/992 ━━━━━━━━━━━━━━━━━━━━ 00:28:31\n",
      "Accuracy: 0.8514 - Loss: 0.4269\n",
      "\n",
      "Batch 685/992 ━━━━━━━━━━━━━━━━━━━━ 00:28:43\n",
      "Accuracy: 0.8515 - Loss: 0.4267\n",
      "\n",
      "Batch 686/992 ━━━━━━━━━━━━━━━━━━━━ 00:28:54\n",
      "Accuracy: 0.8515 - Loss: 0.4267\n",
      "\n",
      "Batch 687/992 ━━━━━━━━━━━━━━━━━━━━ 00:29:04\n",
      "Accuracy: 0.8515 - Loss: 0.4265\n",
      "\n",
      "Batch 688/992 ━━━━━━━━━━━━━━━━━━━━ 00:29:14\n",
      "Accuracy: 0.8517 - Loss: 0.4263\n",
      "\n",
      "Batch 689/992 ━━━━━━━━━━━━━━━━━━━━ 00:29:25\n",
      "Accuracy: 0.8520 - Loss: 0.4260\n",
      "\n",
      "Batch 690/992 ━━━━━━━━━━━━━━━━━━━━ 00:29:35\n",
      "Accuracy: 0.8520 - Loss: 0.4258\n",
      "\n",
      "Batch 691/992 ━━━━━━━━━━━━━━━━━━━━ 00:29:46\n",
      "Accuracy: 0.8518 - Loss: 0.4260\n",
      "\n",
      "Batch 692/992 ━━━━━━━━━━━━━━━━━━━━ 00:29:57\n",
      "Accuracy: 0.8519 - Loss: 0.4260\n",
      "\n",
      "Batch 693/992 ━━━━━━━━━━━━━━━━━━━━ 00:30:09\n",
      "Accuracy: 0.8521 - Loss: 0.4254\n",
      "\n",
      "Batch 694/992 ━━━━━━━━━━━━━━━━━━━━ 00:30:21\n",
      "Accuracy: 0.8521 - Loss: 0.4255\n",
      "\n",
      "Batch 695/992 ━━━━━━━━━━━━━━━━━━━━ 00:30:32\n",
      "Accuracy: 0.8522 - Loss: 0.4252\n",
      "\n",
      "Batch 696/992 ━━━━━━━━━━━━━━━━━━━━ 00:30:42\n",
      "Accuracy: 0.8518 - Loss: 0.4263\n",
      "\n",
      "Batch 697/992 ━━━━━━━━━━━━━━━━━━━━ 00:30:53\n",
      "Accuracy: 0.8519 - Loss: 0.4263\n",
      "\n",
      "Batch 698/992 ━━━━━━━━━━━━━━━━━━━━ 00:31:04\n",
      "Accuracy: 0.8517 - Loss: 0.4264\n",
      "\n",
      "Batch 699/992 ━━━━━━━━━━━━━━━━━━━━ 00:31:14\n",
      "Accuracy: 0.8516 - Loss: 0.4269\n",
      "\n",
      "Batch 700/992 ━━━━━━━━━━━━━━━━━━━━ 00:31:25\n",
      "Accuracy: 0.8516 - Loss: 0.4271\n",
      "\n",
      "Batch 701/992 ━━━━━━━━━━━━━━━━━━━━ 00:31:35\n",
      "Accuracy: 0.8515 - Loss: 0.4272\n",
      "\n",
      "Batch 702/992 ━━━━━━━━━━━━━━━━━━━━ 00:31:46\n",
      "Accuracy: 0.8517 - Loss: 0.4269\n",
      "\n",
      "Batch 703/992 ━━━━━━━━━━━━━━━━━━━━ 00:31:56\n",
      "Accuracy: 0.8519 - Loss: 0.4264\n",
      "\n",
      "Batch 704/992 ━━━━━━━━━━━━━━━━━━━━ 00:32:07\n",
      "Accuracy: 0.8521 - Loss: 0.4262\n",
      "\n",
      "Batch 705/992 ━━━━━━━━━━━━━━━━━━━━ 00:32:18\n",
      "Accuracy: 0.8518 - Loss: 0.4268\n",
      "\n",
      "Batch 706/992 ━━━━━━━━━━━━━━━━━━━━ 00:32:28\n",
      "Accuracy: 0.8518 - Loss: 0.4268\n",
      "\n",
      "Batch 707/992 ━━━━━━━━━━━━━━━━━━━━ 00:32:38\n",
      "Accuracy: 0.8517 - Loss: 0.4271\n",
      "\n",
      "Batch 708/992 ━━━━━━━━━━━━━━━━━━━━ 00:32:49\n",
      "Accuracy: 0.8517 - Loss: 0.4278\n",
      "\n",
      "Batch 709/992 ━━━━━━━━━━━━━━━━━━━━ 00:32:59\n",
      "Accuracy: 0.8514 - Loss: 0.4287\n",
      "\n",
      "Batch 710/992 ━━━━━━━━━━━━━━━━━━━━ 00:33:09\n",
      "Accuracy: 0.8511 - Loss: 0.4290\n",
      "\n",
      "Batch 711/992 ━━━━━━━━━━━━━━━━━━━━ 00:33:20\n",
      "Accuracy: 0.8513 - Loss: 0.4286\n",
      "\n",
      "Batch 712/992 ━━━━━━━━━━━━━━━━━━━━ 00:33:31\n",
      "Accuracy: 0.8513 - Loss: 0.4284\n",
      "\n",
      "Batch 713/992 ━━━━━━━━━━━━━━━━━━━━ 00:33:44\n",
      "Accuracy: 0.8513 - Loss: 0.4285\n",
      "\n",
      "Batch 714/992 ━━━━━━━━━━━━━━━━━━━━ 00:33:54\n",
      "Accuracy: 0.8515 - Loss: 0.4282\n",
      "\n",
      "Batch 715/992 ━━━━━━━━━━━━━━━━━━━━ 00:34:05\n",
      "Accuracy: 0.8516 - Loss: 0.4283\n",
      "\n",
      "Batch 716/992 ━━━━━━━━━━━━━━━━━━━━ 00:34:16\n",
      "Accuracy: 0.8518 - Loss: 0.4277\n",
      "\n",
      "Batch 717/992 ━━━━━━━━━━━━━━━━━━━━ 00:34:26\n",
      "Accuracy: 0.8518 - Loss: 0.4276\n",
      "\n",
      "Batch 718/992 ━━━━━━━━━━━━━━━━━━━━ 00:34:37\n",
      "Accuracy: 0.8518 - Loss: 0.4279\n",
      "\n",
      "Batch 719/992 ━━━━━━━━━━━━━━━━━━━━ 00:34:47\n",
      "Accuracy: 0.8521 - Loss: 0.4274\n",
      "\n",
      "Batch 720/992 ━━━━━━━━━━━━━━━━━━━━ 00:34:58\n",
      "Accuracy: 0.8521 - Loss: 0.4276\n",
      "\n",
      "Batch 721/992 ━━━━━━━━━━━━━━━━━━━━ 00:35:09\n",
      "Accuracy: 0.8523 - Loss: 0.4272\n",
      "\n",
      "Batch 722/992 ━━━━━━━━━━━━━━━━━━━━ 00:35:20\n",
      "Accuracy: 0.8523 - Loss: 0.4270\n",
      "\n",
      "Batch 723/992 ━━━━━━━━━━━━━━━━━━━━ 00:35:31\n",
      "Accuracy: 0.8525 - Loss: 0.4266\n",
      "\n",
      "Batch 724/992 ━━━━━━━━━━━━━━━━━━━━ 00:35:41\n",
      "Accuracy: 0.8524 - Loss: 0.4277\n",
      "\n",
      "Batch 725/992 ━━━━━━━━━━━━━━━━━━━━ 00:35:52\n",
      "Accuracy: 0.8524 - Loss: 0.4278\n",
      "\n",
      "Batch 726/992 ━━━━━━━━━━━━━━━━━━━━ 00:36:02\n",
      "Accuracy: 0.8524 - Loss: 0.4276\n",
      "\n",
      "Batch 727/992 ━━━━━━━━━━━━━━━━━━━━ 00:36:13\n",
      "Accuracy: 0.8525 - Loss: 0.4278\n",
      "\n",
      "Batch 728/992 ━━━━━━━━━━━━━━━━━━━━ 00:36:24\n",
      "Accuracy: 0.8523 - Loss: 0.4278\n",
      "\n",
      "Batch 729/992 ━━━━━━━━━━━━━━━━━━━━ 00:36:34\n",
      "Accuracy: 0.8525 - Loss: 0.4274\n",
      "\n",
      "Batch 730/992 ━━━━━━━━━━━━━━━━━━━━ 00:36:45\n",
      "Accuracy: 0.8526 - Loss: 0.4273\n",
      "\n",
      "Batch 731/992 ━━━━━━━━━━━━━━━━━━━━ 00:36:56\n",
      "Accuracy: 0.8524 - Loss: 0.4274\n",
      "\n",
      "Batch 732/992 ━━━━━━━━━━━━━━━━━━━━ 00:37:06\n",
      "Accuracy: 0.8526 - Loss: 0.4272\n",
      "\n",
      "Batch 733/992 ━━━━━━━━━━━━━━━━━━━━ 00:37:17\n",
      "Accuracy: 0.8527 - Loss: 0.4270\n",
      "\n",
      "Batch 734/992 ━━━━━━━━━━━━━━━━━━━━ 00:37:27\n",
      "Accuracy: 0.8527 - Loss: 0.4267\n",
      "\n",
      "Batch 735/992 ━━━━━━━━━━━━━━━━━━━━ 00:37:38\n",
      "Accuracy: 0.8527 - Loss: 0.4265\n",
      "\n",
      "Batch 736/992 ━━━━━━━━━━━━━━━━━━━━ 00:37:49\n",
      "Accuracy: 0.8526 - Loss: 0.4266\n",
      "\n",
      "Batch 737/992 ━━━━━━━━━━━━━━━━━━━━ 00:37:59\n",
      "Accuracy: 0.8528 - Loss: 0.4262\n",
      "\n",
      "Batch 738/992 ━━━━━━━━━━━━━━━━━━━━ 00:38:10\n",
      "Accuracy: 0.8528 - Loss: 0.4262\n",
      "\n",
      "Batch 739/992 ━━━━━━━━━━━━━━━━━━━━ 00:38:21\n",
      "Accuracy: 0.8527 - Loss: 0.4270\n",
      "\n",
      "Batch 740/992 ━━━━━━━━━━━━━━━━━━━━ 00:38:31\n",
      "Accuracy: 0.8524 - Loss: 0.4276\n",
      "\n",
      "Batch 741/992 ━━━━━━━━━━━━━━━━━━━━ 00:38:42\n",
      "Accuracy: 0.8526 - Loss: 0.4272\n",
      "\n",
      "Batch 742/992 ━━━━━━━━━━━━━━━━━━━━ 00:38:53\n",
      "Accuracy: 0.8524 - Loss: 0.4275\n",
      "\n",
      "Batch 743/992 ━━━━━━━━━━━━━━━━━━━━ 00:39:04\n",
      "Accuracy: 0.8526 - Loss: 0.4270\n",
      "\n",
      "Batch 744/992 ━━━━━━━━━━━━━━━━━━━━ 00:39:15\n",
      "Accuracy: 0.8528 - Loss: 0.4269\n",
      "\n",
      "Batch 745/992 ━━━━━━━━━━━━━━━━━━━━ 00:39:26\n",
      "Accuracy: 0.8529 - Loss: 0.4268\n",
      "\n",
      "Batch 746/992 ━━━━━━━━━━━━━━━━━━━━ 00:39:37\n",
      "Accuracy: 0.8525 - Loss: 0.4281\n",
      "\n",
      "Batch 747/992 ━━━━━━━━━━━━━━━━━━━━ 00:39:48\n",
      "Accuracy: 0.8526 - Loss: 0.4282\n",
      "\n",
      "Batch 748/992 ━━━━━━━━━━━━━━━━━━━━ 00:39:58\n",
      "Accuracy: 0.8526 - Loss: 0.4284\n",
      "\n",
      "Batch 749/992 ━━━━━━━━━━━━━━━━━━━━ 00:40:12\n",
      "Accuracy: 0.8526 - Loss: 0.4284\n",
      "\n",
      "Batch 750/992 ━━━━━━━━━━━━━━━━━━━━ 00:40:23\n",
      "Accuracy: 0.8525 - Loss: 0.4282\n",
      "\n",
      "Batch 751/992 ━━━━━━━━━━━━━━━━━━━━ 00:40:35\n",
      "Accuracy: 0.8525 - Loss: 0.4282\n",
      "\n",
      "Batch 752/992 ━━━━━━━━━━━━━━━━━━━━ 00:40:45\n",
      "Accuracy: 0.8526 - Loss: 0.4279\n",
      "\n",
      "Batch 753/992 ━━━━━━━━━━━━━━━━━━━━ 00:40:56\n",
      "Accuracy: 0.8526 - Loss: 0.4281\n",
      "\n",
      "Batch 754/992 ━━━━━━━━━━━━━━━━━━━━ 00:41:06\n",
      "Accuracy: 0.8528 - Loss: 0.4277\n",
      "\n",
      "Batch 755/992 ━━━━━━━━━━━━━━━━━━━━ 00:41:17\n",
      "Accuracy: 0.8530 - Loss: 0.4275\n",
      "\n",
      "Batch 756/992 ━━━━━━━━━━━━━━━━━━━━ 00:41:28\n",
      "Accuracy: 0.8530 - Loss: 0.4276\n",
      "\n",
      "Batch 757/992 ━━━━━━━━━━━━━━━━━━━━ 00:41:38\n",
      "Accuracy: 0.8532 - Loss: 0.4275\n",
      "\n",
      "Batch 758/992 ━━━━━━━━━━━━━━━━━━━━ 00:41:48\n",
      "Accuracy: 0.8532 - Loss: 0.4274\n",
      "\n",
      "Batch 759/992 ━━━━━━━━━━━━━━━━━━━━ 00:41:58\n",
      "Accuracy: 0.8531 - Loss: 0.4277\n",
      "\n",
      "Batch 760/992 ━━━━━━━━━━━━━━━━━━━━ 00:42:08\n",
      "Accuracy: 0.8531 - Loss: 0.4281\n",
      "\n",
      "Batch 761/992 ━━━━━━━━━━━━━━━━━━━━ 00:42:18\n",
      "Accuracy: 0.8533 - Loss: 0.4277\n",
      "\n",
      "Batch 762/992 ━━━━━━━━━━━━━━━━━━━━ 00:42:28\n",
      "Accuracy: 0.8532 - Loss: 0.4278\n",
      "\n",
      "Batch 763/992 ━━━━━━━━━━━━━━━━━━━━ 00:42:39\n",
      "Accuracy: 0.8534 - Loss: 0.4275\n",
      "\n",
      "Batch 764/992 ━━━━━━━━━━━━━━━━━━━━ 00:42:49\n",
      "Accuracy: 0.8536 - Loss: 0.4270\n",
      "\n",
      "Batch 765/992 ━━━━━━━━━━━━━━━━━━━━ 00:42:59\n",
      "Accuracy: 0.8534 - Loss: 0.4273\n",
      "\n",
      "Batch 766/992 ━━━━━━━━━━━━━━━━━━━━ 00:43:09\n",
      "Accuracy: 0.8533 - Loss: 0.4275\n",
      "\n",
      "Batch 767/992 ━━━━━━━━━━━━━━━━━━━━ 00:43:20\n",
      "Accuracy: 0.8533 - Loss: 0.4273\n",
      "\n",
      "Batch 768/992 ━━━━━━━━━━━━━━━━━━━━ 00:43:30\n",
      "Accuracy: 0.8532 - Loss: 0.4274\n",
      "\n",
      "Batch 769/992 ━━━━━━━━━━━━━━━━━━━━ 00:43:40\n",
      "Accuracy: 0.8531 - Loss: 0.4274\n",
      "\n",
      "Batch 770/992 ━━━━━━━━━━━━━━━━━━━━ 00:43:50\n",
      "Accuracy: 0.8532 - Loss: 0.4271\n",
      "\n",
      "Batch 771/992 ━━━━━━━━━━━━━━━━━━━━ 00:44:00\n",
      "Accuracy: 0.8533 - Loss: 0.4271\n",
      "\n",
      "Batch 772/992 ━━━━━━━━━━━━━━━━━━━━ 00:44:10\n",
      "Accuracy: 0.8535 - Loss: 0.4266\n",
      "\n",
      "Batch 773/992 ━━━━━━━━━━━━━━━━━━━━ 00:44:20\n",
      "Accuracy: 0.8537 - Loss: 0.4262\n",
      "\n",
      "Batch 774/992 ━━━━━━━━━━━━━━━━━━━━ 00:44:30\n",
      "Accuracy: 0.8537 - Loss: 0.4262\n",
      "\n",
      "Batch 775/992 ━━━━━━━━━━━━━━━━━━━━ 00:44:40\n",
      "Accuracy: 0.8539 - Loss: 0.4258\n",
      "\n",
      "Batch 776/992 ━━━━━━━━━━━━━━━━━━━━ 00:44:51\n",
      "Accuracy: 0.8537 - Loss: 0.4257\n",
      "\n",
      "Batch 777/992 ━━━━━━━━━━━━━━━━━━━━ 00:45:01\n",
      "Accuracy: 0.8536 - Loss: 0.4256\n",
      "\n",
      "Batch 778/992 ━━━━━━━━━━━━━━━━━━━━ 00:45:11\n",
      "Accuracy: 0.8538 - Loss: 0.4252\n",
      "\n",
      "Batch 779/992 ━━━━━━━━━━━━━━━━━━━━ 00:45:21\n",
      "Accuracy: 0.8538 - Loss: 0.4250\n",
      "\n",
      "Batch 780/992 ━━━━━━━━━━━━━━━━━━━━ 00:45:32\n",
      "Accuracy: 0.8540 - Loss: 0.4246\n",
      "\n",
      "Batch 781/992 ━━━━━━━━━━━━━━━━━━━━ 00:45:42\n",
      "Accuracy: 0.8540 - Loss: 0.4245\n",
      "\n",
      "Batch 782/992 ━━━━━━━━━━━━━━━━━━━━ 00:45:52\n",
      "Accuracy: 0.8541 - Loss: 0.4243\n",
      "\n",
      "Batch 783/992 ━━━━━━━━━━━━━━━━━━━━ 00:46:02\n",
      "Accuracy: 0.8541 - Loss: 0.4243\n",
      "\n",
      "Batch 784/992 ━━━━━━━━━━━━━━━━━━━━ 00:46:12\n",
      "Accuracy: 0.8541 - Loss: 0.4242\n",
      "\n",
      "Batch 785/992 ━━━━━━━━━━━━━━━━━━━━ 00:46:22\n",
      "Accuracy: 0.8543 - Loss: 0.4238\n",
      "\n",
      "Batch 786/992 ━━━━━━━━━━━━━━━━━━━━ 00:46:32\n",
      "Accuracy: 0.8542 - Loss: 0.4243\n",
      "\n",
      "Batch 787/992 ━━━━━━━━━━━━━━━━━━━━ 00:46:42\n",
      "Accuracy: 0.8540 - Loss: 0.4249\n",
      "\n",
      "Batch 788/992 ━━━━━━━━━━━━━━━━━━━━ 00:46:53\n",
      "Accuracy: 0.8541 - Loss: 0.4246\n",
      "\n",
      "Batch 789/992 ━━━━━━━━━━━━━━━━━━━━ 00:47:03\n",
      "Accuracy: 0.8541 - Loss: 0.4243\n",
      "\n",
      "Batch 790/992 ━━━━━━━━━━━━━━━━━━━━ 00:47:13\n",
      "Accuracy: 0.8541 - Loss: 0.4241\n",
      "\n",
      "Batch 791/992 ━━━━━━━━━━━━━━━━━━━━ 00:47:23\n",
      "Accuracy: 0.8540 - Loss: 0.4240\n",
      "\n",
      "Batch 792/992 ━━━━━━━━━━━━━━━━━━━━ 00:47:33\n",
      "Accuracy: 0.8542 - Loss: 0.4237\n",
      "\n",
      "Batch 793/992 ━━━━━━━━━━━━━━━━━━━━ 00:47:43\n",
      "Accuracy: 0.8544 - Loss: 0.4233\n",
      "\n",
      "Batch 794/992 ━━━━━━━━━━━━━━━━━━━━ 00:47:54\n",
      "Accuracy: 0.8545 - Loss: 0.4231\n",
      "\n",
      "Batch 795/992 ━━━━━━━━━━━━━━━━━━━━ 00:48:04\n",
      "Accuracy: 0.8547 - Loss: 0.4226\n",
      "\n",
      "Batch 796/992 ━━━━━━━━━━━━━━━━━━━━ 00:48:14\n",
      "Accuracy: 0.8546 - Loss: 0.4225\n",
      "\n",
      "Batch 797/992 ━━━━━━━━━━━━━━━━━━━━ 00:48:24\n",
      "Accuracy: 0.8545 - Loss: 0.4227\n",
      "\n",
      "Batch 798/992 ━━━━━━━━━━━━━━━━━━━━ 00:48:34\n",
      "Accuracy: 0.8546 - Loss: 0.4224\n",
      "\n",
      "Batch 799/992 ━━━━━━━━━━━━━━━━━━━━ 00:48:44\n",
      "Accuracy: 0.8547 - Loss: 0.4221\n",
      "\n",
      "Batch 800/992 ━━━━━━━━━━━━━━━━━━━━ 00:48:54\n",
      "Accuracy: 0.8545 - Loss: 0.4223\n",
      "\n",
      "Batch 801/992 ━━━━━━━━━━━━━━━━━━━━ 00:49:04\n",
      "Accuracy: 0.8546 - Loss: 0.4221\n",
      "\n",
      "Batch 802/992 ━━━━━━━━━━━━━━━━━━━━ 00:49:14\n",
      "Accuracy: 0.8547 - Loss: 0.4218\n",
      "\n",
      "Batch 803/992 ━━━━━━━━━━━━━━━━━━━━ 00:49:25\n",
      "Accuracy: 0.8549 - Loss: 0.4214\n",
      "\n",
      "Batch 804/992 ━━━━━━━━━━━━━━━━━━━━ 00:49:35\n",
      "Accuracy: 0.8551 - Loss: 0.4209\n",
      "\n",
      "Batch 805/992 ━━━━━━━━━━━━━━━━━━━━ 00:49:46\n",
      "Accuracy: 0.8553 - Loss: 0.4205\n",
      "\n",
      "Batch 806/992 ━━━━━━━━━━━━━━━━━━━━ 00:49:56\n",
      "Accuracy: 0.8551 - Loss: 0.4209\n",
      "\n",
      "Batch 807/992 ━━━━━━━━━━━━━━━━━━━━ 00:50:06\n",
      "Accuracy: 0.8550 - Loss: 0.4208\n",
      "\n",
      "Batch 808/992 ━━━━━━━━━━━━━━━━━━━━ 00:50:17\n",
      "Accuracy: 0.8552 - Loss: 0.4205\n",
      "\n",
      "Batch 809/992 ━━━━━━━━━━━━━━━━━━━━ 00:50:27\n",
      "Accuracy: 0.8552 - Loss: 0.4202\n",
      "\n",
      "Batch 810/992 ━━━━━━━━━━━━━━━━━━━━ 00:50:37\n",
      "Accuracy: 0.8551 - Loss: 0.4206\n",
      "\n",
      "Batch 811/992 ━━━━━━━━━━━━━━━━━━━━ 00:50:47\n",
      "Accuracy: 0.8551 - Loss: 0.4204\n",
      "\n",
      "Batch 812/992 ━━━━━━━━━━━━━━━━━━━━ 00:50:57\n",
      "Accuracy: 0.8550 - Loss: 0.4204\n",
      "\n",
      "Batch 813/992 ━━━━━━━━━━━━━━━━━━━━ 00:51:07\n",
      "Accuracy: 0.8549 - Loss: 0.4209\n",
      "\n",
      "Batch 814/992 ━━━━━━━━━━━━━━━━━━━━ 00:51:17\n",
      "Accuracy: 0.8549 - Loss: 0.4209\n",
      "\n",
      "Batch 815/992 ━━━━━━━━━━━━━━━━━━━━ 00:51:27\n",
      "Accuracy: 0.8546 - Loss: 0.4212\n",
      "\n",
      "Batch 816/992 ━━━━━━━━━━━━━━━━━━━━ 00:51:37\n",
      "Accuracy: 0.8545 - Loss: 0.4214\n",
      "\n",
      "Batch 817/992 ━━━━━━━━━━━━━━━━━━━━ 00:51:47\n",
      "Accuracy: 0.8547 - Loss: 0.4210\n",
      "\n",
      "Batch 818/992 ━━━━━━━━━━━━━━━━━━━━ 00:51:57\n",
      "Accuracy: 0.8548 - Loss: 0.4205\n",
      "\n",
      "Batch 819/992 ━━━━━━━━━━━━━━━━━━━━ 00:52:08\n",
      "Accuracy: 0.8545 - Loss: 0.4208\n",
      "\n",
      "Batch 820/992 ━━━━━━━━━━━━━━━━━━━━ 00:52:18\n",
      "Accuracy: 0.8546 - Loss: 0.4207\n",
      "\n",
      "Batch 821/992 ━━━━━━━━━━━━━━━━━━━━ 00:52:28\n",
      "Accuracy: 0.8544 - Loss: 0.4208\n",
      "\n",
      "Batch 822/992 ━━━━━━━━━━━━━━━━━━━━ 00:52:38\n",
      "Accuracy: 0.8543 - Loss: 0.4210\n",
      "\n",
      "Batch 823/992 ━━━━━━━━━━━━━━━━━━━━ 00:52:48\n",
      "Accuracy: 0.8543 - Loss: 0.4208\n",
      "\n",
      "Batch 824/992 ━━━━━━━━━━━━━━━━━━━━ 00:52:58\n",
      "Accuracy: 0.8545 - Loss: 0.4204\n",
      "\n",
      "Batch 825/992 ━━━━━━━━━━━━━━━━━━━━ 00:53:08\n",
      "Accuracy: 0.8544 - Loss: 0.4208\n",
      "\n",
      "Batch 826/992 ━━━━━━━━━━━━━━━━━━━━ 00:53:18\n",
      "Accuracy: 0.8541 - Loss: 0.4210\n",
      "\n",
      "Batch 827/992 ━━━━━━━━━━━━━━━━━━━━ 00:53:29\n",
      "Accuracy: 0.8541 - Loss: 0.4210\n",
      "\n",
      "Batch 828/992 ━━━━━━━━━━━━━━━━━━━━ 00:53:39\n",
      "Accuracy: 0.8539 - Loss: 0.4215\n",
      "\n",
      "Batch 829/992 ━━━━━━━━━━━━━━━━━━━━ 00:53:49\n",
      "Accuracy: 0.8537 - Loss: 0.4215\n",
      "\n",
      "Batch 830/992 ━━━━━━━━━━━━━━━━━━━━ 00:53:59\n",
      "Accuracy: 0.8535 - Loss: 0.4220\n",
      "\n",
      "Batch 831/992 ━━━━━━━━━━━━━━━━━━━━ 00:54:09\n",
      "Accuracy: 0.8535 - Loss: 0.4218\n",
      "\n",
      "Batch 832/992 ━━━━━━━━━━━━━━━━━━━━ 00:54:20\n",
      "Accuracy: 0.8534 - Loss: 0.4217\n",
      "\n",
      "Batch 833/992 ━━━━━━━━━━━━━━━━━━━━ 00:54:30\n",
      "Accuracy: 0.8532 - Loss: 0.4220\n",
      "\n",
      "Batch 834/992 ━━━━━━━━━━━━━━━━━━━━ 00:54:40\n",
      "Accuracy: 0.8531 - Loss: 0.4220\n",
      "\n",
      "Batch 835/992 ━━━━━━━━━━━━━━━━━━━━ 00:54:50\n",
      "Accuracy: 0.8530 - Loss: 0.4229\n",
      "\n",
      "Batch 836/992 ━━━━━━━━━━━━━━━━━━━━ 00:55:00\n",
      "Accuracy: 0.8532 - Loss: 0.4225\n",
      "\n",
      "Batch 837/992 ━━━━━━━━━━━━━━━━━━━━ 00:55:10\n",
      "Accuracy: 0.8532 - Loss: 0.4223\n",
      "\n",
      "Batch 838/992 ━━━━━━━━━━━━━━━━━━━━ 00:55:20\n",
      "Accuracy: 0.8532 - Loss: 0.4224\n",
      "\n",
      "Batch 839/992 ━━━━━━━━━━━━━━━━━━━━ 00:55:30\n",
      "Accuracy: 0.8531 - Loss: 0.4222\n",
      "\n",
      "Batch 840/992 ━━━━━━━━━━━━━━━━━━━━ 00:55:40\n",
      "Accuracy: 0.8533 - Loss: 0.4217\n",
      "\n",
      "Batch 841/992 ━━━━━━━━━━━━━━━━━━━━ 00:55:50\n",
      "Accuracy: 0.8533 - Loss: 0.4218\n",
      "\n",
      "Batch 842/992 ━━━━━━━━━━━━━━━━━━━━ 00:56:00\n",
      "Accuracy: 0.8530 - Loss: 0.4219\n",
      "\n",
      "Batch 843/992 ━━━━━━━━━━━━━━━━━━━━ 00:56:11\n",
      "Accuracy: 0.8529 - Loss: 0.4220\n",
      "\n",
      "Batch 844/992 ━━━━━━━━━━━━━━━━━━━━ 00:56:21\n",
      "Accuracy: 0.8528 - Loss: 0.4220\n",
      "\n",
      "Batch 845/992 ━━━━━━━━━━━━━━━━━━━━ 00:56:32\n",
      "Accuracy: 0.8527 - Loss: 0.4222\n",
      "\n",
      "Batch 846/992 ━━━━━━━━━━━━━━━━━━━━ 00:56:42\n",
      "Accuracy: 0.8528 - Loss: 0.4218\n",
      "\n",
      "Batch 847/992 ━━━━━━━━━━━━━━━━━━━━ 00:56:52\n",
      "Accuracy: 0.8527 - Loss: 0.4223\n",
      "\n",
      "Batch 848/992 ━━━━━━━━━━━━━━━━━━━━ 00:57:02\n",
      "Accuracy: 0.8527 - Loss: 0.4225\n",
      "\n",
      "Batch 849/992 ━━━━━━━━━━━━━━━━━━━━ 00:57:12\n",
      "Accuracy: 0.8525 - Loss: 0.4231\n",
      "\n",
      "Batch 850/992 ━━━━━━━━━━━━━━━━━━━━ 00:57:22\n",
      "Accuracy: 0.8526 - Loss: 0.4228\n",
      "\n",
      "Batch 851/992 ━━━━━━━━━━━━━━━━━━━━ 00:57:32\n",
      "Accuracy: 0.8527 - Loss: 0.4227\n",
      "\n",
      "Batch 852/992 ━━━━━━━━━━━━━━━━━━━━ 00:57:42\n",
      "Accuracy: 0.8524 - Loss: 0.4235\n",
      "\n",
      "Batch 853/992 ━━━━━━━━━━━━━━━━━━━━ 00:57:52\n",
      "Accuracy: 0.8524 - Loss: 0.4234\n",
      "\n",
      "Batch 854/992 ━━━━━━━━━━━━━━━━━━━━ 00:58:02\n",
      "Accuracy: 0.8526 - Loss: 0.4232\n",
      "\n",
      "Batch 855/992 ━━━━━━━━━━━━━━━━━━━━ 00:58:13\n",
      "Accuracy: 0.8525 - Loss: 0.4232\n",
      "\n",
      "Batch 856/992 ━━━━━━━━━━━━━━━━━━━━ 00:58:23\n",
      "Accuracy: 0.8524 - Loss: 0.4235\n",
      "\n",
      "Batch 857/992 ━━━━━━━━━━━━━━━━━━━━ 00:58:33\n",
      "Accuracy: 0.8524 - Loss: 0.4234\n",
      "\n",
      "Batch 858/992 ━━━━━━━━━━━━━━━━━━━━ 00:58:43\n",
      "Accuracy: 0.8524 - Loss: 0.4234\n",
      "\n",
      "Batch 859/992 ━━━━━━━━━━━━━━━━━━━━ 00:58:53\n",
      "Accuracy: 0.8526 - Loss: 0.4230\n",
      "\n",
      "Batch 860/992 ━━━━━━━━━━━━━━━━━━━━ 00:59:03\n",
      "Accuracy: 0.8528 - Loss: 0.4228\n",
      "\n",
      "Batch 861/992 ━━━━━━━━━━━━━━━━━━━━ 00:59:13\n",
      "Accuracy: 0.8529 - Loss: 0.4226\n",
      "\n",
      "Batch 862/992 ━━━━━━━━━━━━━━━━━━━━ 00:59:23\n",
      "Accuracy: 0.8530 - Loss: 0.4224\n",
      "\n",
      "Batch 863/992 ━━━━━━━━━━━━━━━━━━━━ 00:59:33\n",
      "Accuracy: 0.8530 - Loss: 0.4223\n",
      "\n",
      "Batch 864/992 ━━━━━━━━━━━━━━━━━━━━ 00:59:44\n",
      "Accuracy: 0.8530 - Loss: 0.4227\n",
      "\n",
      "Batch 865/992 ━━━━━━━━━━━━━━━━━━━━ 00:59:54\n",
      "Accuracy: 0.8529 - Loss: 0.4227\n",
      "\n",
      "Batch 866/992 ━━━━━━━━━━━━━━━━━━━━ 01:00:04\n",
      "Accuracy: 0.8531 - Loss: 0.4223\n",
      "\n",
      "Batch 867/992 ━━━━━━━━━━━━━━━━━━━━ 01:00:14\n",
      "Accuracy: 0.8532 - Loss: 0.4220\n",
      "\n",
      "Batch 868/992 ━━━━━━━━━━━━━━━━━━━━ 01:00:25\n",
      "Accuracy: 0.8533 - Loss: 0.4220\n",
      "\n",
      "Batch 869/992 ━━━━━━━━━━━━━━━━━━━━ 01:00:35\n",
      "Accuracy: 0.8534 - Loss: 0.4216\n",
      "\n",
      "Batch 870/992 ━━━━━━━━━━━━━━━━━━━━ 01:00:45\n",
      "Accuracy: 0.8533 - Loss: 0.4219\n",
      "\n",
      "Batch 871/992 ━━━━━━━━━━━━━━━━━━━━ 01:00:55\n",
      "Accuracy: 0.8533 - Loss: 0.4217\n",
      "\n",
      "Batch 872/992 ━━━━━━━━━━━━━━━━━━━━ 01:01:05\n",
      "Accuracy: 0.8534 - Loss: 0.4215\n",
      "\n",
      "Batch 873/992 ━━━━━━━━━━━━━━━━━━━━ 01:01:15\n",
      "Accuracy: 0.8535 - Loss: 0.4211\n",
      "\n",
      "Batch 874/992 ━━━━━━━━━━━━━━━━━━━━ 01:01:25\n",
      "Accuracy: 0.8535 - Loss: 0.4209\n",
      "\n",
      "Batch 875/992 ━━━━━━━━━━━━━━━━━━━━ 01:01:35\n",
      "Accuracy: 0.8534 - Loss: 0.4211\n",
      "\n",
      "Batch 876/992 ━━━━━━━━━━━━━━━━━━━━ 01:01:45\n",
      "Accuracy: 0.8533 - Loss: 0.4215\n",
      "\n",
      "Batch 877/992 ━━━━━━━━━━━━━━━━━━━━ 01:01:55\n",
      "Accuracy: 0.8533 - Loss: 0.4215\n",
      "\n",
      "Batch 878/992 ━━━━━━━━━━━━━━━━━━━━ 01:02:05\n",
      "Accuracy: 0.8534 - Loss: 0.4215\n",
      "\n",
      "Batch 879/992 ━━━━━━━━━━━━━━━━━━━━ 01:02:15\n",
      "Accuracy: 0.8534 - Loss: 0.4214\n",
      "\n",
      "Batch 880/992 ━━━━━━━━━━━━━━━━━━━━ 01:02:26\n",
      "Accuracy: 0.8536 - Loss: 0.4209\n",
      "\n",
      "Batch 881/992 ━━━━━━━━━━━━━━━━━━━━ 01:02:36\n",
      "Accuracy: 0.8536 - Loss: 0.4208\n",
      "\n",
      "Batch 882/992 ━━━━━━━━━━━━━━━━━━━━ 01:02:46\n",
      "Accuracy: 0.8536 - Loss: 0.4206\n",
      "\n",
      "Batch 883/992 ━━━━━━━━━━━━━━━━━━━━ 01:02:56\n",
      "Accuracy: 0.8536 - Loss: 0.4204\n",
      "\n",
      "Batch 884/992 ━━━━━━━━━━━━━━━━━━━━ 01:03:06\n",
      "Accuracy: 0.8538 - Loss: 0.4200\n",
      "\n",
      "Batch 885/992 ━━━━━━━━━━━━━━━━━━━━ 01:03:16\n",
      "Accuracy: 0.8537 - Loss: 0.4200\n",
      "\n",
      "Batch 886/992 ━━━━━━━━━━━━━━━━━━━━ 01:03:26\n",
      "Accuracy: 0.8537 - Loss: 0.4197\n",
      "\n",
      "Batch 887/992 ━━━━━━━━━━━━━━━━━━━━ 01:03:36\n",
      "Accuracy: 0.8533 - Loss: 0.4200\n",
      "\n",
      "Batch 888/992 ━━━━━━━━━━━━━━━━━━━━ 01:03:46\n",
      "Accuracy: 0.8535 - Loss: 0.4197\n",
      "\n",
      "Batch 889/992 ━━━━━━━━━━━━━━━━━━━━ 01:03:56\n",
      "Accuracy: 0.8535 - Loss: 0.4196\n",
      "\n",
      "Batch 890/992 ━━━━━━━━━━━━━━━━━━━━ 01:04:07\n",
      "Accuracy: 0.8535 - Loss: 0.4195\n",
      "\n",
      "Batch 891/992 ━━━━━━━━━━━━━━━━━━━━ 01:04:17\n",
      "Accuracy: 0.8537 - Loss: 0.4193\n",
      "\n",
      "Batch 892/992 ━━━━━━━━━━━━━━━━━━━━ 01:04:27\n",
      "Accuracy: 0.8538 - Loss: 0.4190\n",
      "\n",
      "Batch 893/992 ━━━━━━━━━━━━━━━━━━━━ 01:04:37\n",
      "Accuracy: 0.8536 - Loss: 0.4195\n",
      "\n",
      "Batch 894/992 ━━━━━━━━━━━━━━━━━━━━ 01:04:47\n",
      "Accuracy: 0.8537 - Loss: 0.4192\n",
      "\n",
      "Batch 895/992 ━━━━━━━━━━━━━━━━━━━━ 01:04:57\n",
      "Accuracy: 0.8538 - Loss: 0.4192\n",
      "\n",
      "Batch 896/992 ━━━━━━━━━━━━━━━━━━━━ 01:05:08\n",
      "Accuracy: 0.8535 - Loss: 0.4195\n",
      "\n",
      "Batch 897/992 ━━━━━━━━━━━━━━━━━━━━ 01:05:18\n",
      "Accuracy: 0.8537 - Loss: 0.4190\n",
      "\n",
      "Batch 898/992 ━━━━━━━━━━━━━━━━━━━━ 01:05:28\n",
      "Accuracy: 0.8536 - Loss: 0.4189\n",
      "\n",
      "Batch 899/992 ━━━━━━━━━━━━━━━━━━━━ 01:05:38\n",
      "Accuracy: 0.8534 - Loss: 0.4190\n",
      "\n",
      "Batch 900/992 ━━━━━━━━━━━━━━━━━━━━ 01:05:48\n",
      "Accuracy: 0.8533 - Loss: 0.4191\n",
      "\n",
      "Batch 901/992 ━━━━━━━━━━━━━━━━━━━━ 01:05:58\n",
      "Accuracy: 0.8535 - Loss: 0.4189\n",
      "\n",
      "Batch 902/992 ━━━━━━━━━━━━━━━━━━━━ 01:06:08\n",
      "Accuracy: 0.8534 - Loss: 0.4190\n",
      "\n",
      "Batch 903/992 ━━━━━━━━━━━━━━━━━━━━ 01:06:18\n",
      "Accuracy: 0.8535 - Loss: 0.4186\n",
      "\n",
      "Batch 904/992 ━━━━━━━━━━━━━━━━━━━━ 01:06:29\n",
      "Accuracy: 0.8530 - Loss: 0.4194\n",
      "\n",
      "Batch 905/992 ━━━━━━━━━━━━━━━━━━━━ 01:06:39\n",
      "Accuracy: 0.8529 - Loss: 0.4194\n",
      "\n",
      "Batch 906/992 ━━━━━━━━━━━━━━━━━━━━ 01:06:49\n",
      "Accuracy: 0.8526 - Loss: 0.4198\n",
      "\n",
      "Batch 907/992 ━━━━━━━━━━━━━━━━━━━━ 01:06:59\n",
      "Accuracy: 0.8528 - Loss: 0.4195\n",
      "\n",
      "Batch 908/992 ━━━━━━━━━━━━━━━━━━━━ 01:07:09\n",
      "Accuracy: 0.8528 - Loss: 0.4195\n",
      "\n",
      "Batch 909/992 ━━━━━━━━━━━━━━━━━━━━ 01:07:19\n",
      "Accuracy: 0.8529 - Loss: 0.4195\n",
      "\n",
      "Batch 910/992 ━━━━━━━━━━━━━━━━━━━━ 01:07:29\n",
      "Accuracy: 0.8525 - Loss: 0.4201\n",
      "\n",
      "Batch 911/992 ━━━━━━━━━━━━━━━━━━━━ 01:07:40\n",
      "Accuracy: 0.8524 - Loss: 0.4201\n",
      "\n",
      "Batch 912/992 ━━━━━━━━━━━━━━━━━━━━ 01:07:50\n",
      "Accuracy: 0.8524 - Loss: 0.4200\n",
      "\n",
      "Batch 913/992 ━━━━━━━━━━━━━━━━━━━━ 01:08:00\n",
      "Accuracy: 0.8523 - Loss: 0.4200\n",
      "\n",
      "Batch 914/992 ━━━━━━━━━━━━━━━━━━━━ 01:08:10\n",
      "Accuracy: 0.8523 - Loss: 0.4201\n",
      "\n",
      "Batch 915/992 ━━━━━━━━━━━━━━━━━━━━ 01:08:20\n",
      "Accuracy: 0.8522 - Loss: 0.4201\n",
      "\n",
      "Batch 916/992 ━━━━━━━━━━━━━━━━━━━━ 01:08:31\n",
      "Accuracy: 0.8522 - Loss: 0.4200\n",
      "\n",
      "Batch 917/992 ━━━━━━━━━━━━━━━━━━━━ 01:08:41\n",
      "Accuracy: 0.8524 - Loss: 0.4197\n",
      "\n",
      "Batch 918/992 ━━━━━━━━━━━━━━━━━━━━ 01:08:51\n",
      "Accuracy: 0.8523 - Loss: 0.4203\n",
      "\n",
      "Batch 919/992 ━━━━━━━━━━━━━━━━━━━━ 01:09:01\n",
      "Accuracy: 0.8524 - Loss: 0.4200\n",
      "\n",
      "Batch 920/992 ━━━━━━━━━━━━━━━━━━━━ 01:09:11\n",
      "Accuracy: 0.8526 - Loss: 0.4197\n",
      "\n",
      "Batch 921/992 ━━━━━━━━━━━━━━━━━━━━ 01:09:21\n",
      "Accuracy: 0.8525 - Loss: 0.4200\n",
      "\n",
      "Batch 922/992 ━━━━━━━━━━━━━━━━━━━━ 01:09:32\n",
      "Accuracy: 0.8526 - Loss: 0.4196\n",
      "\n",
      "Batch 923/992 ━━━━━━━━━━━━━━━━━━━━ 01:09:42\n",
      "Accuracy: 0.8527 - Loss: 0.4196\n",
      "\n",
      "Batch 924/992 ━━━━━━━━━━━━━━━━━━━━ 01:09:52\n",
      "Accuracy: 0.8527 - Loss: 0.4197\n",
      "\n",
      "Batch 925/992 ━━━━━━━━━━━━━━━━━━━━ 01:10:02\n",
      "Accuracy: 0.8528 - Loss: 0.4193\n",
      "\n",
      "Batch 926/992 ━━━━━━━━━━━━━━━━━━━━ 01:10:12\n",
      "Accuracy: 0.8527 - Loss: 0.4194\n",
      "\n",
      "Batch 927/992 ━━━━━━━━━━━━━━━━━━━━ 01:10:22\n",
      "Accuracy: 0.8526 - Loss: 0.4196\n",
      "\n",
      "Batch 928/992 ━━━━━━━━━━━━━━━━━━━━ 01:10:33\n",
      "Accuracy: 0.8524 - Loss: 0.4200\n",
      "\n",
      "Batch 929/992 ━━━━━━━━━━━━━━━━━━━━ 01:10:43\n",
      "Accuracy: 0.8523 - Loss: 0.4203\n",
      "\n",
      "Batch 930/992 ━━━━━━━━━━━━━━━━━━━━ 01:10:53\n",
      "Accuracy: 0.8524 - Loss: 0.4202\n",
      "\n",
      "Batch 931/992 ━━━━━━━━━━━━━━━━━━━━ 01:11:03\n",
      "Accuracy: 0.8524 - Loss: 0.4201\n",
      "\n",
      "Batch 932/992 ━━━━━━━━━━━━━━━━━━━━ 01:11:13\n",
      "Accuracy: 0.8526 - Loss: 0.4198\n",
      "\n",
      "Batch 933/992 ━━━━━━━━━━━━━━━━━━━━ 01:11:23\n",
      "Accuracy: 0.8528 - Loss: 0.4196\n",
      "\n",
      "Batch 934/992 ━━━━━━━━━━━━━━━━━━━━ 01:11:33\n",
      "Accuracy: 0.8526 - Loss: 0.4196\n",
      "\n",
      "Batch 935/992 ━━━━━━━━━━━━━━━━━━━━ 01:11:43\n",
      "Accuracy: 0.8528 - Loss: 0.4194\n",
      "\n",
      "Batch 936/992 ━━━━━━━━━━━━━━━━━━━━ 01:11:54\n",
      "Accuracy: 0.8528 - Loss: 0.4193\n",
      "\n",
      "Batch 937/992 ━━━━━━━━━━━━━━━━━━━━ 01:12:04\n",
      "Accuracy: 0.8530 - Loss: 0.4190\n",
      "\n",
      "Batch 938/992 ━━━━━━━━━━━━━━━━━━━━ 01:12:14\n",
      "Accuracy: 0.8530 - Loss: 0.4189\n",
      "\n",
      "Batch 939/992 ━━━━━━━━━━━━━━━━━━━━ 01:12:24\n",
      "Accuracy: 0.8532 - Loss: 0.4186\n",
      "\n",
      "Batch 940/992 ━━━━━━━━━━━━━━━━━━━━ 01:12:34\n",
      "Accuracy: 0.8531 - Loss: 0.4189\n",
      "\n",
      "Batch 941/992 ━━━━━━━━━━━━━━━━━━━━ 01:12:45\n",
      "Accuracy: 0.8531 - Loss: 0.4187\n",
      "\n",
      "Batch 942/992 ━━━━━━━━━━━━━━━━━━━━ 01:12:55\n",
      "Accuracy: 0.8528 - Loss: 0.4195\n",
      "\n",
      "Batch 943/992 ━━━━━━━━━━━━━━━━━━━━ 01:13:05\n",
      "Accuracy: 0.8529 - Loss: 0.4194\n",
      "\n",
      "Batch 944/992 ━━━━━━━━━━━━━━━━━━━━ 01:13:15\n",
      "Accuracy: 0.8528 - Loss: 0.4197\n",
      "\n",
      "Batch 945/992 ━━━━━━━━━━━━━━━━━━━━ 01:13:25\n",
      "Accuracy: 0.8526 - Loss: 0.4201\n",
      "\n",
      "Batch 946/992 ━━━━━━━━━━━━━━━━━━━━ 01:13:35\n",
      "Accuracy: 0.8527 - Loss: 0.4200\n",
      "\n",
      "Batch 947/992 ━━━━━━━━━━━━━━━━━━━━ 01:13:45\n",
      "Accuracy: 0.8527 - Loss: 0.4198\n",
      "\n",
      "Batch 948/992 ━━━━━━━━━━━━━━━━━━━━ 01:13:55\n",
      "Accuracy: 0.8527 - Loss: 0.4199\n",
      "\n",
      "Batch 949/992 ━━━━━━━━━━━━━━━━━━━━ 01:14:05\n",
      "Accuracy: 0.8527 - Loss: 0.4198\n",
      "\n",
      "Batch 950/992 ━━━━━━━━━━━━━━━━━━━━ 01:14:16\n",
      "Accuracy: 0.8528 - Loss: 0.4196\n",
      "\n",
      "Batch 951/992 ━━━━━━━━━━━━━━━━━━━━ 01:14:26\n",
      "Accuracy: 0.8528 - Loss: 0.4196\n",
      "\n",
      "Batch 952/992 ━━━━━━━━━━━━━━━━━━━━ 01:14:36\n",
      "Accuracy: 0.8529 - Loss: 0.4192\n",
      "\n",
      "Batch 953/992 ━━━━━━━━━━━━━━━━━━━━ 01:14:46\n",
      "Accuracy: 0.8528 - Loss: 0.4193\n",
      "\n",
      "Batch 954/992 ━━━━━━━━━━━━━━━━━━━━ 01:14:56\n",
      "Accuracy: 0.8530 - Loss: 0.4189\n",
      "\n",
      "Batch 955/992 ━━━━━━━━━━━━━━━━━━━━ 01:15:06\n",
      "Accuracy: 0.8530 - Loss: 0.4187\n",
      "\n",
      "Batch 956/992 ━━━━━━━━━━━━━━━━━━━━ 01:15:17\n",
      "Accuracy: 0.8530 - Loss: 0.4185\n",
      "\n",
      "Batch 957/992 ━━━━━━━━━━━━━━━━━━━━ 01:15:27\n",
      "Accuracy: 0.8529 - Loss: 0.4187\n",
      "\n",
      "Batch 958/992 ━━━━━━━━━━━━━━━━━━━━ 01:15:37\n",
      "Accuracy: 0.8529 - Loss: 0.4189\n",
      "\n",
      "Batch 959/992 ━━━━━━━━━━━━━━━━━━━━ 01:15:47\n",
      "Accuracy: 0.8530 - Loss: 0.4187\n",
      "\n",
      "Batch 960/992 ━━━━━━━━━━━━━━━━━━━━ 01:15:57\n",
      "Accuracy: 0.8531 - Loss: 0.4183\n",
      "\n",
      "Batch 961/992 ━━━━━━━━━━━━━━━━━━━━ 01:16:07\n",
      "Accuracy: 0.8533 - Loss: 0.4180\n",
      "\n",
      "Batch 962/992 ━━━━━━━━━━━━━━━━━━━━ 01:16:17\n",
      "Accuracy: 0.8532 - Loss: 0.4181\n",
      "\n",
      "Batch 963/992 ━━━━━━━━━━━━━━━━━━━━ 01:16:27\n",
      "Accuracy: 0.8533 - Loss: 0.4177\n",
      "\n",
      "Batch 964/992 ━━━━━━━━━━━━━━━━━━━━ 01:16:38\n",
      "Accuracy: 0.8533 - Loss: 0.4179\n",
      "\n",
      "Batch 965/992 ━━━━━━━━━━━━━━━━━━━━ 01:16:48\n",
      "Accuracy: 0.8534 - Loss: 0.4177\n",
      "\n",
      "Batch 966/992 ━━━━━━━━━━━━━━━━━━━━ 01:16:58\n",
      "Accuracy: 0.8533 - Loss: 0.4178\n",
      "\n",
      "Batch 967/992 ━━━━━━━━━━━━━━━━━━━━ 01:17:08\n",
      "Accuracy: 0.8533 - Loss: 0.4177\n",
      "\n",
      "Batch 968/992 ━━━━━━━━━━━━━━━━━━━━ 01:17:18\n",
      "Accuracy: 0.8534 - Loss: 0.4173\n",
      "\n",
      "Batch 969/992 ━━━━━━━━━━━━━━━━━━━━ 01:17:28\n",
      "Accuracy: 0.8536 - Loss: 0.4170\n",
      "\n",
      "Batch 970/992 ━━━━━━━━━━━━━━━━━━━━ 01:17:38\n",
      "Accuracy: 0.8535 - Loss: 0.4175\n",
      "\n",
      "Batch 971/992 ━━━━━━━━━━━━━━━━━━━━ 01:17:49\n",
      "Accuracy: 0.8536 - Loss: 0.4172\n",
      "\n",
      "Batch 972/992 ━━━━━━━━━━━━━━━━━━━━ 01:17:59\n",
      "Accuracy: 0.8535 - Loss: 0.4175\n",
      "\n",
      "Batch 973/992 ━━━━━━━━━━━━━━━━━━━━ 01:18:09\n",
      "Accuracy: 0.8537 - Loss: 0.4171\n",
      "\n",
      "Batch 974/992 ━━━━━━━━━━━━━━━━━━━━ 01:18:19\n",
      "Accuracy: 0.8537 - Loss: 0.4171\n",
      "\n",
      "Batch 975/992 ━━━━━━━━━━━━━━━━━━━━ 01:18:29\n",
      "Accuracy: 0.8537 - Loss: 0.4170\n",
      "\n",
      "Batch 976/992 ━━━━━━━━━━━━━━━━━━━━ 01:18:39\n",
      "Accuracy: 0.8539 - Loss: 0.4166\n",
      "\n",
      "Batch 977/992 ━━━━━━━━━━━━━━━━━━━━ 01:18:50\n",
      "Accuracy: 0.8540 - Loss: 0.4163\n",
      "\n",
      "Batch 978/992 ━━━━━━━━━━━━━━━━━━━━ 01:19:00\n",
      "Accuracy: 0.8539 - Loss: 0.4162\n",
      "\n",
      "Batch 979/992 ━━━━━━━━━━━━━━━━━━━━ 01:19:10\n",
      "Accuracy: 0.8537 - Loss: 0.4171\n",
      "\n",
      "Batch 980/992 ━━━━━━━━━━━━━━━━━━━━ 01:19:20\n",
      "Accuracy: 0.8537 - Loss: 0.4172\n",
      "\n",
      "Batch 981/992 ━━━━━━━━━━━━━━━━━━━━ 01:19:30\n",
      "Accuracy: 0.8537 - Loss: 0.4172\n",
      "\n",
      "Batch 982/992 ━━━━━━━━━━━━━━━━━━━━ 01:19:40\n",
      "Accuracy: 0.8539 - Loss: 0.4168\n",
      "\n",
      "Batch 983/992 ━━━━━━━━━━━━━━━━━━━━ 01:19:50\n",
      "Accuracy: 0.8539 - Loss: 0.4168\n",
      "\n",
      "Batch 984/992 ━━━━━━━━━━━━━━━━━━━━ 01:20:00\n",
      "Accuracy: 0.8540 - Loss: 0.4165\n",
      "\n",
      "Batch 985/992 ━━━━━━━━━━━━━━━━━━━━ 01:20:10\n",
      "Accuracy: 0.8542 - Loss: 0.4162\n",
      "\n",
      "Batch 986/992 ━━━━━━━━━━━━━━━━━━━━ 01:20:20\n",
      "Accuracy: 0.8541 - Loss: 0.4165\n",
      "\n",
      "Batch 987/992 ━━━━━━━━━━━━━━━━━━━━ 01:20:30\n",
      "Accuracy: 0.8542 - Loss: 0.4163\n",
      "\n",
      "Batch 988/992 ━━━━━━━━━━━━━━━━━━━━ 01:20:41\n",
      "Accuracy: 0.8540 - Loss: 0.4170\n",
      "\n",
      "Batch 989/992 ━━━━━━━━━━━━━━━━━━━━ 01:20:51\n",
      "Accuracy: 0.8540 - Loss: 0.4168\n",
      "\n",
      "Batch 990/992 ━━━━━━━━━━━━━━━━━━━━ 01:21:01\n",
      "Accuracy: 0.8540 - Loss: 0.4166\n",
      "\n",
      "Batch 991/992 ━━━━━━━━━━━━━━━━━━━━ 01:21:11\n",
      "Accuracy: 0.8542 - Loss: 0.4163\n",
      "\n",
      "Batch 992/992 ━━━━━━━━━━━━━━━━━━━━ 01:21:21\n",
      "Accuracy: 0.8542 - Loss: 0.4162\n",
      "\n",
      "\n",
      "Epoch 4/10\n",
      "Batch 1/992 ━━━━━━━━━━━━━━━━━━━━ 01:37:47\n",
      "Accuracy: 0.7500 - Loss: 0.4697\n",
      "\n",
      "Batch 2/992 ━━━━━━━━━━━━━━━━━━━━ 01:37:57\n",
      "Accuracy: 0.7500 - Loss: 0.5850\n",
      "\n",
      "Batch 3/992 ━━━━━━━━━━━━━━━━━━━━ 01:38:07\n",
      "Accuracy: 0.8333 - Loss: 0.4417\n",
      "\n",
      "Batch 4/992 ━━━━━━━━━━━━━━━━━━━━ 01:38:17\n",
      "Accuracy: 0.8125 - Loss: 0.4360\n",
      "\n",
      "Batch 5/992 ━━━━━━━━━━━━━━━━━━━━ 01:38:27\n",
      "Accuracy: 0.8250 - Loss: 0.4104\n",
      "\n",
      "Batch 6/992 ━━━━━━━━━━━━━━━━━━━━ 01:38:37\n",
      "Accuracy: 0.8333 - Loss: 0.3846\n",
      "\n",
      "Batch 7/992 ━━━━━━━━━━━━━━━━━━━━ 01:38:48\n",
      "Accuracy: 0.8393 - Loss: 0.3823\n",
      "\n",
      "Batch 8/992 ━━━━━━━━━━━━━━━━━━━━ 01:38:58\n",
      "Accuracy: 0.8438 - Loss: 0.3687\n",
      "\n",
      "Batch 9/992 ━━━━━━━━━━━━━━━━━━━━ 01:39:09\n",
      "Accuracy: 0.8611 - Loss: 0.3372\n",
      "\n",
      "Batch 10/992 ━━━━━━━━━━━━━━━━━━━━ 01:39:19\n",
      "Accuracy: 0.8750 - Loss: 0.3152\n",
      "\n",
      "Batch 11/992 ━━━━━━━━━━━━━━━━━━━━ 01:39:29\n",
      "Accuracy: 0.8636 - Loss: 0.3326\n",
      "\n",
      "Batch 12/992 ━━━━━━━━━━━━━━━━━━━━ 01:39:39\n",
      "Accuracy: 0.8750 - Loss: 0.3103\n",
      "\n",
      "Batch 13/992 ━━━━━━━━━━━━━━━━━━━━ 01:39:49\n",
      "Accuracy: 0.8846 - Loss: 0.3062\n",
      "\n",
      "Batch 14/992 ━━━━━━━━━━━━━━━━━━━━ 01:39:59\n",
      "Accuracy: 0.8839 - Loss: 0.3012\n",
      "\n",
      "Batch 15/992 ━━━━━━━━━━━━━━━━━━━━ 01:40:09\n",
      "Accuracy: 0.8833 - Loss: 0.3010\n",
      "\n",
      "Batch 16/992 ━━━━━━━━━━━━━━━━━━━━ 01:40:19\n",
      "Accuracy: 0.8906 - Loss: 0.2946\n",
      "\n",
      "Batch 17/992 ━━━━━━━━━━━━━━━━━━━━ 01:40:29\n",
      "Accuracy: 0.8897 - Loss: 0.2928\n",
      "\n",
      "Batch 18/992 ━━━━━━━━━━━━━━━━━━━━ 01:40:39\n",
      "Accuracy: 0.8958 - Loss: 0.2819\n",
      "\n",
      "Batch 19/992 ━━━━━━━━━━━━━━━━━━━━ 01:40:50\n",
      "Accuracy: 0.9013 - Loss: 0.2756\n",
      "\n",
      "Batch 20/992 ━━━━━━━━━━━━━━━━━━━━ 01:41:00\n",
      "Accuracy: 0.9062 - Loss: 0.2717\n",
      "\n",
      "Batch 21/992 ━━━━━━━━━━━━━━━━━━━━ 01:41:10\n",
      "Accuracy: 0.9107 - Loss: 0.2687\n",
      "\n",
      "Batch 22/992 ━━━━━━━━━━━━━━━━━━━━ 01:41:21\n",
      "Accuracy: 0.9034 - Loss: 0.2810\n",
      "\n",
      "Batch 23/992 ━━━━━━━━━━━━━━━━━━━━ 01:41:31\n",
      "Accuracy: 0.9076 - Loss: 0.2707\n",
      "\n",
      "Batch 24/992 ━━━━━━━━━━━━━━━━━━━━ 01:41:41\n",
      "Accuracy: 0.9115 - Loss: 0.2707\n",
      "\n",
      "Batch 25/992 ━━━━━━━━━━━━━━━━━━━━ 01:41:51\n",
      "Accuracy: 0.9100 - Loss: 0.2677\n",
      "\n",
      "Batch 26/992 ━━━━━━━━━━━━━━━━━━━━ 01:42:01\n",
      "Accuracy: 0.9135 - Loss: 0.2635\n",
      "\n",
      "Batch 27/992 ━━━━━━━━━━━━━━━━━━━━ 01:42:11\n",
      "Accuracy: 0.9120 - Loss: 0.2692\n",
      "\n",
      "Batch 28/992 ━━━━━━━━━━━━━━━━━━━━ 01:42:21\n",
      "Accuracy: 0.9107 - Loss: 0.2707\n",
      "\n",
      "Batch 29/992 ━━━━━━━━━━━━━━━━━━━━ 01:42:31\n",
      "Accuracy: 0.9095 - Loss: 0.2798\n",
      "\n",
      "Batch 30/992 ━━━━━━━━━━━━━━━━━━━━ 01:42:41\n",
      "Accuracy: 0.9083 - Loss: 0.2775\n",
      "\n",
      "Batch 31/992 ━━━━━━━━━━━━━━━━━━━━ 01:42:52\n",
      "Accuracy: 0.8992 - Loss: 0.2971\n",
      "\n",
      "Batch 32/992 ━━━━━━━━━━━━━━━━━━━━ 01:43:02\n",
      "Accuracy: 0.8945 - Loss: 0.3026\n",
      "\n",
      "Batch 33/992 ━━━━━━━━━━━━━━━━━━━━ 01:43:12\n",
      "Accuracy: 0.8939 - Loss: 0.3066\n",
      "\n",
      "Batch 34/992 ━━━━━━━━━━━━━━━━━━━━ 01:43:22\n",
      "Accuracy: 0.8934 - Loss: 0.3098\n",
      "\n",
      "Batch 35/992 ━━━━━━━━━━━━━━━━━━━━ 01:43:32\n",
      "Accuracy: 0.8964 - Loss: 0.3022\n",
      "\n",
      "Batch 36/992 ━━━━━━━━━━━━━━━━━━━━ 01:43:42\n",
      "Accuracy: 0.8958 - Loss: 0.3003\n",
      "\n",
      "Batch 37/992 ━━━━━━━━━━━━━━━━━━━━ 01:43:52\n",
      "Accuracy: 0.8986 - Loss: 0.2999\n",
      "\n",
      "Batch 38/992 ━━━━━━━━━━━━━━━━━━━━ 01:44:02\n",
      "Accuracy: 0.8980 - Loss: 0.3029\n",
      "\n",
      "Batch 39/992 ━━━━━━━━━━━━━━━━━━━━ 01:44:12\n",
      "Accuracy: 0.9006 - Loss: 0.3023\n",
      "\n",
      "Batch 40/992 ━━━━━━━━━━━━━━━━━━━━ 01:44:22\n",
      "Accuracy: 0.9031 - Loss: 0.2979\n",
      "\n",
      "Batch 41/992 ━━━━━━━━━━━━━━━━━━━━ 01:44:32\n",
      "Accuracy: 0.9024 - Loss: 0.2954\n",
      "\n",
      "Batch 42/992 ━━━━━━━━━━━━━━━━━━━━ 01:44:42\n",
      "Accuracy: 0.9018 - Loss: 0.2980\n",
      "\n",
      "Batch 43/992 ━━━━━━━━━━━━━━━━━━━━ 01:44:52\n",
      "Accuracy: 0.9041 - Loss: 0.2937\n",
      "\n",
      "Batch 44/992 ━━━━━━━━━━━━━━━━━━━━ 01:45:03\n",
      "Accuracy: 0.9006 - Loss: 0.3026\n",
      "\n",
      "Batch 45/992 ━━━━━━━━━━━━━━━━━━━━ 01:45:13\n",
      "Accuracy: 0.9000 - Loss: 0.3011\n",
      "\n",
      "Batch 46/992 ━━━━━━━━━━━━━━━━━━━━ 01:45:23\n",
      "Accuracy: 0.9022 - Loss: 0.2964\n",
      "\n",
      "Batch 47/992 ━━━━━━━━━━━━━━━━━━━━ 01:45:33\n",
      "Accuracy: 0.9043 - Loss: 0.2922\n",
      "\n",
      "Batch 48/992 ━━━━━━━━━━━━━━━━━━━━ 01:45:43\n",
      "Accuracy: 0.9062 - Loss: 0.2883\n",
      "\n",
      "Batch 49/992 ━━━━━━━━━━━━━━━━━━━━ 01:45:53\n",
      "Accuracy: 0.9082 - Loss: 0.2852\n",
      "\n",
      "Batch 50/992 ━━━━━━━━━━━━━━━━━━━━ 01:46:03\n",
      "Accuracy: 0.9075 - Loss: 0.2834\n",
      "\n",
      "Batch 51/992 ━━━━━━━━━━━━━━━━━━━━ 01:46:13\n",
      "Accuracy: 0.9093 - Loss: 0.2792\n",
      "\n",
      "Batch 52/992 ━━━━━━━━━━━━━━━━━━━━ 01:46:23\n",
      "Accuracy: 0.9087 - Loss: 0.2773\n",
      "\n",
      "Batch 53/992 ━━━━━━━━━━━━━━━━━━━━ 01:46:34\n",
      "Accuracy: 0.9033 - Loss: 0.2835\n",
      "\n",
      "Batch 54/992 ━━━━━━━━━━━━━━━━━━━━ 01:46:44\n",
      "Accuracy: 0.9051 - Loss: 0.2806\n",
      "\n",
      "Batch 55/992 ━━━━━━━━━━━━━━━━━━━━ 01:46:54\n",
      "Accuracy: 0.9045 - Loss: 0.2814\n",
      "\n",
      "Batch 56/992 ━━━━━━━━━━━━━━━━━━━━ 01:47:04\n",
      "Accuracy: 0.9062 - Loss: 0.2795\n",
      "\n",
      "Batch 57/992 ━━━━━━━━━━━━━━━━━━━━ 01:47:15\n",
      "Accuracy: 0.9079 - Loss: 0.2789\n",
      "\n",
      "Batch 58/992 ━━━━━━━━━━━━━━━━━━━━ 01:47:25\n",
      "Accuracy: 0.9095 - Loss: 0.2745\n",
      "\n",
      "Batch 59/992 ━━━━━━━━━━━━━━━━━━━━ 01:47:35\n",
      "Accuracy: 0.9110 - Loss: 0.2720\n",
      "\n",
      "Batch 60/992 ━━━━━━━━━━━━━━━━━━━━ 01:47:45\n",
      "Accuracy: 0.9104 - Loss: 0.2729\n",
      "\n",
      "Batch 61/992 ━━━━━━━━━━━━━━━━━━━━ 01:47:55\n",
      "Accuracy: 0.9098 - Loss: 0.2736\n",
      "\n",
      "Batch 62/992 ━━━━━━━━━━━━━━━━━━━━ 01:48:05\n",
      "Accuracy: 0.9093 - Loss: 0.2730\n",
      "\n",
      "Batch 63/992 ━━━━━━━━━━━━━━━━━━━━ 01:48:15\n",
      "Accuracy: 0.9087 - Loss: 0.2737\n",
      "\n",
      "Batch 64/992 ━━━━━━━━━━━━━━━━━━━━ 01:48:25\n",
      "Accuracy: 0.9102 - Loss: 0.2711\n",
      "\n",
      "Batch 65/992 ━━━━━━━━━━━━━━━━━━━━ 01:48:35\n",
      "Accuracy: 0.9115 - Loss: 0.2685\n",
      "\n",
      "Batch 66/992 ━━━━━━━━━━━━━━━━━━━━ 01:48:45\n",
      "Accuracy: 0.9129 - Loss: 0.2648\n",
      "\n",
      "Batch 67/992 ━━━━━━━━━━━━━━━━━━━━ 01:48:55\n",
      "Accuracy: 0.9123 - Loss: 0.2650\n",
      "\n",
      "Batch 68/992 ━━━━━━━━━━━━━━━━━━━━ 01:49:06\n",
      "Accuracy: 0.9136 - Loss: 0.2625\n",
      "\n",
      "Batch 69/992 ━━━━━━━━━━━━━━━━━━━━ 01:49:16\n",
      "Accuracy: 0.9130 - Loss: 0.2611\n",
      "\n",
      "Batch 70/992 ━━━━━━━━━━━━━━━━━━━━ 01:49:26\n",
      "Accuracy: 0.9143 - Loss: 0.2575\n",
      "\n",
      "Batch 71/992 ━━━━━━━━━━━━━━━━━━━━ 01:49:36\n",
      "Accuracy: 0.9155 - Loss: 0.2544\n",
      "\n",
      "Batch 72/992 ━━━━━━━━━━━━━━━━━━━━ 01:49:47\n",
      "Accuracy: 0.9132 - Loss: 0.2573\n",
      "\n",
      "Batch 73/992 ━━━━━━━━━━━━━━━━━━━━ 01:49:57\n",
      "Accuracy: 0.9127 - Loss: 0.2565\n",
      "\n",
      "Batch 74/992 ━━━━━━━━━━━━━━━━━━━━ 01:50:07\n",
      "Accuracy: 0.9105 - Loss: 0.2577\n",
      "\n",
      "Batch 75/992 ━━━━━━━━━━━━━━━━━━━━ 01:50:17\n",
      "Accuracy: 0.9083 - Loss: 0.2577\n",
      "\n",
      "Batch 76/992 ━━━━━━━━━━━━━━━━━━━━ 01:50:27\n",
      "Accuracy: 0.9095 - Loss: 0.2561\n",
      "\n",
      "Batch 77/992 ━━━━━━━━━━━━━━━━━━━━ 01:50:37\n",
      "Accuracy: 0.9107 - Loss: 0.2530\n",
      "\n",
      "Batch 78/992 ━━━━━━━━━━━━━━━━━━━━ 01:50:47\n",
      "Accuracy: 0.9119 - Loss: 0.2507\n",
      "\n",
      "Batch 79/992 ━━━━━━━━━━━━━━━━━━━━ 01:50:57\n",
      "Accuracy: 0.9114 - Loss: 0.2539\n",
      "\n",
      "Batch 80/992 ━━━━━━━━━━━━━━━━━━━━ 01:51:08\n",
      "Accuracy: 0.9125 - Loss: 0.2537\n",
      "\n",
      "Batch 81/992 ━━━━━━━━━━━━━━━━━━━━ 01:51:18\n",
      "Accuracy: 0.9136 - Loss: 0.2518\n",
      "\n",
      "Batch 82/992 ━━━━━━━━━━━━━━━━━━━━ 01:51:28\n",
      "Accuracy: 0.9146 - Loss: 0.2492\n",
      "\n",
      "Batch 83/992 ━━━━━━━━━━━━━━━━━━━━ 01:51:38\n",
      "Accuracy: 0.9157 - Loss: 0.2472\n",
      "\n",
      "Batch 84/992 ━━━━━━━━━━━━━━━━━━━━ 01:51:48\n",
      "Accuracy: 0.9152 - Loss: 0.2469\n",
      "\n",
      "Batch 85/992 ━━━━━━━━━━━━━━━━━━━━ 01:51:58\n",
      "Accuracy: 0.9162 - Loss: 0.2467\n",
      "\n",
      "Batch 86/992 ━━━━━━━━━━━━━━━━━━━━ 01:52:08\n",
      "Accuracy: 0.9172 - Loss: 0.2444\n",
      "\n",
      "Batch 87/992 ━━━━━━━━━━━━━━━━━━━━ 01:52:18\n",
      "Accuracy: 0.9167 - Loss: 0.2436\n",
      "\n",
      "Batch 88/992 ━━━━━━━━━━━━━━━━━━━━ 01:52:28\n",
      "Accuracy: 0.9162 - Loss: 0.2447\n",
      "\n",
      "Batch 89/992 ━━━━━━━━━━━━━━━━━━━━ 01:52:38\n",
      "Accuracy: 0.9143 - Loss: 0.2475\n",
      "\n",
      "Batch 90/992 ━━━━━━━━━━━━━━━━━━━━ 01:52:49\n",
      "Accuracy: 0.9153 - Loss: 0.2452\n",
      "\n",
      "Batch 91/992 ━━━━━━━━━━━━━━━━━━━━ 01:52:59\n",
      "Accuracy: 0.9135 - Loss: 0.2467\n",
      "\n",
      "Batch 92/992 ━━━━━━━━━━━━━━━━━━━━ 01:53:09\n",
      "Accuracy: 0.9117 - Loss: 0.2507\n",
      "\n",
      "Batch 93/992 ━━━━━━━━━━━━━━━━━━━━ 01:53:19\n",
      "Accuracy: 0.9126 - Loss: 0.2484\n",
      "\n",
      "Batch 94/992 ━━━━━━━━━━━━━━━━━━━━ 01:53:30\n",
      "Accuracy: 0.9122 - Loss: 0.2487\n",
      "\n",
      "Batch 95/992 ━━━━━━━━━━━━━━━━━━━━ 01:53:40\n",
      "Accuracy: 0.9118 - Loss: 0.2483\n",
      "\n",
      "Batch 96/992 ━━━━━━━━━━━━━━━━━━━━ 01:53:50\n",
      "Accuracy: 0.9128 - Loss: 0.2461\n",
      "\n",
      "Batch 97/992 ━━━━━━━━━━━━━━━━━━━━ 01:54:00\n",
      "Accuracy: 0.9111 - Loss: 0.2517\n",
      "\n",
      "Batch 98/992 ━━━━━━━━━━━━━━━━━━━━ 01:54:10\n",
      "Accuracy: 0.9094 - Loss: 0.2533\n",
      "\n",
      "Batch 99/992 ━━━━━━━━━━━━━━━━━━━━ 01:54:20\n",
      "Accuracy: 0.9091 - Loss: 0.2532\n",
      "\n",
      "Batch 100/992 ━━━━━━━━━━━━━━━━━━━━ 01:54:30\n",
      "Accuracy: 0.9087 - Loss: 0.2546\n",
      "\n",
      "Batch 101/992 ━━━━━━━━━━━━━━━━━━━━ 01:54:40\n",
      "Accuracy: 0.9084 - Loss: 0.2538\n",
      "\n",
      "Batch 102/992 ━━━━━━━━━━━━━━━━━━━━ 01:54:50\n",
      "Accuracy: 0.9056 - Loss: 0.2571\n",
      "\n",
      "Batch 103/992 ━━━━━━━━━━━━━━━━━━━━ 01:55:01\n",
      "Accuracy: 0.9053 - Loss: 0.2591\n",
      "\n",
      "Batch 104/992 ━━━━━━━━━━━━━━━━━━━━ 01:55:11\n",
      "Accuracy: 0.9050 - Loss: 0.2588\n",
      "\n",
      "Batch 105/992 ━━━━━━━━━━━━━━━━━━━━ 01:55:21\n",
      "Accuracy: 0.9060 - Loss: 0.2570\n",
      "\n",
      "Batch 106/992 ━━━━━━━━━━━━━━━━━━━━ 01:55:31\n",
      "Accuracy: 0.9045 - Loss: 0.2606\n",
      "\n",
      "Batch 107/992 ━━━━━━━━━━━━━━━━━━━━ 01:55:41\n",
      "Accuracy: 0.9030 - Loss: 0.2700\n",
      "\n",
      "Batch 108/992 ━━━━━━━━━━━━━━━━━━━━ 01:55:52\n",
      "Accuracy: 0.9028 - Loss: 0.2708\n",
      "\n",
      "Batch 109/992 ━━━━━━━━━━━━━━━━━━━━ 01:56:02\n",
      "Accuracy: 0.9037 - Loss: 0.2692\n",
      "\n",
      "Batch 110/992 ━━━━━━━━━━━━━━━━━━━━ 01:56:12\n",
      "Accuracy: 0.9034 - Loss: 0.2690\n",
      "\n",
      "Batch 111/992 ━━━━━━━━━━━━━━━━━━━━ 01:56:22\n",
      "Accuracy: 0.9043 - Loss: 0.2668\n",
      "\n",
      "Batch 112/992 ━━━━━━━━━━━━━━━━━━━━ 01:56:32\n",
      "Accuracy: 0.9051 - Loss: 0.2663\n",
      "\n",
      "Batch 113/992 ━━━━━━━━━━━━━━━━━━━━ 01:56:42\n",
      "Accuracy: 0.9027 - Loss: 0.2700\n",
      "\n",
      "Batch 114/992 ━━━━━━━━━━━━━━━━━━━━ 01:56:52\n",
      "Accuracy: 0.9024 - Loss: 0.2707\n",
      "\n",
      "Batch 115/992 ━━━━━━━━━━━━━━━━━━━━ 01:57:02\n",
      "Accuracy: 0.9011 - Loss: 0.2722\n",
      "\n",
      "Batch 116/992 ━━━━━━━━━━━━━━━━━━━━ 01:57:12\n",
      "Accuracy: 0.9019 - Loss: 0.2711\n",
      "\n",
      "Batch 117/992 ━━━━━━━━━━━━━━━━━━━━ 01:57:23\n",
      "Accuracy: 0.9028 - Loss: 0.2695\n",
      "\n",
      "Batch 118/992 ━━━━━━━━━━━━━━━━━━━━ 01:57:33\n",
      "Accuracy: 0.9015 - Loss: 0.2737\n",
      "\n",
      "Batch 119/992 ━━━━━━━━━━━━━━━━━━━━ 01:57:43\n",
      "Accuracy: 0.9013 - Loss: 0.2747\n",
      "\n",
      "Batch 120/992 ━━━━━━━━━━━━━━━━━━━━ 01:57:53\n",
      "Accuracy: 0.9021 - Loss: 0.2735\n",
      "\n",
      "Batch 121/992 ━━━━━━━━━━━━━━━━━━━━ 01:58:03\n",
      "Accuracy: 0.9019 - Loss: 0.2748\n",
      "\n",
      "Batch 122/992 ━━━━━━━━━━━━━━━━━━━━ 01:58:13\n",
      "Accuracy: 0.8986 - Loss: 0.2799\n",
      "\n",
      "Batch 123/992 ━━━━━━━━━━━━━━━━━━━━ 01:58:23\n",
      "Accuracy: 0.8963 - Loss: 0.2841\n",
      "\n",
      "Batch 124/992 ━━━━━━━━━━━━━━━━━━━━ 01:58:34\n",
      "Accuracy: 0.8962 - Loss: 0.2838\n",
      "\n",
      "Batch 125/992 ━━━━━━━━━━━━━━━━━━━━ 01:58:44\n",
      "Accuracy: 0.8970 - Loss: 0.2827\n",
      "\n",
      "Batch 126/992 ━━━━━━━━━━━━━━━━━━━━ 01:58:54\n",
      "Accuracy: 0.8978 - Loss: 0.2807\n",
      "\n",
      "Batch 127/992 ━━━━━━━━━━━━━━━━━━━━ 01:59:04\n",
      "Accuracy: 0.8976 - Loss: 0.2811\n",
      "\n",
      "Batch 128/992 ━━━━━━━━━━━━━━━━━━━━ 01:59:14\n",
      "Accuracy: 0.8975 - Loss: 0.2807\n",
      "\n",
      "Batch 129/992 ━━━━━━━━━━━━━━━━━━━━ 01:59:24\n",
      "Accuracy: 0.8983 - Loss: 0.2807\n",
      "\n",
      "Batch 130/992 ━━━━━━━━━━━━━━━━━━━━ 01:59:35\n",
      "Accuracy: 0.8981 - Loss: 0.2827\n",
      "\n",
      "Batch 131/992 ━━━━━━━━━━━━━━━━━━━━ 01:59:45\n",
      "Accuracy: 0.8969 - Loss: 0.2839\n",
      "\n",
      "Batch 132/992 ━━━━━━━━━━━━━━━━━━━━ 01:59:55\n",
      "Accuracy: 0.8977 - Loss: 0.2834\n",
      "\n",
      "Batch 133/992 ━━━━━━━━━━━━━━━━━━━━ 02:00:05\n",
      "Accuracy: 0.8985 - Loss: 0.2829\n",
      "\n",
      "Batch 134/992 ━━━━━━━━━━━━━━━━━━━━ 02:00:15\n",
      "Accuracy: 0.8983 - Loss: 0.2831\n",
      "\n",
      "Batch 135/992 ━━━━━━━━━━━━━━━━━━━━ 02:00:25\n",
      "Accuracy: 0.8991 - Loss: 0.2824\n",
      "\n",
      "Batch 136/992 ━━━━━━━━━━━━━━━━━━━━ 02:00:35\n",
      "Accuracy: 0.8989 - Loss: 0.2822\n",
      "\n",
      "Batch 137/992 ━━━━━━━━━━━━━━━━━━━━ 02:00:45\n",
      "Accuracy: 0.8996 - Loss: 0.2814\n",
      "\n",
      "Batch 138/992 ━━━━━━━━━━━━━━━━━━━━ 02:00:55\n",
      "Accuracy: 0.8995 - Loss: 0.2824\n",
      "\n",
      "Batch 139/992 ━━━━━━━━━━━━━━━━━━━━ 02:01:05\n",
      "Accuracy: 0.8993 - Loss: 0.2836\n",
      "\n",
      "Batch 140/992 ━━━━━━━━━━━━━━━━━━━━ 02:01:15\n",
      "Accuracy: 0.9000 - Loss: 0.2819\n",
      "\n",
      "Batch 141/992 ━━━━━━━━━━━━━━━━━━━━ 02:01:26\n",
      "Accuracy: 0.9007 - Loss: 0.2809\n",
      "\n",
      "Batch 142/992 ━━━━━━━━━━━━━━━━━━━━ 02:01:36\n",
      "Accuracy: 0.9014 - Loss: 0.2793\n",
      "\n",
      "Batch 143/992 ━━━━━━━━━━━━━━━━━━━━ 02:01:46\n",
      "Accuracy: 0.9021 - Loss: 0.2778\n",
      "\n",
      "Batch 144/992 ━━━━━━━━━━━━━━━━━━━━ 02:01:56\n",
      "Accuracy: 0.9028 - Loss: 0.2773\n",
      "\n",
      "Batch 145/992 ━━━━━━━━━━━━━━━━━━━━ 02:02:06\n",
      "Accuracy: 0.9026 - Loss: 0.2783\n",
      "\n",
      "Batch 146/992 ━━━━━━━━━━━━━━━━━━━━ 02:02:17\n",
      "Accuracy: 0.9007 - Loss: 0.2826\n",
      "\n",
      "Batch 147/992 ━━━━━━━━━━━━━━━━━━━━ 02:02:27\n",
      "Accuracy: 0.8988 - Loss: 0.2851\n",
      "\n",
      "Batch 148/992 ━━━━━━━━━━━━━━━━━━━━ 02:02:37\n",
      "Accuracy: 0.8986 - Loss: 0.2867\n",
      "\n",
      "Batch 149/992 ━━━━━━━━━━━━━━━━━━━━ 02:02:47\n",
      "Accuracy: 0.8968 - Loss: 0.2893\n",
      "\n",
      "Batch 150/992 ━━━━━━━━━━━━━━━━━━━━ 02:02:57\n",
      "Accuracy: 0.8958 - Loss: 0.2902\n",
      "\n",
      "Batch 151/992 ━━━━━━━━━━━━━━━━━━━━ 02:03:07\n",
      "Accuracy: 0.8957 - Loss: 0.2897\n",
      "\n",
      "Batch 152/992 ━━━━━━━━━━━━━━━━━━━━ 02:03:17\n",
      "Accuracy: 0.8964 - Loss: 0.2897\n",
      "\n",
      "Batch 153/992 ━━━━━━━━━━━━━━━━━━━━ 02:03:27\n",
      "Accuracy: 0.8971 - Loss: 0.2883\n",
      "\n",
      "Batch 154/992 ━━━━━━━━━━━━━━━━━━━━ 02:03:38\n",
      "Accuracy: 0.8977 - Loss: 0.2869\n",
      "\n",
      "Batch 155/992 ━━━━━━━━━━━━━━━━━━━━ 02:03:48\n",
      "Accuracy: 0.8984 - Loss: 0.2859\n",
      "\n",
      "Batch 156/992 ━━━━━━━━━━━━━━━━━━━━ 02:03:58\n",
      "Accuracy: 0.8974 - Loss: 0.2876\n",
      "\n",
      "Batch 157/992 ━━━━━━━━━━━━━━━━━━━━ 02:04:08\n",
      "Accuracy: 0.8973 - Loss: 0.2902\n",
      "\n",
      "Batch 158/992 ━━━━━━━━━━━━━━━━━━━━ 02:04:18\n",
      "Accuracy: 0.8972 - Loss: 0.2924\n",
      "\n",
      "Batch 159/992 ━━━━━━━━━━━━━━━━━━━━ 02:04:28\n",
      "Accuracy: 0.8978 - Loss: 0.2910\n",
      "\n",
      "Batch 160/992 ━━━━━━━━━━━━━━━━━━━━ 02:04:38\n",
      "Accuracy: 0.8969 - Loss: 0.2932\n",
      "\n",
      "Batch 161/992 ━━━━━━━━━━━━━━━━━━━━ 02:04:48\n",
      "Accuracy: 0.8952 - Loss: 0.2967\n",
      "\n",
      "Batch 162/992 ━━━━━━━━━━━━━━━━━━━━ 02:04:58\n",
      "Accuracy: 0.8951 - Loss: 0.2971\n",
      "\n",
      "Batch 163/992 ━━━━━━━━━━━━━━━━━━━━ 02:05:08\n",
      "Accuracy: 0.8957 - Loss: 0.2961\n",
      "\n",
      "Batch 164/992 ━━━━━━━━━━━━━━━━━━━━ 02:05:18\n",
      "Accuracy: 0.8963 - Loss: 0.2950\n",
      "\n",
      "Batch 165/992 ━━━━━━━━━━━━━━━━━━━━ 02:05:29\n",
      "Accuracy: 0.8962 - Loss: 0.2963\n",
      "\n",
      "Batch 166/992 ━━━━━━━━━━━━━━━━━━━━ 02:05:39\n",
      "Accuracy: 0.8968 - Loss: 0.2952\n",
      "\n",
      "Batch 167/992 ━━━━━━━━━━━━━━━━━━━━ 02:05:49\n",
      "Accuracy: 0.8975 - Loss: 0.2938\n",
      "\n",
      "Batch 168/992 ━━━━━━━━━━━━━━━━━━━━ 02:05:59\n",
      "Accuracy: 0.8981 - Loss: 0.2930\n",
      "\n",
      "Batch 169/992 ━━━━━━━━━━━━━━━━━━━━ 02:06:09\n",
      "Accuracy: 0.8979 - Loss: 0.2921\n",
      "\n",
      "Batch 170/992 ━━━━━━━━━━━━━━━━━━━━ 02:06:19\n",
      "Accuracy: 0.8985 - Loss: 0.2915\n",
      "\n",
      "Batch 171/992 ━━━━━━━━━━━━━━━━━━━━ 02:06:29\n",
      "Accuracy: 0.8991 - Loss: 0.2903\n",
      "\n",
      "Batch 172/992 ━━━━━━━━━━━━━━━━━━━━ 02:06:39\n",
      "Accuracy: 0.8997 - Loss: 0.2890\n",
      "\n",
      "Batch 173/992 ━━━━━━━━━━━━━━━━━━━━ 02:06:49\n",
      "Accuracy: 0.9003 - Loss: 0.2876\n",
      "\n",
      "Batch 174/992 ━━━━━━━━━━━━━━━━━━━━ 02:07:00\n",
      "Accuracy: 0.9009 - Loss: 0.2869\n",
      "\n",
      "Batch 175/992 ━━━━━━━━━━━━━━━━━━━━ 02:07:10\n",
      "Accuracy: 0.9014 - Loss: 0.2855\n",
      "\n",
      "Batch 176/992 ━━━━━━━━━━━━━━━━━━━━ 02:07:20\n",
      "Accuracy: 0.9013 - Loss: 0.2849\n",
      "\n",
      "Batch 177/992 ━━━━━━━━━━━━━━━━━━━━ 02:07:30\n",
      "Accuracy: 0.9018 - Loss: 0.2842\n",
      "\n",
      "Batch 178/992 ━━━━━━━━━━━━━━━━━━━━ 02:07:41\n",
      "Accuracy: 0.9010 - Loss: 0.2875\n",
      "\n",
      "Batch 179/992 ━━━━━━━━━━━━━━━━━━━━ 02:07:51\n",
      "Accuracy: 0.9008 - Loss: 0.2887\n",
      "\n",
      "Batch 180/992 ━━━━━━━━━━━━━━━━━━━━ 02:08:01\n",
      "Accuracy: 0.9007 - Loss: 0.2891\n",
      "\n",
      "Batch 181/992 ━━━━━━━━━━━━━━━━━━━━ 02:08:11\n",
      "Accuracy: 0.8999 - Loss: 0.2900\n",
      "\n",
      "Batch 182/992 ━━━━━━━━━━━━━━━━━━━━ 02:08:21\n",
      "Accuracy: 0.8997 - Loss: 0.2892\n",
      "\n",
      "Batch 183/992 ━━━━━━━━━━━━━━━━━━━━ 02:08:31\n",
      "Accuracy: 0.8996 - Loss: 0.2893\n",
      "\n",
      "Batch 184/992 ━━━━━━━━━━━━━━━━━━━━ 02:08:41\n",
      "Accuracy: 0.9001 - Loss: 0.2880\n",
      "\n",
      "Batch 185/992 ━━━━━━━━━━━━━━━━━━━━ 02:08:51\n",
      "Accuracy: 0.8993 - Loss: 0.2889\n",
      "\n",
      "Batch 186/992 ━━━━━━━━━━━━━━━━━━━━ 02:09:01\n",
      "Accuracy: 0.8999 - Loss: 0.2884\n",
      "\n",
      "Batch 187/992 ━━━━━━━━━━━━━━━━━━━━ 02:09:11\n",
      "Accuracy: 0.8991 - Loss: 0.2896\n",
      "\n",
      "Batch 188/992 ━━━━━━━━━━━━━━━━━━━━ 02:09:21\n",
      "Accuracy: 0.8996 - Loss: 0.2886\n",
      "\n",
      "Batch 189/992 ━━━━━━━━━━━━━━━━━━━━ 02:09:32\n",
      "Accuracy: 0.8995 - Loss: 0.2896\n",
      "\n",
      "Batch 190/992 ━━━━━━━━━━━━━━━━━━━━ 02:09:42\n",
      "Accuracy: 0.9000 - Loss: 0.2887\n",
      "\n",
      "Batch 191/992 ━━━━━━━━━━━━━━━━━━━━ 02:09:53\n",
      "Accuracy: 0.8999 - Loss: 0.2887\n",
      "\n",
      "Batch 192/992 ━━━━━━━━━━━━━━━━━━━━ 02:10:03\n",
      "Accuracy: 0.8997 - Loss: 0.2886\n",
      "\n",
      "Batch 193/992 ━━━━━━━━━━━━━━━━━━━━ 02:10:13\n",
      "Accuracy: 0.8996 - Loss: 0.2890\n",
      "\n",
      "Batch 194/992 ━━━━━━━━━━━━━━━━━━━━ 02:10:23\n",
      "Accuracy: 0.8995 - Loss: 0.2885\n",
      "\n",
      "Batch 195/992 ━━━━━━━━━━━━━━━━━━━━ 02:10:33\n",
      "Accuracy: 0.8994 - Loss: 0.2896\n",
      "\n",
      "Batch 196/992 ━━━━━━━━━━━━━━━━━━━━ 02:10:43\n",
      "Accuracy: 0.8986 - Loss: 0.2912\n",
      "\n",
      "Batch 197/992 ━━━━━━━━━━━━━━━━━━━━ 02:10:53\n",
      "Accuracy: 0.8985 - Loss: 0.2920\n",
      "\n",
      "Batch 198/992 ━━━━━━━━━━━━━━━━━━━━ 02:11:03\n",
      "Accuracy: 0.8977 - Loss: 0.2940\n",
      "\n",
      "Batch 199/992 ━━━━━━━━━━━━━━━━━━━━ 02:11:13\n",
      "Accuracy: 0.8976 - Loss: 0.2938\n",
      "\n",
      "Batch 200/992 ━━━━━━━━━━━━━━━━━━━━ 02:11:23\n",
      "Accuracy: 0.8981 - Loss: 0.2933\n",
      "\n",
      "Batch 201/992 ━━━━━━━━━━━━━━━━━━━━ 02:11:33\n",
      "Accuracy: 0.8980 - Loss: 0.2927\n",
      "\n",
      "Batch 202/992 ━━━━━━━━━━━━━━━━━━━━ 02:11:43\n",
      "Accuracy: 0.8979 - Loss: 0.2926\n",
      "\n",
      "Batch 203/992 ━━━━━━━━━━━━━━━━━━━━ 02:11:54\n",
      "Accuracy: 0.8984 - Loss: 0.2916\n",
      "\n",
      "Batch 204/992 ━━━━━━━━━━━━━━━━━━━━ 02:12:04\n",
      "Accuracy: 0.8989 - Loss: 0.2907\n",
      "\n",
      "Batch 205/992 ━━━━━━━━━━━━━━━━━━━━ 02:12:14\n",
      "Accuracy: 0.8982 - Loss: 0.2931\n",
      "\n",
      "Batch 206/992 ━━━━━━━━━━━━━━━━━━━━ 02:12:24\n",
      "Accuracy: 0.8981 - Loss: 0.2943\n",
      "\n",
      "Batch 207/992 ━━━━━━━━━━━━━━━━━━━━ 02:12:34\n",
      "Accuracy: 0.8979 - Loss: 0.2950\n",
      "\n",
      "Batch 208/992 ━━━━━━━━━━━━━━━━━━━━ 02:12:45\n",
      "Accuracy: 0.8978 - Loss: 0.2949\n",
      "\n",
      "Batch 209/992 ━━━━━━━━━━━━━━━━━━━━ 02:12:55\n",
      "Accuracy: 0.8977 - Loss: 0.2956\n",
      "\n",
      "Batch 210/992 ━━━━━━━━━━━━━━━━━━━━ 02:13:05\n",
      "Accuracy: 0.8970 - Loss: 0.2961\n",
      "\n",
      "Batch 211/992 ━━━━━━━━━━━━━━━━━━━━ 02:13:15\n",
      "Accuracy: 0.8969 - Loss: 0.2959\n",
      "\n",
      "Batch 212/992 ━━━━━━━━━━━━━━━━━━━━ 02:13:25\n",
      "Accuracy: 0.8962 - Loss: 0.2974\n",
      "\n",
      "Batch 213/992 ━━━━━━━━━━━━━━━━━━━━ 02:13:35\n",
      "Accuracy: 0.8961 - Loss: 0.2971\n",
      "\n",
      "Batch 214/992 ━━━━━━━━━━━━━━━━━━━━ 02:13:45\n",
      "Accuracy: 0.8954 - Loss: 0.2979\n",
      "\n",
      "Batch 215/992 ━━━━━━━━━━━━━━━━━━━━ 02:13:55\n",
      "Accuracy: 0.8953 - Loss: 0.2987\n",
      "\n",
      "Batch 216/992 ━━━━━━━━━━━━━━━━━━━━ 02:14:05\n",
      "Accuracy: 0.8953 - Loss: 0.2995\n",
      "\n",
      "Batch 217/992 ━━━━━━━━━━━━━━━━━━━━ 02:14:15\n",
      "Accuracy: 0.8952 - Loss: 0.2990\n",
      "\n",
      "Batch 218/992 ━━━━━━━━━━━━━━━━━━━━ 02:14:26\n",
      "Accuracy: 0.8951 - Loss: 0.2997\n",
      "\n",
      "Batch 219/992 ━━━━━━━━━━━━━━━━━━━━ 02:14:36\n",
      "Accuracy: 0.8950 - Loss: 0.3001\n",
      "\n",
      "Batch 220/992 ━━━━━━━━━━━━━━━━━━━━ 02:14:46\n",
      "Accuracy: 0.8955 - Loss: 0.2991\n",
      "\n",
      "Batch 221/992 ━━━━━━━━━━━━━━━━━━━━ 02:14:56\n",
      "Accuracy: 0.8954 - Loss: 0.2984\n",
      "\n",
      "Batch 222/992 ━━━━━━━━━━━━━━━━━━━━ 02:15:06\n",
      "Accuracy: 0.8958 - Loss: 0.2978\n",
      "\n",
      "Batch 223/992 ━━━━━━━━━━━━━━━━━━━━ 02:15:16\n",
      "Accuracy: 0.8957 - Loss: 0.2976\n",
      "\n",
      "Batch 224/992 ━━━━━━━━━━━━━━━━━━━━ 02:15:26\n",
      "Accuracy: 0.8962 - Loss: 0.2972\n",
      "\n",
      "Batch 225/992 ━━━━━━━━━━━━━━━━━━━━ 02:15:36\n",
      "Accuracy: 0.8956 - Loss: 0.2983\n",
      "\n",
      "Batch 226/992 ━━━━━━━━━━━━━━━━━━━━ 02:15:46\n",
      "Accuracy: 0.8955 - Loss: 0.3006\n",
      "\n",
      "Batch 227/992 ━━━━━━━━━━━━━━━━━━━━ 02:15:56\n",
      "Accuracy: 0.8959 - Loss: 0.2996\n",
      "\n",
      "Batch 228/992 ━━━━━━━━━━━━━━━━━━━━ 02:16:07\n",
      "Accuracy: 0.8953 - Loss: 0.3000\n",
      "\n",
      "Batch 229/992 ━━━━━━━━━━━━━━━━━━━━ 02:16:17\n",
      "Accuracy: 0.8952 - Loss: 0.3001\n",
      "\n",
      "Batch 230/992 ━━━━━━━━━━━━━━━━━━━━ 02:16:27\n",
      "Accuracy: 0.8951 - Loss: 0.2997\n",
      "\n",
      "Batch 231/992 ━━━━━━━━━━━━━━━━━━━━ 02:16:37\n",
      "Accuracy: 0.8945 - Loss: 0.3005\n",
      "\n",
      "Batch 232/992 ━━━━━━━━━━━━━━━━━━━━ 02:16:48\n",
      "Accuracy: 0.8944 - Loss: 0.3014\n",
      "\n",
      "Batch 233/992 ━━━━━━━━━━━━━━━━━━━━ 02:16:58\n",
      "Accuracy: 0.8943 - Loss: 0.3011\n",
      "\n",
      "Batch 234/992 ━━━━━━━━━━━━━━━━━━━━ 02:17:08\n",
      "Accuracy: 0.8942 - Loss: 0.3012\n",
      "\n",
      "Batch 235/992 ━━━━━━━━━━━━━━━━━━━━ 02:17:18\n",
      "Accuracy: 0.8936 - Loss: 0.3018\n",
      "\n",
      "Batch 236/992 ━━━━━━━━━━━━━━━━━━━━ 02:17:28\n",
      "Accuracy: 0.8941 - Loss: 0.3011\n",
      "\n",
      "Batch 237/992 ━━━━━━━━━━━━━━━━━━━━ 02:17:38\n",
      "Accuracy: 0.8945 - Loss: 0.3004\n",
      "\n",
      "Batch 238/992 ━━━━━━━━━━━━━━━━━━━━ 02:17:48\n",
      "Accuracy: 0.8929 - Loss: 0.3054\n",
      "\n",
      "Batch 239/992 ━━━━━━━━━━━━━━━━━━━━ 02:17:58\n",
      "Accuracy: 0.8933 - Loss: 0.3044\n",
      "\n",
      "Batch 240/992 ━━━━━━━━━━━━━━━━━━━━ 02:18:08\n",
      "Accuracy: 0.8938 - Loss: 0.3035\n",
      "\n",
      "Batch 241/992 ━━━━━━━━━━━━━━━━━━━━ 02:18:18\n",
      "Accuracy: 0.8932 - Loss: 0.3045\n",
      "\n",
      "Batch 242/992 ━━━━━━━━━━━━━━━━━━━━ 02:18:29\n",
      "Accuracy: 0.8931 - Loss: 0.3050\n",
      "\n",
      "Batch 243/992 ━━━━━━━━━━━━━━━━━━━━ 02:18:39\n",
      "Accuracy: 0.8935 - Loss: 0.3044\n",
      "\n",
      "Batch 244/992 ━━━━━━━━━━━━━━━━━━━━ 02:18:49\n",
      "Accuracy: 0.8934 - Loss: 0.3047\n",
      "\n",
      "Batch 245/992 ━━━━━━━━━━━━━━━━━━━━ 02:18:59\n",
      "Accuracy: 0.8929 - Loss: 0.3063\n",
      "\n",
      "Batch 246/992 ━━━━━━━━━━━━━━━━━━━━ 02:19:10\n",
      "Accuracy: 0.8933 - Loss: 0.3056\n",
      "\n",
      "Batch 247/992 ━━━━━━━━━━━━━━━━━━━━ 02:19:20\n",
      "Accuracy: 0.8932 - Loss: 0.3060\n",
      "\n",
      "Batch 248/992 ━━━━━━━━━━━━━━━━━━━━ 02:19:30\n",
      "Accuracy: 0.8931 - Loss: 0.3063\n",
      "\n",
      "Batch 249/992 ━━━━━━━━━━━━━━━━━━━━ 02:19:40\n",
      "Accuracy: 0.8926 - Loss: 0.3080\n",
      "\n",
      "Batch 250/992 ━━━━━━━━━━━━━━━━━━━━ 02:19:50\n",
      "Accuracy: 0.8925 - Loss: 0.3078\n",
      "\n",
      "Batch 251/992 ━━━━━━━━━━━━━━━━━━━━ 02:20:00\n",
      "Accuracy: 0.8924 - Loss: 0.3080\n",
      "\n",
      "Batch 252/992 ━━━━━━━━━━━━━━━━━━━━ 02:20:10\n",
      "Accuracy: 0.8924 - Loss: 0.3080\n",
      "\n",
      "Batch 253/992 ━━━━━━━━━━━━━━━━━━━━ 02:20:20\n",
      "Accuracy: 0.8928 - Loss: 0.3072\n",
      "\n",
      "Batch 254/992 ━━━━━━━━━━━━━━━━━━━━ 02:20:30\n",
      "Accuracy: 0.8932 - Loss: 0.3069\n",
      "\n",
      "Batch 255/992 ━━━━━━━━━━━━━━━━━━━━ 02:20:41\n",
      "Accuracy: 0.8922 - Loss: 0.3085\n",
      "\n",
      "Batch 256/992 ━━━━━━━━━━━━━━━━━━━━ 02:20:51\n",
      "Accuracy: 0.8916 - Loss: 0.3108\n",
      "\n",
      "Batch 257/992 ━━━━━━━━━━━━━━━━━━━━ 02:21:01\n",
      "Accuracy: 0.8911 - Loss: 0.3116\n",
      "\n",
      "Batch 258/992 ━━━━━━━━━━━━━━━━━━━━ 02:21:11\n",
      "Accuracy: 0.8910 - Loss: 0.3113\n",
      "\n",
      "Batch 259/992 ━━━━━━━━━━━━━━━━━━━━ 02:21:21\n",
      "Accuracy: 0.8914 - Loss: 0.3104\n",
      "\n",
      "Batch 260/992 ━━━━━━━━━━━━━━━━━━━━ 02:21:31\n",
      "Accuracy: 0.8904 - Loss: 0.3117\n",
      "\n",
      "Batch 261/992 ━━━━━━━━━━━━━━━━━━━━ 02:21:41\n",
      "Accuracy: 0.8903 - Loss: 0.3134\n",
      "\n",
      "Batch 262/992 ━━━━━━━━━━━━━━━━━━━━ 02:21:51\n",
      "Accuracy: 0.8903 - Loss: 0.3139\n",
      "\n",
      "Batch 263/992 ━━━━━━━━━━━━━━━━━━━━ 02:22:02\n",
      "Accuracy: 0.8907 - Loss: 0.3135\n",
      "\n",
      "Batch 264/992 ━━━━━━━━━━━━━━━━━━━━ 02:22:12\n",
      "Accuracy: 0.8911 - Loss: 0.3132\n",
      "\n",
      "Batch 265/992 ━━━━━━━━━━━━━━━━━━━━ 02:22:22\n",
      "Accuracy: 0.8906 - Loss: 0.3134\n",
      "\n",
      "Batch 266/992 ━━━━━━━━━━━━━━━━━━━━ 02:22:32\n",
      "Accuracy: 0.8905 - Loss: 0.3137\n",
      "\n",
      "Batch 267/992 ━━━━━━━━━━━━━━━━━━━━ 02:22:42\n",
      "Accuracy: 0.8904 - Loss: 0.3136\n",
      "\n",
      "Batch 268/992 ━━━━━━━━━━━━━━━━━━━━ 02:22:53\n",
      "Accuracy: 0.8899 - Loss: 0.3149\n",
      "\n",
      "Batch 269/992 ━━━━━━━━━━━━━━━━━━━━ 02:23:03\n",
      "Accuracy: 0.8899 - Loss: 0.3144\n",
      "\n",
      "Batch 270/992 ━━━━━━━━━━━━━━━━━━━━ 02:23:13\n",
      "Accuracy: 0.8898 - Loss: 0.3157\n",
      "\n",
      "Batch 271/992 ━━━━━━━━━━━━━━━━━━━━ 02:23:23\n",
      "Accuracy: 0.8898 - Loss: 0.3149\n",
      "\n",
      "Batch 272/992 ━━━━━━━━━━━━━━━━━━━━ 02:23:33\n",
      "Accuracy: 0.8897 - Loss: 0.3155\n",
      "\n",
      "Batch 273/992 ━━━━━━━━━━━━━━━━━━━━ 02:23:43\n",
      "Accuracy: 0.8901 - Loss: 0.3147\n",
      "\n",
      "Batch 274/992 ━━━━━━━━━━━━━━━━━━━━ 02:23:53\n",
      "Accuracy: 0.8901 - Loss: 0.3148\n",
      "\n",
      "Batch 275/992 ━━━━━━━━━━━━━━━━━━━━ 02:24:03\n",
      "Accuracy: 0.8905 - Loss: 0.3140\n",
      "\n",
      "Batch 276/992 ━━━━━━━━━━━━━━━━━━━━ 02:24:13\n",
      "Accuracy: 0.8895 - Loss: 0.3151\n",
      "\n",
      "Batch 277/992 ━━━━━━━━━━━━━━━━━━━━ 02:24:23\n",
      "Accuracy: 0.8894 - Loss: 0.3150\n",
      "\n",
      "Batch 278/992 ━━━━━━━━━━━━━━━━━━━━ 02:24:33\n",
      "Accuracy: 0.8889 - Loss: 0.3159\n",
      "\n",
      "Batch 279/992 ━━━━━━━━━━━━━━━━━━━━ 02:24:44\n",
      "Accuracy: 0.8884 - Loss: 0.3163\n",
      "\n",
      "Batch 280/992 ━━━━━━━━━━━━━━━━━━━━ 02:24:54\n",
      "Accuracy: 0.8884 - Loss: 0.3160\n",
      "\n",
      "Batch 281/992 ━━━━━━━━━━━━━━━━━━━━ 02:25:04\n",
      "Accuracy: 0.8879 - Loss: 0.3188\n",
      "\n",
      "Batch 282/992 ━━━━━━━━━━━━━━━━━━━━ 02:25:14\n",
      "Accuracy: 0.8879 - Loss: 0.3185\n",
      "\n",
      "Batch 283/992 ━━━━━━━━━━━━━━━━━━━━ 02:25:24\n",
      "Accuracy: 0.8883 - Loss: 0.3177\n",
      "\n",
      "Batch 284/992 ━━━━━━━━━━━━━━━━━━━━ 02:25:34\n",
      "Accuracy: 0.8886 - Loss: 0.3169\n",
      "\n",
      "Batch 285/992 ━━━━━━━━━━━━━━━━━━━━ 02:25:44\n",
      "Accuracy: 0.8886 - Loss: 0.3169\n",
      "\n",
      "Batch 286/992 ━━━━━━━━━━━━━━━━━━━━ 02:25:54\n",
      "Accuracy: 0.8890 - Loss: 0.3162\n",
      "\n",
      "Batch 287/992 ━━━━━━━━━━━━━━━━━━━━ 02:26:05\n",
      "Accuracy: 0.8889 - Loss: 0.3157\n",
      "\n",
      "Batch 288/992 ━━━━━━━━━━━━━━━━━━━━ 02:26:15\n",
      "Accuracy: 0.8889 - Loss: 0.3158\n",
      "\n",
      "Batch 289/992 ━━━━━━━━━━━━━━━━━━━━ 02:26:25\n",
      "Accuracy: 0.8875 - Loss: 0.3197\n",
      "\n",
      "Batch 290/992 ━━━━━━━━━━━━━━━━━━━━ 02:26:35\n",
      "Accuracy: 0.8875 - Loss: 0.3195\n",
      "\n",
      "Batch 291/992 ━━━━━━━━━━━━━━━━━━━━ 02:26:45\n",
      "Accuracy: 0.8879 - Loss: 0.3190\n",
      "\n",
      "Batch 292/992 ━━━━━━━━━━━━━━━━━━━━ 02:26:56\n",
      "Accuracy: 0.8874 - Loss: 0.3202\n",
      "\n",
      "Batch 293/992 ━━━━━━━━━━━━━━━━━━━━ 02:27:06\n",
      "Accuracy: 0.8878 - Loss: 0.3197\n",
      "\n",
      "Batch 294/992 ━━━━━━━━━━━━━━━━━━━━ 02:27:16\n",
      "Accuracy: 0.8873 - Loss: 0.3202\n",
      "\n",
      "Batch 295/992 ━━━━━━━━━━━━━━━━━━━━ 02:27:26\n",
      "Accuracy: 0.8869 - Loss: 0.3221\n",
      "\n",
      "Batch 296/992 ━━━━━━━━━━━━━━━━━━━━ 02:27:36\n",
      "Accuracy: 0.8860 - Loss: 0.3231\n",
      "\n",
      "Batch 297/992 ━━━━━━━━━━━━━━━━━━━━ 02:27:46\n",
      "Accuracy: 0.8851 - Loss: 0.3245\n",
      "\n",
      "Batch 298/992 ━━━━━━━━━━━━━━━━━━━━ 02:27:56\n",
      "Accuracy: 0.8851 - Loss: 0.3245\n",
      "\n",
      "Batch 299/992 ━━━━━━━━━━━━━━━━━━━━ 02:28:06\n",
      "Accuracy: 0.8850 - Loss: 0.3242\n",
      "\n",
      "Batch 300/992 ━━━━━━━━━━━━━━━━━━━━ 02:28:16\n",
      "Accuracy: 0.8846 - Loss: 0.3246\n",
      "\n",
      "Batch 301/992 ━━━━━━━━━━━━━━━━━━━━ 02:28:26\n",
      "Accuracy: 0.8846 - Loss: 0.3248\n",
      "\n",
      "Batch 302/992 ━━━━━━━━━━━━━━━━━━━━ 02:28:36\n",
      "Accuracy: 0.8837 - Loss: 0.3277\n",
      "\n",
      "Batch 303/992 ━━━━━━━━━━━━━━━━━━━━ 02:28:47\n",
      "Accuracy: 0.8837 - Loss: 0.3272\n",
      "\n",
      "Batch 304/992 ━━━━━━━━━━━━━━━━━━━━ 02:28:57\n",
      "Accuracy: 0.8832 - Loss: 0.3280\n",
      "\n",
      "Batch 305/992 ━━━━━━━━━━━━━━━━━━━━ 02:29:07\n",
      "Accuracy: 0.8824 - Loss: 0.3296\n",
      "\n",
      "Batch 306/992 ━━━━━━━━━━━━━━━━━━━━ 02:29:17\n",
      "Accuracy: 0.8828 - Loss: 0.3289\n",
      "\n",
      "Batch 307/992 ━━━━━━━━━━━━━━━━━━━━ 02:29:27\n",
      "Accuracy: 0.8823 - Loss: 0.3292\n",
      "\n",
      "Batch 308/992 ━━━━━━━━━━━━━━━━━━━━ 02:29:38\n",
      "Accuracy: 0.8823 - Loss: 0.3295\n",
      "\n",
      "Batch 309/992 ━━━━━━━━━━━━━━━━━━━━ 02:29:48\n",
      "Accuracy: 0.8823 - Loss: 0.3297\n",
      "\n",
      "Batch 310/992 ━━━━━━━━━━━━━━━━━━━━ 02:29:58\n",
      "Accuracy: 0.8827 - Loss: 0.3291\n",
      "\n",
      "Batch 311/992 ━━━━━━━━━━━━━━━━━━━━ 02:30:08\n",
      "Accuracy: 0.8826 - Loss: 0.3292\n",
      "\n",
      "Batch 312/992 ━━━━━━━━━━━━━━━━━━━━ 02:30:18\n",
      "Accuracy: 0.8826 - Loss: 0.3294\n",
      "\n",
      "Batch 313/992 ━━━━━━━━━━━━━━━━━━━━ 02:30:28\n",
      "Accuracy: 0.8830 - Loss: 0.3289\n",
      "\n",
      "Batch 314/992 ━━━━━━━━━━━━━━━━━━━━ 02:30:38\n",
      "Accuracy: 0.8826 - Loss: 0.3291\n",
      "\n",
      "Batch 315/992 ━━━━━━━━━━━━━━━━━━━━ 02:30:48\n",
      "Accuracy: 0.8825 - Loss: 0.3294\n",
      "\n",
      "Batch 316/992 ━━━━━━━━━━━━━━━━━━━━ 02:30:59\n",
      "Accuracy: 0.8825 - Loss: 0.3289\n",
      "\n",
      "Batch 317/992 ━━━━━━━━━━━━━━━━━━━━ 02:31:09\n",
      "Accuracy: 0.8813 - Loss: 0.3298\n",
      "\n",
      "Batch 318/992 ━━━━━━━━━━━━━━━━━━━━ 02:31:19\n",
      "Accuracy: 0.8813 - Loss: 0.3303\n",
      "\n",
      "Batch 319/992 ━━━━━━━━━━━━━━━━━━━━ 02:31:29\n",
      "Accuracy: 0.8813 - Loss: 0.3303\n",
      "\n",
      "Batch 320/992 ━━━━━━━━━━━━━━━━━━━━ 02:31:39\n",
      "Accuracy: 0.8813 - Loss: 0.3302\n",
      "\n",
      "Batch 321/992 ━━━━━━━━━━━━━━━━━━━━ 02:31:49\n",
      "Accuracy: 0.8808 - Loss: 0.3316\n",
      "\n",
      "Batch 322/992 ━━━━━━━━━━━━━━━━━━━━ 02:31:59\n",
      "Accuracy: 0.8812 - Loss: 0.3311\n",
      "\n",
      "Batch 323/992 ━━━━━━━━━━━━━━━━━━━━ 02:32:09\n",
      "Accuracy: 0.8812 - Loss: 0.3311\n",
      "\n",
      "Batch 324/992 ━━━━━━━━━━━━━━━━━━━━ 02:32:20\n",
      "Accuracy: 0.8812 - Loss: 0.3316\n",
      "\n",
      "Batch 325/992 ━━━━━━━━━━━━━━━━━━━━ 02:32:30\n",
      "Accuracy: 0.8800 - Loss: 0.3330\n",
      "\n",
      "Batch 326/992 ━━━━━━━━━━━━━━━━━━━━ 02:32:40\n",
      "Accuracy: 0.8804 - Loss: 0.3324\n",
      "\n",
      "Batch 327/992 ━━━━━━━━━━━━━━━━━━━━ 02:32:50\n",
      "Accuracy: 0.8800 - Loss: 0.3329\n",
      "\n",
      "Batch 328/992 ━━━━━━━━━━━━━━━━━━━━ 02:33:00\n",
      "Accuracy: 0.8800 - Loss: 0.3327\n",
      "\n",
      "Batch 329/992 ━━━━━━━━━━━━━━━━━━━━ 02:33:11\n",
      "Accuracy: 0.8803 - Loss: 0.3320\n",
      "\n",
      "Batch 330/992 ━━━━━━━━━━━━━━━━━━━━ 02:33:21\n",
      "Accuracy: 0.8807 - Loss: 0.3320\n",
      "\n",
      "Batch 331/992 ━━━━━━━━━━━━━━━━━━━━ 02:33:31\n",
      "Accuracy: 0.8807 - Loss: 0.3322\n",
      "\n",
      "Batch 332/992 ━━━━━━━━━━━━━━━━━━━━ 02:33:41\n",
      "Accuracy: 0.8810 - Loss: 0.3317\n",
      "\n",
      "Batch 333/992 ━━━━━━━━━━━━━━━━━━━━ 02:33:51\n",
      "Accuracy: 0.8810 - Loss: 0.3315\n",
      "\n",
      "Batch 334/992 ━━━━━━━━━━━━━━━━━━━━ 02:34:01\n",
      "Accuracy: 0.8814 - Loss: 0.3308\n",
      "\n",
      "Batch 335/992 ━━━━━━━━━━━━━━━━━━━━ 02:34:11\n",
      "Accuracy: 0.8817 - Loss: 0.3300\n",
      "\n",
      "Batch 336/992 ━━━━━━━━━━━━━━━━━━━━ 02:34:21\n",
      "Accuracy: 0.8821 - Loss: 0.3295\n",
      "\n",
      "Batch 337/992 ━━━━━━━━━━━━━━━━━━━━ 02:34:31\n",
      "Accuracy: 0.8820 - Loss: 0.3295\n",
      "\n",
      "Batch 338/992 ━━━━━━━━━━━━━━━━━━━━ 02:34:41\n",
      "Accuracy: 0.8820 - Loss: 0.3302\n",
      "\n",
      "Batch 339/992 ━━━━━━━━━━━━━━━━━━━━ 02:34:51\n",
      "Accuracy: 0.8820 - Loss: 0.3299\n",
      "\n",
      "Batch 340/992 ━━━━━━━━━━━━━━━━━━━━ 02:35:01\n",
      "Accuracy: 0.8820 - Loss: 0.3295\n",
      "\n",
      "Batch 341/992 ━━━━━━━━━━━━━━━━━━━━ 02:35:12\n",
      "Accuracy: 0.8816 - Loss: 0.3298\n",
      "\n",
      "Batch 342/992 ━━━━━━━━━━━━━━━━━━━━ 02:35:22\n",
      "Accuracy: 0.8819 - Loss: 0.3299\n",
      "\n",
      "Batch 343/992 ━━━━━━━━━━━━━━━━━━━━ 02:35:32\n",
      "Accuracy: 0.8819 - Loss: 0.3294\n",
      "\n",
      "Batch 344/992 ━━━━━━━━━━━━━━━━━━━━ 02:35:42\n",
      "Accuracy: 0.8819 - Loss: 0.3289\n",
      "\n",
      "Batch 345/992 ━━━━━━━━━━━━━━━━━━━━ 02:35:52\n",
      "Accuracy: 0.8822 - Loss: 0.3282\n",
      "\n",
      "Batch 346/992 ━━━━━━━━━━━━━━━━━━━━ 02:36:02\n",
      "Accuracy: 0.8822 - Loss: 0.3285\n",
      "\n",
      "Batch 347/992 ━━━━━━━━━━━━━━━━━━━━ 02:36:12\n",
      "Accuracy: 0.8826 - Loss: 0.3280\n",
      "\n",
      "Batch 348/992 ━━━━━━━━━━━━━━━━━━━━ 02:36:22\n",
      "Accuracy: 0.8829 - Loss: 0.3273\n",
      "\n",
      "Batch 349/992 ━━━━━━━━━━━━━━━━━━━━ 02:36:33\n",
      "Accuracy: 0.8832 - Loss: 0.3269\n",
      "\n",
      "Batch 350/992 ━━━━━━━━━━━━━━━━━━━━ 02:36:43\n",
      "Accuracy: 0.8836 - Loss: 0.3263\n",
      "\n",
      "Batch 351/992 ━━━━━━━━━━━━━━━━━━━━ 02:36:53\n",
      "Accuracy: 0.8835 - Loss: 0.3272\n",
      "\n",
      "Batch 352/992 ━━━━━━━━━━━━━━━━━━━━ 02:37:03\n",
      "Accuracy: 0.8832 - Loss: 0.3272\n",
      "\n",
      "Batch 353/992 ━━━━━━━━━━━━━━━━━━━━ 02:37:13\n",
      "Accuracy: 0.8835 - Loss: 0.3264\n",
      "\n",
      "Batch 354/992 ━━━━━━━━━━━━━━━━━━━━ 02:37:23\n",
      "Accuracy: 0.8831 - Loss: 0.3277\n",
      "\n",
      "Batch 355/992 ━━━━━━━━━━━━━━━━━━━━ 02:37:34\n",
      "Accuracy: 0.8835 - Loss: 0.3269\n",
      "\n",
      "Batch 356/992 ━━━━━━━━━━━━━━━━━━━━ 02:37:44\n",
      "Accuracy: 0.8834 - Loss: 0.3283\n",
      "\n",
      "Batch 357/992 ━━━━━━━━━━━━━━━━━━━━ 02:37:54\n",
      "Accuracy: 0.8838 - Loss: 0.3276\n",
      "\n",
      "Batch 358/992 ━━━━━━━━━━━━━━━━━━━━ 02:38:04\n",
      "Accuracy: 0.8837 - Loss: 0.3275\n",
      "\n",
      "Batch 359/992 ━━━━━━━━━━━━━━━━━━━━ 02:38:14\n",
      "Accuracy: 0.8834 - Loss: 0.3293\n",
      "\n",
      "Batch 360/992 ━━━━━━━━━━━━━━━━━━━━ 02:38:24\n",
      "Accuracy: 0.8830 - Loss: 0.3297\n",
      "\n",
      "Batch 361/992 ━━━━━━━━━━━━━━━━━━━━ 02:38:34\n",
      "Accuracy: 0.8833 - Loss: 0.3296\n",
      "\n",
      "Batch 362/992 ━━━━━━━━━━━━━━━━━━━━ 02:38:44\n",
      "Accuracy: 0.8833 - Loss: 0.3304\n",
      "\n",
      "Batch 363/992 ━━━━━━━━━━━━━━━━━━━━ 02:38:54\n",
      "Accuracy: 0.8833 - Loss: 0.3312\n",
      "\n",
      "Batch 364/992 ━━━━━━━━━━━━━━━━━━━━ 02:39:04\n",
      "Accuracy: 0.8836 - Loss: 0.3304\n",
      "\n",
      "Batch 365/992 ━━━━━━━━━━━━━━━━━━━━ 02:39:14\n",
      "Accuracy: 0.8836 - Loss: 0.3312\n",
      "\n",
      "Batch 366/992 ━━━━━━━━━━━━━━━━━━━━ 02:39:25\n",
      "Accuracy: 0.8835 - Loss: 0.3312\n",
      "\n",
      "Batch 367/992 ━━━━━━━━━━━━━━━━━━━━ 02:39:35\n",
      "Accuracy: 0.8839 - Loss: 0.3310\n",
      "\n",
      "Batch 368/992 ━━━━━━━━━━━━━━━━━━━━ 02:39:45\n",
      "Accuracy: 0.8838 - Loss: 0.3307\n",
      "\n",
      "Batch 369/992 ━━━━━━━━━━━━━━━━━━━━ 02:39:55\n",
      "Accuracy: 0.8838 - Loss: 0.3309\n",
      "\n",
      "Batch 370/992 ━━━━━━━━━━━━━━━━━━━━ 02:40:05\n",
      "Accuracy: 0.8834 - Loss: 0.3312\n",
      "\n",
      "Batch 371/992 ━━━━━━━━━━━━━━━━━━━━ 02:40:15\n",
      "Accuracy: 0.8838 - Loss: 0.3308\n",
      "\n",
      "Batch 372/992 ━━━━━━━━━━━━━━━━━━━━ 02:40:25\n",
      "Accuracy: 0.8834 - Loss: 0.3318\n",
      "\n",
      "Batch 373/992 ━━━━━━━━━━━━━━━━━━━━ 02:40:35\n",
      "Accuracy: 0.8834 - Loss: 0.3320\n",
      "\n",
      "Batch 374/992 ━━━━━━━━━━━━━━━━━━━━ 02:40:45\n",
      "Accuracy: 0.8837 - Loss: 0.3318\n",
      "\n",
      "Batch 375/992 ━━━━━━━━━━━━━━━━━━━━ 02:40:55\n",
      "Accuracy: 0.8837 - Loss: 0.3319\n",
      "\n",
      "Batch 376/992 ━━━━━━━━━━━━━━━━━━━━ 02:41:05\n",
      "Accuracy: 0.8840 - Loss: 0.3315\n",
      "\n",
      "Batch 377/992 ━━━━━━━━━━━━━━━━━━━━ 02:41:16\n",
      "Accuracy: 0.8843 - Loss: 0.3309\n",
      "\n",
      "Batch 378/992 ━━━━━━━━━━━━━━━━━━━━ 02:41:26\n",
      "Accuracy: 0.8843 - Loss: 0.3306\n",
      "\n",
      "Batch 379/992 ━━━━━━━━━━━━━━━━━━━━ 02:41:37\n",
      "Accuracy: 0.8839 - Loss: 0.3307\n",
      "\n",
      "Batch 380/992 ━━━━━━━━━━━━━━━━━━━━ 02:41:47\n",
      "Accuracy: 0.8839 - Loss: 0.3303\n",
      "\n",
      "Batch 381/992 ━━━━━━━━━━━━━━━━━━━━ 02:41:57\n",
      "Accuracy: 0.8842 - Loss: 0.3298\n",
      "\n",
      "Batch 382/992 ━━━━━━━━━━━━━━━━━━━━ 02:42:07\n",
      "Accuracy: 0.8845 - Loss: 0.3291\n",
      "\n",
      "Batch 383/992 ━━━━━━━━━━━━━━━━━━━━ 02:42:17\n",
      "Accuracy: 0.8841 - Loss: 0.3295\n",
      "\n",
      "Batch 384/992 ━━━━━━━━━━━━━━━━━━━━ 02:42:27\n",
      "Accuracy: 0.8835 - Loss: 0.3306\n",
      "\n",
      "Batch 385/992 ━━━━━━━━━━━━━━━━━━━━ 02:42:37\n",
      "Accuracy: 0.8838 - Loss: 0.3298\n",
      "\n",
      "Batch 386/992 ━━━━━━━━━━━━━━━━━━━━ 02:42:47\n",
      "Accuracy: 0.8841 - Loss: 0.3292\n",
      "\n",
      "Batch 387/992 ━━━━━━━━━━━━━━━━━━━━ 02:42:57\n",
      "Accuracy: 0.8837 - Loss: 0.3297\n",
      "\n",
      "Batch 388/992 ━━━━━━━━━━━━━━━━━━━━ 02:43:07\n",
      "Accuracy: 0.8834 - Loss: 0.3305\n",
      "\n",
      "Batch 389/992 ━━━━━━━━━━━━━━━━━━━━ 02:43:18\n",
      "Accuracy: 0.8830 - Loss: 0.3308\n",
      "\n",
      "Batch 390/992 ━━━━━━━━━━━━━━━━━━━━ 02:43:28\n",
      "Accuracy: 0.8833 - Loss: 0.3302\n",
      "\n",
      "Batch 391/992 ━━━━━━━━━━━━━━━━━━━━ 02:43:38\n",
      "Accuracy: 0.8833 - Loss: 0.3299\n",
      "\n",
      "Batch 392/992 ━━━━━━━━━━━━━━━━━━━━ 02:43:48\n",
      "Accuracy: 0.8833 - Loss: 0.3299\n",
      "\n",
      "Batch 393/992 ━━━━━━━━━━━━━━━━━━━━ 02:43:58\n",
      "Accuracy: 0.8836 - Loss: 0.3298\n",
      "\n",
      "Batch 394/992 ━━━━━━━━━━━━━━━━━━━━ 02:44:08\n",
      "Accuracy: 0.8839 - Loss: 0.3295\n",
      "\n",
      "Batch 395/992 ━━━━━━━━━━━━━━━━━━━━ 02:44:18\n",
      "Accuracy: 0.8839 - Loss: 0.3293\n",
      "\n",
      "Batch 396/992 ━━━━━━━━━━━━━━━━━━━━ 02:44:28\n",
      "Accuracy: 0.8842 - Loss: 0.3289\n",
      "\n",
      "Batch 397/992 ━━━━━━━━━━━━━━━━━━━━ 02:44:39\n",
      "Accuracy: 0.8841 - Loss: 0.3290\n",
      "\n",
      "Batch 398/992 ━━━━━━━━━━━━━━━━━━━━ 02:44:49\n",
      "Accuracy: 0.8844 - Loss: 0.3284\n",
      "\n",
      "Batch 399/992 ━━━━━━━━━━━━━━━━━━━━ 02:44:59\n",
      "Accuracy: 0.8847 - Loss: 0.3278\n",
      "\n",
      "Batch 400/992 ━━━━━━━━━━━━━━━━━━━━ 02:45:09\n",
      "Accuracy: 0.8847 - Loss: 0.3274\n",
      "\n",
      "Batch 401/992 ━━━━━━━━━━━━━━━━━━━━ 02:45:19\n",
      "Accuracy: 0.8850 - Loss: 0.3270\n",
      "\n",
      "Batch 402/992 ━━━━━━━━━━━━━━━━━━━━ 02:45:30\n",
      "Accuracy: 0.8850 - Loss: 0.3274\n",
      "\n",
      "Batch 403/992 ━━━━━━━━━━━━━━━━━━━━ 02:45:40\n",
      "Accuracy: 0.8852 - Loss: 0.3269\n",
      "\n",
      "Batch 404/992 ━━━━━━━━━━━━━━━━━━━━ 02:45:50\n",
      "Accuracy: 0.8855 - Loss: 0.3264\n",
      "\n",
      "Batch 405/992 ━━━━━━━━━━━━━━━━━━━━ 02:46:00\n",
      "Accuracy: 0.8855 - Loss: 0.3267\n",
      "\n",
      "Batch 406/992 ━━━━━━━━━━━━━━━━━━━━ 02:46:10\n",
      "Accuracy: 0.8852 - Loss: 0.3272\n",
      "\n",
      "Batch 407/992 ━━━━━━━━━━━━━━━━━━━━ 02:46:20\n",
      "Accuracy: 0.8848 - Loss: 0.3275\n",
      "\n",
      "Batch 408/992 ━━━━━━━━━━━━━━━━━━━━ 02:46:30\n",
      "Accuracy: 0.8845 - Loss: 0.3278\n",
      "\n",
      "Batch 409/992 ━━━━━━━━━━━━━━━━━━━━ 02:46:40\n",
      "Accuracy: 0.8848 - Loss: 0.3274\n",
      "\n",
      "Batch 410/992 ━━━━━━━━━━━━━━━━━━━━ 02:46:50\n",
      "Accuracy: 0.8848 - Loss: 0.3279\n",
      "\n",
      "Batch 411/992 ━━━━━━━━━━━━━━━━━━━━ 02:47:00\n",
      "Accuracy: 0.8850 - Loss: 0.3271\n",
      "\n",
      "Batch 412/992 ━━━━━━━━━━━━━━━━━━━━ 02:47:10\n",
      "Accuracy: 0.8850 - Loss: 0.3267\n",
      "\n",
      "Batch 413/992 ━━━━━━━━━━━━━━━━━━━━ 02:47:21\n",
      "Accuracy: 0.8850 - Loss: 0.3271\n",
      "\n",
      "Batch 414/992 ━━━━━━━━━━━━━━━━━━━━ 02:47:31\n",
      "Accuracy: 0.8853 - Loss: 0.3266\n",
      "\n",
      "Batch 415/992 ━━━━━━━━━━━━━━━━━━━━ 02:47:41\n",
      "Accuracy: 0.8855 - Loss: 0.3261\n",
      "\n",
      "Batch 416/992 ━━━━━━━━━━━━━━━━━━━━ 02:47:51\n",
      "Accuracy: 0.8852 - Loss: 0.3264\n",
      "\n",
      "Batch 417/992 ━━━━━━━━━━━━━━━━━━━━ 02:48:01\n",
      "Accuracy: 0.8852 - Loss: 0.3264\n",
      "\n",
      "Batch 418/992 ━━━━━━━━━━━━━━━━━━━━ 02:48:11\n",
      "Accuracy: 0.8855 - Loss: 0.3258\n",
      "\n",
      "Batch 419/992 ━━━━━━━━━━━━━━━━━━━━ 02:48:21\n",
      "Accuracy: 0.8851 - Loss: 0.3271\n",
      "\n",
      "Batch 420/992 ━━━━━━━━━━━━━━━━━━━━ 02:48:32\n",
      "Accuracy: 0.8854 - Loss: 0.3265\n",
      "\n",
      "Batch 421/992 ━━━━━━━━━━━━━━━━━━━━ 02:48:42\n",
      "Accuracy: 0.8848 - Loss: 0.3272\n",
      "\n",
      "Batch 422/992 ━━━━━━━━━━━━━━━━━━━━ 02:48:52\n",
      "Accuracy: 0.8848 - Loss: 0.3269\n",
      "\n",
      "Batch 423/992 ━━━━━━━━━━━━━━━━━━━━ 02:49:02\n",
      "Accuracy: 0.8845 - Loss: 0.3268\n",
      "\n",
      "Batch 424/992 ━━━━━━━━━━━━━━━━━━━━ 02:49:12\n",
      "Accuracy: 0.8844 - Loss: 0.3274\n",
      "\n",
      "Batch 425/992 ━━━━━━━━━━━━━━━━━━━━ 02:49:22\n",
      "Accuracy: 0.8844 - Loss: 0.3275\n",
      "\n",
      "Batch 426/992 ━━━━━━━━━━━━━━━━━━━━ 02:49:33\n",
      "Accuracy: 0.8847 - Loss: 0.3272\n",
      "\n",
      "Batch 427/992 ━━━━━━━━━━━━━━━━━━━━ 02:49:43\n",
      "Accuracy: 0.8850 - Loss: 0.3269\n",
      "\n",
      "Batch 428/992 ━━━━━━━━━━━━━━━━━━━━ 02:49:53\n",
      "Accuracy: 0.8841 - Loss: 0.3283\n",
      "\n",
      "Batch 429/992 ━━━━━━━━━━━━━━━━━━━━ 02:50:03\n",
      "Accuracy: 0.8843 - Loss: 0.3277\n",
      "\n",
      "Batch 430/992 ━━━━━━━━━━━━━━━━━━━━ 02:50:13\n",
      "Accuracy: 0.8846 - Loss: 0.3271\n",
      "\n",
      "Batch 431/992 ━━━━━━━━━━━━━━━━━━━━ 02:50:23\n",
      "Accuracy: 0.8849 - Loss: 0.3268\n",
      "\n",
      "Batch 432/992 ━━━━━━━━━━━━━━━━━━━━ 02:50:33\n",
      "Accuracy: 0.8843 - Loss: 0.3289\n",
      "\n",
      "Batch 433/992 ━━━━━━━━━━━━━━━━━━━━ 02:50:43\n",
      "Accuracy: 0.8839 - Loss: 0.3296\n",
      "\n",
      "Batch 434/992 ━━━━━━━━━━━━━━━━━━━━ 02:50:54\n",
      "Accuracy: 0.8836 - Loss: 0.3303\n",
      "\n",
      "Batch 435/992 ━━━━━━━━━━━━━━━━━━━━ 02:51:04\n",
      "Accuracy: 0.8839 - Loss: 0.3300\n",
      "\n",
      "Batch 436/992 ━━━━━━━━━━━━━━━━━━━━ 02:51:14\n",
      "Accuracy: 0.8839 - Loss: 0.3299\n",
      "\n",
      "Batch 437/992 ━━━━━━━━━━━━━━━━━━━━ 02:51:24\n",
      "Accuracy: 0.8839 - Loss: 0.3298\n",
      "\n",
      "Batch 438/992 ━━━━━━━━━━━━━━━━━━━━ 02:51:34\n",
      "Accuracy: 0.8836 - Loss: 0.3306\n",
      "\n",
      "Batch 439/992 ━━━━━━━━━━━━━━━━━━━━ 02:51:44\n",
      "Accuracy: 0.8838 - Loss: 0.3302\n",
      "\n",
      "Batch 440/992 ━━━━━━━━━━━━━━━━━━━━ 02:51:55\n",
      "Accuracy: 0.8835 - Loss: 0.3309\n",
      "\n",
      "Batch 441/992 ━━━━━━━━━━━━━━━━━━━━ 02:52:05\n",
      "Accuracy: 0.8832 - Loss: 0.3321\n",
      "\n",
      "Batch 442/992 ━━━━━━━━━━━━━━━━━━━━ 02:52:15\n",
      "Accuracy: 0.8829 - Loss: 0.3324\n",
      "\n",
      "Batch 443/992 ━━━━━━━━━━━━━━━━━━━━ 02:52:25\n",
      "Accuracy: 0.8829 - Loss: 0.3328\n",
      "\n",
      "Batch 444/992 ━━━━━━━━━━━━━━━━━━━━ 02:52:35\n",
      "Accuracy: 0.8832 - Loss: 0.3323\n",
      "\n",
      "Batch 445/992 ━━━━━━━━━━━━━━━━━━━━ 02:52:45\n",
      "Accuracy: 0.8829 - Loss: 0.3324\n",
      "\n",
      "Batch 446/992 ━━━━━━━━━━━━━━━━━━━━ 02:52:55\n",
      "Accuracy: 0.8831 - Loss: 0.3322\n",
      "\n",
      "Batch 447/992 ━━━━━━━━━━━━━━━━━━━━ 02:53:05\n",
      "Accuracy: 0.8831 - Loss: 0.3317\n",
      "\n",
      "Batch 448/992 ━━━━━━━━━━━━━━━━━━━━ 02:53:15\n",
      "Accuracy: 0.8831 - Loss: 0.3313\n",
      "\n",
      "Batch 449/992 ━━━━━━━━━━━━━━━━━━━━ 02:53:25\n",
      "Accuracy: 0.8831 - Loss: 0.3314\n",
      "\n",
      "Batch 450/992 ━━━━━━━━━━━━━━━━━━━━ 02:53:36\n",
      "Accuracy: 0.8828 - Loss: 0.3325\n",
      "\n",
      "Batch 451/992 ━━━━━━━━━━━━━━━━━━━━ 02:53:46\n",
      "Accuracy: 0.8828 - Loss: 0.3326\n",
      "\n",
      "Batch 452/992 ━━━━━━━━━━━━━━━━━━━━ 02:53:56\n",
      "Accuracy: 0.8827 - Loss: 0.3323\n",
      "\n",
      "Batch 453/992 ━━━━━━━━━━━━━━━━━━━━ 02:54:06\n",
      "Accuracy: 0.8830 - Loss: 0.3318\n",
      "\n",
      "Batch 454/992 ━━━━━━━━━━━━━━━━━━━━ 02:54:16\n",
      "Accuracy: 0.8824 - Loss: 0.3327\n",
      "\n",
      "Batch 455/992 ━━━━━━━━━━━━━━━━━━━━ 02:54:26\n",
      "Accuracy: 0.8821 - Loss: 0.3338\n",
      "\n",
      "Batch 456/992 ━━━━━━━━━━━━━━━━━━━━ 02:54:36\n",
      "Accuracy: 0.8824 - Loss: 0.3332\n",
      "\n",
      "Batch 457/992 ━━━━━━━━━━━━━━━━━━━━ 02:54:46\n",
      "Accuracy: 0.8824 - Loss: 0.3334\n",
      "\n",
      "Batch 458/992 ━━━━━━━━━━━━━━━━━━━━ 02:54:57\n",
      "Accuracy: 0.8826 - Loss: 0.3332\n",
      "\n",
      "Batch 459/992 ━━━━━━━━━━━━━━━━━━━━ 02:55:07\n",
      "Accuracy: 0.8829 - Loss: 0.3328\n",
      "\n",
      "Batch 460/992 ━━━━━━━━━━━━━━━━━━━━ 02:55:17\n",
      "Accuracy: 0.8829 - Loss: 0.3327\n",
      "\n",
      "Batch 461/992 ━━━━━━━━━━━━━━━━━━━━ 02:55:27\n",
      "Accuracy: 0.8826 - Loss: 0.3326\n",
      "\n",
      "Batch 462/992 ━━━━━━━━━━━━━━━━━━━━ 02:55:37\n",
      "Accuracy: 0.8826 - Loss: 0.3331\n",
      "\n",
      "Batch 463/992 ━━━━━━━━━━━━━━━━━━━━ 02:55:47\n",
      "Accuracy: 0.8826 - Loss: 0.3329\n",
      "\n",
      "Batch 464/992 ━━━━━━━━━━━━━━━━━━━━ 02:55:58\n",
      "Accuracy: 0.8825 - Loss: 0.3327\n",
      "\n",
      "Batch 465/992 ━━━━━━━━━━━━━━━━━━━━ 02:56:08\n",
      "Accuracy: 0.8823 - Loss: 0.3328\n",
      "\n",
      "Batch 466/992 ━━━━━━━━━━━━━━━━━━━━ 02:56:18\n",
      "Accuracy: 0.8817 - Loss: 0.3332\n",
      "\n",
      "Batch 467/992 ━━━━━━━━━━━━━━━━━━━━ 02:56:28\n",
      "Accuracy: 0.8817 - Loss: 0.3333\n",
      "\n",
      "Batch 468/992 ━━━━━━━━━━━━━━━━━━━━ 02:56:38\n",
      "Accuracy: 0.8819 - Loss: 0.3326\n",
      "\n",
      "Batch 469/992 ━━━━━━━━━━━━━━━━━━━━ 02:56:48\n",
      "Accuracy: 0.8819 - Loss: 0.3328\n",
      "\n",
      "Batch 470/992 ━━━━━━━━━━━━━━━━━━━━ 02:56:58\n",
      "Accuracy: 0.8822 - Loss: 0.3322\n",
      "\n",
      "Batch 471/992 ━━━━━━━━━━━━━━━━━━━━ 02:57:08\n",
      "Accuracy: 0.8816 - Loss: 0.3332\n",
      "\n",
      "Batch 472/992 ━━━━━━━━━━━━━━━━━━━━ 02:57:18\n",
      "Accuracy: 0.8816 - Loss: 0.3328\n",
      "\n",
      "Batch 473/992 ━━━━━━━━━━━━━━━━━━━━ 02:57:28\n",
      "Accuracy: 0.8819 - Loss: 0.3324\n",
      "\n",
      "Batch 474/992 ━━━━━━━━━━━━━━━━━━━━ 02:57:39\n",
      "Accuracy: 0.8821 - Loss: 0.3319\n",
      "\n",
      "Batch 475/992 ━━━━━━━━━━━━━━━━━━━━ 02:57:49\n",
      "Accuracy: 0.8818 - Loss: 0.3320\n",
      "\n",
      "Batch 476/992 ━━━━━━━━━━━━━━━━━━━━ 02:58:00\n",
      "Accuracy: 0.8818 - Loss: 0.3319\n",
      "\n",
      "Batch 477/992 ━━━━━━━━━━━━━━━━━━━━ 02:58:10\n",
      "Accuracy: 0.8818 - Loss: 0.3315\n",
      "\n",
      "Batch 478/992 ━━━━━━━━━━━━━━━━━━━━ 02:58:20\n",
      "Accuracy: 0.8821 - Loss: 0.3311\n",
      "\n",
      "Batch 479/992 ━━━━━━━━━━━━━━━━━━━━ 02:58:30\n",
      "Accuracy: 0.8820 - Loss: 0.3313\n",
      "\n",
      "Batch 480/992 ━━━━━━━━━━━━━━━━━━━━ 02:58:41\n",
      "Accuracy: 0.8820 - Loss: 0.3312\n",
      "\n",
      "Batch 481/992 ━━━━━━━━━━━━━━━━━━━━ 02:58:51\n",
      "Accuracy: 0.8818 - Loss: 0.3311\n",
      "\n",
      "Batch 482/992 ━━━━━━━━━━━━━━━━━━━━ 02:59:01\n",
      "Accuracy: 0.8820 - Loss: 0.3308\n",
      "\n",
      "Batch 483/992 ━━━━━━━━━━━━━━━━━━━━ 02:59:11\n",
      "Accuracy: 0.8822 - Loss: 0.3303\n",
      "\n",
      "Batch 484/992 ━━━━━━━━━━━━━━━━━━━━ 02:59:21\n",
      "Accuracy: 0.8822 - Loss: 0.3300\n",
      "\n",
      "Batch 485/992 ━━━━━━━━━━━━━━━━━━━━ 02:59:31\n",
      "Accuracy: 0.8825 - Loss: 0.3297\n",
      "\n",
      "Batch 486/992 ━━━━━━━━━━━━━━━━━━━━ 02:59:41\n",
      "Accuracy: 0.8822 - Loss: 0.3310\n",
      "\n",
      "Batch 487/992 ━━━━━━━━━━━━━━━━━━━━ 02:59:52\n",
      "Accuracy: 0.8819 - Loss: 0.3316\n",
      "\n",
      "Batch 488/992 ━━━━━━━━━━━━━━━━━━━━ 03:00:02\n",
      "Accuracy: 0.8822 - Loss: 0.3311\n",
      "\n",
      "Batch 489/992 ━━━━━━━━━━━━━━━━━━━━ 03:00:12\n",
      "Accuracy: 0.8824 - Loss: 0.3309\n",
      "\n",
      "Batch 490/992 ━━━━━━━━━━━━━━━━━━━━ 03:00:22\n",
      "Accuracy: 0.8824 - Loss: 0.3307\n",
      "\n",
      "Batch 491/992 ━━━━━━━━━━━━━━━━━━━━ 03:00:32\n",
      "Accuracy: 0.8824 - Loss: 0.3306\n",
      "\n",
      "Batch 492/992 ━━━━━━━━━━━━━━━━━━━━ 03:00:42\n",
      "Accuracy: 0.8816 - Loss: 0.3318\n",
      "\n",
      "Batch 493/992 ━━━━━━━━━━━━━━━━━━━━ 03:00:52\n",
      "Accuracy: 0.8816 - Loss: 0.3317\n",
      "\n",
      "Batch 494/992 ━━━━━━━━━━━━━━━━━━━━ 03:01:02\n",
      "Accuracy: 0.8813 - Loss: 0.3324\n",
      "\n",
      "Batch 495/992 ━━━━━━━━━━━━━━━━━━━━ 03:01:12\n",
      "Accuracy: 0.8816 - Loss: 0.3320\n",
      "\n",
      "Batch 496/992 ━━━━━━━━━━━━━━━━━━━━ 03:01:22\n",
      "Accuracy: 0.8813 - Loss: 0.3323\n",
      "\n",
      "Batch 497/992 ━━━━━━━━━━━━━━━━━━━━ 03:01:32\n",
      "Accuracy: 0.8815 - Loss: 0.3319\n",
      "\n",
      "Batch 498/992 ━━━━━━━━━━━━━━━━━━━━ 03:01:43\n",
      "Accuracy: 0.8813 - Loss: 0.3329\n",
      "\n",
      "Batch 499/992 ━━━━━━━━━━━━━━━━━━━━ 03:01:53\n",
      "Accuracy: 0.8810 - Loss: 0.3332\n",
      "\n",
      "Batch 500/992 ━━━━━━━━━━━━━━━━━━━━ 03:02:03\n",
      "Accuracy: 0.8813 - Loss: 0.3327\n",
      "\n",
      "Batch 501/992 ━━━━━━━━━━━━━━━━━━━━ 03:02:13\n",
      "Accuracy: 0.8812 - Loss: 0.3336\n",
      "\n",
      "Batch 502/992 ━━━━━━━━━━━━━━━━━━━━ 03:02:24\n",
      "Accuracy: 0.8812 - Loss: 0.3342\n",
      "\n",
      "Batch 503/992 ━━━━━━━━━━━━━━━━━━━━ 03:02:34\n",
      "Accuracy: 0.8812 - Loss: 0.3341\n",
      "\n",
      "Batch 504/992 ━━━━━━━━━━━━━━━━━━━━ 03:02:44\n",
      "Accuracy: 0.8810 - Loss: 0.3352\n",
      "\n",
      "Batch 505/992 ━━━━━━━━━━━━━━━━━━━━ 03:02:54\n",
      "Accuracy: 0.8812 - Loss: 0.3346\n",
      "\n",
      "Batch 506/992 ━━━━━━━━━━━━━━━━━━━━ 03:03:04\n",
      "Accuracy: 0.8814 - Loss: 0.3340\n",
      "\n",
      "Batch 507/992 ━━━━━━━━━━━━━━━━━━━━ 03:03:14\n",
      "Accuracy: 0.8817 - Loss: 0.3336\n",
      "\n",
      "Batch 508/992 ━━━━━━━━━━━━━━━━━━━━ 03:03:24\n",
      "Accuracy: 0.8812 - Loss: 0.3348\n",
      "\n",
      "Batch 509/992 ━━━━━━━━━━━━━━━━━━━━ 03:03:34\n",
      "Accuracy: 0.8809 - Loss: 0.3347\n",
      "\n",
      "Batch 510/992 ━━━━━━━━━━━━━━━━━━━━ 03:03:44\n",
      "Accuracy: 0.8809 - Loss: 0.3353\n",
      "\n",
      "Batch 511/992 ━━━━━━━━━━━━━━━━━━━━ 03:03:55\n",
      "Accuracy: 0.8809 - Loss: 0.3351\n",
      "\n",
      "Batch 512/992 ━━━━━━━━━━━━━━━━━━━━ 03:04:05\n",
      "Accuracy: 0.8804 - Loss: 0.3365\n",
      "\n",
      "Batch 513/992 ━━━━━━━━━━━━━━━━━━━━ 03:04:15\n",
      "Accuracy: 0.8804 - Loss: 0.3365\n",
      "\n",
      "Batch 514/992 ━━━━━━━━━━━━━━━━━━━━ 03:04:25\n",
      "Accuracy: 0.8804 - Loss: 0.3364\n",
      "\n",
      "Batch 515/992 ━━━━━━━━━━━━━━━━━━━━ 03:04:36\n",
      "Accuracy: 0.8796 - Loss: 0.3374\n",
      "\n",
      "Batch 516/992 ━━━━━━━━━━━━━━━━━━━━ 03:04:48\n",
      "Accuracy: 0.8796 - Loss: 0.3370\n",
      "\n",
      "Batch 517/992 ━━━━━━━━━━━━━━━━━━━━ 03:05:00\n",
      "Accuracy: 0.8798 - Loss: 0.3364\n",
      "\n",
      "Batch 518/992 ━━━━━━━━━━━━━━━━━━━━ 03:05:11\n",
      "Accuracy: 0.8798 - Loss: 0.3362\n",
      "\n",
      "Batch 519/992 ━━━━━━━━━━━━━━━━━━━━ 03:05:22\n",
      "Accuracy: 0.8798 - Loss: 0.3359\n",
      "\n",
      "Batch 520/992 ━━━━━━━━━━━━━━━━━━━━ 03:05:32\n",
      "Accuracy: 0.8800 - Loss: 0.3355\n",
      "\n",
      "Batch 521/992 ━━━━━━━━━━━━━━━━━━━━ 03:05:42\n",
      "Accuracy: 0.8803 - Loss: 0.3350\n",
      "\n",
      "Batch 522/992 ━━━━━━━━━━━━━━━━━━━━ 03:05:52\n",
      "Accuracy: 0.8798 - Loss: 0.3358\n",
      "\n",
      "Batch 523/992 ━━━━━━━━━━━━━━━━━━━━ 03:06:02\n",
      "Accuracy: 0.8798 - Loss: 0.3355\n",
      "\n",
      "Batch 524/992 ━━━━━━━━━━━━━━━━━━━━ 03:06:12\n",
      "Accuracy: 0.8795 - Loss: 0.3363\n",
      "\n",
      "Batch 525/992 ━━━━━━━━━━━━━━━━━━━━ 03:06:22\n",
      "Accuracy: 0.8795 - Loss: 0.3366\n",
      "\n",
      "Batch 526/992 ━━━━━━━━━━━━━━━━━━━━ 03:06:32\n",
      "Accuracy: 0.8798 - Loss: 0.3363\n",
      "\n",
      "Batch 527/992 ━━━━━━━━━━━━━━━━━━━━ 03:06:42\n",
      "Accuracy: 0.8795 - Loss: 0.3369\n",
      "\n",
      "Batch 528/992 ━━━━━━━━━━━━━━━━━━━━ 03:06:52\n",
      "Accuracy: 0.8797 - Loss: 0.3366\n",
      "\n",
      "Batch 529/992 ━━━━━━━━━━━━━━━━━━━━ 03:07:02\n",
      "Accuracy: 0.8795 - Loss: 0.3367\n",
      "\n",
      "Batch 530/992 ━━━━━━━━━━━━━━━━━━━━ 03:07:12\n",
      "Accuracy: 0.8797 - Loss: 0.3362\n",
      "\n",
      "Batch 531/992 ━━━━━━━━━━━━━━━━━━━━ 03:07:23\n",
      "Accuracy: 0.8799 - Loss: 0.3358\n",
      "\n",
      "Batch 532/992 ━━━━━━━━━━━━━━━━━━━━ 03:07:33\n",
      "Accuracy: 0.8802 - Loss: 0.3353\n",
      "\n",
      "Batch 533/992 ━━━━━━━━━━━━━━━━━━━━ 03:07:43\n",
      "Accuracy: 0.8799 - Loss: 0.3359\n",
      "\n",
      "Batch 534/992 ━━━━━━━━━━━━━━━━━━━━ 03:07:53\n",
      "Accuracy: 0.8794 - Loss: 0.3362\n",
      "\n",
      "Batch 535/992 ━━━━━━━━━━━━━━━━━━━━ 03:08:03\n",
      "Accuracy: 0.8797 - Loss: 0.3359\n",
      "\n",
      "Batch 536/992 ━━━━━━━━━━━━━━━━━━━━ 03:08:13\n",
      "Accuracy: 0.8797 - Loss: 0.3357\n",
      "\n",
      "Batch 537/992 ━━━━━━━━━━━━━━━━━━━━ 03:08:24\n",
      "Accuracy: 0.8797 - Loss: 0.3365\n",
      "\n",
      "Batch 538/992 ━━━━━━━━━━━━━━━━━━━━ 03:08:34\n",
      "Accuracy: 0.8796 - Loss: 0.3365\n",
      "\n",
      "Batch 539/992 ━━━━━━━━━━━━━━━━━━━━ 03:08:44\n",
      "Accuracy: 0.8796 - Loss: 0.3366\n",
      "\n",
      "Batch 540/992 ━━━━━━━━━━━━━━━━━━━━ 03:08:54\n",
      "Accuracy: 0.8799 - Loss: 0.3361\n",
      "\n",
      "Batch 541/992 ━━━━━━━━━━━━━━━━━━━━ 03:09:04\n",
      "Accuracy: 0.8801 - Loss: 0.3358\n",
      "\n",
      "Batch 542/992 ━━━━━━━━━━━━━━━━━━━━ 03:09:14\n",
      "Accuracy: 0.8801 - Loss: 0.3358\n",
      "\n",
      "Batch 543/992 ━━━━━━━━━━━━━━━━━━━━ 03:09:24\n",
      "Accuracy: 0.8803 - Loss: 0.3353\n",
      "\n",
      "Batch 544/992 ━━━━━━━━━━━━━━━━━━━━ 03:09:34\n",
      "Accuracy: 0.8803 - Loss: 0.3351\n",
      "\n",
      "Batch 545/992 ━━━━━━━━━━━━━━━━━━━━ 03:09:44\n",
      "Accuracy: 0.8805 - Loss: 0.3349\n",
      "\n",
      "Batch 546/992 ━━━━━━━━━━━━━━━━━━━━ 03:09:54\n",
      "Accuracy: 0.8805 - Loss: 0.3348\n",
      "\n",
      "Batch 547/992 ━━━━━━━━━━━━━━━━━━━━ 03:10:04\n",
      "Accuracy: 0.8807 - Loss: 0.3345\n",
      "\n",
      "Batch 548/992 ━━━━━━━━━━━━━━━━━━━━ 03:10:14\n",
      "Accuracy: 0.8809 - Loss: 0.3341\n",
      "\n",
      "Batch 549/992 ━━━━━━━━━━━━━━━━━━━━ 03:10:24\n",
      "Accuracy: 0.8811 - Loss: 0.3339\n",
      "\n",
      "Batch 550/992 ━━━━━━━━━━━━━━━━━━━━ 03:10:35\n",
      "Accuracy: 0.8811 - Loss: 0.3336\n",
      "\n",
      "Batch 551/992 ━━━━━━━━━━━━━━━━━━━━ 03:10:45\n",
      "Accuracy: 0.8811 - Loss: 0.3337\n",
      "\n",
      "Batch 552/992 ━━━━━━━━━━━━━━━━━━━━ 03:10:55\n",
      "Accuracy: 0.8811 - Loss: 0.3337\n",
      "\n",
      "Batch 553/992 ━━━━━━━━━━━━━━━━━━━━ 03:11:05\n",
      "Accuracy: 0.8813 - Loss: 0.3334\n",
      "\n",
      "Batch 554/992 ━━━━━━━━━━━━━━━━━━━━ 03:11:15\n",
      "Accuracy: 0.8815 - Loss: 0.3332\n",
      "\n",
      "Batch 555/992 ━━━━━━━━━━━━━━━━━━━━ 03:11:25\n",
      "Accuracy: 0.8815 - Loss: 0.3334\n",
      "\n",
      "Batch 556/992 ━━━━━━━━━━━━━━━━━━━━ 03:11:35\n",
      "Accuracy: 0.8817 - Loss: 0.3329\n",
      "\n",
      "Batch 557/992 ━━━━━━━━━━━━━━━━━━━━ 03:11:45\n",
      "Accuracy: 0.8820 - Loss: 0.3325\n",
      "\n",
      "Batch 558/992 ━━━━━━━━━━━━━━━━━━━━ 03:11:55\n",
      "Accuracy: 0.8822 - Loss: 0.3324\n",
      "\n",
      "Batch 559/992 ━━━━━━━━━━━━━━━━━━━━ 03:12:05\n",
      "Accuracy: 0.8822 - Loss: 0.3324\n",
      "\n",
      "Batch 560/992 ━━━━━━━━━━━━━━━━━━━━ 03:12:15\n",
      "Accuracy: 0.8817 - Loss: 0.3330\n",
      "\n",
      "Batch 561/992 ━━━━━━━━━━━━━━━━━━━━ 03:12:26\n",
      "Accuracy: 0.8817 - Loss: 0.3332\n",
      "\n",
      "Batch 562/992 ━━━━━━━━━━━━━━━━━━━━ 03:12:36\n",
      "Accuracy: 0.8819 - Loss: 0.3330\n",
      "\n",
      "Batch 563/992 ━━━━━━━━━━━━━━━━━━━━ 03:12:46\n",
      "Accuracy: 0.8819 - Loss: 0.3328\n",
      "\n",
      "Batch 564/992 ━━━━━━━━━━━━━━━━━━━━ 03:12:56\n",
      "Accuracy: 0.8821 - Loss: 0.3324\n",
      "\n",
      "Batch 565/992 ━━━━━━━━━━━━━━━━━━━━ 03:13:06\n",
      "Accuracy: 0.8816 - Loss: 0.3332\n",
      "\n",
      "Batch 566/992 ━━━━━━━━━━━━━━━━━━━━ 03:13:16\n",
      "Accuracy: 0.8818 - Loss: 0.3328\n",
      "\n",
      "Batch 567/992 ━━━━━━━━━━━━━━━━━━━━ 03:13:26\n",
      "Accuracy: 0.8818 - Loss: 0.3331\n",
      "\n",
      "Batch 568/992 ━━━━━━━━━━━━━━━━━━━━ 03:13:36\n",
      "Accuracy: 0.8820 - Loss: 0.3327\n",
      "\n",
      "Batch 569/992 ━━━━━━━━━━━━━━━━━━━━ 03:13:46\n",
      "Accuracy: 0.8820 - Loss: 0.3326\n",
      "\n",
      "Batch 570/992 ━━━━━━━━━━━━━━━━━━━━ 03:13:57\n",
      "Accuracy: 0.8820 - Loss: 0.3325\n",
      "\n",
      "Batch 571/992 ━━━━━━━━━━━━━━━━━━━━ 03:14:07\n",
      "Accuracy: 0.8822 - Loss: 0.3320\n",
      "\n",
      "Batch 572/992 ━━━━━━━━━━━━━━━━━━━━ 03:14:17\n",
      "Accuracy: 0.8818 - Loss: 0.3321\n",
      "\n",
      "Batch 573/992 ━━━━━━━━━━━━━━━━━━━━ 03:14:27\n",
      "Accuracy: 0.8818 - Loss: 0.3320\n",
      "\n",
      "Batch 574/992 ━━━━━━━━━━━━━━━━━━━━ 03:14:37\n",
      "Accuracy: 0.8815 - Loss: 0.3321\n",
      "\n",
      "Batch 575/992 ━━━━━━━━━━━━━━━━━━━━ 03:14:47\n",
      "Accuracy: 0.8815 - Loss: 0.3323\n",
      "\n",
      "Batch 576/992 ━━━━━━━━━━━━━━━━━━━━ 03:14:57\n",
      "Accuracy: 0.8817 - Loss: 0.3320\n",
      "\n",
      "Batch 577/992 ━━━━━━━━━━━━━━━━━━━━ 03:15:07\n",
      "Accuracy: 0.8817 - Loss: 0.3319\n",
      "\n",
      "Batch 578/992 ━━━━━━━━━━━━━━━━━━━━ 03:15:17\n",
      "Accuracy: 0.8815 - Loss: 0.3324\n",
      "\n",
      "Batch 579/992 ━━━━━━━━━━━━━━━━━━━━ 03:15:27\n",
      "Accuracy: 0.8813 - Loss: 0.3326\n",
      "\n",
      "Batch 580/992 ━━━━━━━━━━━━━━━━━━━━ 03:15:37\n",
      "Accuracy: 0.8808 - Loss: 0.3333\n",
      "\n",
      "Batch 581/992 ━━━━━━━━━━━━━━━━━━━━ 03:15:47\n",
      "Accuracy: 0.8808 - Loss: 0.3330\n",
      "\n",
      "Batch 582/992 ━━━━━━━━━━━━━━━━━━━━ 03:15:57\n",
      "Accuracy: 0.8810 - Loss: 0.3324\n",
      "\n",
      "Batch 583/992 ━━━━━━━━━━━━━━━━━━━━ 03:16:07\n",
      "Accuracy: 0.8812 - Loss: 0.3321\n",
      "\n",
      "Batch 584/992 ━━━━━━━━━━━━━━━━━━━━ 03:16:17\n",
      "Accuracy: 0.8814 - Loss: 0.3318\n",
      "\n",
      "Batch 585/992 ━━━━━━━━━━━━━━━━━━━━ 03:16:27\n",
      "Accuracy: 0.8814 - Loss: 0.3318\n",
      "\n",
      "Batch 586/992 ━━━━━━━━━━━━━━━━━━━━ 03:16:38\n",
      "Accuracy: 0.8812 - Loss: 0.3322\n",
      "\n",
      "Batch 587/992 ━━━━━━━━━━━━━━━━━━━━ 03:16:48\n",
      "Accuracy: 0.8814 - Loss: 0.3319\n",
      "\n",
      "Batch 588/992 ━━━━━━━━━━━━━━━━━━━━ 03:16:58\n",
      "Accuracy: 0.8816 - Loss: 0.3315\n",
      "\n",
      "Batch 589/992 ━━━━━━━━━━━━━━━━━━━━ 03:17:08\n",
      "Accuracy: 0.8818 - Loss: 0.3314\n",
      "\n",
      "Batch 590/992 ━━━━━━━━━━━━━━━━━━━━ 03:17:18\n",
      "Accuracy: 0.8816 - Loss: 0.3317\n",
      "\n",
      "Batch 591/992 ━━━━━━━━━━━━━━━━━━━━ 03:17:28\n",
      "Accuracy: 0.8816 - Loss: 0.3321\n",
      "\n",
      "Batch 592/992 ━━━━━━━━━━━━━━━━━━━━ 03:17:38\n",
      "Accuracy: 0.8818 - Loss: 0.3316\n",
      "\n",
      "Batch 593/992 ━━━━━━━━━━━━━━━━━━━━ 03:17:48\n",
      "Accuracy: 0.8820 - Loss: 0.3312\n",
      "\n",
      "Batch 594/992 ━━━━━━━━━━━━━━━━━━━━ 03:17:58\n",
      "Accuracy: 0.8815 - Loss: 0.3320\n",
      "\n",
      "Batch 595/992 ━━━━━━━━━━━━━━━━━━━━ 03:18:08\n",
      "Accuracy: 0.8815 - Loss: 0.3321\n",
      "\n",
      "Batch 596/992 ━━━━━━━━━━━━━━━━━━━━ 03:18:18\n",
      "Accuracy: 0.8815 - Loss: 0.3322\n",
      "\n",
      "Batch 597/992 ━━━━━━━━━━━━━━━━━━━━ 03:18:28\n",
      "Accuracy: 0.8815 - Loss: 0.3322\n",
      "\n",
      "Batch 598/992 ━━━━━━━━━━━━━━━━━━━━ 03:18:38\n",
      "Accuracy: 0.8817 - Loss: 0.3318\n",
      "\n",
      "Batch 599/992 ━━━━━━━━━━━━━━━━━━━━ 03:18:48\n",
      "Accuracy: 0.8817 - Loss: 0.3319\n",
      "\n",
      "Batch 600/992 ━━━━━━━━━━━━━━━━━━━━ 03:18:58\n",
      "Accuracy: 0.8817 - Loss: 0.3316\n",
      "\n",
      "Batch 601/992 ━━━━━━━━━━━━━━━━━━━━ 03:19:08\n",
      "Accuracy: 0.8819 - Loss: 0.3310\n",
      "\n",
      "Batch 602/992 ━━━━━━━━━━━━━━━━━━━━ 03:19:18\n",
      "Accuracy: 0.8821 - Loss: 0.3306\n",
      "\n",
      "Batch 603/992 ━━━━━━━━━━━━━━━━━━━━ 03:19:28\n",
      "Accuracy: 0.8820 - Loss: 0.3307\n",
      "\n",
      "Batch 604/992 ━━━━━━━━━━━━━━━━━━━━ 03:19:38\n",
      "Accuracy: 0.8814 - Loss: 0.3323\n",
      "\n",
      "Batch 605/992 ━━━━━━━━━━━━━━━━━━━━ 03:19:48\n",
      "Accuracy: 0.8816 - Loss: 0.3318\n",
      "\n",
      "Batch 606/992 ━━━━━━━━━━━━━━━━━━━━ 03:19:58\n",
      "Accuracy: 0.8818 - Loss: 0.3316\n",
      "\n",
      "Batch 607/992 ━━━━━━━━━━━━━━━━━━━━ 03:20:08\n",
      "Accuracy: 0.8818 - Loss: 0.3313\n",
      "\n",
      "Batch 608/992 ━━━━━━━━━━━━━━━━━━━━ 03:20:18\n",
      "Accuracy: 0.8820 - Loss: 0.3310\n",
      "\n",
      "Batch 609/992 ━━━━━━━━━━━━━━━━━━━━ 03:20:28\n",
      "Accuracy: 0.8818 - Loss: 0.3312\n",
      "\n",
      "Batch 610/992 ━━━━━━━━━━━━━━━━━━━━ 03:20:38\n",
      "Accuracy: 0.8818 - Loss: 0.3314\n",
      "\n",
      "Batch 611/992 ━━━━━━━━━━━━━━━━━━━━ 03:20:48\n",
      "Accuracy: 0.8815 - Loss: 0.3316\n",
      "\n",
      "Batch 612/992 ━━━━━━━━━━━━━━━━━━━━ 03:20:58\n",
      "Accuracy: 0.8813 - Loss: 0.3315\n",
      "\n",
      "Batch 613/992 ━━━━━━━━━━━━━━━━━━━━ 03:21:09\n",
      "Accuracy: 0.8815 - Loss: 0.3313\n",
      "\n",
      "Batch 614/992 ━━━━━━━━━━━━━━━━━━━━ 03:21:19\n",
      "Accuracy: 0.8815 - Loss: 0.3314\n",
      "\n",
      "Batch 615/992 ━━━━━━━━━━━━━━━━━━━━ 03:21:29\n",
      "Accuracy: 0.8815 - Loss: 0.3316\n",
      "\n",
      "Batch 616/992 ━━━━━━━━━━━━━━━━━━━━ 03:21:39\n",
      "Accuracy: 0.8815 - Loss: 0.3318\n",
      "\n",
      "Batch 617/992 ━━━━━━━━━━━━━━━━━━━━ 03:21:50\n",
      "Accuracy: 0.8813 - Loss: 0.3328\n",
      "\n",
      "Batch 618/992 ━━━━━━━━━━━━━━━━━━━━ 03:22:00\n",
      "Accuracy: 0.8815 - Loss: 0.3325\n",
      "\n",
      "Batch 619/992 ━━━━━━━━━━━━━━━━━━━━ 03:22:10\n",
      "Accuracy: 0.8813 - Loss: 0.3334\n",
      "\n",
      "Batch 620/992 ━━━━━━━━━━━━━━━━━━━━ 03:22:20\n",
      "Accuracy: 0.8815 - Loss: 0.3330\n",
      "\n",
      "Batch 621/992 ━━━━━━━━━━━━━━━━━━━━ 03:22:30\n",
      "Accuracy: 0.8816 - Loss: 0.3327\n",
      "\n",
      "Batch 622/992 ━━━━━━━━━━━━━━━━━━━━ 03:22:40\n",
      "Accuracy: 0.8818 - Loss: 0.3325\n",
      "\n",
      "Batch 623/992 ━━━━━━━━━━━━━━━━━━━━ 03:22:50\n",
      "Accuracy: 0.8818 - Loss: 0.3329\n",
      "\n",
      "Batch 624/992 ━━━━━━━━━━━━━━━━━━━━ 03:23:00\n",
      "Accuracy: 0.8816 - Loss: 0.3330\n",
      "\n",
      "Batch 625/992 ━━━━━━━━━━━━━━━━━━━━ 03:23:10\n",
      "Accuracy: 0.8812 - Loss: 0.3338\n",
      "\n",
      "Batch 626/992 ━━━━━━━━━━━━━━━━━━━━ 03:23:21\n",
      "Accuracy: 0.8812 - Loss: 0.3342\n",
      "\n",
      "Batch 627/992 ━━━━━━━━━━━━━━━━━━━━ 03:23:31\n",
      "Accuracy: 0.8812 - Loss: 0.3342\n",
      "\n",
      "Batch 628/992 ━━━━━━━━━━━━━━━━━━━━ 03:23:41\n",
      "Accuracy: 0.8810 - Loss: 0.3343\n",
      "\n",
      "Batch 629/992 ━━━━━━━━━━━━━━━━━━━━ 03:23:51\n",
      "Accuracy: 0.8812 - Loss: 0.3338\n",
      "\n",
      "Batch 630/992 ━━━━━━━━━━━━━━━━━━━━ 03:24:01\n",
      "Accuracy: 0.8812 - Loss: 0.3342\n",
      "\n",
      "Batch 631/992 ━━━━━━━━━━━━━━━━━━━━ 03:24:11\n",
      "Accuracy: 0.8809 - Loss: 0.3349\n",
      "\n",
      "Batch 632/992 ━━━━━━━━━━━━━━━━━━━━ 03:24:21\n",
      "Accuracy: 0.8805 - Loss: 0.3353\n",
      "\n",
      "Batch 633/992 ━━━━━━━━━━━━━━━━━━━━ 03:24:31\n",
      "Accuracy: 0.8805 - Loss: 0.3361\n",
      "\n",
      "Batch 634/992 ━━━━━━━━━━━━━━━━━━━━ 03:24:41\n",
      "Accuracy: 0.8807 - Loss: 0.3358\n",
      "\n",
      "Batch 635/992 ━━━━━━━━━━━━━━━━━━━━ 03:24:51\n",
      "Accuracy: 0.8805 - Loss: 0.3361\n",
      "\n",
      "Batch 636/992 ━━━━━━━━━━━━━━━━━━━━ 03:25:02\n",
      "Accuracy: 0.8805 - Loss: 0.3364\n",
      "\n",
      "Batch 637/992 ━━━━━━━━━━━━━━━━━━━━ 03:25:12\n",
      "Accuracy: 0.8805 - Loss: 0.3365\n",
      "\n",
      "Batch 638/992 ━━━━━━━━━━━━━━━━━━━━ 03:25:22\n",
      "Accuracy: 0.8805 - Loss: 0.3364\n",
      "\n",
      "Batch 639/992 ━━━━━━━━━━━━━━━━━━━━ 03:25:32\n",
      "Accuracy: 0.8803 - Loss: 0.3366\n",
      "\n",
      "Batch 640/992 ━━━━━━━━━━━━━━━━━━━━ 03:25:43\n",
      "Accuracy: 0.8803 - Loss: 0.3370\n",
      "\n",
      "Batch 641/992 ━━━━━━━━━━━━━━━━━━━━ 03:25:53\n",
      "Accuracy: 0.8801 - Loss: 0.3375\n",
      "\n",
      "Batch 642/992 ━━━━━━━━━━━━━━━━━━━━ 03:26:03\n",
      "Accuracy: 0.8801 - Loss: 0.3374\n",
      "\n",
      "Batch 643/992 ━━━━━━━━━━━━━━━━━━━━ 03:26:13\n",
      "Accuracy: 0.8801 - Loss: 0.3375\n",
      "\n",
      "Batch 644/992 ━━━━━━━━━━━━━━━━━━━━ 03:26:23\n",
      "Accuracy: 0.8800 - Loss: 0.3380\n",
      "\n",
      "Batch 645/992 ━━━━━━━━━━━━━━━━━━━━ 03:26:33\n",
      "Accuracy: 0.8800 - Loss: 0.3378\n",
      "\n",
      "Batch 646/992 ━━━━━━━━━━━━━━━━━━━━ 03:26:43\n",
      "Accuracy: 0.8800 - Loss: 0.3376\n",
      "\n",
      "Batch 647/992 ━━━━━━━━━━━━━━━━━━━━ 03:26:53\n",
      "Accuracy: 0.8802 - Loss: 0.3372\n",
      "\n",
      "Batch 648/992 ━━━━━━━━━━━━━━━━━━━━ 03:27:03\n",
      "Accuracy: 0.8804 - Loss: 0.3370\n",
      "\n",
      "Batch 649/992 ━━━━━━━━━━━━━━━━━━━━ 03:27:13\n",
      "Accuracy: 0.8800 - Loss: 0.3377\n",
      "\n",
      "Batch 650/992 ━━━━━━━━━━━━━━━━━━━━ 03:27:24\n",
      "Accuracy: 0.8800 - Loss: 0.3375\n",
      "\n",
      "Batch 651/992 ━━━━━━━━━━━━━━━━━━━━ 03:27:34\n",
      "Accuracy: 0.8798 - Loss: 0.3382\n",
      "\n",
      "Batch 652/992 ━━━━━━━━━━━━━━━━━━━━ 03:27:44\n",
      "Accuracy: 0.8800 - Loss: 0.3377\n",
      "\n",
      "Batch 653/992 ━━━━━━━━━━━━━━━━━━━━ 03:27:54\n",
      "Accuracy: 0.8800 - Loss: 0.3376\n",
      "\n",
      "Batch 654/992 ━━━━━━━━━━━━━━━━━━━━ 03:28:04\n",
      "Accuracy: 0.8802 - Loss: 0.3372\n",
      "\n",
      "Batch 655/992 ━━━━━━━━━━━━━━━━━━━━ 03:28:15\n",
      "Accuracy: 0.8803 - Loss: 0.3368\n",
      "\n",
      "Batch 656/992 ━━━━━━━━━━━━━━━━━━━━ 03:28:25\n",
      "Accuracy: 0.8805 - Loss: 0.3366\n",
      "\n",
      "Batch 657/992 ━━━━━━━━━━━━━━━━━━━━ 03:28:35\n",
      "Accuracy: 0.8807 - Loss: 0.3361\n",
      "\n",
      "Batch 658/992 ━━━━━━━━━━━━━━━━━━━━ 03:28:45\n",
      "Accuracy: 0.8809 - Loss: 0.3360\n",
      "\n",
      "Batch 659/992 ━━━━━━━━━━━━━━━━━━━━ 03:28:55\n",
      "Accuracy: 0.8811 - Loss: 0.3355\n",
      "\n",
      "Batch 660/992 ━━━━━━━━━━━━━━━━━━━━ 03:29:05\n",
      "Accuracy: 0.8813 - Loss: 0.3351\n",
      "\n",
      "Batch 661/992 ━━━━━━━━━━━━━━━━━━━━ 03:29:15\n",
      "Accuracy: 0.8809 - Loss: 0.3356\n",
      "\n",
      "Batch 662/992 ━━━━━━━━━━━━━━━━━━━━ 03:29:26\n",
      "Accuracy: 0.8810 - Loss: 0.3353\n",
      "\n",
      "Batch 663/992 ━━━━━━━━━━━━━━━━━━━━ 03:29:36\n",
      "Accuracy: 0.8812 - Loss: 0.3350\n",
      "\n",
      "Batch 664/992 ━━━━━━━━━━━━━━━━━━━━ 03:29:46\n",
      "Accuracy: 0.8812 - Loss: 0.3351\n",
      "\n",
      "Batch 665/992 ━━━━━━━━━━━━━━━━━━━━ 03:29:56\n",
      "Accuracy: 0.8812 - Loss: 0.3355\n",
      "\n",
      "Batch 666/992 ━━━━━━━━━━━━━━━━━━━━ 03:30:06\n",
      "Accuracy: 0.8812 - Loss: 0.3352\n",
      "\n",
      "Batch 667/992 ━━━━━━━━━━━━━━━━━━━━ 03:30:16\n",
      "Accuracy: 0.8812 - Loss: 0.3353\n",
      "\n",
      "Batch 668/992 ━━━━━━━━━━━━━━━━━━━━ 03:30:26\n",
      "Accuracy: 0.8812 - Loss: 0.3353\n",
      "\n",
      "Batch 669/992 ━━━━━━━━━━━━━━━━━━━━ 03:30:37\n",
      "Accuracy: 0.8814 - Loss: 0.3349\n",
      "\n",
      "Batch 670/992 ━━━━━━━━━━━━━━━━━━━━ 03:30:47\n",
      "Accuracy: 0.8813 - Loss: 0.3347\n",
      "\n",
      "Batch 671/992 ━━━━━━━━━━━━━━━━━━━━ 03:30:57\n",
      "Accuracy: 0.8811 - Loss: 0.3353\n",
      "\n",
      "Batch 672/992 ━━━━━━━━━━━━━━━━━━━━ 03:31:07\n",
      "Accuracy: 0.8810 - Loss: 0.3369\n",
      "\n",
      "Batch 673/992 ━━━━━━━━━━━━━━━━━━━━ 03:31:17\n",
      "Accuracy: 0.8811 - Loss: 0.3366\n",
      "\n",
      "Batch 674/992 ━━━━━━━━━━━━━━━━━━━━ 03:31:27\n",
      "Accuracy: 0.8813 - Loss: 0.3363\n",
      "\n",
      "Batch 675/992 ━━━━━━━━━━━━━━━━━━━━ 03:31:38\n",
      "Accuracy: 0.8813 - Loss: 0.3363\n",
      "\n",
      "Batch 676/992 ━━━━━━━━━━━━━━━━━━━━ 03:31:48\n",
      "Accuracy: 0.8813 - Loss: 0.3363\n",
      "\n",
      "Batch 677/992 ━━━━━━━━━━━━━━━━━━━━ 03:31:58\n",
      "Accuracy: 0.8813 - Loss: 0.3365\n",
      "\n",
      "Batch 678/992 ━━━━━━━━━━━━━━━━━━━━ 03:32:08\n",
      "Accuracy: 0.8813 - Loss: 0.3364\n",
      "\n",
      "Batch 679/992 ━━━━━━━━━━━━━━━━━━━━ 03:32:18\n",
      "Accuracy: 0.8811 - Loss: 0.3370\n",
      "\n",
      "Batch 680/992 ━━━━━━━━━━━━━━━━━━━━ 03:32:28\n",
      "Accuracy: 0.8811 - Loss: 0.3370\n",
      "\n",
      "Batch 681/992 ━━━━━━━━━━━━━━━━━━━━ 03:32:38\n",
      "Accuracy: 0.8812 - Loss: 0.3366\n",
      "\n",
      "Batch 682/992 ━━━━━━━━━━━━━━━━━━━━ 03:32:48\n",
      "Accuracy: 0.8812 - Loss: 0.3366\n",
      "\n",
      "Batch 683/992 ━━━━━━━━━━━━━━━━━━━━ 03:32:58\n",
      "Accuracy: 0.8812 - Loss: 0.3364\n",
      "\n",
      "Batch 684/992 ━━━━━━━━━━━━━━━━━━━━ 03:33:08\n",
      "Accuracy: 0.8814 - Loss: 0.3361\n",
      "\n",
      "Batch 685/992 ━━━━━━━━━━━━━━━━━━━━ 03:33:18\n",
      "Accuracy: 0.8816 - Loss: 0.3359\n",
      "\n",
      "Batch 686/992 ━━━━━━━━━━━━━━━━━━━━ 03:33:29\n",
      "Accuracy: 0.8817 - Loss: 0.3356\n",
      "\n",
      "Batch 687/992 ━━━━━━━━━━━━━━━━━━━━ 03:33:39\n",
      "Accuracy: 0.8817 - Loss: 0.3356\n",
      "\n",
      "Batch 688/992 ━━━━━━━━━━━━━━━━━━━━ 03:33:49\n",
      "Accuracy: 0.8817 - Loss: 0.3356\n",
      "\n",
      "Batch 689/992 ━━━━━━━━━━━━━━━━━━━━ 03:33:59\n",
      "Accuracy: 0.8817 - Loss: 0.3358\n",
      "\n",
      "Batch 690/992 ━━━━━━━━━━━━━━━━━━━━ 03:34:10\n",
      "Accuracy: 0.8815 - Loss: 0.3359\n",
      "\n",
      "Batch 691/992 ━━━━━━━━━━━━━━━━━━━━ 03:34:20\n",
      "Accuracy: 0.8815 - Loss: 0.3359\n",
      "\n",
      "Batch 692/992 ━━━━━━━━━━━━━━━━━━━━ 03:34:30\n",
      "Accuracy: 0.8815 - Loss: 0.3360\n",
      "\n",
      "Batch 693/992 ━━━━━━━━━━━━━━━━━━━━ 03:34:40\n",
      "Accuracy: 0.8817 - Loss: 0.3359\n",
      "\n",
      "Batch 694/992 ━━━━━━━━━━━━━━━━━━━━ 03:34:50\n",
      "Accuracy: 0.8818 - Loss: 0.3356\n",
      "\n",
      "Batch 695/992 ━━━━━━━━━━━━━━━━━━━━ 03:35:00\n",
      "Accuracy: 0.8820 - Loss: 0.3353\n",
      "\n",
      "Batch 696/992 ━━━━━━━━━━━━━━━━━━━━ 03:35:10\n",
      "Accuracy: 0.8818 - Loss: 0.3358\n",
      "\n",
      "Batch 697/992 ━━━━━━━━━━━━━━━━━━━━ 03:35:20\n",
      "Accuracy: 0.8820 - Loss: 0.3354\n",
      "\n",
      "Batch 698/992 ━━━━━━━━━━━━━━━━━━━━ 03:35:30\n",
      "Accuracy: 0.8818 - Loss: 0.3358\n",
      "\n",
      "Batch 699/992 ━━━━━━━━━━━━━━━━━━━━ 03:35:41\n",
      "Accuracy: 0.8818 - Loss: 0.3359\n",
      "\n",
      "Batch 700/992 ━━━━━━━━━━━━━━━━━━━━ 03:35:51\n",
      "Accuracy: 0.8813 - Loss: 0.3366\n",
      "\n",
      "Batch 701/992 ━━━━━━━━━━━━━━━━━━━━ 03:36:01\n",
      "Accuracy: 0.8811 - Loss: 0.3366\n",
      "\n",
      "Batch 702/992 ━━━━━━━━━━━━━━━━━━━━ 03:36:11\n",
      "Accuracy: 0.8811 - Loss: 0.3365\n",
      "\n",
      "Batch 703/992 ━━━━━━━━━━━━━━━━━━━━ 03:36:21\n",
      "Accuracy: 0.8810 - Loss: 0.3363\n",
      "\n",
      "Batch 704/992 ━━━━━━━━━━━━━━━━━━━━ 03:36:31\n",
      "Accuracy: 0.8812 - Loss: 0.3360\n",
      "\n",
      "Batch 705/992 ━━━━━━━━━━━━━━━━━━━━ 03:36:41\n",
      "Accuracy: 0.8814 - Loss: 0.3357\n",
      "\n",
      "Batch 706/992 ━━━━━━━━━━━━━━━━━━━━ 03:36:52\n",
      "Accuracy: 0.8814 - Loss: 0.3356\n",
      "\n",
      "Batch 707/992 ━━━━━━━━━━━━━━━━━━━━ 03:37:02\n",
      "Accuracy: 0.8815 - Loss: 0.3354\n",
      "\n",
      "Batch 708/992 ━━━━━━━━━━━━━━━━━━━━ 03:37:12\n",
      "Accuracy: 0.8812 - Loss: 0.3361\n",
      "\n",
      "Batch 709/992 ━━━━━━━━━━━━━━━━━━━━ 03:37:22\n",
      "Accuracy: 0.8812 - Loss: 0.3359\n",
      "\n",
      "Batch 710/992 ━━━━━━━━━━━━━━━━━━━━ 03:37:32\n",
      "Accuracy: 0.8813 - Loss: 0.3358\n",
      "\n",
      "Batch 711/992 ━━━━━━━━━━━━━━━━━━━━ 03:37:42\n",
      "Accuracy: 0.8815 - Loss: 0.3355\n",
      "\n",
      "Batch 712/992 ━━━━━━━━━━━━━━━━━━━━ 03:37:53\n",
      "Accuracy: 0.8817 - Loss: 0.3355\n",
      "\n",
      "Batch 713/992 ━━━━━━━━━━━━━━━━━━━━ 03:38:03\n",
      "Accuracy: 0.8818 - Loss: 0.3352\n",
      "\n",
      "Batch 714/992 ━━━━━━━━━━━━━━━━━━━━ 03:38:13\n",
      "Accuracy: 0.8817 - Loss: 0.3356\n",
      "\n",
      "Batch 715/992 ━━━━━━━━━━━━━━━━━━━━ 03:38:23\n",
      "Accuracy: 0.8816 - Loss: 0.3357\n",
      "\n",
      "Batch 716/992 ━━━━━━━━━━━━━━━━━━━━ 03:38:33\n",
      "Accuracy: 0.8816 - Loss: 0.3362\n",
      "\n",
      "Batch 717/992 ━━━━━━━━━━━━━━━━━━━━ 03:38:43\n",
      "Accuracy: 0.8818 - Loss: 0.3357\n",
      "\n",
      "Batch 718/992 ━━━━━━━━━━━━━━━━━━━━ 03:38:53\n",
      "Accuracy: 0.8816 - Loss: 0.3358\n",
      "\n",
      "Batch 719/992 ━━━━━━━━━━━━━━━━━━━━ 03:39:04\n",
      "Accuracy: 0.8818 - Loss: 0.3355\n",
      "\n",
      "Batch 720/992 ━━━━━━━━━━━━━━━━━━━━ 03:39:14\n",
      "Accuracy: 0.8818 - Loss: 0.3356\n",
      "\n",
      "Batch 721/992 ━━━━━━━━━━━━━━━━━━━━ 03:39:24\n",
      "Accuracy: 0.8816 - Loss: 0.3361\n",
      "\n",
      "Batch 722/992 ━━━━━━━━━━━━━━━━━━━━ 03:39:34\n",
      "Accuracy: 0.8818 - Loss: 0.3357\n",
      "\n",
      "Batch 723/992 ━━━━━━━━━━━━━━━━━━━━ 03:39:44\n",
      "Accuracy: 0.8819 - Loss: 0.3353\n",
      "\n",
      "Batch 724/992 ━━━━━━━━━━━━━━━━━━━━ 03:39:55\n",
      "Accuracy: 0.8821 - Loss: 0.3351\n",
      "\n",
      "Batch 725/992 ━━━━━━━━━━━━━━━━━━━━ 03:40:05\n",
      "Accuracy: 0.8821 - Loss: 0.3349\n",
      "\n",
      "Batch 726/992 ━━━━━━━━━━━━━━━━━━━━ 03:40:15\n",
      "Accuracy: 0.8821 - Loss: 0.3350\n",
      "\n",
      "Batch 727/992 ━━━━━━━━━━━━━━━━━━━━ 03:40:25\n",
      "Accuracy: 0.8820 - Loss: 0.3353\n",
      "\n",
      "Batch 728/992 ━━━━━━━━━━━━━━━━━━━━ 03:40:35\n",
      "Accuracy: 0.8820 - Loss: 0.3353\n",
      "\n",
      "Batch 729/992 ━━━━━━━━━━━━━━━━━━━━ 03:40:45\n",
      "Accuracy: 0.8822 - Loss: 0.3350\n",
      "\n",
      "Batch 730/992 ━━━━━━━━━━━━━━━━━━━━ 03:40:55\n",
      "Accuracy: 0.8824 - Loss: 0.3346\n",
      "\n",
      "Batch 731/992 ━━━━━━━━━━━━━━━━━━━━ 03:41:05\n",
      "Accuracy: 0.8825 - Loss: 0.3344\n",
      "\n",
      "Batch 732/992 ━━━━━━━━━━━━━━━━━━━━ 03:41:16\n",
      "Accuracy: 0.8825 - Loss: 0.3343\n",
      "\n",
      "Batch 733/992 ━━━━━━━━━━━━━━━━━━━━ 03:41:26\n",
      "Accuracy: 0.8823 - Loss: 0.3344\n",
      "\n",
      "Batch 734/992 ━━━━━━━━━━━━━━━━━━━━ 03:41:36\n",
      "Accuracy: 0.8823 - Loss: 0.3344\n",
      "\n",
      "Batch 735/992 ━━━━━━━━━━━━━━━━━━━━ 03:41:47\n",
      "Accuracy: 0.8825 - Loss: 0.3341\n",
      "\n",
      "Batch 736/992 ━━━━━━━━━━━━━━━━━━━━ 03:41:58\n",
      "Accuracy: 0.8826 - Loss: 0.3340\n",
      "\n",
      "Batch 737/992 ━━━━━━━━━━━━━━━━━━━━ 03:42:08\n",
      "Accuracy: 0.8828 - Loss: 0.3337\n",
      "\n",
      "Batch 738/992 ━━━━━━━━━━━━━━━━━━━━ 03:42:18\n",
      "Accuracy: 0.8828 - Loss: 0.3336\n",
      "\n",
      "Batch 739/992 ━━━━━━━━━━━━━━━━━━━━ 03:42:28\n",
      "Accuracy: 0.8829 - Loss: 0.3333\n",
      "\n",
      "Batch 740/992 ━━━━━━━━━━━━━━━━━━━━ 03:42:38\n",
      "Accuracy: 0.8828 - Loss: 0.3334\n",
      "\n",
      "Batch 741/992 ━━━━━━━━━━━━━━━━━━━━ 03:42:48\n",
      "Accuracy: 0.8828 - Loss: 0.3339\n",
      "\n",
      "Batch 742/992 ━━━━━━━━━━━━━━━━━━━━ 03:42:58\n",
      "Accuracy: 0.8829 - Loss: 0.3338\n",
      "\n",
      "Batch 743/992 ━━━━━━━━━━━━━━━━━━━━ 03:43:08\n",
      "Accuracy: 0.8831 - Loss: 0.3339\n",
      "\n",
      "Batch 744/992 ━━━━━━━━━━━━━━━━━━━━ 03:43:18\n",
      "Accuracy: 0.8831 - Loss: 0.3337\n",
      "\n",
      "Batch 745/992 ━━━━━━━━━━━━━━━━━━━━ 03:43:29\n",
      "Accuracy: 0.8832 - Loss: 0.3334\n",
      "\n",
      "Batch 746/992 ━━━━━━━━━━━━━━━━━━━━ 03:43:39\n",
      "Accuracy: 0.8832 - Loss: 0.3332\n",
      "\n",
      "Batch 747/992 ━━━━━━━━━━━━━━━━━━━━ 03:43:49\n",
      "Accuracy: 0.8832 - Loss: 0.3332\n",
      "\n",
      "Batch 748/992 ━━━━━━━━━━━━━━━━━━━━ 03:43:59\n",
      "Accuracy: 0.8834 - Loss: 0.3331\n",
      "\n",
      "Batch 749/992 ━━━━━━━━━━━━━━━━━━━━ 03:44:10\n",
      "Accuracy: 0.8833 - Loss: 0.3329\n",
      "\n",
      "Batch 750/992 ━━━━━━━━━━━━━━━━━━━━ 03:44:20\n",
      "Accuracy: 0.8835 - Loss: 0.3326\n",
      "\n",
      "Batch 751/992 ━━━━━━━━━━━━━━━━━━━━ 03:44:30\n",
      "Accuracy: 0.8832 - Loss: 0.3331\n",
      "\n",
      "Batch 752/992 ━━━━━━━━━━━━━━━━━━━━ 03:44:40\n",
      "Accuracy: 0.8831 - Loss: 0.3330\n",
      "\n",
      "Batch 753/992 ━━━━━━━━━━━━━━━━━━━━ 03:44:50\n",
      "Accuracy: 0.8833 - Loss: 0.3327\n",
      "\n",
      "Batch 754/992 ━━━━━━━━━━━━━━━━━━━━ 03:45:00\n",
      "Accuracy: 0.8833 - Loss: 0.3327\n",
      "\n",
      "Batch 755/992 ━━━━━━━━━━━━━━━━━━━━ 03:45:10\n",
      "Accuracy: 0.8828 - Loss: 0.3339\n",
      "\n",
      "Batch 756/992 ━━━━━━━━━━━━━━━━━━━━ 03:45:20\n",
      "Accuracy: 0.8824 - Loss: 0.3342\n",
      "\n",
      "Batch 757/992 ━━━━━━━━━━━━━━━━━━━━ 03:45:30\n",
      "Accuracy: 0.8823 - Loss: 0.3343\n",
      "\n",
      "Batch 758/992 ━━━━━━━━━━━━━━━━━━━━ 03:45:40\n",
      "Accuracy: 0.8821 - Loss: 0.3346\n",
      "\n",
      "Batch 759/992 ━━━━━━━━━━━━━━━━━━━━ 03:45:51\n",
      "Accuracy: 0.8821 - Loss: 0.3346\n",
      "\n",
      "Batch 760/992 ━━━━━━━━━━━━━━━━━━━━ 03:46:01\n",
      "Accuracy: 0.8819 - Loss: 0.3346\n",
      "\n",
      "Batch 761/992 ━━━━━━━━━━━━━━━━━━━━ 03:46:11\n",
      "Accuracy: 0.8821 - Loss: 0.3343\n",
      "\n",
      "Batch 762/992 ━━━━━━━━━━━━━━━━━━━━ 03:46:21\n",
      "Accuracy: 0.8822 - Loss: 0.3340\n",
      "\n",
      "Batch 763/992 ━━━━━━━━━━━━━━━━━━━━ 03:46:32\n",
      "Accuracy: 0.8822 - Loss: 0.3339\n",
      "\n",
      "Batch 764/992 ━━━━━━━━━━━━━━━━━━━━ 03:46:42\n",
      "Accuracy: 0.8824 - Loss: 0.3336\n",
      "\n",
      "Batch 765/992 ━━━━━━━━━━━━━━━━━━━━ 03:46:52\n",
      "Accuracy: 0.8822 - Loss: 0.3342\n",
      "\n",
      "Batch 766/992 ━━━━━━━━━━━━━━━━━━━━ 03:47:02\n",
      "Accuracy: 0.8823 - Loss: 0.3340\n",
      "\n",
      "Batch 767/992 ━━━━━━━━━━━━━━━━━━━━ 03:47:12\n",
      "Accuracy: 0.8823 - Loss: 0.3339\n",
      "\n",
      "Batch 768/992 ━━━━━━━━━━━━━━━━━━━━ 03:47:22\n",
      "Accuracy: 0.8822 - Loss: 0.3346\n",
      "\n",
      "Batch 769/992 ━━━━━━━━━━━━━━━━━━━━ 03:47:32\n",
      "Accuracy: 0.8822 - Loss: 0.3345\n",
      "\n",
      "Batch 770/992 ━━━━━━━━━━━━━━━━━━━━ 03:47:42\n",
      "Accuracy: 0.8821 - Loss: 0.3346\n",
      "\n",
      "Batch 771/992 ━━━━━━━━━━━━━━━━━━━━ 03:47:52\n",
      "Accuracy: 0.8821 - Loss: 0.3345\n",
      "\n",
      "Batch 772/992 ━━━━━━━━━━━━━━━━━━━━ 03:48:03\n",
      "Accuracy: 0.8823 - Loss: 0.3341\n",
      "\n",
      "Batch 773/992 ━━━━━━━━━━━━━━━━━━━━ 03:48:13\n",
      "Accuracy: 0.8824 - Loss: 0.3338\n",
      "\n",
      "Batch 774/992 ━━━━━━━━━━━━━━━━━━━━ 03:48:23\n",
      "Accuracy: 0.8824 - Loss: 0.3337\n",
      "\n",
      "Batch 775/992 ━━━━━━━━━━━━━━━━━━━━ 03:48:33\n",
      "Accuracy: 0.8823 - Loss: 0.3339\n",
      "\n",
      "Batch 776/992 ━━━━━━━━━━━━━━━━━━━━ 03:48:43\n",
      "Accuracy: 0.8822 - Loss: 0.3340\n",
      "\n",
      "Batch 777/992 ━━━━━━━━━━━━━━━━━━━━ 03:48:53\n",
      "Accuracy: 0.8822 - Loss: 0.3339\n",
      "\n",
      "Batch 778/992 ━━━━━━━━━━━━━━━━━━━━ 03:49:03\n",
      "Accuracy: 0.8819 - Loss: 0.3349\n",
      "\n",
      "Batch 779/992 ━━━━━━━━━━━━━━━━━━━━ 03:49:13\n",
      "Accuracy: 0.8821 - Loss: 0.3346\n",
      "\n",
      "Batch 780/992 ━━━━━━━━━━━━━━━━━━━━ 03:49:23\n",
      "Accuracy: 0.8821 - Loss: 0.3346\n",
      "\n",
      "Batch 781/992 ━━━━━━━━━━━━━━━━━━━━ 03:49:33\n",
      "Accuracy: 0.8820 - Loss: 0.3346\n",
      "\n",
      "Batch 782/992 ━━━━━━━━━━━━━━━━━━━━ 03:49:43\n",
      "Accuracy: 0.8820 - Loss: 0.3346\n",
      "\n",
      "Batch 783/992 ━━━━━━━━━━━━━━━━━━━━ 03:49:54\n",
      "Accuracy: 0.8822 - Loss: 0.3344\n",
      "\n",
      "Batch 784/992 ━━━━━━━━━━━━━━━━━━━━ 03:50:04\n",
      "Accuracy: 0.8820 - Loss: 0.3344\n",
      "\n",
      "Batch 785/992 ━━━━━━━━━━━━━━━━━━━━ 03:50:14\n",
      "Accuracy: 0.8820 - Loss: 0.3343\n",
      "\n",
      "Batch 786/992 ━━━━━━━━━━━━━━━━━━━━ 03:50:24\n",
      "Accuracy: 0.8820 - Loss: 0.3345\n",
      "\n",
      "Batch 787/992 ━━━━━━━━━━━━━━━━━━━━ 03:50:34\n",
      "Accuracy: 0.8818 - Loss: 0.3347\n",
      "\n",
      "Batch 788/992 ━━━━━━━━━━━━━━━━━━━━ 03:50:45\n",
      "Accuracy: 0.8818 - Loss: 0.3347\n",
      "\n",
      "Batch 789/992 ━━━━━━━━━━━━━━━━━━━━ 03:50:55\n",
      "Accuracy: 0.8818 - Loss: 0.3347\n",
      "\n",
      "Batch 790/992 ━━━━━━━━━━━━━━━━━━━━ 03:51:05\n",
      "Accuracy: 0.8818 - Loss: 0.3345\n",
      "\n",
      "Batch 791/992 ━━━━━━━━━━━━━━━━━━━━ 03:51:15\n",
      "Accuracy: 0.8820 - Loss: 0.3344\n",
      "\n",
      "Batch 792/992 ━━━━━━━━━━━━━━━━━━━━ 03:51:25\n",
      "Accuracy: 0.8819 - Loss: 0.3345\n",
      "\n",
      "Batch 793/992 ━━━━━━━━━━━━━━━━━━━━ 03:51:35\n",
      "Accuracy: 0.8818 - Loss: 0.3345\n",
      "\n",
      "Batch 794/992 ━━━━━━━━━━━━━━━━━━━━ 03:51:45\n",
      "Accuracy: 0.8819 - Loss: 0.3342\n",
      "\n",
      "Batch 795/992 ━━━━━━━━━━━━━━━━━━━━ 03:51:55\n",
      "Accuracy: 0.8819 - Loss: 0.3341\n",
      "\n",
      "Batch 796/992 ━━━━━━━━━━━━━━━━━━━━ 03:52:05\n",
      "Accuracy: 0.8818 - Loss: 0.3341\n",
      "\n",
      "Batch 797/992 ━━━━━━━━━━━━━━━━━━━━ 03:52:16\n",
      "Accuracy: 0.8817 - Loss: 0.3341\n",
      "\n",
      "Batch 798/992 ━━━━━━━━━━━━━━━━━━━━ 03:52:26\n",
      "Accuracy: 0.8817 - Loss: 0.3342\n",
      "\n",
      "Batch 799/992 ━━━━━━━━━━━━━━━━━━━━ 03:52:36\n",
      "Accuracy: 0.8816 - Loss: 0.3347\n",
      "\n",
      "Batch 800/992 ━━━━━━━━━━━━━━━━━━━━ 03:52:46\n",
      "Accuracy: 0.8816 - Loss: 0.3348\n",
      "\n",
      "Batch 801/992 ━━━━━━━━━━━━━━━━━━━━ 03:52:56\n",
      "Accuracy: 0.8816 - Loss: 0.3349\n",
      "\n",
      "Batch 802/992 ━━━━━━━━━━━━━━━━━━━━ 03:53:06\n",
      "Accuracy: 0.8817 - Loss: 0.3345\n",
      "\n",
      "Batch 803/992 ━━━━━━━━━━━━━━━━━━━━ 03:53:16\n",
      "Accuracy: 0.8815 - Loss: 0.3352\n",
      "\n",
      "Batch 804/992 ━━━━━━━━━━━━━━━━━━━━ 03:53:26\n",
      "Accuracy: 0.8814 - Loss: 0.3355\n",
      "\n",
      "Batch 805/992 ━━━━━━━━━━━━━━━━━━━━ 03:53:36\n",
      "Accuracy: 0.8812 - Loss: 0.3357\n",
      "\n",
      "Batch 806/992 ━━━━━━━━━━━━━━━━━━━━ 03:53:46\n",
      "Accuracy: 0.8812 - Loss: 0.3357\n",
      "\n",
      "Batch 807/992 ━━━━━━━━━━━━━━━━━━━━ 03:53:56\n",
      "Accuracy: 0.8814 - Loss: 0.3353\n",
      "\n",
      "Batch 808/992 ━━━━━━━━━━━━━━━━━━━━ 03:54:07\n",
      "Accuracy: 0.8813 - Loss: 0.3353\n",
      "\n",
      "Batch 809/992 ━━━━━━━━━━━━━━━━━━━━ 03:54:17\n",
      "Accuracy: 0.8815 - Loss: 0.3350\n",
      "\n",
      "Batch 810/992 ━━━━━━━━━━━━━━━━━━━━ 03:54:28\n",
      "Accuracy: 0.8815 - Loss: 0.3350\n",
      "\n",
      "Batch 811/992 ━━━━━━━━━━━━━━━━━━━━ 03:54:38\n",
      "Accuracy: 0.8813 - Loss: 0.3352\n",
      "\n",
      "Batch 812/992 ━━━━━━━━━━━━━━━━━━━━ 03:54:48\n",
      "Accuracy: 0.8812 - Loss: 0.3355\n",
      "\n",
      "Batch 813/992 ━━━━━━━━━━━━━━━━━━━━ 03:54:58\n",
      "Accuracy: 0.8810 - Loss: 0.3358\n",
      "\n",
      "Batch 814/992 ━━━━━━━━━━━━━━━━━━━━ 03:55:08\n",
      "Accuracy: 0.8810 - Loss: 0.3359\n",
      "\n",
      "Batch 815/992 ━━━━━━━━━━━━━━━━━━━━ 03:55:18\n",
      "Accuracy: 0.8810 - Loss: 0.3361\n",
      "\n",
      "Batch 816/992 ━━━━━━━━━━━━━━━━━━━━ 03:55:28\n",
      "Accuracy: 0.8811 - Loss: 0.3358\n",
      "\n",
      "Batch 817/992 ━━━━━━━━━━━━━━━━━━━━ 03:55:38\n",
      "Accuracy: 0.8813 - Loss: 0.3356\n",
      "\n",
      "Batch 818/992 ━━━━━━━━━━━━━━━━━━━━ 03:55:48\n",
      "Accuracy: 0.8814 - Loss: 0.3352\n",
      "\n",
      "Batch 819/992 ━━━━━━━━━━━━━━━━━━━━ 03:55:58\n",
      "Accuracy: 0.8813 - Loss: 0.3355\n",
      "\n",
      "Batch 820/992 ━━━━━━━━━━━━━━━━━━━━ 03:56:09\n",
      "Accuracy: 0.8813 - Loss: 0.3357\n",
      "\n",
      "Batch 821/992 ━━━━━━━━━━━━━━━━━━━━ 03:56:19\n",
      "Accuracy: 0.8814 - Loss: 0.3356\n",
      "\n",
      "Batch 822/992 ━━━━━━━━━━━━━━━━━━━━ 03:56:29\n",
      "Accuracy: 0.8812 - Loss: 0.3360\n",
      "\n",
      "Batch 823/992 ━━━━━━━━━━━━━━━━━━━━ 03:56:39\n",
      "Accuracy: 0.8812 - Loss: 0.3365\n",
      "\n",
      "Batch 824/992 ━━━━━━━━━━━━━━━━━━━━ 03:56:49\n",
      "Accuracy: 0.8812 - Loss: 0.3363\n",
      "\n",
      "Batch 825/992 ━━━━━━━━━━━━━━━━━━━━ 03:56:59\n",
      "Accuracy: 0.8814 - Loss: 0.3359\n",
      "\n",
      "Batch 826/992 ━━━━━━━━━━━━━━━━━━━━ 03:57:09\n",
      "Accuracy: 0.8814 - Loss: 0.3359\n",
      "\n",
      "Batch 827/992 ━━━━━━━━━━━━━━━━━━━━ 03:57:19\n",
      "Accuracy: 0.8813 - Loss: 0.3360\n",
      "\n",
      "Batch 828/992 ━━━━━━━━━━━━━━━━━━━━ 03:57:30\n",
      "Accuracy: 0.8812 - Loss: 0.3363\n",
      "\n",
      "Batch 829/992 ━━━━━━━━━━━━━━━━━━━━ 03:57:40\n",
      "Accuracy: 0.8810 - Loss: 0.3364\n",
      "\n",
      "Batch 830/992 ━━━━━━━━━━━━━━━━━━━━ 03:57:50\n",
      "Accuracy: 0.8809 - Loss: 0.3368\n",
      "\n",
      "Batch 831/992 ━━━━━━━━━━━━━━━━━━━━ 03:58:00\n",
      "Accuracy: 0.8810 - Loss: 0.3364\n",
      "\n",
      "Batch 832/992 ━━━━━━━━━━━━━━━━━━━━ 03:58:10\n",
      "Accuracy: 0.8812 - Loss: 0.3361\n",
      "\n",
      "Batch 833/992 ━━━━━━━━━━━━━━━━━━━━ 03:58:21\n",
      "Accuracy: 0.8810 - Loss: 0.3367\n",
      "\n",
      "Batch 834/992 ━━━━━━━━━━━━━━━━━━━━ 03:58:31\n",
      "Accuracy: 0.8810 - Loss: 0.3365\n",
      "\n",
      "Batch 835/992 ━━━━━━━━━━━━━━━━━━━━ 03:58:42\n",
      "Accuracy: 0.8808 - Loss: 0.3371\n",
      "\n",
      "Batch 836/992 ━━━━━━━━━━━━━━━━━━━━ 03:58:52\n",
      "Accuracy: 0.8810 - Loss: 0.3368\n",
      "\n",
      "Batch 837/992 ━━━━━━━━━━━━━━━━━━━━ 03:59:02\n",
      "Accuracy: 0.8811 - Loss: 0.3365\n",
      "\n",
      "Batch 838/992 ━━━━━━━━━━━━━━━━━━━━ 03:59:12\n",
      "Accuracy: 0.8810 - Loss: 0.3366\n",
      "\n",
      "Batch 839/992 ━━━━━━━━━━━━━━━━━━━━ 03:59:22\n",
      "Accuracy: 0.8807 - Loss: 0.3371\n",
      "\n",
      "Batch 840/992 ━━━━━━━━━━━━━━━━━━━━ 03:59:32\n",
      "Accuracy: 0.8807 - Loss: 0.3371\n",
      "\n",
      "Batch 841/992 ━━━━━━━━━━━━━━━━━━━━ 03:59:42\n",
      "Accuracy: 0.8806 - Loss: 0.3371\n",
      "\n",
      "Batch 842/992 ━━━━━━━━━━━━━━━━━━━━ 03:59:52\n",
      "Accuracy: 0.8805 - Loss: 0.3372\n",
      "\n",
      "Batch 843/992 ━━━━━━━━━━━━━━━━━━━━ 04:00:02\n",
      "Accuracy: 0.8805 - Loss: 0.3373\n",
      "\n",
      "Batch 844/992 ━━━━━━━━━━━━━━━━━━━━ 04:00:12\n",
      "Accuracy: 0.8803 - Loss: 0.3373\n",
      "\n",
      "Batch 845/992 ━━━━━━━━━━━━━━━━━━━━ 04:00:23\n",
      "Accuracy: 0.8805 - Loss: 0.3370\n",
      "\n",
      "Batch 846/992 ━━━━━━━━━━━━━━━━━━━━ 04:00:33\n",
      "Accuracy: 0.8805 - Loss: 0.3371\n",
      "\n",
      "Batch 847/992 ━━━━━━━━━━━━━━━━━━━━ 04:00:43\n",
      "Accuracy: 0.8806 - Loss: 0.3369\n",
      "\n",
      "Batch 848/992 ━━━━━━━━━━━━━━━━━━━━ 04:00:53\n",
      "Accuracy: 0.8803 - Loss: 0.3375\n",
      "\n",
      "Batch 849/992 ━━━━━━━━━━━━━━━━━━━━ 04:01:04\n",
      "Accuracy: 0.8804 - Loss: 0.3373\n",
      "\n",
      "Batch 850/992 ━━━━━━━━━━━━━━━━━━━━ 04:01:14\n",
      "Accuracy: 0.8804 - Loss: 0.3372\n",
      "\n",
      "Batch 851/992 ━━━━━━━━━━━━━━━━━━━━ 04:01:24\n",
      "Accuracy: 0.8806 - Loss: 0.3370\n",
      "\n",
      "Batch 852/992 ━━━━━━━━━━━━━━━━━━━━ 04:01:34\n",
      "Accuracy: 0.8806 - Loss: 0.3371\n",
      "\n",
      "Batch 853/992 ━━━━━━━━━━━━━━━━━━━━ 04:01:44\n",
      "Accuracy: 0.8804 - Loss: 0.3372\n",
      "\n",
      "Batch 854/992 ━━━━━━━━━━━━━━━━━━━━ 04:01:54\n",
      "Accuracy: 0.8804 - Loss: 0.3373\n",
      "\n",
      "Batch 855/992 ━━━━━━━━━━━━━━━━━━━━ 04:02:04\n",
      "Accuracy: 0.8806 - Loss: 0.3370\n",
      "\n",
      "Batch 856/992 ━━━━━━━━━━━━━━━━━━━━ 04:02:14\n",
      "Accuracy: 0.8805 - Loss: 0.3368\n",
      "\n",
      "Batch 857/992 ━━━━━━━━━━━━━━━━━━━━ 04:02:24\n",
      "Accuracy: 0.8805 - Loss: 0.3367\n",
      "\n",
      "Batch 858/992 ━━━━━━━━━━━━━━━━━━━━ 04:02:34\n",
      "Accuracy: 0.8807 - Loss: 0.3364\n",
      "\n",
      "Batch 859/992 ━━━━━━━━━━━━━━━━━━━━ 04:02:44\n",
      "Accuracy: 0.8808 - Loss: 0.3362\n",
      "\n",
      "Batch 860/992 ━━━━━━━━━━━━━━━━━━━━ 04:02:55\n",
      "Accuracy: 0.8810 - Loss: 0.3359\n",
      "\n",
      "Batch 861/992 ━━━━━━━━━━━━━━━━━━━━ 04:03:05\n",
      "Accuracy: 0.8810 - Loss: 0.3360\n",
      "\n",
      "Batch 862/992 ━━━━━━━━━━━━━━━━━━━━ 04:03:15\n",
      "Accuracy: 0.8808 - Loss: 0.3362\n",
      "\n",
      "Batch 863/992 ━━━━━━━━━━━━━━━━━━━━ 04:03:25\n",
      "Accuracy: 0.8808 - Loss: 0.3360\n",
      "\n",
      "Batch 864/992 ━━━━━━━━━━━━━━━━━━━━ 04:03:35\n",
      "Accuracy: 0.8808 - Loss: 0.3358\n",
      "\n",
      "Batch 865/992 ━━━━━━━━━━━━━━━━━━━━ 04:03:45\n",
      "Accuracy: 0.8808 - Loss: 0.3359\n",
      "\n",
      "Batch 866/992 ━━━━━━━━━━━━━━━━━━━━ 04:03:55\n",
      "Accuracy: 0.8803 - Loss: 0.3367\n",
      "\n",
      "Batch 867/992 ━━━━━━━━━━━━━━━━━━━━ 04:04:05\n",
      "Accuracy: 0.8805 - Loss: 0.3365\n",
      "\n",
      "Batch 868/992 ━━━━━━━━━━━━━━━━━━━━ 04:04:15\n",
      "Accuracy: 0.8805 - Loss: 0.3366\n",
      "\n",
      "Batch 869/992 ━━━━━━━━━━━━━━━━━━━━ 04:04:25\n",
      "Accuracy: 0.8803 - Loss: 0.3366\n",
      "\n",
      "Batch 870/992 ━━━━━━━━━━━━━━━━━━━━ 04:04:35\n",
      "Accuracy: 0.8803 - Loss: 0.3370\n",
      "\n",
      "Batch 871/992 ━━━━━━━━━━━━━━━━━━━━ 04:04:45\n",
      "Accuracy: 0.8803 - Loss: 0.3372\n",
      "\n",
      "Batch 872/992 ━━━━━━━━━━━━━━━━━━━━ 04:04:55\n",
      "Accuracy: 0.8804 - Loss: 0.3369\n",
      "\n",
      "Batch 873/992 ━━━━━━━━━━━━━━━━━━━━ 04:05:05\n",
      "Accuracy: 0.8804 - Loss: 0.3369\n",
      "\n",
      "Batch 874/992 ━━━━━━━━━━━━━━━━━━━━ 04:05:15\n",
      "Accuracy: 0.8806 - Loss: 0.3366\n",
      "\n",
      "Batch 875/992 ━━━━━━━━━━━━━━━━━━━━ 04:05:25\n",
      "Accuracy: 0.8806 - Loss: 0.3364\n",
      "\n",
      "Batch 876/992 ━━━━━━━━━━━━━━━━━━━━ 04:05:35\n",
      "Accuracy: 0.8807 - Loss: 0.3362\n",
      "\n",
      "Batch 877/992 ━━━━━━━━━━━━━━━━━━━━ 04:05:46\n",
      "Accuracy: 0.8807 - Loss: 0.3365\n",
      "\n",
      "Batch 878/992 ━━━━━━━━━━━━━━━━━━━━ 04:05:56\n",
      "Accuracy: 0.8807 - Loss: 0.3365\n",
      "\n",
      "Batch 879/992 ━━━━━━━━━━━━━━━━━━━━ 04:06:06\n",
      "Accuracy: 0.8805 - Loss: 0.3367\n",
      "\n",
      "Batch 880/992 ━━━━━━━━━━━━━━━━━━━━ 04:06:16\n",
      "Accuracy: 0.8803 - Loss: 0.3369\n",
      "\n",
      "Batch 881/992 ━━━━━━━━━━━━━━━━━━━━ 04:06:26\n",
      "Accuracy: 0.8804 - Loss: 0.3366\n",
      "\n",
      "Batch 882/992 ━━━━━━━━━━━━━━━━━━━━ 04:06:36\n",
      "Accuracy: 0.8804 - Loss: 0.3365\n",
      "\n",
      "Batch 883/992 ━━━━━━━━━━━━━━━━━━━━ 04:06:46\n",
      "Accuracy: 0.8804 - Loss: 0.3363\n",
      "\n",
      "Batch 884/992 ━━━━━━━━━━━━━━━━━━━━ 04:06:57\n",
      "Accuracy: 0.8804 - Loss: 0.3364\n",
      "\n",
      "Batch 885/992 ━━━━━━━━━━━━━━━━━━━━ 04:07:07\n",
      "Accuracy: 0.8802 - Loss: 0.3365\n",
      "\n",
      "Batch 886/992 ━━━━━━━━━━━━━━━━━━━━ 04:07:17\n",
      "Accuracy: 0.8804 - Loss: 0.3363\n",
      "\n",
      "Batch 887/992 ━━━━━━━━━━━━━━━━━━━━ 04:07:27\n",
      "Accuracy: 0.8804 - Loss: 0.3361\n",
      "\n",
      "Batch 888/992 ━━━━━━━━━━━━━━━━━━━━ 04:07:37\n",
      "Accuracy: 0.8799 - Loss: 0.3368\n",
      "\n",
      "Batch 889/992 ━━━━━━━━━━━━━━━━━━━━ 04:07:47\n",
      "Accuracy: 0.8801 - Loss: 0.3364\n",
      "\n",
      "Batch 890/992 ━━━━━━━━━━━━━━━━━━━━ 04:07:57\n",
      "Accuracy: 0.8802 - Loss: 0.3361\n",
      "\n",
      "Batch 891/992 ━━━━━━━━━━━━━━━━━━━━ 04:08:08\n",
      "Accuracy: 0.8799 - Loss: 0.3366\n",
      "\n",
      "Batch 892/992 ━━━━━━━━━━━━━━━━━━━━ 04:08:18\n",
      "Accuracy: 0.8800 - Loss: 0.3363\n",
      "\n",
      "Batch 893/992 ━━━━━━━━━━━━━━━━━━━━ 04:08:28\n",
      "Accuracy: 0.8802 - Loss: 0.3362\n",
      "\n",
      "Batch 894/992 ━━━━━━━━━━━━━━━━━━━━ 04:08:38\n",
      "Accuracy: 0.8800 - Loss: 0.3362\n",
      "\n",
      "Batch 895/992 ━━━━━━━━━━━━━━━━━━━━ 04:08:48\n",
      "Accuracy: 0.8802 - Loss: 0.3359\n",
      "\n",
      "Batch 896/992 ━━━━━━━━━━━━━━━━━━━━ 04:08:58\n",
      "Accuracy: 0.8799 - Loss: 0.3363\n",
      "\n",
      "Batch 897/992 ━━━━━━━━━━━━━━━━━━━━ 04:09:08\n",
      "Accuracy: 0.8799 - Loss: 0.3362\n",
      "\n",
      "Batch 898/992 ━━━━━━━━━━━━━━━━━━━━ 04:09:18\n",
      "Accuracy: 0.8797 - Loss: 0.3366\n",
      "\n",
      "Batch 899/992 ━━━━━━━━━━━━━━━━━━━━ 04:09:28\n",
      "Accuracy: 0.8799 - Loss: 0.3363\n",
      "\n",
      "Batch 900/992 ━━━━━━━━━━━━━━━━━━━━ 04:09:39\n",
      "Accuracy: 0.8799 - Loss: 0.3363\n",
      "\n",
      "Batch 901/992 ━━━━━━━━━━━━━━━━━━━━ 04:09:49\n",
      "Accuracy: 0.8797 - Loss: 0.3363\n",
      "\n",
      "Batch 902/992 ━━━━━━━━━━━━━━━━━━━━ 04:09:59\n",
      "Accuracy: 0.8797 - Loss: 0.3364\n",
      "\n",
      "Batch 903/992 ━━━━━━━━━━━━━━━━━━━━ 04:10:09\n",
      "Accuracy: 0.8796 - Loss: 0.3368\n",
      "\n",
      "Batch 904/992 ━━━━━━━━━━━━━━━━━━━━ 04:10:20\n",
      "Accuracy: 0.8796 - Loss: 0.3371\n",
      "\n",
      "Batch 905/992 ━━━━━━━━━━━━━━━━━━━━ 04:10:30\n",
      "Accuracy: 0.8796 - Loss: 0.3371\n",
      "\n",
      "Batch 906/992 ━━━━━━━━━━━━━━━━━━━━ 04:10:40\n",
      "Accuracy: 0.8797 - Loss: 0.3368\n",
      "\n",
      "Batch 907/992 ━━━━━━━━━━━━━━━━━━━━ 04:10:50\n",
      "Accuracy: 0.8797 - Loss: 0.3367\n",
      "\n",
      "Batch 908/992 ━━━━━━━━━━━━━━━━━━━━ 04:11:00\n",
      "Accuracy: 0.8798 - Loss: 0.3365\n",
      "\n",
      "Batch 909/992 ━━━━━━━━━━━━━━━━━━━━ 04:11:10\n",
      "Accuracy: 0.8800 - Loss: 0.3362\n",
      "\n",
      "Batch 910/992 ━━━━━━━━━━━━━━━━━━━━ 04:11:20\n",
      "Accuracy: 0.8798 - Loss: 0.3361\n",
      "\n",
      "Batch 911/992 ━━━━━━━━━━━━━━━━━━━━ 04:11:30\n",
      "Accuracy: 0.8799 - Loss: 0.3358\n",
      "\n",
      "Batch 912/992 ━━━━━━━━━━━━━━━━━━━━ 04:11:40\n",
      "Accuracy: 0.8801 - Loss: 0.3356\n",
      "\n",
      "Batch 913/992 ━━━━━━━━━━━━━━━━━━━━ 04:11:50\n",
      "Accuracy: 0.8801 - Loss: 0.3359\n",
      "\n",
      "Batch 914/992 ━━━━━━━━━━━━━━━━━━━━ 04:12:01\n",
      "Accuracy: 0.8801 - Loss: 0.3359\n",
      "\n",
      "Batch 915/992 ━━━━━━━━━━━━━━━━━━━━ 04:12:11\n",
      "Accuracy: 0.8802 - Loss: 0.3356\n",
      "\n",
      "Batch 916/992 ━━━━━━━━━━━━━━━━━━━━ 04:12:21\n",
      "Accuracy: 0.8802 - Loss: 0.3356\n",
      "\n",
      "Batch 917/992 ━━━━━━━━━━━━━━━━━━━━ 04:12:31\n",
      "Accuracy: 0.8802 - Loss: 0.3357\n",
      "\n",
      "Batch 918/992 ━━━━━━━━━━━━━━━━━━━━ 04:12:41\n",
      "Accuracy: 0.8800 - Loss: 0.3358\n",
      "\n",
      "Batch 919/992 ━━━━━━━━━━━━━━━━━━━━ 04:12:51\n",
      "Accuracy: 0.8800 - Loss: 0.3356\n",
      "\n",
      "Batch 920/992 ━━━━━━━━━━━━━━━━━━━━ 04:13:01\n",
      "Accuracy: 0.8799 - Loss: 0.3361\n",
      "\n",
      "Batch 921/992 ━━━━━━━━━━━━━━━━━━━━ 04:13:11\n",
      "Accuracy: 0.8800 - Loss: 0.3358\n",
      "\n",
      "Batch 922/992 ━━━━━━━━━━━━━━━━━━━━ 04:13:21\n",
      "Accuracy: 0.8797 - Loss: 0.3364\n",
      "\n",
      "Batch 923/992 ━━━━━━━━━━━━━━━━━━━━ 04:13:31\n",
      "Accuracy: 0.8799 - Loss: 0.3361\n",
      "\n",
      "Batch 924/992 ━━━━━━━━━━━━━━━━━━━━ 04:13:42\n",
      "Accuracy: 0.8799 - Loss: 0.3360\n",
      "\n",
      "Batch 925/992 ━━━━━━━━━━━━━━━━━━━━ 04:13:52\n",
      "Accuracy: 0.8799 - Loss: 0.3360\n",
      "\n",
      "Batch 926/992 ━━━━━━━━━━━━━━━━━━━━ 04:14:02\n",
      "Accuracy: 0.8800 - Loss: 0.3357\n",
      "\n",
      "Batch 927/992 ━━━━━━━━━━━━━━━━━━━━ 04:14:13\n",
      "Accuracy: 0.8799 - Loss: 0.3357\n",
      "\n",
      "Batch 928/992 ━━━━━━━━━━━━━━━━━━━━ 04:14:23\n",
      "Accuracy: 0.8796 - Loss: 0.3360\n",
      "\n",
      "Batch 929/992 ━━━━━━━━━━━━━━━━━━━━ 04:14:33\n",
      "Accuracy: 0.8796 - Loss: 0.3360\n",
      "\n",
      "Batch 930/992 ━━━━━━━━━━━━━━━━━━━━ 04:14:43\n",
      "Accuracy: 0.8797 - Loss: 0.3357\n",
      "\n",
      "Batch 931/992 ━━━━━━━━━━━━━━━━━━━━ 04:14:53\n",
      "Accuracy: 0.8798 - Loss: 0.3356\n",
      "\n",
      "Batch 932/992 ━━━━━━━━━━━━━━━━━━━━ 04:15:03\n",
      "Accuracy: 0.8797 - Loss: 0.3361\n",
      "\n",
      "Batch 933/992 ━━━━━━━━━━━━━━━━━━━━ 04:15:13\n",
      "Accuracy: 0.8796 - Loss: 0.3366\n",
      "\n",
      "Batch 934/992 ━━━━━━━━━━━━━━━━━━━━ 04:15:23\n",
      "Accuracy: 0.8794 - Loss: 0.3379\n",
      "\n",
      "Batch 935/992 ━━━━━━━━━━━━━━━━━━━━ 04:15:33\n",
      "Accuracy: 0.8793 - Loss: 0.3379\n",
      "\n",
      "Batch 936/992 ━━━━━━━━━━━━━━━━━━━━ 04:15:43\n",
      "Accuracy: 0.8794 - Loss: 0.3376\n",
      "\n",
      "Batch 937/992 ━━━━━━━━━━━━━━━━━━━━ 04:15:53\n",
      "Accuracy: 0.8795 - Loss: 0.3374\n",
      "\n",
      "Batch 938/992 ━━━━━━━━━━━━━━━━━━━━ 04:16:04\n",
      "Accuracy: 0.8794 - Loss: 0.3377\n",
      "\n",
      "Batch 939/992 ━━━━━━━━━━━━━━━━━━━━ 04:16:14\n",
      "Accuracy: 0.8793 - Loss: 0.3379\n",
      "\n",
      "Batch 940/992 ━━━━━━━━━━━━━━━━━━━━ 04:16:24\n",
      "Accuracy: 0.8794 - Loss: 0.3377\n",
      "\n",
      "Batch 941/992 ━━━━━━━━━━━━━━━━━━━━ 04:16:34\n",
      "Accuracy: 0.8793 - Loss: 0.3377\n",
      "\n",
      "Batch 942/992 ━━━━━━━━━━━━━━━━━━━━ 04:16:44\n",
      "Accuracy: 0.8792 - Loss: 0.3377\n",
      "\n",
      "Batch 943/992 ━━━━━━━━━━━━━━━━━━━━ 04:16:54\n",
      "Accuracy: 0.8792 - Loss: 0.3376\n",
      "\n",
      "Batch 944/992 ━━━━━━━━━━━━━━━━━━━━ 04:17:05\n",
      "Accuracy: 0.8794 - Loss: 0.3375\n",
      "\n",
      "Batch 945/992 ━━━━━━━━━━━━━━━━━━━━ 04:17:15\n",
      "Accuracy: 0.8795 - Loss: 0.3372\n",
      "\n",
      "Batch 946/992 ━━━━━━━━━━━━━━━━━━━━ 04:17:25\n",
      "Accuracy: 0.8791 - Loss: 0.3386\n",
      "\n",
      "Batch 947/992 ━━━━━━━━━━━━━━━━━━━━ 04:17:35\n",
      "Accuracy: 0.8791 - Loss: 0.3384\n",
      "\n",
      "Batch 948/992 ━━━━━━━━━━━━━━━━━━━━ 04:17:45\n",
      "Accuracy: 0.8791 - Loss: 0.3386\n",
      "\n",
      "Batch 949/992 ━━━━━━━━━━━━━━━━━━━━ 04:17:55\n",
      "Accuracy: 0.8791 - Loss: 0.3385\n",
      "\n",
      "Batch 950/992 ━━━━━━━━━━━━━━━━━━━━ 04:18:06\n",
      "Accuracy: 0.8791 - Loss: 0.3384\n",
      "\n",
      "Batch 951/992 ━━━━━━━━━━━━━━━━━━━━ 04:18:16\n",
      "Accuracy: 0.8791 - Loss: 0.3384\n",
      "\n",
      "Batch 952/992 ━━━━━━━━━━━━━━━━━━━━ 04:18:26\n",
      "Accuracy: 0.8789 - Loss: 0.3385\n",
      "\n",
      "Batch 953/992 ━━━━━━━━━━━━━━━━━━━━ 04:18:36\n",
      "Accuracy: 0.8788 - Loss: 0.3388\n",
      "\n",
      "Batch 954/992 ━━━━━━━━━━━━━━━━━━━━ 04:18:46\n",
      "Accuracy: 0.8789 - Loss: 0.3386\n",
      "\n",
      "Batch 955/992 ━━━━━━━━━━━━━━━━━━━━ 04:18:56\n",
      "Accuracy: 0.8791 - Loss: 0.3384\n",
      "\n",
      "Batch 956/992 ━━━━━━━━━━━━━━━━━━━━ 04:19:06\n",
      "Accuracy: 0.8789 - Loss: 0.3386\n",
      "\n",
      "Batch 957/992 ━━━━━━━━━━━━━━━━━━━━ 04:19:17\n",
      "Accuracy: 0.8788 - Loss: 0.3390\n",
      "\n",
      "Batch 958/992 ━━━━━━━━━━━━━━━━━━━━ 04:19:27\n",
      "Accuracy: 0.8787 - Loss: 0.3395\n",
      "\n",
      "Batch 959/992 ━━━━━━━━━━━━━━━━━━━━ 04:19:37\n",
      "Accuracy: 0.8788 - Loss: 0.3392\n",
      "\n",
      "Batch 960/992 ━━━━━━━━━━━━━━━━━━━━ 04:19:47\n",
      "Accuracy: 0.8788 - Loss: 0.3393\n",
      "\n",
      "Batch 961/992 ━━━━━━━━━━━━━━━━━━━━ 04:19:57\n",
      "Accuracy: 0.8788 - Loss: 0.3397\n",
      "\n",
      "Batch 962/992 ━━━━━━━━━━━━━━━━━━━━ 04:20:07\n",
      "Accuracy: 0.8788 - Loss: 0.3398\n",
      "\n",
      "Batch 963/992 ━━━━━━━━━━━━━━━━━━━━ 04:20:18\n",
      "Accuracy: 0.8789 - Loss: 0.3398\n",
      "\n",
      "Batch 964/992 ━━━━━━━━━━━━━━━━━━━━ 04:20:28\n",
      "Accuracy: 0.8790 - Loss: 0.3396\n",
      "\n",
      "Batch 965/992 ━━━━━━━━━━━━━━━━━━━━ 04:20:38\n",
      "Accuracy: 0.8789 - Loss: 0.3398\n",
      "\n",
      "Batch 966/992 ━━━━━━━━━━━━━━━━━━━━ 04:20:48\n",
      "Accuracy: 0.8788 - Loss: 0.3400\n",
      "\n",
      "Batch 967/992 ━━━━━━━━━━━━━━━━━━━━ 04:20:58\n",
      "Accuracy: 0.8786 - Loss: 0.3399\n",
      "\n",
      "Batch 968/992 ━━━━━━━━━━━━━━━━━━━━ 04:21:08\n",
      "Accuracy: 0.8785 - Loss: 0.3403\n",
      "\n",
      "Batch 969/992 ━━━━━━━━━━━━━━━━━━━━ 04:21:18\n",
      "Accuracy: 0.8785 - Loss: 0.3404\n",
      "\n",
      "Batch 970/992 ━━━━━━━━━━━━━━━━━━━━ 04:21:28\n",
      "Accuracy: 0.8786 - Loss: 0.3401\n",
      "\n",
      "Batch 971/992 ━━━━━━━━━━━━━━━━━━━━ 04:21:38\n",
      "Accuracy: 0.8787 - Loss: 0.3401\n",
      "\n",
      "Batch 972/992 ━━━━━━━━━━━━━━━━━━━━ 04:21:48\n",
      "Accuracy: 0.8789 - Loss: 0.3398\n",
      "\n",
      "Batch 973/992 ━━━━━━━━━━━━━━━━━━━━ 04:21:59\n",
      "Accuracy: 0.8790 - Loss: 0.3396\n",
      "\n",
      "Batch 974/992 ━━━━━━━━━━━━━━━━━━━━ 04:22:09\n",
      "Accuracy: 0.8791 - Loss: 0.3394\n",
      "\n",
      "Batch 975/992 ━━━━━━━━━━━━━━━━━━━━ 04:22:20\n",
      "Accuracy: 0.8790 - Loss: 0.3399\n",
      "\n",
      "Batch 976/992 ━━━━━━━━━━━━━━━━━━━━ 04:22:30\n",
      "Accuracy: 0.8788 - Loss: 0.3400\n",
      "\n",
      "Batch 977/992 ━━━━━━━━━━━━━━━━━━━━ 04:22:40\n",
      "Accuracy: 0.8790 - Loss: 0.3398\n",
      "\n",
      "Batch 978/992 ━━━━━━━━━━━━━━━━━━━━ 04:22:50\n",
      "Accuracy: 0.8791 - Loss: 0.3397\n",
      "\n",
      "Batch 979/992 ━━━━━━━━━━━━━━━━━━━━ 04:23:00\n",
      "Accuracy: 0.8792 - Loss: 0.3395\n",
      "\n",
      "Batch 980/992 ━━━━━━━━━━━━━━━━━━━━ 04:23:10\n",
      "Accuracy: 0.8792 - Loss: 0.3394\n",
      "\n",
      "Batch 981/992 ━━━━━━━━━━━━━━━━━━━━ 04:23:20\n",
      "Accuracy: 0.8793 - Loss: 0.3391\n",
      "\n",
      "Batch 982/992 ━━━━━━━━━━━━━━━━━━━━ 04:23:31\n",
      "Accuracy: 0.8793 - Loss: 0.3396\n",
      "\n",
      "Batch 983/992 ━━━━━━━━━━━━━━━━━━━━ 04:23:41\n",
      "Accuracy: 0.8795 - Loss: 0.3394\n",
      "\n",
      "Batch 984/992 ━━━━━━━━━━━━━━━━━━━━ 04:23:51\n",
      "Accuracy: 0.8792 - Loss: 0.3399\n",
      "\n",
      "Batch 985/992 ━━━━━━━━━━━━━━━━━━━━ 04:24:01\n",
      "Accuracy: 0.8792 - Loss: 0.3398\n",
      "\n",
      "Batch 986/992 ━━━━━━━━━━━━━━━━━━━━ 04:24:11\n",
      "Accuracy: 0.8792 - Loss: 0.3398\n",
      "\n",
      "Batch 987/992 ━━━━━━━━━━━━━━━━━━━━ 04:24:22\n",
      "Accuracy: 0.8792 - Loss: 0.3399\n",
      "\n",
      "Batch 988/992 ━━━━━━━━━━━━━━━━━━━━ 04:24:32\n",
      "Accuracy: 0.8792 - Loss: 0.3398\n",
      "\n",
      "Batch 989/992 ━━━━━━━━━━━━━━━━━━━━ 04:24:42\n",
      "Accuracy: 0.8793 - Loss: 0.3397\n",
      "\n",
      "Batch 990/992 ━━━━━━━━━━━━━━━━━━━━ 04:24:52\n",
      "Accuracy: 0.8794 - Loss: 0.3394\n",
      "\n",
      "Batch 991/992 ━━━━━━━━━━━━━━━━━━━━ 04:25:02\n",
      "Accuracy: 0.8793 - Loss: 0.3395\n",
      "\n",
      "Batch 992/992 ━━━━━━━━━━━━━━━━━━━━ 04:25:12\n",
      "Accuracy: 0.8793 - Loss: 0.3396\n",
      "\n",
      "\n",
      "Epoch 5/10\n",
      "Batch 1/992 ━━━━━━━━━━━━━━━━━━━━ 04:41:20\n",
      "Accuracy: 1.0000 - Loss: 0.0570\n",
      "\n",
      "Batch 2/992 ━━━━━━━━━━━━━━━━━━━━ 04:41:30\n",
      "Accuracy: 1.0000 - Loss: 0.0935\n",
      "\n",
      "Batch 3/992 ━━━━━━━━━━━━━━━━━━━━ 04:41:40\n",
      "Accuracy: 0.9583 - Loss: 0.1589\n",
      "\n",
      "Batch 4/992 ━━━━━━━━━━━━━━━━━━━━ 04:41:51\n",
      "Accuracy: 0.9375 - Loss: 0.1699\n",
      "\n",
      "Batch 5/992 ━━━━━━━━━━━━━━━━━━━━ 04:42:01\n",
      "Accuracy: 0.9500 - Loss: 0.1581\n",
      "\n",
      "Batch 6/992 ━━━━━━━━━━━━━━━━━━━━ 04:42:11\n",
      "Accuracy: 0.9583 - Loss: 0.1483\n",
      "\n",
      "Batch 7/992 ━━━━━━━━━━━━━━━━━━━━ 04:42:22\n",
      "Accuracy: 0.9286 - Loss: 0.2005\n",
      "\n",
      "Batch 8/992 ━━━━━━━━━━━━━━━━━━━━ 04:42:32\n",
      "Accuracy: 0.9375 - Loss: 0.1940\n",
      "\n",
      "Batch 9/992 ━━━━━━━━━━━━━━━━━━━━ 04:42:43\n",
      "Accuracy: 0.9444 - Loss: 0.1926\n",
      "\n",
      "Batch 10/992 ━━━━━━━━━━━━━━━━━━━━ 04:42:53\n",
      "Accuracy: 0.9375 - Loss: 0.2078\n",
      "\n",
      "Batch 11/992 ━━━━━━━━━━━━━━━━━━━━ 04:43:03\n",
      "Accuracy: 0.9318 - Loss: 0.2097\n",
      "\n",
      "Batch 12/992 ━━━━━━━━━━━━━━━━━━━━ 04:43:13\n",
      "Accuracy: 0.9375 - Loss: 0.2006\n",
      "\n",
      "Batch 13/992 ━━━━━━━━━━━━━━━━━━━━ 04:43:23\n",
      "Accuracy: 0.9327 - Loss: 0.2008\n",
      "\n",
      "Batch 14/992 ━━━━━━━━━━━━━━━━━━━━ 04:43:33\n",
      "Accuracy: 0.9375 - Loss: 0.1940\n",
      "\n",
      "Batch 15/992 ━━━━━━━━━━━━━━━━━━━━ 04:43:43\n",
      "Accuracy: 0.9417 - Loss: 0.1836\n",
      "\n",
      "Batch 16/992 ━━━━━━━━━━━━━━━━━━━━ 04:43:53\n",
      "Accuracy: 0.9453 - Loss: 0.1782\n",
      "\n",
      "Batch 17/992 ━━━━━━━━━━━━━━━━━━━━ 04:44:03\n",
      "Accuracy: 0.9412 - Loss: 0.1784\n",
      "\n",
      "Batch 18/992 ━━━━━━━━━━━━━━━━━━━━ 04:44:13\n",
      "Accuracy: 0.9444 - Loss: 0.1721\n",
      "\n",
      "Batch 19/992 ━━━━━━━━━━━━━━━━━━━━ 04:44:24\n",
      "Accuracy: 0.9408 - Loss: 0.1761\n",
      "\n",
      "Batch 20/992 ━━━━━━━━━━━━━━━━━━━━ 04:44:34\n",
      "Accuracy: 0.9438 - Loss: 0.1712\n",
      "\n",
      "Batch 21/992 ━━━━━━━━━━━━━━━━━━━━ 04:44:44\n",
      "Accuracy: 0.9464 - Loss: 0.1672\n",
      "\n",
      "Batch 22/992 ━━━━━━━━━━━━━━━━━━━━ 04:44:54\n",
      "Accuracy: 0.9489 - Loss: 0.1616\n",
      "\n",
      "Batch 23/992 ━━━━━━━━━━━━━━━━━━━━ 04:45:04\n",
      "Accuracy: 0.9402 - Loss: 0.1730\n",
      "\n",
      "Batch 24/992 ━━━━━━━━━━━━━━━━━━━━ 04:45:14\n",
      "Accuracy: 0.9323 - Loss: 0.1901\n",
      "\n",
      "Batch 25/992 ━━━━━━━━━━━━━━━━━━━━ 04:45:24\n",
      "Accuracy: 0.9350 - Loss: 0.1856\n",
      "\n",
      "Batch 26/992 ━━━━━━━━━━━━━━━━━━━━ 04:45:34\n",
      "Accuracy: 0.9375 - Loss: 0.1857\n",
      "\n",
      "Batch 27/992 ━━━━━━━━━━━━━━━━━━━━ 04:45:45\n",
      "Accuracy: 0.9398 - Loss: 0.1849\n",
      "\n",
      "Batch 28/992 ━━━━━━━━━━━━━━━━━━━━ 04:45:55\n",
      "Accuracy: 0.9420 - Loss: 0.1842\n",
      "\n",
      "Batch 29/992 ━━━━━━━━━━━━━━━━━━━━ 04:46:05\n",
      "Accuracy: 0.9440 - Loss: 0.1816\n",
      "\n",
      "Batch 30/992 ━━━━━━━━━━━━━━━━━━━━ 04:46:15\n",
      "Accuracy: 0.9417 - Loss: 0.1813\n",
      "\n",
      "Batch 31/992 ━━━━━━━━━━━━━━━━━━━━ 04:46:25\n",
      "Accuracy: 0.9435 - Loss: 0.1798\n",
      "\n",
      "Batch 32/992 ━━━━━━━━━━━━━━━━━━━━ 04:46:35\n",
      "Accuracy: 0.9453 - Loss: 0.1791\n",
      "\n",
      "Batch 33/992 ━━━━━━━━━━━━━━━━━━━━ 04:46:46\n",
      "Accuracy: 0.9432 - Loss: 0.1861\n",
      "\n",
      "Batch 34/992 ━━━━━━━━━━━━━━━━━━━━ 04:46:56\n",
      "Accuracy: 0.9412 - Loss: 0.1856\n",
      "\n",
      "Batch 35/992 ━━━━━━━━━━━━━━━━━━━━ 04:47:06\n",
      "Accuracy: 0.9429 - Loss: 0.1816\n",
      "\n",
      "Batch 36/992 ━━━━━━━━━━━━━━━━━━━━ 04:47:16\n",
      "Accuracy: 0.9444 - Loss: 0.1801\n",
      "\n",
      "Batch 37/992 ━━━━━━━━━━━━━━━━━━━━ 04:47:26\n",
      "Accuracy: 0.9459 - Loss: 0.1784\n",
      "\n",
      "Batch 38/992 ━━━━━━━━━━━━━━━━━━━━ 04:47:36\n",
      "Accuracy: 0.9474 - Loss: 0.1769\n",
      "\n",
      "Batch 39/992 ━━━━━━━━━━━━━━━━━━━━ 04:47:46\n",
      "Accuracy: 0.9455 - Loss: 0.1783\n",
      "\n",
      "Batch 40/992 ━━━━━━━━━━━━━━━━━━━━ 04:47:56\n",
      "Accuracy: 0.9438 - Loss: 0.1818\n",
      "\n",
      "Batch 41/992 ━━━━━━━━━━━━━━━━━━━━ 04:48:07\n",
      "Accuracy: 0.9421 - Loss: 0.1859\n",
      "\n",
      "Batch 42/992 ━━━━━━━━━━━━━━━━━━━━ 04:48:17\n",
      "Accuracy: 0.9435 - Loss: 0.1832\n",
      "\n",
      "Batch 43/992 ━━━━━━━━━━━━━━━━━━━━ 04:48:27\n",
      "Accuracy: 0.9448 - Loss: 0.1797\n",
      "\n",
      "Batch 44/992 ━━━━━━━━━━━━━━━━━━━━ 04:48:37\n",
      "Accuracy: 0.9432 - Loss: 0.1806\n",
      "\n",
      "Batch 45/992 ━━━━━━━━━━━━━━━━━━━━ 04:48:47\n",
      "Accuracy: 0.9444 - Loss: 0.1780\n",
      "\n",
      "Batch 46/992 ━━━━━━━━━━━━━━━━━━━━ 04:48:57\n",
      "Accuracy: 0.9429 - Loss: 0.1823\n",
      "\n",
      "Batch 47/992 ━━━━━━━━━━━━━━━━━━━━ 04:49:08\n",
      "Accuracy: 0.9415 - Loss: 0.1892\n",
      "\n",
      "Batch 48/992 ━━━━━━━━━━━━━━━━━━━━ 04:49:18\n",
      "Accuracy: 0.9401 - Loss: 0.1921\n",
      "\n",
      "Batch 49/992 ━━━━━━━━━━━━━━━━━━━━ 04:49:28\n",
      "Accuracy: 0.9413 - Loss: 0.1899\n",
      "\n",
      "Batch 50/992 ━━━━━━━━━━━━━━━━━━━━ 04:49:38\n",
      "Accuracy: 0.9425 - Loss: 0.1882\n",
      "\n",
      "Batch 51/992 ━━━━━━━━━━━━━━━━━━━━ 04:49:48\n",
      "Accuracy: 0.9436 - Loss: 0.1857\n",
      "\n",
      "Batch 52/992 ━━━━━━━━━━━━━━━━━━━━ 04:49:58\n",
      "Accuracy: 0.9423 - Loss: 0.1887\n",
      "\n",
      "Batch 53/992 ━━━━━━━━━━━━━━━━━━━━ 04:50:08\n",
      "Accuracy: 0.9434 - Loss: 0.1865\n",
      "\n",
      "Batch 54/992 ━━━━━━━━━━━━━━━━━━━━ 04:50:18\n",
      "Accuracy: 0.9444 - Loss: 0.1842\n",
      "\n",
      "Batch 55/992 ━━━━━━━━━━━━━━━━━━━━ 04:50:28\n",
      "Accuracy: 0.9432 - Loss: 0.1861\n",
      "\n",
      "Batch 56/992 ━━━━━━━━━━━━━━━━━━━━ 04:50:38\n",
      "Accuracy: 0.9420 - Loss: 0.1862\n",
      "\n",
      "Batch 57/992 ━━━━━━━━━━━━━━━━━━━━ 04:50:48\n",
      "Accuracy: 0.9430 - Loss: 0.1841\n",
      "\n",
      "Batch 58/992 ━━━━━━━━━━━━━━━━━━━━ 04:50:59\n",
      "Accuracy: 0.9418 - Loss: 0.1834\n",
      "\n",
      "Batch 59/992 ━━━━━━━━━━━━━━━━━━━━ 04:51:09\n",
      "Accuracy: 0.9407 - Loss: 0.1866\n",
      "\n",
      "Batch 60/992 ━━━━━━━━━━━━━━━━━━━━ 04:51:19\n",
      "Accuracy: 0.9417 - Loss: 0.1847\n",
      "\n",
      "Batch 61/992 ━━━━━━━━━━━━━━━━━━━━ 04:51:29\n",
      "Accuracy: 0.9426 - Loss: 0.1826\n",
      "\n",
      "Batch 62/992 ━━━━━━━━━━━━━━━━━━━━ 04:51:39\n",
      "Accuracy: 0.9435 - Loss: 0.1802\n",
      "\n",
      "Batch 63/992 ━━━━━━━━━━━━━━━━━━━━ 04:51:50\n",
      "Accuracy: 0.9425 - Loss: 0.1821\n",
      "\n",
      "Batch 64/992 ━━━━━━━━━━━━━━━━━━━━ 04:52:00\n",
      "Accuracy: 0.9414 - Loss: 0.1827\n",
      "\n",
      "Batch 65/992 ━━━━━━━━━━━━━━━━━━━━ 04:52:10\n",
      "Accuracy: 0.9423 - Loss: 0.1812\n",
      "\n",
      "Batch 66/992 ━━━━━━━━━━━━━━━━━━━━ 04:52:20\n",
      "Accuracy: 0.9432 - Loss: 0.1798\n",
      "\n",
      "Batch 67/992 ━━━━━━━━━━━━━━━━━━━━ 04:52:30\n",
      "Accuracy: 0.9422 - Loss: 0.1818\n",
      "\n",
      "Batch 68/992 ━━━━━━━━━━━━━━━━━━━━ 04:52:40\n",
      "Accuracy: 0.9412 - Loss: 0.1813\n",
      "\n",
      "Batch 69/992 ━━━━━━━━━━━━━━━━━━━━ 04:52:50\n",
      "Accuracy: 0.9420 - Loss: 0.1795\n",
      "\n",
      "Batch 70/992 ━━━━━━━━━━━━━━━━━━━━ 04:53:01\n",
      "Accuracy: 0.9411 - Loss: 0.1804\n",
      "\n",
      "Batch 71/992 ━━━━━━━━━━━━━━━━━━━━ 04:53:11\n",
      "Accuracy: 0.9384 - Loss: 0.1821\n",
      "\n",
      "Batch 72/992 ━━━━━━━━━━━━━━━━━━━━ 04:53:21\n",
      "Accuracy: 0.9392 - Loss: 0.1823\n",
      "\n",
      "Batch 73/992 ━━━━━━━━━━━━━━━━━━━━ 04:53:31\n",
      "Accuracy: 0.9401 - Loss: 0.1804\n",
      "\n",
      "Batch 74/992 ━━━━━━━━━━━━━━━━━━━━ 04:53:41\n",
      "Accuracy: 0.9375 - Loss: 0.1825\n",
      "\n",
      "Batch 75/992 ━━━━━━━━━━━━━━━━━━━━ 04:53:51\n",
      "Accuracy: 0.9367 - Loss: 0.1886\n",
      "\n",
      "Batch 76/992 ━━━━━━━━━━━━━━━━━━━━ 04:54:01\n",
      "Accuracy: 0.9359 - Loss: 0.1885\n",
      "\n",
      "Batch 77/992 ━━━━━━━━━━━━━━━━━━━━ 04:54:12\n",
      "Accuracy: 0.9367 - Loss: 0.1878\n",
      "\n",
      "Batch 78/992 ━━━━━━━━━━━━━━━━━━━━ 04:54:23\n",
      "Accuracy: 0.9359 - Loss: 0.1883\n",
      "\n",
      "Batch 79/992 ━━━━━━━━━━━━━━━━━━━━ 04:54:33\n",
      "Accuracy: 0.9351 - Loss: 0.1884\n",
      "\n",
      "Batch 80/992 ━━━━━━━━━━━━━━━━━━━━ 04:54:43\n",
      "Accuracy: 0.9344 - Loss: 0.1895\n",
      "\n",
      "Batch 81/992 ━━━━━━━━━━━━━━━━━━━━ 04:54:53\n",
      "Accuracy: 0.9336 - Loss: 0.1899\n",
      "\n",
      "Batch 82/992 ━━━━━━━━━━━━━━━━━━━━ 04:55:03\n",
      "Accuracy: 0.9299 - Loss: 0.1951\n",
      "\n",
      "Batch 83/992 ━━━━━━━━━━━━━━━━━━━━ 04:55:14\n",
      "Accuracy: 0.9292 - Loss: 0.1955\n",
      "\n",
      "Batch 84/992 ━━━━━━━━━━━━━━━━━━━━ 04:55:24\n",
      "Accuracy: 0.9256 - Loss: 0.1992\n",
      "\n",
      "Batch 85/992 ━━━━━━━━━━━━━━━━━━━━ 04:55:34\n",
      "Accuracy: 0.9265 - Loss: 0.1985\n",
      "\n",
      "Batch 86/992 ━━━━━━━━━━━━━━━━━━━━ 04:55:44\n",
      "Accuracy: 0.9273 - Loss: 0.1976\n",
      "\n",
      "Batch 87/992 ━━━━━━━━━━━━━━━━━━━━ 04:55:54\n",
      "Accuracy: 0.9282 - Loss: 0.1964\n",
      "\n",
      "Batch 88/992 ━━━━━━━━━━━━━━━━━━━━ 04:56:05\n",
      "Accuracy: 0.9290 - Loss: 0.1947\n",
      "\n",
      "Batch 89/992 ━━━━━━━━━━━━━━━━━━━━ 04:56:15\n",
      "Accuracy: 0.9298 - Loss: 0.1932\n",
      "\n",
      "Batch 90/992 ━━━━━━━━━━━━━━━━━━━━ 04:56:25\n",
      "Accuracy: 0.9292 - Loss: 0.1958\n",
      "\n",
      "Batch 91/992 ━━━━━━━━━━━━━━━━━━━━ 04:56:35\n",
      "Accuracy: 0.9286 - Loss: 0.1970\n",
      "\n",
      "Batch 92/992 ━━━━━━━━━━━━━━━━━━━━ 04:56:45\n",
      "Accuracy: 0.9280 - Loss: 0.1971\n",
      "\n",
      "Batch 93/992 ━━━━━━━━━━━━━━━━━━━━ 04:56:55\n",
      "Accuracy: 0.9261 - Loss: 0.2023\n",
      "\n",
      "Batch 94/992 ━━━━━━━━━━━━━━━━━━━━ 04:57:05\n",
      "Accuracy: 0.9269 - Loss: 0.2007\n",
      "\n",
      "Batch 95/992 ━━━━━━━━━━━━━━━━━━━━ 04:57:15\n",
      "Accuracy: 0.9250 - Loss: 0.2024\n",
      "\n",
      "Batch 96/992 ━━━━━━━━━━━━━━━━━━━━ 04:57:26\n",
      "Accuracy: 0.9258 - Loss: 0.2010\n",
      "\n",
      "Batch 97/992 ━━━━━━━━━━━━━━━━━━━━ 04:57:36\n",
      "Accuracy: 0.9265 - Loss: 0.2000\n",
      "\n",
      "Batch 98/992 ━━━━━━━━━━━━━━━━━━━━ 04:57:46\n",
      "Accuracy: 0.9260 - Loss: 0.2027\n",
      "\n",
      "Batch 99/992 ━━━━━━━━━━━━━━━━━━━━ 04:57:56\n",
      "Accuracy: 0.9255 - Loss: 0.2030\n",
      "\n",
      "Batch 100/992 ━━━━━━━━━━━━━━━━━━━━ 04:58:06\n",
      "Accuracy: 0.9262 - Loss: 0.2016\n",
      "\n",
      "Batch 101/992 ━━━━━━━━━━━━━━━━━━━━ 04:58:17\n",
      "Accuracy: 0.9245 - Loss: 0.2039\n",
      "\n",
      "Batch 102/992 ━━━━━━━━━━━━━━━━━━━━ 04:58:27\n",
      "Accuracy: 0.9252 - Loss: 0.2021\n",
      "\n",
      "Batch 103/992 ━━━━━━━━━━━━━━━━━━━━ 04:58:37\n",
      "Accuracy: 0.9248 - Loss: 0.2026\n",
      "\n",
      "Batch 104/992 ━━━━━━━━━━━━━━━━━━━━ 04:58:47\n",
      "Accuracy: 0.9231 - Loss: 0.2045\n",
      "\n",
      "Batch 105/992 ━━━━━━━━━━━━━━━━━━━━ 04:58:57\n",
      "Accuracy: 0.9238 - Loss: 0.2036\n",
      "\n",
      "Batch 106/992 ━━━━━━━━━━━━━━━━━━━━ 04:59:07\n",
      "Accuracy: 0.9222 - Loss: 0.2071\n",
      "\n",
      "Batch 107/992 ━━━━━━━━━━━━━━━━━━━━ 04:59:17\n",
      "Accuracy: 0.9229 - Loss: 0.2074\n",
      "\n",
      "Batch 108/992 ━━━━━━━━━━━━━━━━━━━━ 04:59:27\n",
      "Accuracy: 0.9236 - Loss: 0.2076\n",
      "\n",
      "Batch 109/992 ━━━━━━━━━━━━━━━━━━━━ 04:59:38\n",
      "Accuracy: 0.9243 - Loss: 0.2070\n",
      "\n",
      "Batch 110/992 ━━━━━━━━━━━━━━━━━━━━ 04:59:48\n",
      "Accuracy: 0.9250 - Loss: 0.2051\n",
      "\n",
      "Batch 111/992 ━━━━━━━━━━━━━━━━━━━━ 04:59:58\n",
      "Accuracy: 0.9234 - Loss: 0.2070\n",
      "\n",
      "Batch 112/992 ━━━━━━━━━━━━━━━━━━━━ 05:00:08\n",
      "Accuracy: 0.9230 - Loss: 0.2084\n",
      "\n",
      "Batch 113/992 ━━━━━━━━━━━━━━━━━━━━ 05:00:18\n",
      "Accuracy: 0.9226 - Loss: 0.2090\n",
      "\n",
      "Batch 114/992 ━━━━━━━━━━━━━━━━━━━━ 05:00:29\n",
      "Accuracy: 0.9221 - Loss: 0.2086\n",
      "\n",
      "Batch 115/992 ━━━━━━━━━━━━━━━━━━━━ 05:00:39\n",
      "Accuracy: 0.9207 - Loss: 0.2096\n",
      "\n",
      "Batch 116/992 ━━━━━━━━━━━━━━━━━━━━ 05:00:49\n",
      "Accuracy: 0.9213 - Loss: 0.2086\n",
      "\n",
      "Batch 117/992 ━━━━━━━━━━━━━━━━━━━━ 05:00:59\n",
      "Accuracy: 0.9209 - Loss: 0.2094\n",
      "\n",
      "Batch 118/992 ━━━━━━━━━━━━━━━━━━━━ 05:01:09\n",
      "Accuracy: 0.9216 - Loss: 0.2082\n",
      "\n",
      "Batch 119/992 ━━━━━━━━━━━━━━━━━━━━ 05:01:19\n",
      "Accuracy: 0.9202 - Loss: 0.2116\n",
      "\n",
      "Batch 120/992 ━━━━━━━━━━━━━━━━━━━━ 05:01:29\n",
      "Accuracy: 0.9208 - Loss: 0.2110\n",
      "\n",
      "Batch 121/992 ━━━━━━━━━━━━━━━━━━━━ 05:01:39\n",
      "Accuracy: 0.9215 - Loss: 0.2095\n",
      "\n",
      "Batch 122/992 ━━━━━━━━━━━━━━━━━━━━ 05:01:50\n",
      "Accuracy: 0.9211 - Loss: 0.2108\n",
      "\n",
      "Batch 123/992 ━━━━━━━━━━━━━━━━━━━━ 05:02:00\n",
      "Accuracy: 0.9217 - Loss: 0.2101\n",
      "\n",
      "Batch 124/992 ━━━━━━━━━━━━━━━━━━━━ 05:02:10\n",
      "Accuracy: 0.9224 - Loss: 0.2091\n",
      "\n",
      "Batch 125/992 ━━━━━━━━━━━━━━━━━━━━ 05:02:20\n",
      "Accuracy: 0.9230 - Loss: 0.2084\n",
      "\n",
      "Batch 126/992 ━━━━━━━━━━━━━━━━━━━━ 05:02:30\n",
      "Accuracy: 0.9226 - Loss: 0.2126\n",
      "\n",
      "Batch 127/992 ━━━━━━━━━━━━━━━━━━━━ 05:02:40\n",
      "Accuracy: 0.9232 - Loss: 0.2122\n",
      "\n",
      "Batch 128/992 ━━━━━━━━━━━━━━━━━━━━ 05:02:50\n",
      "Accuracy: 0.9238 - Loss: 0.2114\n",
      "\n",
      "Batch 129/992 ━━━━━━━━━━━━━━━━━━━━ 05:03:00\n",
      "Accuracy: 0.9244 - Loss: 0.2109\n",
      "\n",
      "Batch 130/992 ━━━━━━━━━━━━━━━━━━━━ 05:03:10\n",
      "Accuracy: 0.9240 - Loss: 0.2126\n",
      "\n",
      "Batch 131/992 ━━━━━━━━━━━━━━━━━━━━ 05:03:20\n",
      "Accuracy: 0.9227 - Loss: 0.2140\n",
      "\n",
      "Batch 132/992 ━━━━━━━━━━━━━━━━━━━━ 05:03:30\n",
      "Accuracy: 0.9223 - Loss: 0.2137\n",
      "\n",
      "Batch 133/992 ━━━━━━━━━━━━━━━━━━━━ 05:03:40\n",
      "Accuracy: 0.9220 - Loss: 0.2161\n",
      "\n",
      "Batch 134/992 ━━━━━━━━━━━━━━━━━━━━ 05:03:51\n",
      "Accuracy: 0.9216 - Loss: 0.2167\n",
      "\n",
      "Batch 135/992 ━━━━━━━━━━━━━━━━━━━━ 05:04:01\n",
      "Accuracy: 0.9222 - Loss: 0.2157\n",
      "\n",
      "Batch 136/992 ━━━━━━━━━━━━━━━━━━━━ 05:04:11\n",
      "Accuracy: 0.9219 - Loss: 0.2159\n",
      "\n",
      "Batch 137/992 ━━━━━━━━━━━━━━━━━━━━ 05:04:21\n",
      "Accuracy: 0.9215 - Loss: 0.2169\n",
      "\n",
      "Batch 138/992 ━━━━━━━━━━━━━━━━━━━━ 05:04:31\n",
      "Accuracy: 0.9212 - Loss: 0.2167\n",
      "\n",
      "Batch 139/992 ━━━━━━━━━━━━━━━━━━━━ 05:04:41\n",
      "Accuracy: 0.9209 - Loss: 0.2162\n",
      "\n",
      "Batch 140/992 ━━━━━━━━━━━━━━━━━━━━ 05:04:51\n",
      "Accuracy: 0.9205 - Loss: 0.2171\n",
      "\n",
      "Batch 141/992 ━━━━━━━━━━━━━━━━━━━━ 05:05:01\n",
      "Accuracy: 0.9202 - Loss: 0.2192\n",
      "\n",
      "Batch 142/992 ━━━━━━━━━━━━━━━━━━━━ 05:05:11\n",
      "Accuracy: 0.9190 - Loss: 0.2219\n",
      "\n",
      "Batch 143/992 ━━━━━━━━━━━━━━━━━━━━ 05:05:21\n",
      "Accuracy: 0.9196 - Loss: 0.2208\n",
      "\n",
      "Batch 144/992 ━━━━━━━━━━━━━━━━━━━━ 05:05:32\n",
      "Accuracy: 0.9201 - Loss: 0.2197\n",
      "\n",
      "Batch 145/992 ━━━━━━━━━━━━━━━━━━━━ 05:05:42\n",
      "Accuracy: 0.9207 - Loss: 0.2183\n",
      "\n",
      "Batch 146/992 ━━━━━━━━━━━━━━━━━━━━ 05:05:52\n",
      "Accuracy: 0.9212 - Loss: 0.2171\n",
      "\n",
      "Batch 147/992 ━━━━━━━━━━━━━━━━━━━━ 05:06:02\n",
      "Accuracy: 0.9209 - Loss: 0.2167\n",
      "\n",
      "Batch 148/992 ━━━━━━━━━━━━━━━━━━━━ 05:06:12\n",
      "Accuracy: 0.9215 - Loss: 0.2157\n",
      "\n",
      "Batch 149/992 ━━━━━━━━━━━━━━━━━━━━ 05:06:22\n",
      "Accuracy: 0.9211 - Loss: 0.2155\n",
      "\n",
      "Batch 150/992 ━━━━━━━━━━━━━━━━━━━━ 05:06:32\n",
      "Accuracy: 0.9208 - Loss: 0.2159\n",
      "\n",
      "Batch 151/992 ━━━━━━━━━━━━━━━━━━━━ 05:06:42\n",
      "Accuracy: 0.9214 - Loss: 0.2147\n",
      "\n",
      "Batch 152/992 ━━━━━━━━━━━━━━━━━━━━ 05:06:52\n",
      "Accuracy: 0.9211 - Loss: 0.2148\n",
      "\n",
      "Batch 153/992 ━━━━━━━━━━━━━━━━━━━━ 05:07:02\n",
      "Accuracy: 0.9216 - Loss: 0.2142\n",
      "\n",
      "Batch 154/992 ━━━━━━━━━━━━━━━━━━━━ 05:07:12\n",
      "Accuracy: 0.9213 - Loss: 0.2145\n",
      "\n",
      "Batch 155/992 ━━━━━━━━━━━━━━━━━━━━ 05:07:23\n",
      "Accuracy: 0.9218 - Loss: 0.2140\n",
      "\n",
      "Batch 156/992 ━━━━━━━━━━━━━━━━━━━━ 05:07:33\n",
      "Accuracy: 0.9215 - Loss: 0.2155\n",
      "\n",
      "Batch 157/992 ━━━━━━━━━━━━━━━━━━━━ 05:07:43\n",
      "Accuracy: 0.9212 - Loss: 0.2163\n",
      "\n",
      "Batch 158/992 ━━━━━━━━━━━━━━━━━━━━ 05:07:53\n",
      "Accuracy: 0.9209 - Loss: 0.2190\n",
      "\n",
      "Batch 159/992 ━━━━━━━━━━━━━━━━━━━━ 05:08:03\n",
      "Accuracy: 0.9214 - Loss: 0.2180\n",
      "\n",
      "Batch 160/992 ━━━━━━━━━━━━━━━━━━━━ 05:08:13\n",
      "Accuracy: 0.9211 - Loss: 0.2190\n",
      "\n",
      "Batch 161/992 ━━━━━━━━━━━━━━━━━━━━ 05:08:23\n",
      "Accuracy: 0.9208 - Loss: 0.2191\n",
      "\n",
      "Batch 162/992 ━━━━━━━━━━━━━━━━━━━━ 05:08:33\n",
      "Accuracy: 0.9198 - Loss: 0.2207\n",
      "\n",
      "Batch 163/992 ━━━━━━━━━━━━━━━━━━━━ 05:08:43\n",
      "Accuracy: 0.9202 - Loss: 0.2202\n",
      "\n",
      "Batch 164/992 ━━━━━━━━━━━━━━━━━━━━ 05:08:53\n",
      "Accuracy: 0.9192 - Loss: 0.2210\n",
      "\n",
      "Batch 165/992 ━━━━━━━━━━━━━━━━━━━━ 05:09:03\n",
      "Accuracy: 0.9197 - Loss: 0.2206\n",
      "\n",
      "Batch 166/992 ━━━━━━━━━━━━━━━━━━━━ 05:09:13\n",
      "Accuracy: 0.9202 - Loss: 0.2196\n",
      "\n",
      "Batch 167/992 ━━━━━━━━━━━━━━━━━━━━ 05:09:23\n",
      "Accuracy: 0.9192 - Loss: 0.2209\n",
      "\n",
      "Batch 168/992 ━━━━━━━━━━━━━━━━━━━━ 05:09:33\n",
      "Accuracy: 0.9182 - Loss: 0.2224\n",
      "\n",
      "Batch 169/992 ━━━━━━━━━━━━━━━━━━━━ 05:09:44\n",
      "Accuracy: 0.9186 - Loss: 0.2218\n",
      "\n",
      "Batch 170/992 ━━━━━━━━━━━━━━━━━━━━ 05:09:54\n",
      "Accuracy: 0.9191 - Loss: 0.2207\n",
      "\n",
      "Batch 171/992 ━━━━━━━━━━━━━━━━━━━━ 05:10:04\n",
      "Accuracy: 0.9181 - Loss: 0.2242\n",
      "\n",
      "Batch 172/992 ━━━━━━━━━━━━━━━━━━━━ 05:10:14\n",
      "Accuracy: 0.9172 - Loss: 0.2254\n",
      "\n",
      "Batch 173/992 ━━━━━━━━━━━━━━━━━━━━ 05:10:25\n",
      "Accuracy: 0.9176 - Loss: 0.2251\n",
      "\n",
      "Batch 174/992 ━━━━━━━━━━━━━━━━━━━━ 05:10:35\n",
      "Accuracy: 0.9174 - Loss: 0.2263\n",
      "\n",
      "Batch 175/992 ━━━━━━━━━━━━━━━━━━━━ 05:10:45\n",
      "Accuracy: 0.9171 - Loss: 0.2266\n",
      "\n",
      "Batch 176/992 ━━━━━━━━━━━━━━━━━━━━ 05:10:55\n",
      "Accuracy: 0.9176 - Loss: 0.2261\n",
      "\n",
      "Batch 177/992 ━━━━━━━━━━━━━━━━━━━━ 05:11:05\n",
      "Accuracy: 0.9181 - Loss: 0.2264\n",
      "\n",
      "Batch 178/992 ━━━━━━━━━━━━━━━━━━━━ 05:11:15\n",
      "Accuracy: 0.9185 - Loss: 0.2260\n",
      "\n",
      "Batch 179/992 ━━━━━━━━━━━━━━━━━━━━ 05:11:25\n",
      "Accuracy: 0.9190 - Loss: 0.2253\n",
      "\n",
      "Batch 180/992 ━━━━━━━━━━━━━━━━━━━━ 05:11:35\n",
      "Accuracy: 0.9194 - Loss: 0.2244\n",
      "\n",
      "Batch 181/992 ━━━━━━━━━━━━━━━━━━━━ 05:11:46\n",
      "Accuracy: 0.9192 - Loss: 0.2260\n",
      "\n",
      "Batch 182/992 ━━━━━━━━━━━━━━━━━━━━ 05:11:56\n",
      "Accuracy: 0.9190 - Loss: 0.2263\n",
      "\n",
      "Batch 183/992 ━━━━━━━━━━━━━━━━━━━━ 05:12:07\n",
      "Accuracy: 0.9180 - Loss: 0.2269\n",
      "\n",
      "Batch 184/992 ━━━━━━━━━━━━━━━━━━━━ 05:12:17\n",
      "Accuracy: 0.9178 - Loss: 0.2276\n",
      "\n",
      "Batch 185/992 ━━━━━━━━━━━━━━━━━━━━ 05:12:27\n",
      "Accuracy: 0.9182 - Loss: 0.2268\n",
      "\n",
      "Batch 186/992 ━━━━━━━━━━━━━━━━━━━━ 05:12:37\n",
      "Accuracy: 0.9187 - Loss: 0.2259\n",
      "\n",
      "Batch 187/992 ━━━━━━━━━━━━━━━━━━━━ 05:12:47\n",
      "Accuracy: 0.9184 - Loss: 0.2275\n",
      "\n",
      "Batch 188/992 ━━━━━━━━━━━━━━━━━━━━ 05:12:57\n",
      "Accuracy: 0.9189 - Loss: 0.2266\n",
      "\n",
      "Batch 189/992 ━━━━━━━━━━━━━━━━━━━━ 05:13:07\n",
      "Accuracy: 0.9193 - Loss: 0.2258\n",
      "\n",
      "Batch 190/992 ━━━━━━━━━━━━━━━━━━━━ 05:13:17\n",
      "Accuracy: 0.9197 - Loss: 0.2249\n",
      "\n",
      "Batch 191/992 ━━━━━━━━━━━━━━━━━━━━ 05:13:27\n",
      "Accuracy: 0.9202 - Loss: 0.2246\n",
      "\n",
      "Batch 192/992 ━━━━━━━━━━━━━━━━━━━━ 05:13:37\n",
      "Accuracy: 0.9206 - Loss: 0.2235\n",
      "\n",
      "Batch 193/992 ━━━━━━━━━━━━━━━━━━━━ 05:13:47\n",
      "Accuracy: 0.9203 - Loss: 0.2236\n",
      "\n",
      "Batch 194/992 ━━━━━━━━━━━━━━━━━━━━ 05:13:58\n",
      "Accuracy: 0.9207 - Loss: 0.2232\n",
      "\n",
      "Batch 195/992 ━━━━━━━━━━━━━━━━━━━━ 05:14:08\n",
      "Accuracy: 0.9192 - Loss: 0.2272\n",
      "\n",
      "Batch 196/992 ━━━━━━━━━━━━━━━━━━━━ 05:14:18\n",
      "Accuracy: 0.9190 - Loss: 0.2269\n",
      "\n",
      "Batch 197/992 ━━━━━━━━━━━━━━━━━━━━ 05:14:28\n",
      "Accuracy: 0.9194 - Loss: 0.2261\n",
      "\n",
      "Batch 198/992 ━━━━━━━━━━━━━━━━━━━━ 05:14:38\n",
      "Accuracy: 0.9192 - Loss: 0.2270\n",
      "\n",
      "Batch 199/992 ━━━━━━━━━━━━━━━━━━━━ 05:14:48\n",
      "Accuracy: 0.9190 - Loss: 0.2267\n",
      "\n",
      "Batch 200/992 ━━━━━━━━━━━━━━━━━━━━ 05:14:58\n",
      "Accuracy: 0.9187 - Loss: 0.2268\n",
      "\n",
      "Batch 201/992 ━━━━━━━━━━━━━━━━━━━━ 05:15:08\n",
      "Accuracy: 0.9185 - Loss: 0.2265\n",
      "\n",
      "Batch 202/992 ━━━━━━━━━━━━━━━━━━━━ 05:15:19\n",
      "Accuracy: 0.9183 - Loss: 0.2263\n",
      "\n",
      "Batch 203/992 ━━━━━━━━━━━━━━━━━━━━ 05:15:29\n",
      "Accuracy: 0.9187 - Loss: 0.2255\n",
      "\n",
      "Batch 204/992 ━━━━━━━━━━━━━━━━━━━━ 05:15:39\n",
      "Accuracy: 0.9191 - Loss: 0.2248\n",
      "\n",
      "Batch 205/992 ━━━━━━━━━━━━━━━━━━━━ 05:15:49\n",
      "Accuracy: 0.9195 - Loss: 0.2243\n",
      "\n",
      "Batch 206/992 ━━━━━━━━━━━━━━━━━━━━ 05:15:59\n",
      "Accuracy: 0.9187 - Loss: 0.2252\n",
      "\n",
      "Batch 207/992 ━━━━━━━━━━━━━━━━━━━━ 05:16:09\n",
      "Accuracy: 0.9191 - Loss: 0.2248\n",
      "\n",
      "Batch 208/992 ━━━━━━━━━━━━━━━━━━━━ 05:16:19\n",
      "Accuracy: 0.9195 - Loss: 0.2239\n",
      "\n",
      "Batch 209/992 ━━━━━━━━━━━━━━━━━━━━ 05:16:30\n",
      "Accuracy: 0.9187 - Loss: 0.2259\n",
      "\n",
      "Batch 210/992 ━━━━━━━━━━━━━━━━━━━━ 05:16:40\n",
      "Accuracy: 0.9190 - Loss: 0.2255\n",
      "\n",
      "Batch 211/992 ━━━━━━━━━━━━━━━━━━━━ 05:16:50\n",
      "Accuracy: 0.9194 - Loss: 0.2256\n",
      "\n",
      "Batch 212/992 ━━━━━━━━━━━━━━━━━━━━ 05:17:00\n",
      "Accuracy: 0.9198 - Loss: 0.2247\n",
      "\n",
      "Batch 213/992 ━━━━━━━━━━━━━━━━━━━━ 05:17:10\n",
      "Accuracy: 0.9202 - Loss: 0.2244\n",
      "\n",
      "Batch 214/992 ━━━━━━━━━━━━━━━━━━━━ 05:17:20\n",
      "Accuracy: 0.9200 - Loss: 0.2251\n",
      "\n",
      "Batch 215/992 ━━━━━━━━━━━━━━━━━━━━ 05:17:30\n",
      "Accuracy: 0.9198 - Loss: 0.2250\n",
      "\n",
      "Batch 216/992 ━━━━━━━━━━━━━━━━━━━━ 05:17:40\n",
      "Accuracy: 0.9196 - Loss: 0.2258\n",
      "\n",
      "Batch 217/992 ━━━━━━━━━━━━━━━━━━━━ 05:17:50\n",
      "Accuracy: 0.9188 - Loss: 0.2288\n",
      "\n",
      "Batch 218/992 ━━━━━━━━━━━━━━━━━━━━ 05:18:00\n",
      "Accuracy: 0.9186 - Loss: 0.2284\n",
      "\n",
      "Batch 219/992 ━━━━━━━━━━━━━━━━━━━━ 05:18:10\n",
      "Accuracy: 0.9184 - Loss: 0.2288\n",
      "\n",
      "Batch 220/992 ━━━━━━━━━━━━━━━━━━━━ 05:18:20\n",
      "Accuracy: 0.9182 - Loss: 0.2288\n",
      "\n",
      "Batch 221/992 ━━━━━━━━━━━━━━━━━━━━ 05:18:30\n",
      "Accuracy: 0.9186 - Loss: 0.2286\n",
      "\n",
      "Batch 222/992 ━━━━━━━━━━━━━━━━━━━━ 05:18:40\n",
      "Accuracy: 0.9189 - Loss: 0.2277\n",
      "\n",
      "Batch 223/992 ━━━━━━━━━━━━━━━━━━━━ 05:18:50\n",
      "Accuracy: 0.9193 - Loss: 0.2269\n",
      "\n",
      "Batch 224/992 ━━━━━━━━━━━━━━━━━━━━ 05:19:01\n",
      "Accuracy: 0.9191 - Loss: 0.2272\n",
      "\n",
      "Batch 225/992 ━━━━━━━━━━━━━━━━━━━━ 05:19:11\n",
      "Accuracy: 0.9194 - Loss: 0.2271\n",
      "\n",
      "Batch 226/992 ━━━━━━━━━━━━━━━━━━━━ 05:19:21\n",
      "Accuracy: 0.9198 - Loss: 0.2266\n",
      "\n",
      "Batch 227/992 ━━━━━━━━━━━━━━━━━━━━ 05:19:31\n",
      "Accuracy: 0.9196 - Loss: 0.2265\n",
      "\n",
      "Batch 228/992 ━━━━━━━━━━━━━━━━━━━━ 05:19:41\n",
      "Accuracy: 0.9189 - Loss: 0.2288\n",
      "\n",
      "Batch 229/992 ━━━━━━━━━━━━━━━━━━━━ 05:19:51\n",
      "Accuracy: 0.9187 - Loss: 0.2300\n",
      "\n",
      "Batch 230/992 ━━━━━━━━━━━━━━━━━━━━ 05:20:02\n",
      "Accuracy: 0.9179 - Loss: 0.2304\n",
      "\n",
      "Batch 231/992 ━━━━━━━━━━━━━━━━━━━━ 05:20:12\n",
      "Accuracy: 0.9177 - Loss: 0.2303\n",
      "\n",
      "Batch 232/992 ━━━━━━━━━━━━━━━━━━━━ 05:20:22\n",
      "Accuracy: 0.9181 - Loss: 0.2293\n",
      "\n",
      "Batch 233/992 ━━━━━━━━━━━━━━━━━━━━ 05:20:32\n",
      "Accuracy: 0.9185 - Loss: 0.2286\n",
      "\n",
      "Batch 234/992 ━━━━━━━━━━━━━━━━━━━━ 05:20:42\n",
      "Accuracy: 0.9183 - Loss: 0.2285\n",
      "\n",
      "Batch 235/992 ━━━━━━━━━━━━━━━━━━━━ 05:20:52\n",
      "Accuracy: 0.9186 - Loss: 0.2278\n",
      "\n",
      "Batch 236/992 ━━━━━━━━━━━━━━━━━━━━ 05:21:02\n",
      "Accuracy: 0.9184 - Loss: 0.2278\n",
      "\n",
      "Batch 237/992 ━━━━━━━━━━━━━━━━━━━━ 05:21:13\n",
      "Accuracy: 0.9188 - Loss: 0.2269\n",
      "\n",
      "Batch 238/992 ━━━━━━━━━━━━━━━━━━━━ 05:21:23\n",
      "Accuracy: 0.9186 - Loss: 0.2272\n",
      "\n",
      "Batch 239/992 ━━━━━━━━━━━━━━━━━━━━ 05:21:33\n",
      "Accuracy: 0.9189 - Loss: 0.2268\n",
      "\n",
      "Batch 240/992 ━━━━━━━━━━━━━━━━━━━━ 05:21:43\n",
      "Accuracy: 0.9187 - Loss: 0.2271\n",
      "\n",
      "Batch 241/992 ━━━━━━━━━━━━━━━━━━━━ 05:21:53\n",
      "Accuracy: 0.9186 - Loss: 0.2270\n",
      "\n",
      "Batch 242/992 ━━━━━━━━━━━━━━━━━━━━ 05:22:03\n",
      "Accuracy: 0.9184 - Loss: 0.2277\n",
      "\n",
      "Batch 243/992 ━━━━━━━━━━━━━━━━━━━━ 05:22:14\n",
      "Accuracy: 0.9182 - Loss: 0.2273\n",
      "\n",
      "Batch 244/992 ━━━━━━━━━━━━━━━━━━━━ 05:22:24\n",
      "Accuracy: 0.9185 - Loss: 0.2265\n",
      "\n",
      "Batch 245/992 ━━━━━━━━━━━━━━━━━━━━ 05:22:34\n",
      "Accuracy: 0.9179 - Loss: 0.2286\n",
      "\n",
      "Batch 246/992 ━━━━━━━━━━━━━━━━━━━━ 05:22:44\n",
      "Accuracy: 0.9182 - Loss: 0.2279\n",
      "\n",
      "Batch 247/992 ━━━━━━━━━━━━━━━━━━━━ 05:22:54\n",
      "Accuracy: 0.9185 - Loss: 0.2275\n",
      "\n",
      "Batch 248/992 ━━━━━━━━━━━━━━━━━━━━ 05:23:04\n",
      "Accuracy: 0.9189 - Loss: 0.2267\n",
      "\n",
      "Batch 249/992 ━━━━━━━━━━━━━━━━━━━━ 05:23:15\n",
      "Accuracy: 0.9182 - Loss: 0.2294\n",
      "\n",
      "Batch 250/992 ━━━━━━━━━━━━━━━━━━━━ 05:23:25\n",
      "Accuracy: 0.9180 - Loss: 0.2298\n",
      "\n",
      "Batch 251/992 ━━━━━━━━━━━━━━━━━━━━ 05:23:35\n",
      "Accuracy: 0.9183 - Loss: 0.2292\n",
      "\n",
      "Batch 252/992 ━━━━━━━━━━━━━━━━━━━━ 05:23:45\n",
      "Accuracy: 0.9182 - Loss: 0.2292\n",
      "\n",
      "Batch 253/992 ━━━━━━━━━━━━━━━━━━━━ 05:23:55\n",
      "Accuracy: 0.9180 - Loss: 0.2309\n",
      "\n",
      "Batch 254/992 ━━━━━━━━━━━━━━━━━━━━ 05:24:05\n",
      "Accuracy: 0.9183 - Loss: 0.2301\n",
      "\n",
      "Batch 255/992 ━━━━━━━━━━━━━━━━━━━━ 05:24:15\n",
      "Accuracy: 0.9181 - Loss: 0.2298\n",
      "\n",
      "Batch 256/992 ━━━━━━━━━━━━━━━━━━━━ 05:24:25\n",
      "Accuracy: 0.9180 - Loss: 0.2297\n",
      "\n",
      "Batch 257/992 ━━━━━━━━━━━━━━━━━━━━ 05:24:35\n",
      "Accuracy: 0.9178 - Loss: 0.2313\n",
      "\n",
      "Batch 258/992 ━━━━━━━━━━━━━━━━━━━━ 05:24:45\n",
      "Accuracy: 0.9176 - Loss: 0.2310\n",
      "\n",
      "Batch 259/992 ━━━━━━━━━━━━━━━━━━━━ 05:24:55\n",
      "Accuracy: 0.9180 - Loss: 0.2301\n",
      "\n",
      "Batch 260/992 ━━━━━━━━━━━━━━━━━━━━ 05:25:06\n",
      "Accuracy: 0.9183 - Loss: 0.2296\n",
      "\n",
      "Batch 261/992 ━━━━━━━━━━━━━━━━━━━━ 05:25:16\n",
      "Accuracy: 0.9186 - Loss: 0.2292\n",
      "\n",
      "Batch 262/992 ━━━━━━━━━━━━━━━━━━━━ 05:25:26\n",
      "Accuracy: 0.9184 - Loss: 0.2292\n",
      "\n",
      "Batch 263/992 ━━━━━━━━━━━━━━━━━━━━ 05:25:37\n",
      "Accuracy: 0.9173 - Loss: 0.2308\n",
      "\n",
      "Batch 264/992 ━━━━━━━━━━━━━━━━━━━━ 05:25:47\n",
      "Accuracy: 0.9176 - Loss: 0.2301\n",
      "\n",
      "Batch 265/992 ━━━━━━━━━━━━━━━━━━━━ 05:25:57\n",
      "Accuracy: 0.9179 - Loss: 0.2306\n",
      "\n",
      "Batch 266/992 ━━━━━━━━━━━━━━━━━━━━ 05:26:07\n",
      "Accuracy: 0.9178 - Loss: 0.2306\n",
      "\n",
      "Batch 267/992 ━━━━━━━━━━━━━━━━━━━━ 05:26:17\n",
      "Accuracy: 0.9176 - Loss: 0.2304\n",
      "\n",
      "Batch 268/992 ━━━━━━━━━━━━━━━━━━━━ 05:26:27\n",
      "Accuracy: 0.9174 - Loss: 0.2308\n",
      "\n",
      "Batch 269/992 ━━━━━━━━━━━━━━━━━━━━ 05:26:37\n",
      "Accuracy: 0.9168 - Loss: 0.2332\n",
      "\n",
      "Batch 270/992 ━━━━━━━━━━━━━━━━━━━━ 05:26:47\n",
      "Accuracy: 0.9167 - Loss: 0.2333\n",
      "\n",
      "Batch 271/992 ━━━━━━━━━━━━━━━━━━━━ 05:26:57\n",
      "Accuracy: 0.9170 - Loss: 0.2326\n",
      "\n",
      "Batch 272/992 ━━━━━━━━━━━━━━━━━━━━ 05:27:07\n",
      "Accuracy: 0.9154 - Loss: 0.2358\n",
      "\n",
      "Batch 273/992 ━━━━━━━━━━━━━━━━━━━━ 05:27:17\n",
      "Accuracy: 0.9158 - Loss: 0.2354\n",
      "\n",
      "Batch 274/992 ━━━━━━━━━━━━━━━━━━━━ 05:27:28\n",
      "Accuracy: 0.9161 - Loss: 0.2348\n",
      "\n",
      "Batch 275/992 ━━━━━━━━━━━━━━━━━━━━ 05:27:38\n",
      "Accuracy: 0.9159 - Loss: 0.2350\n",
      "\n",
      "Batch 276/992 ━━━━━━━━━━━━━━━━━━━━ 05:27:48\n",
      "Accuracy: 0.9158 - Loss: 0.2359\n",
      "\n",
      "Batch 277/992 ━━━━━━━━━━━━━━━━━━━━ 05:27:58\n",
      "Accuracy: 0.9152 - Loss: 0.2367\n",
      "\n",
      "Batch 278/992 ━━━━━━━━━━━━━━━━━━━━ 05:28:08\n",
      "Accuracy: 0.9155 - Loss: 0.2365\n",
      "\n",
      "Batch 279/992 ━━━━━━━━━━━━━━━━━━━━ 05:28:18\n",
      "Accuracy: 0.9149 - Loss: 0.2383\n",
      "\n",
      "Batch 280/992 ━━━━━━━━━━━━━━━━━━━━ 05:28:28\n",
      "Accuracy: 0.9147 - Loss: 0.2383\n",
      "\n",
      "Batch 281/992 ━━━━━━━━━━━━━━━━━━━━ 05:28:38\n",
      "Accuracy: 0.9150 - Loss: 0.2377\n",
      "\n",
      "Batch 282/992 ━━━━━━━━━━━━━━━━━━━━ 05:28:48\n",
      "Accuracy: 0.9149 - Loss: 0.2375\n",
      "\n",
      "Batch 283/992 ━━━━━━━━━━━━━━━━━━━━ 05:28:59\n",
      "Accuracy: 0.9152 - Loss: 0.2371\n",
      "\n",
      "Batch 284/992 ━━━━━━━━━━━━━━━━━━━━ 05:29:09\n",
      "Accuracy: 0.9151 - Loss: 0.2370\n",
      "\n",
      "Batch 285/992 ━━━━━━━━━━━━━━━━━━━━ 05:29:19\n",
      "Accuracy: 0.9149 - Loss: 0.2376\n",
      "\n",
      "Batch 286/992 ━━━━━━━━━━━━━━━━━━━━ 05:29:29\n",
      "Accuracy: 0.9152 - Loss: 0.2373\n",
      "\n",
      "Batch 287/992 ━━━━━━━━━━━━━━━━━━━━ 05:29:39\n",
      "Accuracy: 0.9146 - Loss: 0.2389\n",
      "\n",
      "Batch 288/992 ━━━━━━━━━━━━━━━━━━━━ 05:29:49\n",
      "Accuracy: 0.9149 - Loss: 0.2383\n",
      "\n",
      "Batch 289/992 ━━━━━━━━━━━━━━━━━━━━ 05:29:59\n",
      "Accuracy: 0.9144 - Loss: 0.2388\n",
      "\n",
      "Batch 290/992 ━━━━━━━━━━━━━━━━━━━━ 05:30:09\n",
      "Accuracy: 0.9142 - Loss: 0.2395\n",
      "\n",
      "Batch 291/992 ━━━━━━━━━━━━━━━━━━━━ 05:30:20\n",
      "Accuracy: 0.9145 - Loss: 0.2388\n",
      "\n",
      "Batch 292/992 ━━━━━━━━━━━━━━━━━━━━ 05:30:30\n",
      "Accuracy: 0.9140 - Loss: 0.2417\n",
      "\n",
      "Batch 293/992 ━━━━━━━━━━━━━━━━━━━━ 05:30:40\n",
      "Accuracy: 0.9138 - Loss: 0.2420\n",
      "\n",
      "Batch 294/992 ━━━━━━━━━━━━━━━━━━━━ 05:30:50\n",
      "Accuracy: 0.9137 - Loss: 0.2423\n",
      "\n",
      "Batch 295/992 ━━━━━━━━━━━━━━━━━━━━ 05:31:00\n",
      "Accuracy: 0.9136 - Loss: 0.2419\n",
      "\n",
      "Batch 296/992 ━━━━━━━━━━━━━━━━━━━━ 05:31:10\n",
      "Accuracy: 0.9130 - Loss: 0.2423\n",
      "\n",
      "Batch 297/992 ━━━━━━━━━━━━━━━━━━━━ 05:31:20\n",
      "Accuracy: 0.9129 - Loss: 0.2423\n",
      "\n",
      "Batch 298/992 ━━━━━━━━━━━━━━━━━━━━ 05:31:30\n",
      "Accuracy: 0.9132 - Loss: 0.2420\n",
      "\n",
      "Batch 299/992 ━━━━━━━━━━━━━━━━━━━━ 05:31:40\n",
      "Accuracy: 0.9130 - Loss: 0.2420\n",
      "\n",
      "Batch 300/992 ━━━━━━━━━━━━━━━━━━━━ 05:31:50\n",
      "Accuracy: 0.9129 - Loss: 0.2417\n",
      "\n",
      "Batch 301/992 ━━━━━━━━━━━━━━━━━━━━ 05:32:00\n",
      "Accuracy: 0.9128 - Loss: 0.2421\n",
      "\n",
      "Batch 302/992 ━━━━━━━━━━━━━━━━━━━━ 05:32:11\n",
      "Accuracy: 0.9127 - Loss: 0.2427\n",
      "\n",
      "Batch 303/992 ━━━━━━━━━━━━━━━━━━━━ 05:32:21\n",
      "Accuracy: 0.9130 - Loss: 0.2425\n",
      "\n",
      "Batch 304/992 ━━━━━━━━━━━━━━━━━━━━ 05:32:31\n",
      "Accuracy: 0.9132 - Loss: 0.2419\n",
      "\n",
      "Batch 305/992 ━━━━━━━━━━━━━━━━━━━━ 05:32:41\n",
      "Accuracy: 0.9127 - Loss: 0.2424\n",
      "\n",
      "Batch 306/992 ━━━━━━━━━━━━━━━━━━━━ 05:32:51\n",
      "Accuracy: 0.9126 - Loss: 0.2423\n",
      "\n",
      "Batch 307/992 ━━━━━━━━━━━━━━━━━━━━ 05:33:01\n",
      "Accuracy: 0.9125 - Loss: 0.2421\n",
      "\n",
      "Batch 308/992 ━━━━━━━━━━━━━━━━━━━━ 05:33:11\n",
      "Accuracy: 0.9127 - Loss: 0.2417\n",
      "\n",
      "Batch 309/992 ━━━━━━━━━━━━━━━━━━━━ 05:33:21\n",
      "Accuracy: 0.9126 - Loss: 0.2415\n",
      "\n",
      "Batch 310/992 ━━━━━━━━━━━━━━━━━━━━ 05:33:31\n",
      "Accuracy: 0.9125 - Loss: 0.2421\n",
      "\n",
      "Batch 311/992 ━━━━━━━━━━━━━━━━━━━━ 05:33:41\n",
      "Accuracy: 0.9128 - Loss: 0.2419\n",
      "\n",
      "Batch 312/992 ━━━━━━━━━━━━━━━━━━━━ 05:33:51\n",
      "Accuracy: 0.9131 - Loss: 0.2414\n",
      "\n",
      "Batch 313/992 ━━━━━━━━━━━━━━━━━━━━ 05:34:01\n",
      "Accuracy: 0.9133 - Loss: 0.2407\n",
      "\n",
      "Batch 314/992 ━━━━━━━━━━━━━━━━━━━━ 05:34:12\n",
      "Accuracy: 0.9132 - Loss: 0.2405\n",
      "\n",
      "Batch 315/992 ━━━━━━━━━━━━━━━━━━━━ 05:34:22\n",
      "Accuracy: 0.9127 - Loss: 0.2416\n",
      "\n",
      "Batch 316/992 ━━━━━━━━━━━━━━━━━━━━ 05:34:32\n",
      "Accuracy: 0.9130 - Loss: 0.2414\n",
      "\n",
      "Batch 317/992 ━━━━━━━━━━━━━━━━━━━━ 05:34:42\n",
      "Accuracy: 0.9132 - Loss: 0.2409\n",
      "\n",
      "Batch 318/992 ━━━━━━━━━━━━━━━━━━━━ 05:34:52\n",
      "Accuracy: 0.9131 - Loss: 0.2409\n",
      "\n",
      "Batch 319/992 ━━━━━━━━━━━━━━━━━━━━ 05:35:02\n",
      "Accuracy: 0.9130 - Loss: 0.2412\n",
      "\n",
      "Batch 320/992 ━━━━━━━━━━━━━━━━━━━━ 05:35:12\n",
      "Accuracy: 0.9129 - Loss: 0.2415\n",
      "\n",
      "Batch 321/992 ━━━━━━━━━━━━━━━━━━━━ 05:35:23\n",
      "Accuracy: 0.9132 - Loss: 0.2412\n",
      "\n",
      "Batch 322/992 ━━━━━━━━━━━━━━━━━━━━ 05:35:33\n",
      "Accuracy: 0.9130 - Loss: 0.2423\n",
      "\n",
      "Batch 323/992 ━━━━━━━━━━━━━━━━━━━━ 05:35:43\n",
      "Accuracy: 0.9125 - Loss: 0.2439\n",
      "\n",
      "Batch 324/992 ━━━━━━━━━━━━━━━━━━━━ 05:35:53\n",
      "Accuracy: 0.9124 - Loss: 0.2447\n",
      "\n",
      "Batch 325/992 ━━━━━━━━━━━━━━━━━━━━ 05:36:03\n",
      "Accuracy: 0.9123 - Loss: 0.2447\n",
      "\n",
      "Batch 326/992 ━━━━━━━━━━━━━━━━━━━━ 05:36:13\n",
      "Accuracy: 0.9126 - Loss: 0.2441\n",
      "\n",
      "Batch 327/992 ━━━━━━━━━━━━━━━━━━━━ 05:36:24\n",
      "Accuracy: 0.9128 - Loss: 0.2441\n",
      "\n",
      "Batch 328/992 ━━━━━━━━━━━━━━━━━━━━ 05:36:34\n",
      "Accuracy: 0.9127 - Loss: 0.2454\n",
      "\n",
      "Batch 329/992 ━━━━━━━━━━━━━━━━━━━━ 05:36:44\n",
      "Accuracy: 0.9126 - Loss: 0.2460\n",
      "\n",
      "Batch 330/992 ━━━━━━━━━━━━━━━━━━━━ 05:36:54\n",
      "Accuracy: 0.9129 - Loss: 0.2458\n",
      "\n",
      "Batch 331/992 ━━━━━━━━━━━━━━━━━━━━ 05:37:04\n",
      "Accuracy: 0.9131 - Loss: 0.2454\n",
      "\n",
      "Batch 332/992 ━━━━━━━━━━━━━━━━━━━━ 05:37:14\n",
      "Accuracy: 0.9134 - Loss: 0.2450\n",
      "\n",
      "Batch 333/992 ━━━━━━━━━━━━━━━━━━━━ 05:37:25\n",
      "Accuracy: 0.9137 - Loss: 0.2445\n",
      "\n",
      "Batch 334/992 ━━━━━━━━━━━━━━━━━━━━ 05:37:35\n",
      "Accuracy: 0.9139 - Loss: 0.2440\n",
      "\n",
      "Batch 335/992 ━━━━━━━━━━━━━━━━━━━━ 05:37:45\n",
      "Accuracy: 0.9142 - Loss: 0.2437\n",
      "\n",
      "Batch 336/992 ━━━━━━━━━━━━━━━━━━━━ 05:37:55\n",
      "Accuracy: 0.9141 - Loss: 0.2443\n",
      "\n",
      "Batch 337/992 ━━━━━━━━━━━━━━━━━━━━ 05:38:05\n",
      "Accuracy: 0.9143 - Loss: 0.2439\n",
      "\n",
      "Batch 338/992 ━━━━━━━━━━━━━━━━━━━━ 05:38:15\n",
      "Accuracy: 0.9135 - Loss: 0.2458\n",
      "\n",
      "Batch 339/992 ━━━━━━━━━━━━━━━━━━━━ 05:38:25\n",
      "Accuracy: 0.9130 - Loss: 0.2466\n",
      "\n",
      "Batch 340/992 ━━━━━━━━━━━━━━━━━━━━ 05:38:35\n",
      "Accuracy: 0.9129 - Loss: 0.2466\n",
      "\n",
      "Batch 341/992 ━━━━━━━━━━━━━━━━━━━━ 05:38:46\n",
      "Accuracy: 0.9131 - Loss: 0.2459\n",
      "\n",
      "Batch 342/992 ━━━━━━━━━━━━━━━━━━━━ 05:38:56\n",
      "Accuracy: 0.9134 - Loss: 0.2453\n",
      "\n",
      "Batch 343/992 ━━━━━━━━━━━━━━━━━━━━ 05:39:06\n",
      "Accuracy: 0.9136 - Loss: 0.2447\n",
      "\n",
      "Batch 344/992 ━━━━━━━━━━━━━━━━━━━━ 05:39:16\n",
      "Accuracy: 0.9135 - Loss: 0.2447\n",
      "\n",
      "Batch 345/992 ━━━━━━━━━━━━━━━━━━━━ 05:39:27\n",
      "Accuracy: 0.9138 - Loss: 0.2442\n",
      "\n",
      "Batch 346/992 ━━━━━━━━━━━━━━━━━━━━ 05:39:37\n",
      "Accuracy: 0.9140 - Loss: 0.2440\n",
      "\n",
      "Batch 347/992 ━━━━━━━━━━━━━━━━━━━━ 05:39:47\n",
      "Accuracy: 0.9143 - Loss: 0.2433\n",
      "\n",
      "Batch 348/992 ━━━━━━━━━━━━━━━━━━━━ 05:39:57\n",
      "Accuracy: 0.9142 - Loss: 0.2435\n",
      "\n",
      "Batch 349/992 ━━━━━━━━━━━━━━━━━━━━ 05:40:08\n",
      "Accuracy: 0.9137 - Loss: 0.2438\n",
      "\n",
      "Batch 350/992 ━━━━━━━━━━━━━━━━━━━━ 05:40:18\n",
      "Accuracy: 0.9136 - Loss: 0.2441\n",
      "\n",
      "Batch 351/992 ━━━━━━━━━━━━━━━━━━━━ 05:40:28\n",
      "Accuracy: 0.9131 - Loss: 0.2446\n",
      "\n",
      "Batch 352/992 ━━━━━━━━━━━━━━━━━━━━ 05:40:38\n",
      "Accuracy: 0.9126 - Loss: 0.2475\n",
      "\n",
      "Batch 353/992 ━━━━━━━━━━━━━━━━━━━━ 05:40:48\n",
      "Accuracy: 0.9125 - Loss: 0.2483\n",
      "\n",
      "Batch 354/992 ━━━━━━━━━━━━━━━━━━━━ 05:40:58\n",
      "Accuracy: 0.9121 - Loss: 0.2493\n",
      "\n",
      "Batch 355/992 ━━━━━━━━━━━━━━━━━━━━ 05:41:08\n",
      "Accuracy: 0.9123 - Loss: 0.2487\n",
      "\n",
      "Batch 356/992 ━━━━━━━━━━━━━━━━━━━━ 05:41:18\n",
      "Accuracy: 0.9119 - Loss: 0.2489\n",
      "\n",
      "Batch 357/992 ━━━━━━━━━━━━━━━━━━━━ 05:41:29\n",
      "Accuracy: 0.9121 - Loss: 0.2485\n",
      "\n",
      "Batch 358/992 ━━━━━━━━━━━━━━━━━━━━ 05:41:39\n",
      "Accuracy: 0.9113 - Loss: 0.2496\n",
      "\n",
      "Batch 359/992 ━━━━━━━━━━━━━━━━━━━━ 05:41:49\n",
      "Accuracy: 0.9116 - Loss: 0.2490\n",
      "\n",
      "Batch 360/992 ━━━━━━━━━━━━━━━━━━━━ 05:42:00\n",
      "Accuracy: 0.9118 - Loss: 0.2485\n",
      "\n",
      "Batch 361/992 ━━━━━━━━━━━━━━━━━━━━ 05:42:10\n",
      "Accuracy: 0.9114 - Loss: 0.2485\n",
      "\n",
      "Batch 362/992 ━━━━━━━━━━━━━━━━━━━━ 05:42:20\n",
      "Accuracy: 0.9113 - Loss: 0.2485\n",
      "\n",
      "Batch 363/992 ━━━━━━━━━━━━━━━━━━━━ 05:42:30\n",
      "Accuracy: 0.9112 - Loss: 0.2482\n",
      "\n",
      "Batch 364/992 ━━━━━━━━━━━━━━━━━━━━ 05:42:41\n",
      "Accuracy: 0.9114 - Loss: 0.2479\n",
      "\n",
      "Batch 365/992 ━━━━━━━━━━━━━━━━━━━━ 05:42:51\n",
      "Accuracy: 0.9113 - Loss: 0.2479\n",
      "\n",
      "Batch 366/992 ━━━━━━━━━━━━━━━━━━━━ 05:43:01\n",
      "Accuracy: 0.9115 - Loss: 0.2475\n",
      "\n",
      "Batch 367/992 ━━━━━━━━━━━━━━━━━━━━ 05:43:11\n",
      "Accuracy: 0.9114 - Loss: 0.2480\n",
      "\n",
      "Batch 368/992 ━━━━━━━━━━━━━━━━━━━━ 05:43:21\n",
      "Accuracy: 0.9117 - Loss: 0.2474\n",
      "\n",
      "Batch 369/992 ━━━━━━━━━━━━━━━━━━━━ 05:43:31\n",
      "Accuracy: 0.9116 - Loss: 0.2475\n",
      "\n",
      "Batch 370/992 ━━━━━━━━━━━━━━━━━━━━ 05:43:41\n",
      "Accuracy: 0.9115 - Loss: 0.2475\n",
      "\n",
      "Batch 371/992 ━━━━━━━━━━━━━━━━━━━━ 05:43:52\n",
      "Accuracy: 0.9117 - Loss: 0.2470\n",
      "\n",
      "Batch 372/992 ━━━━━━━━━━━━━━━━━━━━ 05:44:02\n",
      "Accuracy: 0.9120 - Loss: 0.2464\n",
      "\n",
      "Batch 373/992 ━━━━━━━━━━━━━━━━━━━━ 05:44:12\n",
      "Accuracy: 0.9122 - Loss: 0.2460\n",
      "\n",
      "Batch 374/992 ━━━━━━━━━━━━━━━━━━━━ 05:44:22\n",
      "Accuracy: 0.9121 - Loss: 0.2465\n",
      "\n",
      "Batch 375/992 ━━━━━━━━━━━━━━━━━━━━ 05:44:32\n",
      "Accuracy: 0.9123 - Loss: 0.2459\n",
      "\n",
      "Batch 376/992 ━━━━━━━━━━━━━━━━━━━━ 05:44:42\n",
      "Accuracy: 0.9122 - Loss: 0.2457\n",
      "\n",
      "Batch 377/992 ━━━━━━━━━━━━━━━━━━━━ 05:44:53\n",
      "Accuracy: 0.9121 - Loss: 0.2460\n",
      "\n",
      "Batch 378/992 ━━━━━━━━━━━━━━━━━━━━ 05:45:03\n",
      "Accuracy: 0.9120 - Loss: 0.2461\n",
      "\n",
      "Batch 379/992 ━━━━━━━━━━━━━━━━━━━━ 05:45:13\n",
      "Accuracy: 0.9119 - Loss: 0.2461\n",
      "\n",
      "Batch 380/992 ━━━━━━━━━━━━━━━━━━━━ 05:45:23\n",
      "Accuracy: 0.9122 - Loss: 0.2457\n",
      "\n",
      "Batch 381/992 ━━━━━━━━━━━━━━━━━━━━ 05:45:33\n",
      "Accuracy: 0.9124 - Loss: 0.2453\n",
      "\n",
      "Batch 382/992 ━━━━━━━━━━━━━━━━━━━━ 05:45:43\n",
      "Accuracy: 0.9123 - Loss: 0.2451\n",
      "\n",
      "Batch 383/992 ━━━━━━━━━━━━━━━━━━━━ 05:45:54\n",
      "Accuracy: 0.9125 - Loss: 0.2446\n",
      "\n",
      "Batch 384/992 ━━━━━━━━━━━━━━━━━━━━ 05:46:04\n",
      "Accuracy: 0.9121 - Loss: 0.2453\n",
      "\n",
      "Batch 385/992 ━━━━━━━━━━━━━━━━━━━━ 05:46:14\n",
      "Accuracy: 0.9120 - Loss: 0.2467\n",
      "\n",
      "Batch 386/992 ━━━━━━━━━━━━━━━━━━━━ 05:46:24\n",
      "Accuracy: 0.9119 - Loss: 0.2474\n",
      "\n",
      "Batch 387/992 ━━━━━━━━━━━━━━━━━━━━ 05:46:35\n",
      "Accuracy: 0.9118 - Loss: 0.2474\n",
      "\n",
      "Batch 388/992 ━━━━━━━━━━━━━━━━━━━━ 05:46:45\n",
      "Accuracy: 0.9114 - Loss: 0.2481\n",
      "\n",
      "Batch 389/992 ━━━━━━━━━━━━━━━━━━━━ 05:46:55\n",
      "Accuracy: 0.9116 - Loss: 0.2479\n",
      "\n",
      "Batch 390/992 ━━━━━━━━━━━━━━━━━━━━ 05:47:05\n",
      "Accuracy: 0.9115 - Loss: 0.2483\n",
      "\n",
      "Batch 391/992 ━━━━━━━━━━━━━━━━━━━━ 05:47:15\n",
      "Accuracy: 0.9118 - Loss: 0.2480\n",
      "\n",
      "Batch 392/992 ━━━━━━━━━━━━━━━━━━━━ 05:47:25\n",
      "Accuracy: 0.9120 - Loss: 0.2477\n",
      "\n",
      "Batch 393/992 ━━━━━━━━━━━━━━━━━━━━ 05:47:35\n",
      "Accuracy: 0.9119 - Loss: 0.2491\n",
      "\n",
      "Batch 394/992 ━━━━━━━━━━━━━━━━━━━━ 05:47:45\n",
      "Accuracy: 0.9115 - Loss: 0.2505\n",
      "\n",
      "Batch 395/992 ━━━━━━━━━━━━━━━━━━━━ 05:47:56\n",
      "Accuracy: 0.9117 - Loss: 0.2505\n",
      "\n",
      "Batch 396/992 ━━━━━━━━━━━━━━━━━━━━ 05:48:06\n",
      "Accuracy: 0.9113 - Loss: 0.2508\n",
      "\n",
      "Batch 397/992 ━━━━━━━━━━━━━━━━━━━━ 05:48:16\n",
      "Accuracy: 0.9109 - Loss: 0.2512\n",
      "\n",
      "Batch 398/992 ━━━━━━━━━━━━━━━━━━━━ 05:48:27\n",
      "Accuracy: 0.9111 - Loss: 0.2510\n",
      "\n",
      "Batch 399/992 ━━━━━━━━━━━━━━━━━━━━ 05:48:37\n",
      "Accuracy: 0.9113 - Loss: 0.2507\n",
      "\n",
      "Batch 400/992 ━━━━━━━━━━━━━━━━━━━━ 05:48:47\n",
      "Accuracy: 0.9112 - Loss: 0.2504\n",
      "\n",
      "Batch 401/992 ━━━━━━━━━━━━━━━━━━━━ 05:48:57\n",
      "Accuracy: 0.9115 - Loss: 0.2500\n",
      "\n",
      "Batch 402/992 ━━━━━━━━━━━━━━━━━━━━ 05:49:07\n",
      "Accuracy: 0.9117 - Loss: 0.2495\n",
      "\n",
      "Batch 403/992 ━━━━━━━━━━━━━━━━━━━━ 05:49:17\n",
      "Accuracy: 0.9119 - Loss: 0.2489\n",
      "\n",
      "Batch 404/992 ━━━━━━━━━━━━━━━━━━━━ 05:49:27\n",
      "Accuracy: 0.9118 - Loss: 0.2489\n",
      "\n",
      "Batch 405/992 ━━━━━━━━━━━━━━━━━━━━ 05:49:37\n",
      "Accuracy: 0.9120 - Loss: 0.2484\n",
      "\n",
      "Batch 406/992 ━━━━━━━━━━━━━━━━━━━━ 05:49:47\n",
      "Accuracy: 0.9123 - Loss: 0.2480\n",
      "\n",
      "Batch 407/992 ━━━━━━━━━━━━━━━━━━━━ 05:49:58\n",
      "Accuracy: 0.9122 - Loss: 0.2480\n",
      "\n",
      "Batch 408/992 ━━━━━━━━━━━━━━━━━━━━ 05:50:08\n",
      "Accuracy: 0.9121 - Loss: 0.2483\n",
      "\n",
      "Batch 409/992 ━━━━━━━━━━━━━━━━━━━━ 05:50:18\n",
      "Accuracy: 0.9123 - Loss: 0.2483\n",
      "\n",
      "Batch 410/992 ━━━━━━━━━━━━━━━━━━━━ 05:50:28\n",
      "Accuracy: 0.9119 - Loss: 0.2493\n",
      "\n",
      "Batch 411/992 ━━━━━━━━━━━━━━━━━━━━ 05:50:38\n",
      "Accuracy: 0.9121 - Loss: 0.2489\n",
      "\n",
      "Batch 412/992 ━━━━━━━━━━━━━━━━━━━━ 05:50:49\n",
      "Accuracy: 0.9123 - Loss: 0.2487\n",
      "\n",
      "Batch 413/992 ━━━━━━━━━━━━━━━━━━━━ 05:50:59\n",
      "Accuracy: 0.9119 - Loss: 0.2496\n",
      "\n",
      "Batch 414/992 ━━━━━━━━━━━━━━━━━━━━ 05:51:09\n",
      "Accuracy: 0.9118 - Loss: 0.2499\n",
      "\n",
      "Batch 415/992 ━━━━━━━━━━━━━━━━━━━━ 05:51:19\n",
      "Accuracy: 0.9117 - Loss: 0.2502\n",
      "\n",
      "Batch 416/992 ━━━━━━━━━━━━━━━━━━━━ 05:51:30\n",
      "Accuracy: 0.9120 - Loss: 0.2499\n",
      "\n",
      "Batch 417/992 ━━━━━━━━━━━━━━━━━━━━ 05:51:40\n",
      "Accuracy: 0.9119 - Loss: 0.2497\n",
      "\n",
      "Batch 418/992 ━━━━━━━━━━━━━━━━━━━━ 05:51:51\n",
      "Accuracy: 0.9118 - Loss: 0.2498\n",
      "\n",
      "Batch 419/992 ━━━━━━━━━━━━━━━━━━━━ 05:52:01\n",
      "Accuracy: 0.9117 - Loss: 0.2496\n",
      "\n",
      "Batch 420/992 ━━━━━━━━━━━━━━━━━━━━ 05:52:11\n",
      "Accuracy: 0.9116 - Loss: 0.2497\n",
      "\n",
      "Batch 421/992 ━━━━━━━━━━━━━━━━━━━━ 05:52:21\n",
      "Accuracy: 0.9109 - Loss: 0.2530\n",
      "\n",
      "Batch 422/992 ━━━━━━━━━━━━━━━━━━━━ 05:52:32\n",
      "Accuracy: 0.9108 - Loss: 0.2537\n",
      "\n",
      "Batch 423/992 ━━━━━━━━━━━━━━━━━━━━ 05:52:42\n",
      "Accuracy: 0.9111 - Loss: 0.2534\n",
      "\n",
      "Batch 424/992 ━━━━━━━━━━━━━━━━━━━━ 05:52:52\n",
      "Accuracy: 0.9113 - Loss: 0.2529\n",
      "\n",
      "Batch 425/992 ━━━━━━━━━━━━━━━━━━━━ 05:53:02\n",
      "Accuracy: 0.9109 - Loss: 0.2539\n",
      "\n",
      "Batch 426/992 ━━━━━━━━━━━━━━━━━━━━ 05:53:12\n",
      "Accuracy: 0.9108 - Loss: 0.2544\n",
      "\n",
      "Batch 427/992 ━━━━━━━━━━━━━━━━━━━━ 05:53:23\n",
      "Accuracy: 0.9107 - Loss: 0.2544\n",
      "\n",
      "Batch 428/992 ━━━━━━━━━━━━━━━━━━━━ 05:53:33\n",
      "Accuracy: 0.9106 - Loss: 0.2550\n",
      "\n",
      "Batch 429/992 ━━━━━━━━━━━━━━━━━━━━ 05:53:43\n",
      "Accuracy: 0.9108 - Loss: 0.2546\n",
      "\n",
      "Batch 430/992 ━━━━━━━━━━━━━━━━━━━━ 05:53:53\n",
      "Accuracy: 0.9108 - Loss: 0.2548\n",
      "\n",
      "Batch 431/992 ━━━━━━━━━━━━━━━━━━━━ 05:54:03\n",
      "Accuracy: 0.9107 - Loss: 0.2551\n",
      "\n",
      "Batch 432/992 ━━━━━━━━━━━━━━━━━━━━ 05:54:14\n",
      "Accuracy: 0.9109 - Loss: 0.2546\n",
      "\n",
      "Batch 433/992 ━━━━━━━━━━━━━━━━━━━━ 05:54:24\n",
      "Accuracy: 0.9111 - Loss: 0.2543\n",
      "\n",
      "Batch 434/992 ━━━━━━━━━━━━━━━━━━━━ 05:54:34\n",
      "Accuracy: 0.9110 - Loss: 0.2541\n",
      "\n",
      "Batch 435/992 ━━━━━━━━━━━━━━━━━━━━ 05:54:44\n",
      "Accuracy: 0.9109 - Loss: 0.2540\n",
      "\n",
      "Batch 436/992 ━━━━━━━━━━━━━━━━━━━━ 05:54:54\n",
      "Accuracy: 0.9108 - Loss: 0.2540\n",
      "\n",
      "Batch 437/992 ━━━━━━━━━━━━━━━━━━━━ 05:55:05\n",
      "Accuracy: 0.9110 - Loss: 0.2536\n",
      "\n",
      "Batch 438/992 ━━━━━━━━━━━━━━━━━━━━ 05:55:15\n",
      "Accuracy: 0.9112 - Loss: 0.2531\n",
      "\n",
      "Batch 439/992 ━━━━━━━━━━━━━━━━━━━━ 05:55:25\n",
      "Accuracy: 0.9114 - Loss: 0.2528\n",
      "\n",
      "Batch 440/992 ━━━━━━━━━━━━━━━━━━━━ 05:55:35\n",
      "Accuracy: 0.9114 - Loss: 0.2530\n",
      "\n",
      "Batch 441/992 ━━━━━━━━━━━━━━━━━━━━ 05:55:45\n",
      "Accuracy: 0.9113 - Loss: 0.2537\n",
      "\n",
      "Batch 442/992 ━━━━━━━━━━━━━━━━━━━━ 05:55:55\n",
      "Accuracy: 0.9115 - Loss: 0.2536\n",
      "\n",
      "Batch 443/992 ━━━━━━━━━━━━━━━━━━━━ 05:56:06\n",
      "Accuracy: 0.9111 - Loss: 0.2542\n",
      "\n",
      "Batch 444/992 ━━━━━━━━━━━━━━━━━━━━ 05:56:16\n",
      "Accuracy: 0.9113 - Loss: 0.2539\n",
      "\n",
      "Batch 445/992 ━━━━━━━━━━━━━━━━━━━━ 05:56:26\n",
      "Accuracy: 0.9110 - Loss: 0.2546\n",
      "\n",
      "Batch 446/992 ━━━━━━━━━━━━━━━━━━━━ 05:56:36\n",
      "Accuracy: 0.9109 - Loss: 0.2547\n",
      "\n",
      "Batch 447/992 ━━━━━━━━━━━━━━━━━━━━ 05:56:46\n",
      "Accuracy: 0.9108 - Loss: 0.2549\n",
      "\n",
      "Batch 448/992 ━━━━━━━━━━━━━━━━━━━━ 05:56:56\n",
      "Accuracy: 0.9107 - Loss: 0.2548\n",
      "\n",
      "Batch 449/992 ━━━━━━━━━━━━━━━━━━━━ 05:57:06\n",
      "Accuracy: 0.9109 - Loss: 0.2544\n",
      "\n",
      "Batch 450/992 ━━━━━━━━━━━━━━━━━━━━ 05:57:16\n",
      "Accuracy: 0.9111 - Loss: 0.2541\n",
      "\n",
      "Batch 451/992 ━━━━━━━━━━━━━━━━━━━━ 05:57:27\n",
      "Accuracy: 0.9113 - Loss: 0.2538\n",
      "\n",
      "Batch 452/992 ━━━━━━━━━━━━━━━━━━━━ 05:57:37\n",
      "Accuracy: 0.9115 - Loss: 0.2535\n",
      "\n",
      "Batch 453/992 ━━━━━━━━━━━━━━━━━━━━ 05:57:48\n",
      "Accuracy: 0.9114 - Loss: 0.2537\n",
      "\n",
      "Batch 454/992 ━━━━━━━━━━━━━━━━━━━━ 05:57:58\n",
      "Accuracy: 0.9113 - Loss: 0.2538\n",
      "\n",
      "Batch 455/992 ━━━━━━━━━━━━━━━━━━━━ 05:58:08\n",
      "Accuracy: 0.9113 - Loss: 0.2538\n",
      "\n",
      "Batch 456/992 ━━━━━━━━━━━━━━━━━━━━ 05:58:18\n",
      "Accuracy: 0.9109 - Loss: 0.2541\n",
      "\n",
      "Batch 457/992 ━━━━━━━━━━━━━━━━━━━━ 05:58:29\n",
      "Accuracy: 0.9108 - Loss: 0.2540\n",
      "\n",
      "Batch 458/992 ━━━━━━━━━━━━━━━━━━━━ 05:58:39\n",
      "Accuracy: 0.9110 - Loss: 0.2537\n",
      "\n",
      "Batch 459/992 ━━━━━━━━━━━━━━━━━━━━ 05:58:49\n",
      "Accuracy: 0.9107 - Loss: 0.2542\n",
      "\n",
      "Batch 460/992 ━━━━━━━━━━━━━━━━━━━━ 05:58:59\n",
      "Accuracy: 0.9106 - Loss: 0.2544\n",
      "\n",
      "Batch 461/992 ━━━━━━━━━━━━━━━━━━━━ 05:59:09\n",
      "Accuracy: 0.9108 - Loss: 0.2540\n",
      "\n",
      "Batch 462/992 ━━━━━━━━━━━━━━━━━━━━ 05:59:19\n",
      "Accuracy: 0.9107 - Loss: 0.2538\n",
      "\n",
      "Batch 463/992 ━━━━━━━━━━━━━━━━━━━━ 05:59:30\n",
      "Accuracy: 0.9104 - Loss: 0.2548\n",
      "\n",
      "Batch 464/992 ━━━━━━━━━━━━━━━━━━━━ 05:59:40\n",
      "Accuracy: 0.9100 - Loss: 0.2555\n",
      "\n",
      "Batch 465/992 ━━━━━━━━━━━━━━━━━━━━ 05:59:50\n",
      "Accuracy: 0.9097 - Loss: 0.2559\n",
      "\n",
      "Batch 466/992 ━━━━━━━━━━━━━━━━━━━━ 06:00:00\n",
      "Accuracy: 0.9096 - Loss: 0.2563\n",
      "\n",
      "Batch 467/992 ━━━━━━━━━━━━━━━━━━━━ 06:00:11\n",
      "Accuracy: 0.9098 - Loss: 0.2559\n",
      "\n",
      "Batch 468/992 ━━━━━━━━━━━━━━━━━━━━ 06:00:21\n",
      "Accuracy: 0.9097 - Loss: 0.2559\n",
      "\n",
      "Batch 469/992 ━━━━━━━━━━━━━━━━━━━━ 06:00:31\n",
      "Accuracy: 0.9099 - Loss: 0.2554\n",
      "\n",
      "Batch 470/992 ━━━━━━━━━━━━━━━━━━━━ 06:00:41\n",
      "Accuracy: 0.9101 - Loss: 0.2552\n",
      "\n",
      "Batch 471/992 ━━━━━━━━━━━━━━━━━━━━ 06:00:52\n",
      "Accuracy: 0.9103 - Loss: 0.2547\n",
      "\n",
      "Batch 472/992 ━━━━━━━━━━━━━━━━━━━━ 06:01:02\n",
      "Accuracy: 0.9105 - Loss: 0.2542\n",
      "\n",
      "Batch 473/992 ━━━━━━━━━━━━━━━━━━━━ 06:01:12\n",
      "Accuracy: 0.9104 - Loss: 0.2541\n",
      "\n",
      "Batch 474/992 ━━━━━━━━━━━━━━━━━━━━ 06:01:22\n",
      "Accuracy: 0.9103 - Loss: 0.2545\n",
      "\n",
      "Batch 475/992 ━━━━━━━━━━━━━━━━━━━━ 06:01:33\n",
      "Accuracy: 0.9105 - Loss: 0.2544\n",
      "\n",
      "Batch 476/992 ━━━━━━━━━━━━━━━━━━━━ 06:01:43\n",
      "Accuracy: 0.9105 - Loss: 0.2542\n",
      "\n",
      "Batch 477/992 ━━━━━━━━━━━━━━━━━━━━ 06:01:53\n",
      "Accuracy: 0.9104 - Loss: 0.2542\n",
      "\n",
      "Batch 478/992 ━━━━━━━━━━━━━━━━━━━━ 06:02:03\n",
      "Accuracy: 0.9103 - Loss: 0.2545\n",
      "\n",
      "Batch 479/992 ━━━━━━━━━━━━━━━━━━━━ 06:02:13\n",
      "Accuracy: 0.9105 - Loss: 0.2542\n",
      "\n",
      "Batch 480/992 ━━━━━━━━━━━━━━━━━━━━ 06:02:23\n",
      "Accuracy: 0.9104 - Loss: 0.2543\n",
      "\n",
      "Batch 481/992 ━━━━━━━━━━━━━━━━━━━━ 06:02:34\n",
      "Accuracy: 0.9103 - Loss: 0.2544\n",
      "\n",
      "Batch 482/992 ━━━━━━━━━━━━━━━━━━━━ 06:02:44\n",
      "Accuracy: 0.9103 - Loss: 0.2544\n",
      "\n",
      "Batch 483/992 ━━━━━━━━━━━━━━━━━━━━ 06:02:54\n",
      "Accuracy: 0.9102 - Loss: 0.2544\n",
      "\n",
      "Batch 484/992 ━━━━━━━━━━━━━━━━━━━━ 06:03:04\n",
      "Accuracy: 0.9101 - Loss: 0.2545\n",
      "\n",
      "Batch 485/992 ━━━━━━━━━━━━━━━━━━━━ 06:03:14\n",
      "Accuracy: 0.9103 - Loss: 0.2543\n",
      "\n",
      "Batch 486/992 ━━━━━━━━━━━━━━━━━━━━ 06:03:24\n",
      "Accuracy: 0.9105 - Loss: 0.2541\n",
      "\n",
      "Batch 487/992 ━━━━━━━━━━━━━━━━━━━━ 06:03:35\n",
      "Accuracy: 0.9107 - Loss: 0.2540\n",
      "\n",
      "Batch 488/992 ━━━━━━━━━━━━━━━━━━━━ 06:03:45\n",
      "Accuracy: 0.9109 - Loss: 0.2536\n",
      "\n",
      "Batch 489/992 ━━━━━━━━━━━━━━━━━━━━ 06:03:55\n",
      "Accuracy: 0.9108 - Loss: 0.2534\n",
      "\n",
      "Batch 490/992 ━━━━━━━━━━━━━━━━━━━━ 06:04:05\n",
      "Accuracy: 0.9107 - Loss: 0.2536\n",
      "\n",
      "Batch 491/992 ━━━━━━━━━━━━━━━━━━━━ 06:04:15\n",
      "Accuracy: 0.9106 - Loss: 0.2538\n",
      "\n",
      "Batch 492/992 ━━━━━━━━━━━━━━━━━━━━ 06:04:25\n",
      "Accuracy: 0.9108 - Loss: 0.2534\n",
      "\n",
      "Batch 493/992 ━━━━━━━━━━━━━━━━━━━━ 06:04:36\n",
      "Accuracy: 0.9108 - Loss: 0.2537\n",
      "\n",
      "Batch 494/992 ━━━━━━━━━━━━━━━━━━━━ 06:04:46\n",
      "Accuracy: 0.9107 - Loss: 0.2537\n",
      "\n",
      "Batch 495/992 ━━━━━━━━━━━━━━━━━━━━ 06:04:56\n",
      "Accuracy: 0.9104 - Loss: 0.2544\n",
      "\n",
      "Batch 496/992 ━━━━━━━━━━━━━━━━━━━━ 06:05:06\n",
      "Accuracy: 0.9105 - Loss: 0.2542\n",
      "\n",
      "Batch 497/992 ━━━━━━━━━━━━━━━━━━━━ 06:05:16\n",
      "Accuracy: 0.9107 - Loss: 0.2539\n",
      "\n",
      "Batch 498/992 ━━━━━━━━━━━━━━━━━━━━ 06:05:27\n",
      "Accuracy: 0.9109 - Loss: 0.2537\n",
      "\n",
      "Batch 499/992 ━━━━━━━━━━━━━━━━━━━━ 06:05:37\n",
      "Accuracy: 0.9108 - Loss: 0.2537\n",
      "\n",
      "Batch 500/992 ━━━━━━━━━━━━━━━━━━━━ 06:05:47\n",
      "Accuracy: 0.9110 - Loss: 0.2532\n",
      "\n",
      "Batch 501/992 ━━━━━━━━━━━━━━━━━━━━ 06:05:57\n",
      "Accuracy: 0.9107 - Loss: 0.2543\n",
      "\n",
      "Batch 502/992 ━━━━━━━━━━━━━━━━━━━━ 06:06:07\n",
      "Accuracy: 0.9109 - Loss: 0.2539\n",
      "\n",
      "Batch 503/992 ━━━━━━━━━━━━━━━━━━━━ 06:06:17\n",
      "Accuracy: 0.9105 - Loss: 0.2546\n",
      "\n",
      "Batch 504/992 ━━━━━━━━━━━━━━━━━━━━ 06:06:27\n",
      "Accuracy: 0.9105 - Loss: 0.2555\n",
      "\n",
      "Batch 505/992 ━━━━━━━━━━━━━━━━━━━━ 06:06:38\n",
      "Accuracy: 0.9106 - Loss: 0.2552\n",
      "\n",
      "Batch 506/992 ━━━━━━━━━━━━━━━━━━━━ 06:06:48\n",
      "Accuracy: 0.9106 - Loss: 0.2552\n",
      "\n",
      "Batch 507/992 ━━━━━━━━━━━━━━━━━━━━ 06:06:58\n",
      "Accuracy: 0.9107 - Loss: 0.2547\n",
      "\n",
      "Batch 508/992 ━━━━━━━━━━━━━━━━━━━━ 06:07:08\n",
      "Accuracy: 0.9107 - Loss: 0.2547\n",
      "\n",
      "Batch 509/992 ━━━━━━━━━━━━━━━━━━━━ 06:07:18\n",
      "Accuracy: 0.9104 - Loss: 0.2551\n",
      "\n",
      "Batch 510/992 ━━━━━━━━━━━━━━━━━━━━ 06:07:28\n",
      "Accuracy: 0.9105 - Loss: 0.2549\n",
      "\n",
      "Batch 511/992 ━━━━━━━━━━━━━━━━━━━━ 06:07:38\n",
      "Accuracy: 0.9105 - Loss: 0.2547\n",
      "\n",
      "Batch 512/992 ━━━━━━━━━━━━━━━━━━━━ 06:07:48\n",
      "Accuracy: 0.9104 - Loss: 0.2547\n",
      "\n",
      "Batch 513/992 ━━━━━━━━━━━━━━━━━━━━ 06:07:59\n",
      "Accuracy: 0.9106 - Loss: 0.2544\n",
      "\n",
      "Batch 514/992 ━━━━━━━━━━━━━━━━━━━━ 06:08:09\n",
      "Accuracy: 0.9107 - Loss: 0.2542\n",
      "\n",
      "Batch 515/992 ━━━━━━━━━━━━━━━━━━━━ 06:08:19\n",
      "Accuracy: 0.9109 - Loss: 0.2538\n",
      "\n",
      "Batch 516/992 ━━━━━━━━━━━━━━━━━━━━ 06:08:29\n",
      "Accuracy: 0.9106 - Loss: 0.2545\n",
      "\n",
      "Batch 517/992 ━━━━━━━━━━━━━━━━━━━━ 06:08:39\n",
      "Accuracy: 0.9103 - Loss: 0.2551\n",
      "\n",
      "Batch 518/992 ━━━━━━━━━━━━━━━━━━━━ 06:08:49\n",
      "Accuracy: 0.9105 - Loss: 0.2547\n",
      "\n",
      "Batch 519/992 ━━━━━━━━━━━━━━━━━━━━ 06:08:59\n",
      "Accuracy: 0.9106 - Loss: 0.2543\n",
      "\n",
      "Batch 520/992 ━━━━━━━━━━━━━━━━━━━━ 06:09:09\n",
      "Accuracy: 0.9106 - Loss: 0.2549\n",
      "\n",
      "Batch 521/992 ━━━━━━━━━━━━━━━━━━━━ 06:09:19\n",
      "Accuracy: 0.9105 - Loss: 0.2554\n",
      "\n",
      "Batch 522/992 ━━━━━━━━━━━━━━━━━━━━ 06:09:30\n",
      "Accuracy: 0.9107 - Loss: 0.2551\n",
      "\n",
      "Batch 523/992 ━━━━━━━━━━━━━━━━━━━━ 06:09:40\n",
      "Accuracy: 0.9109 - Loss: 0.2547\n",
      "\n",
      "Batch 524/992 ━━━━━━━━━━━━━━━━━━━━ 06:09:50\n",
      "Accuracy: 0.9110 - Loss: 0.2543\n",
      "\n",
      "Batch 525/992 ━━━━━━━━━━━━━━━━━━━━ 06:10:00\n",
      "Accuracy: 0.9110 - Loss: 0.2541\n",
      "\n",
      "Batch 526/992 ━━━━━━━━━━━━━━━━━━━━ 06:10:10\n",
      "Accuracy: 0.9109 - Loss: 0.2547\n",
      "\n",
      "Batch 527/992 ━━━━━━━━━━━━━━━━━━━━ 06:10:20\n",
      "Accuracy: 0.9108 - Loss: 0.2547\n",
      "\n",
      "Batch 528/992 ━━━━━━━━━━━━━━━━━━━━ 06:10:31\n",
      "Accuracy: 0.9107 - Loss: 0.2545\n",
      "\n",
      "Batch 529/992 ━━━━━━━━━━━━━━━━━━━━ 06:10:41\n",
      "Accuracy: 0.9107 - Loss: 0.2545\n",
      "\n",
      "Batch 530/992 ━━━━━━━━━━━━━━━━━━━━ 06:10:51\n",
      "Accuracy: 0.9108 - Loss: 0.2541\n",
      "\n",
      "Batch 531/992 ━━━━━━━━━━━━━━━━━━━━ 06:11:01\n",
      "Accuracy: 0.9110 - Loss: 0.2538\n",
      "\n",
      "Batch 532/992 ━━━━━━━━━━━━━━━━━━━━ 06:11:11\n",
      "Accuracy: 0.9109 - Loss: 0.2537\n",
      "\n",
      "Batch 533/992 ━━━━━━━━━━━━━━━━━━━━ 06:11:21\n",
      "Accuracy: 0.9106 - Loss: 0.2540\n",
      "\n",
      "Batch 534/992 ━━━━━━━━━━━━━━━━━━━━ 06:11:32\n",
      "Accuracy: 0.9103 - Loss: 0.2546\n",
      "\n",
      "Batch 535/992 ━━━━━━━━━━━━━━━━━━━━ 06:11:42\n",
      "Accuracy: 0.9105 - Loss: 0.2542\n",
      "\n",
      "Batch 536/992 ━━━━━━━━━━━━━━━━━━━━ 06:11:52\n",
      "Accuracy: 0.9107 - Loss: 0.2539\n",
      "\n",
      "Batch 537/992 ━━━━━━━━━━━━━━━━━━━━ 06:12:02\n",
      "Accuracy: 0.9106 - Loss: 0.2538\n",
      "\n",
      "Batch 538/992 ━━━━━━━━━━━━━━━━━━━━ 06:12:12\n",
      "Accuracy: 0.9108 - Loss: 0.2535\n",
      "\n",
      "Batch 539/992 ━━━━━━━━━━━━━━━━━━━━ 06:12:22\n",
      "Accuracy: 0.9107 - Loss: 0.2534\n",
      "\n",
      "Batch 540/992 ━━━━━━━━━━━━━━━━━━━━ 06:12:33\n",
      "Accuracy: 0.9109 - Loss: 0.2531\n",
      "\n",
      "Batch 541/992 ━━━━━━━━━━━━━━━━━━━━ 06:12:43\n",
      "Accuracy: 0.9106 - Loss: 0.2540\n",
      "\n",
      "Batch 542/992 ━━━━━━━━━━━━━━━━━━━━ 06:12:53\n",
      "Accuracy: 0.9103 - Loss: 0.2544\n",
      "\n",
      "Batch 543/992 ━━━━━━━━━━━━━━━━━━━━ 06:13:03\n",
      "Accuracy: 0.9105 - Loss: 0.2540\n",
      "\n",
      "Batch 544/992 ━━━━━━━━━━━━━━━━━━━━ 06:13:13\n",
      "Accuracy: 0.9106 - Loss: 0.2537\n",
      "\n",
      "Batch 545/992 ━━━━━━━━━━━━━━━━━━━━ 06:13:23\n",
      "Accuracy: 0.9106 - Loss: 0.2535\n",
      "\n",
      "Batch 546/992 ━━━━━━━━━━━━━━━━━━━━ 06:13:34\n",
      "Accuracy: 0.9105 - Loss: 0.2534\n",
      "\n",
      "Batch 547/992 ━━━━━━━━━━━━━━━━━━━━ 06:13:44\n",
      "Accuracy: 0.9106 - Loss: 0.2531\n",
      "\n",
      "Batch 548/992 ━━━━━━━━━━━━━━━━━━━━ 06:13:54\n",
      "Accuracy: 0.9108 - Loss: 0.2526\n",
      "\n",
      "Batch 549/992 ━━━━━━━━━━━━━━━━━━━━ 06:14:04\n",
      "Accuracy: 0.9107 - Loss: 0.2525\n",
      "\n",
      "Batch 550/992 ━━━━━━━━━━━━━━━━━━━━ 06:14:14\n",
      "Accuracy: 0.9105 - Loss: 0.2530\n",
      "\n",
      "Batch 551/992 ━━━━━━━━━━━━━━━━━━━━ 06:14:24\n",
      "Accuracy: 0.9104 - Loss: 0.2528\n",
      "\n",
      "Batch 552/992 ━━━━━━━━━━━━━━━━━━━━ 06:14:34\n",
      "Accuracy: 0.9106 - Loss: 0.2524\n",
      "\n",
      "Batch 553/992 ━━━━━━━━━━━━━━━━━━━━ 06:14:44\n",
      "Accuracy: 0.9107 - Loss: 0.2521\n",
      "\n",
      "Batch 554/992 ━━━━━━━━━━━━━━━━━━━━ 06:14:54\n",
      "Accuracy: 0.9109 - Loss: 0.2518\n",
      "\n",
      "Batch 555/992 ━━━━━━━━━━━━━━━━━━━━ 06:15:05\n",
      "Accuracy: 0.9110 - Loss: 0.2515\n",
      "\n",
      "Batch 556/992 ━━━━━━━━━━━━━━━━━━━━ 06:15:15\n",
      "Accuracy: 0.9110 - Loss: 0.2515\n",
      "\n",
      "Batch 557/992 ━━━━━━━━━━━━━━━━━━━━ 06:15:25\n",
      "Accuracy: 0.9109 - Loss: 0.2514\n",
      "\n",
      "Batch 558/992 ━━━━━━━━━━━━━━━━━━━━ 06:15:35\n",
      "Accuracy: 0.9104 - Loss: 0.2521\n",
      "\n",
      "Batch 559/992 ━━━━━━━━━━━━━━━━━━━━ 06:15:45\n",
      "Accuracy: 0.9106 - Loss: 0.2519\n",
      "\n",
      "Batch 560/992 ━━━━━━━━━━━━━━━━━━━━ 06:15:55\n",
      "Accuracy: 0.9103 - Loss: 0.2527\n",
      "\n",
      "Batch 561/992 ━━━━━━━━━━━━━━━━━━━━ 06:16:06\n",
      "Accuracy: 0.9102 - Loss: 0.2533\n",
      "\n",
      "Batch 562/992 ━━━━━━━━━━━━━━━━━━━━ 06:16:16\n",
      "Accuracy: 0.9104 - Loss: 0.2531\n",
      "\n",
      "Batch 563/992 ━━━━━━━━━━━━━━━━━━━━ 06:16:26\n",
      "Accuracy: 0.9105 - Loss: 0.2528\n",
      "\n",
      "Batch 564/992 ━━━━━━━━━━━━━━━━━━━━ 06:16:36\n",
      "Accuracy: 0.9107 - Loss: 0.2527\n",
      "\n",
      "Batch 565/992 ━━━━━━━━━━━━━━━━━━━━ 06:16:46\n",
      "Accuracy: 0.9108 - Loss: 0.2523\n",
      "\n",
      "Batch 566/992 ━━━━━━━━━━━━━━━━━━━━ 06:16:57\n",
      "Accuracy: 0.9108 - Loss: 0.2527\n",
      "\n",
      "Batch 567/992 ━━━━━━━━━━━━━━━━━━━━ 06:17:07\n",
      "Accuracy: 0.9107 - Loss: 0.2528\n",
      "\n",
      "Batch 568/992 ━━━━━━━━━━━━━━━━━━━━ 06:17:17\n",
      "Accuracy: 0.9107 - Loss: 0.2533\n",
      "\n",
      "Batch 569/992 ━━━━━━━━━━━━━━━━━━━━ 06:17:27\n",
      "Accuracy: 0.9108 - Loss: 0.2532\n",
      "\n",
      "Batch 570/992 ━━━━━━━━━━━━━━━━━━━━ 06:17:37\n",
      "Accuracy: 0.9110 - Loss: 0.2529\n",
      "\n",
      "Batch 571/992 ━━━━━━━━━━━━━━━━━━━━ 06:17:47\n",
      "Accuracy: 0.9111 - Loss: 0.2527\n",
      "\n",
      "Batch 572/992 ━━━━━━━━━━━━━━━━━━━━ 06:17:57\n",
      "Accuracy: 0.9113 - Loss: 0.2523\n",
      "\n",
      "Batch 573/992 ━━━━━━━━━━━━━━━━━━━━ 06:18:08\n",
      "Accuracy: 0.9114 - Loss: 0.2520\n",
      "\n",
      "Batch 574/992 ━━━━━━━━━━━━━━━━━━━━ 06:18:18\n",
      "Accuracy: 0.9109 - Loss: 0.2529\n",
      "\n",
      "Batch 575/992 ━━━━━━━━━━━━━━━━━━━━ 06:18:28\n",
      "Accuracy: 0.9109 - Loss: 0.2532\n",
      "\n",
      "Batch 576/992 ━━━━━━━━━━━━━━━━━━━━ 06:18:38\n",
      "Accuracy: 0.9108 - Loss: 0.2533\n",
      "\n",
      "Batch 577/992 ━━━━━━━━━━━━━━━━━━━━ 06:18:48\n",
      "Accuracy: 0.9110 - Loss: 0.2530\n",
      "\n",
      "Batch 578/992 ━━━━━━━━━━━━━━━━━━━━ 06:18:58\n",
      "Accuracy: 0.9111 - Loss: 0.2526\n",
      "\n",
      "Batch 579/992 ━━━━━━━━━━━━━━━━━━━━ 06:19:08\n",
      "Accuracy: 0.9113 - Loss: 0.2525\n",
      "\n",
      "Batch 580/992 ━━━━━━━━━━━━━━━━━━━━ 06:19:18\n",
      "Accuracy: 0.9114 - Loss: 0.2522\n",
      "\n",
      "Batch 581/992 ━━━━━━━━━━━━━━━━━━━━ 06:19:29\n",
      "Accuracy: 0.9116 - Loss: 0.2520\n",
      "\n",
      "Batch 582/992 ━━━━━━━━━━━━━━━━━━━━ 06:19:39\n",
      "Accuracy: 0.9115 - Loss: 0.2521\n",
      "\n",
      "Batch 583/992 ━━━━━━━━━━━━━━━━━━━━ 06:19:49\n",
      "Accuracy: 0.9112 - Loss: 0.2522\n",
      "\n",
      "Batch 584/992 ━━━━━━━━━━━━━━━━━━━━ 06:19:59\n",
      "Accuracy: 0.9114 - Loss: 0.2520\n",
      "\n",
      "Batch 585/992 ━━━━━━━━━━━━━━━━━━━━ 06:20:09\n",
      "Accuracy: 0.9115 - Loss: 0.2519\n",
      "\n",
      "Batch 586/992 ━━━━━━━━━━━━━━━━━━━━ 06:20:19\n",
      "Accuracy: 0.9113 - Loss: 0.2523\n",
      "\n",
      "Batch 587/992 ━━━━━━━━━━━━━━━━━━━━ 06:20:29\n",
      "Accuracy: 0.9114 - Loss: 0.2519\n",
      "\n",
      "Batch 588/992 ━━━━━━━━━━━━━━━━━━━━ 06:20:40\n",
      "Accuracy: 0.9114 - Loss: 0.2521\n",
      "\n",
      "Batch 589/992 ━━━━━━━━━━━━━━━━━━━━ 06:20:50\n",
      "Accuracy: 0.9111 - Loss: 0.2526\n",
      "\n",
      "Batch 590/992 ━━━━━━━━━━━━━━━━━━━━ 06:21:00\n",
      "Accuracy: 0.9112 - Loss: 0.2524\n",
      "\n",
      "Batch 591/992 ━━━━━━━━━━━━━━━━━━━━ 06:21:10\n",
      "Accuracy: 0.9112 - Loss: 0.2526\n",
      "\n",
      "Batch 592/992 ━━━━━━━━━━━━━━━━━━━━ 06:21:20\n",
      "Accuracy: 0.9113 - Loss: 0.2523\n",
      "\n",
      "Batch 593/992 ━━━━━━━━━━━━━━━━━━━━ 06:21:30\n",
      "Accuracy: 0.9113 - Loss: 0.2526\n",
      "\n",
      "Batch 594/992 ━━━━━━━━━━━━━━━━━━━━ 06:21:40\n",
      "Accuracy: 0.9112 - Loss: 0.2530\n",
      "\n",
      "Batch 595/992 ━━━━━━━━━━━━━━━━━━━━ 06:21:50\n",
      "Accuracy: 0.9109 - Loss: 0.2533\n",
      "\n",
      "Batch 596/992 ━━━━━━━━━━━━━━━━━━━━ 06:22:01\n",
      "Accuracy: 0.9111 - Loss: 0.2529\n",
      "\n",
      "Batch 597/992 ━━━━━━━━━━━━━━━━━━━━ 06:22:11\n",
      "Accuracy: 0.9112 - Loss: 0.2528\n",
      "\n",
      "Batch 598/992 ━━━━━━━━━━━━━━━━━━━━ 06:22:21\n",
      "Accuracy: 0.9112 - Loss: 0.2535\n",
      "\n",
      "Batch 599/992 ━━━━━━━━━━━━━━━━━━━━ 06:22:31\n",
      "Accuracy: 0.9113 - Loss: 0.2534\n",
      "\n",
      "Batch 600/992 ━━━━━━━━━━━━━━━━━━━━ 06:22:41\n",
      "Accuracy: 0.9112 - Loss: 0.2535\n",
      "\n",
      "Batch 601/992 ━━━━━━━━━━━━━━━━━━━━ 06:22:51\n",
      "Accuracy: 0.9114 - Loss: 0.2532\n",
      "\n",
      "Batch 602/992 ━━━━━━━━━━━━━━━━━━━━ 06:23:01\n",
      "Accuracy: 0.9115 - Loss: 0.2530\n",
      "\n",
      "Batch 603/992 ━━━━━━━━━━━━━━━━━━━━ 06:23:11\n",
      "Accuracy: 0.9117 - Loss: 0.2526\n",
      "\n",
      "Batch 604/992 ━━━━━━━━━━━━━━━━━━━━ 06:23:21\n",
      "Accuracy: 0.9116 - Loss: 0.2528\n",
      "\n",
      "Batch 605/992 ━━━━━━━━━━━━━━━━━━━━ 06:23:32\n",
      "Accuracy: 0.9118 - Loss: 0.2525\n",
      "\n",
      "Batch 606/992 ━━━━━━━━━━━━━━━━━━━━ 06:23:42\n",
      "Accuracy: 0.9119 - Loss: 0.2522\n",
      "\n",
      "Batch 607/992 ━━━━━━━━━━━━━━━━━━━━ 06:23:52\n",
      "Accuracy: 0.9117 - Loss: 0.2522\n",
      "\n",
      "Batch 608/992 ━━━━━━━━━━━━━━━━━━━━ 06:24:02\n",
      "Accuracy: 0.9118 - Loss: 0.2520\n",
      "\n",
      "Batch 609/992 ━━━━━━━━━━━━━━━━━━━━ 06:24:12\n",
      "Accuracy: 0.9119 - Loss: 0.2520\n",
      "\n",
      "Batch 610/992 ━━━━━━━━━━━━━━━━━━━━ 06:24:22\n",
      "Accuracy: 0.9119 - Loss: 0.2518\n",
      "\n",
      "Batch 611/992 ━━━━━━━━━━━━━━━━━━━━ 06:24:33\n",
      "Accuracy: 0.9118 - Loss: 0.2523\n",
      "\n",
      "Batch 612/992 ━━━━━━━━━━━━━━━━━━━━ 06:24:43\n",
      "Accuracy: 0.9118 - Loss: 0.2523\n",
      "\n",
      "Batch 613/992 ━━━━━━━━━━━━━━━━━━━━ 06:24:53\n",
      "Accuracy: 0.9115 - Loss: 0.2528\n",
      "\n",
      "Batch 614/992 ━━━━━━━━━━━━━━━━━━━━ 06:25:03\n",
      "Accuracy: 0.9112 - Loss: 0.2532\n",
      "\n",
      "Batch 615/992 ━━━━━━━━━━━━━━━━━━━━ 06:25:14\n",
      "Accuracy: 0.9108 - Loss: 0.2544\n",
      "\n",
      "Batch 616/992 ━━━━━━━━━━━━━━━━━━━━ 06:25:24\n",
      "Accuracy: 0.9105 - Loss: 0.2550\n",
      "\n",
      "Batch 617/992 ━━━━━━━━━━━━━━━━━━━━ 06:25:34\n",
      "Accuracy: 0.9107 - Loss: 0.2546\n",
      "\n",
      "Batch 618/992 ━━━━━━━━━━━━━━━━━━━━ 06:25:44\n",
      "Accuracy: 0.9104 - Loss: 0.2556\n",
      "\n",
      "Batch 619/992 ━━━━━━━━━━━━━━━━━━━━ 06:25:54\n",
      "Accuracy: 0.9103 - Loss: 0.2558\n",
      "\n",
      "Batch 620/992 ━━━━━━━━━━━━━━━━━━━━ 06:26:05\n",
      "Accuracy: 0.9103 - Loss: 0.2559\n",
      "\n",
      "Batch 621/992 ━━━━━━━━━━━━━━━━━━━━ 06:26:15\n",
      "Accuracy: 0.9098 - Loss: 0.2573\n",
      "\n",
      "Batch 622/992 ━━━━━━━━━━━━━━━━━━━━ 06:26:25\n",
      "Accuracy: 0.9096 - Loss: 0.2576\n",
      "\n",
      "Batch 623/992 ━━━━━━━━━━━━━━━━━━━━ 06:26:36\n",
      "Accuracy: 0.9095 - Loss: 0.2575\n",
      "\n",
      "Batch 624/992 ━━━━━━━━━━━━━━━━━━━━ 06:26:46\n",
      "Accuracy: 0.9095 - Loss: 0.2573\n",
      "\n",
      "Batch 625/992 ━━━━━━━━━━━━━━━━━━━━ 06:26:56\n",
      "Accuracy: 0.9096 - Loss: 0.2571\n",
      "\n",
      "Batch 626/992 ━━━━━━━━━━━━━━━━━━━━ 06:27:06\n",
      "Accuracy: 0.9095 - Loss: 0.2572\n",
      "\n",
      "Batch 627/992 ━━━━━━━━━━━━━━━━━━━━ 06:27:17\n",
      "Accuracy: 0.9095 - Loss: 0.2572\n",
      "\n",
      "Batch 628/992 ━━━━━━━━━━━━━━━━━━━━ 06:27:27\n",
      "Accuracy: 0.9096 - Loss: 0.2571\n",
      "\n",
      "Batch 629/992 ━━━━━━━━━━━━━━━━━━━━ 06:27:37\n",
      "Accuracy: 0.9096 - Loss: 0.2570\n",
      "\n",
      "Batch 630/992 ━━━━━━━━━━━━━━━━━━━━ 06:27:47\n",
      "Accuracy: 0.9093 - Loss: 0.2580\n",
      "\n",
      "Batch 631/992 ━━━━━━━━━━━━━━━━━━━━ 06:27:57\n",
      "Accuracy: 0.9093 - Loss: 0.2582\n",
      "\n",
      "Batch 632/992 ━━━━━━━━━━━━━━━━━━━━ 06:28:08\n",
      "Accuracy: 0.9094 - Loss: 0.2580\n",
      "\n",
      "Batch 633/992 ━━━━━━━━━━━━━━━━━━━━ 06:28:18\n",
      "Accuracy: 0.9096 - Loss: 0.2578\n",
      "\n",
      "Batch 634/992 ━━━━━━━━━━━━━━━━━━━━ 06:28:28\n",
      "Accuracy: 0.9093 - Loss: 0.2583\n",
      "\n",
      "Batch 635/992 ━━━━━━━━━━━━━━━━━━━━ 06:28:38\n",
      "Accuracy: 0.9091 - Loss: 0.2590\n",
      "\n",
      "Batch 636/992 ━━━━━━━━━━━━━━━━━━━━ 06:28:48\n",
      "Accuracy: 0.9088 - Loss: 0.2592\n",
      "\n",
      "Batch 637/992 ━━━━━━━━━━━━━━━━━━━━ 06:28:59\n",
      "Accuracy: 0.9089 - Loss: 0.2589\n",
      "\n",
      "Batch 638/992 ━━━━━━━━━━━━━━━━━━━━ 06:29:09\n",
      "Accuracy: 0.9091 - Loss: 0.2585\n",
      "\n",
      "Batch 639/992 ━━━━━━━━━━━━━━━━━━━━ 06:29:19\n",
      "Accuracy: 0.9092 - Loss: 0.2583\n",
      "\n",
      "Batch 640/992 ━━━━━━━━━━━━━━━━━━━━ 06:29:29\n",
      "Accuracy: 0.9094 - Loss: 0.2579\n",
      "\n",
      "Batch 641/992 ━━━━━━━━━━━━━━━━━━━━ 06:29:40\n",
      "Accuracy: 0.9093 - Loss: 0.2578\n",
      "\n",
      "Batch 642/992 ━━━━━━━━━━━━━━━━━━━━ 06:29:50\n",
      "Accuracy: 0.9093 - Loss: 0.2579\n",
      "\n",
      "Batch 643/992 ━━━━━━━━━━━━━━━━━━━━ 06:30:00\n",
      "Accuracy: 0.9094 - Loss: 0.2576\n",
      "\n",
      "Batch 644/992 ━━━━━━━━━━━━━━━━━━━━ 06:30:10\n",
      "Accuracy: 0.9094 - Loss: 0.2576\n",
      "\n",
      "Batch 645/992 ━━━━━━━━━━━━━━━━━━━━ 06:30:20\n",
      "Accuracy: 0.9093 - Loss: 0.2577\n",
      "\n",
      "Batch 646/992 ━━━━━━━━━━━━━━━━━━━━ 06:30:31\n",
      "Accuracy: 0.9094 - Loss: 0.2574\n",
      "\n",
      "Batch 647/992 ━━━━━━━━━━━━━━━━━━━━ 06:30:41\n",
      "Accuracy: 0.9088 - Loss: 0.2583\n",
      "\n",
      "Batch 648/992 ━━━━━━━━━━━━━━━━━━━━ 06:30:51\n",
      "Accuracy: 0.9090 - Loss: 0.2581\n",
      "\n",
      "Batch 649/992 ━━━━━━━━━━━━━━━━━━━━ 06:31:01\n",
      "Accuracy: 0.9091 - Loss: 0.2578\n",
      "\n",
      "Batch 650/992 ━━━━━━━━━━━━━━━━━━━━ 06:31:11\n",
      "Accuracy: 0.9090 - Loss: 0.2585\n",
      "\n",
      "Batch 651/992 ━━━━━━━━━━━━━━━━━━━━ 06:31:21\n",
      "Accuracy: 0.9092 - Loss: 0.2583\n",
      "\n",
      "Batch 652/992 ━━━━━━━━━━━━━━━━━━━━ 06:31:32\n",
      "Accuracy: 0.9091 - Loss: 0.2585\n",
      "\n",
      "Batch 653/992 ━━━━━━━━━━━━━━━━━━━━ 06:31:42\n",
      "Accuracy: 0.9093 - Loss: 0.2583\n",
      "\n",
      "Batch 654/992 ━━━━━━━━━━━━━━━━━━━━ 06:31:52\n",
      "Accuracy: 0.9094 - Loss: 0.2580\n",
      "\n",
      "Batch 655/992 ━━━━━━━━━━━━━━━━━━━━ 06:32:02\n",
      "Accuracy: 0.9094 - Loss: 0.2580\n",
      "\n",
      "Batch 656/992 ━━━━━━━━━━━━━━━━━━━━ 06:32:12\n",
      "Accuracy: 0.9093 - Loss: 0.2580\n",
      "\n",
      "Batch 657/992 ━━━━━━━━━━━━━━━━━━━━ 06:32:23\n",
      "Accuracy: 0.9091 - Loss: 0.2584\n",
      "\n",
      "Batch 658/992 ━━━━━━━━━━━━━━━━━━━━ 06:32:33\n",
      "Accuracy: 0.9090 - Loss: 0.2587\n",
      "\n",
      "Batch 659/992 ━━━━━━━━━━━━━━━━━━━━ 06:32:43\n",
      "Accuracy: 0.9090 - Loss: 0.2587\n",
      "\n",
      "Batch 660/992 ━━━━━━━━━━━━━━━━━━━━ 06:32:53\n",
      "Accuracy: 0.9091 - Loss: 0.2584\n",
      "\n",
      "Batch 661/992 ━━━━━━━━━━━━━━━━━━━━ 06:33:03\n",
      "Accuracy: 0.9090 - Loss: 0.2584\n",
      "\n",
      "Batch 662/992 ━━━━━━━━━━━━━━━━━━━━ 06:33:13\n",
      "Accuracy: 0.9092 - Loss: 0.2582\n",
      "\n",
      "Batch 663/992 ━━━━━━━━━━━━━━━━━━━━ 06:33:24\n",
      "Accuracy: 0.9093 - Loss: 0.2580\n",
      "\n",
      "Batch 664/992 ━━━━━━━━━━━━━━━━━━━━ 06:33:34\n",
      "Accuracy: 0.9095 - Loss: 0.2578\n",
      "\n",
      "Batch 665/992 ━━━━━━━━━━━━━━━━━━━━ 06:33:44\n",
      "Accuracy: 0.9094 - Loss: 0.2581\n",
      "\n",
      "Batch 666/992 ━━━━━━━━━━━━━━━━━━━━ 06:33:54\n",
      "Accuracy: 0.9095 - Loss: 0.2577\n",
      "\n",
      "Batch 667/992 ━━━━━━━━━━━━━━━━━━━━ 06:34:04\n",
      "Accuracy: 0.9097 - Loss: 0.2576\n",
      "\n",
      "Batch 668/992 ━━━━━━━━━━━━━━━━━━━━ 06:34:14\n",
      "Accuracy: 0.9096 - Loss: 0.2576\n",
      "\n",
      "Batch 669/992 ━━━━━━━━━━━━━━━━━━━━ 06:34:24\n",
      "Accuracy: 0.9098 - Loss: 0.2573\n",
      "\n",
      "Batch 670/992 ━━━━━━━━━━━━━━━━━━━━ 06:34:35\n",
      "Accuracy: 0.9097 - Loss: 0.2572\n",
      "\n",
      "Batch 671/992 ━━━━━━━━━━━━━━━━━━━━ 06:34:45\n",
      "Accuracy: 0.9098 - Loss: 0.2569\n",
      "\n",
      "Batch 672/992 ━━━━━━━━━━━━━━━━━━━━ 06:34:55\n",
      "Accuracy: 0.9100 - Loss: 0.2566\n",
      "\n",
      "Batch 673/992 ━━━━━━━━━━━━━━━━━━━━ 06:35:05\n",
      "Accuracy: 0.9097 - Loss: 0.2567\n",
      "\n",
      "Batch 674/992 ━━━━━━━━━━━━━━━━━━━━ 06:35:15\n",
      "Accuracy: 0.9095 - Loss: 0.2572\n",
      "\n",
      "Batch 675/992 ━━━━━━━━━━━━━━━━━━━━ 06:35:25\n",
      "Accuracy: 0.9096 - Loss: 0.2570\n",
      "\n",
      "Batch 676/992 ━━━━━━━━━━━━━━━━━━━━ 06:35:36\n",
      "Accuracy: 0.9096 - Loss: 0.2572\n",
      "\n",
      "Batch 677/992 ━━━━━━━━━━━━━━━━━━━━ 06:35:46\n",
      "Accuracy: 0.9092 - Loss: 0.2576\n",
      "\n",
      "Batch 678/992 ━━━━━━━━━━━━━━━━━━━━ 06:35:56\n",
      "Accuracy: 0.9089 - Loss: 0.2578\n",
      "\n",
      "Batch 679/992 ━━━━━━━━━━━━━━━━━━━━ 06:36:06\n",
      "Accuracy: 0.9085 - Loss: 0.2588\n",
      "\n",
      "Batch 680/992 ━━━━━━━━━━━━━━━━━━━━ 06:36:16\n",
      "Accuracy: 0.9085 - Loss: 0.2588\n",
      "\n",
      "Batch 681/992 ━━━━━━━━━━━━━━━━━━━━ 06:36:27\n",
      "Accuracy: 0.9086 - Loss: 0.2585\n",
      "\n",
      "Batch 682/992 ━━━━━━━━━━━━━━━━━━━━ 06:36:37\n",
      "Accuracy: 0.9085 - Loss: 0.2585\n",
      "\n",
      "Batch 683/992 ━━━━━━━━━━━━━━━━━━━━ 06:36:47\n",
      "Accuracy: 0.9087 - Loss: 0.2583\n",
      "\n",
      "Batch 684/992 ━━━━━━━━━━━━━━━━━━━━ 06:36:57\n",
      "Accuracy: 0.9084 - Loss: 0.2585\n",
      "\n",
      "Batch 685/992 ━━━━━━━━━━━━━━━━━━━━ 06:37:08\n",
      "Accuracy: 0.9086 - Loss: 0.2582\n",
      "\n",
      "Batch 686/992 ━━━━━━━━━━━━━━━━━━━━ 06:37:18\n",
      "Accuracy: 0.9087 - Loss: 0.2579\n",
      "\n",
      "Batch 687/992 ━━━━━━━━━━━━━━━━━━━━ 06:37:28\n",
      "Accuracy: 0.9088 - Loss: 0.2578\n",
      "\n",
      "Batch 688/992 ━━━━━━━━━━━━━━━━━━━━ 06:37:38\n",
      "Accuracy: 0.9086 - Loss: 0.2582\n",
      "\n",
      "Batch 689/992 ━━━━━━━━━━━━━━━━━━━━ 06:37:48\n",
      "Accuracy: 0.9087 - Loss: 0.2580\n",
      "\n",
      "Batch 690/992 ━━━━━━━━━━━━━━━━━━━━ 06:37:59\n",
      "Accuracy: 0.9085 - Loss: 0.2580\n",
      "\n",
      "Batch 691/992 ━━━━━━━━━━━━━━━━━━━━ 06:38:09\n",
      "Accuracy: 0.9086 - Loss: 0.2579\n",
      "\n",
      "Batch 692/992 ━━━━━━━━━━━━━━━━━━━━ 06:38:19\n",
      "Accuracy: 0.9086 - Loss: 0.2582\n",
      "\n",
      "Batch 693/992 ━━━━━━━━━━━━━━━━━━━━ 06:38:29\n",
      "Accuracy: 0.9085 - Loss: 0.2584\n",
      "\n",
      "Batch 694/992 ━━━━━━━━━━━━━━━━━━━━ 06:38:40\n",
      "Accuracy: 0.9087 - Loss: 0.2581\n",
      "\n",
      "Batch 695/992 ━━━━━━━━━━━━━━━━━━━━ 06:38:50\n",
      "Accuracy: 0.9088 - Loss: 0.2578\n",
      "\n",
      "Batch 696/992 ━━━━━━━━━━━━━━━━━━━━ 06:39:01\n",
      "Accuracy: 0.9089 - Loss: 0.2577\n",
      "\n",
      "Batch 697/992 ━━━━━━━━━━━━━━━━━━━━ 06:39:11\n",
      "Accuracy: 0.9091 - Loss: 0.2574\n",
      "\n",
      "Batch 698/992 ━━━━━━━━━━━━━━━━━━━━ 06:39:21\n",
      "Accuracy: 0.9088 - Loss: 0.2578\n",
      "\n",
      "Batch 699/992 ━━━━━━━━━━━━━━━━━━━━ 06:39:31\n",
      "Accuracy: 0.9088 - Loss: 0.2580\n",
      "\n",
      "Batch 700/992 ━━━━━━━━━━━━━━━━━━━━ 06:39:41\n",
      "Accuracy: 0.9087 - Loss: 0.2580\n",
      "\n",
      "Batch 701/992 ━━━━━━━━━━━━━━━━━━━━ 06:39:52\n",
      "Accuracy: 0.9089 - Loss: 0.2578\n",
      "\n",
      "Batch 702/992 ━━━━━━━━━━━━━━━━━━━━ 06:40:02\n",
      "Accuracy: 0.9088 - Loss: 0.2578\n",
      "\n",
      "Batch 703/992 ━━━━━━━━━━━━━━━━━━━━ 06:40:12\n",
      "Accuracy: 0.9090 - Loss: 0.2575\n",
      "\n",
      "Batch 704/992 ━━━━━━━━━━━━━━━━━━━━ 06:40:22\n",
      "Accuracy: 0.9087 - Loss: 0.2579\n",
      "\n",
      "Batch 705/992 ━━━━━━━━━━━━━━━━━━━━ 06:40:32\n",
      "Accuracy: 0.9089 - Loss: 0.2576\n",
      "\n",
      "Batch 706/992 ━━━━━━━━━━━━━━━━━━━━ 06:40:42\n",
      "Accuracy: 0.9090 - Loss: 0.2574\n",
      "\n",
      "Batch 707/992 ━━━━━━━━━━━━━━━━━━━━ 06:40:53\n",
      "Accuracy: 0.9091 - Loss: 0.2571\n",
      "\n",
      "Batch 708/992 ━━━━━━━━━━━━━━━━━━━━ 06:41:03\n",
      "Accuracy: 0.9091 - Loss: 0.2569\n",
      "\n",
      "Batch 709/992 ━━━━━━━━━━━━━━━━━━━━ 06:41:13\n",
      "Accuracy: 0.9090 - Loss: 0.2570\n",
      "\n",
      "Batch 710/992 ━━━━━━━━━━━━━━━━━━━━ 06:41:23\n",
      "Accuracy: 0.9090 - Loss: 0.2570\n",
      "\n",
      "Batch 711/992 ━━━━━━━━━━━━━━━━━━━━ 06:41:33\n",
      "Accuracy: 0.9091 - Loss: 0.2568\n",
      "\n",
      "Batch 712/992 ━━━━━━━━━━━━━━━━━━━━ 06:41:43\n",
      "Accuracy: 0.9092 - Loss: 0.2566\n",
      "\n",
      "Batch 713/992 ━━━━━━━━━━━━━━━━━━━━ 06:41:54\n",
      "Accuracy: 0.9094 - Loss: 0.2564\n",
      "\n",
      "Batch 714/992 ━━━━━━━━━━━━━━━━━━━━ 06:42:04\n",
      "Accuracy: 0.9093 - Loss: 0.2563\n",
      "\n",
      "Batch 715/992 ━━━━━━━━━━━━━━━━━━━━ 06:42:14\n",
      "Accuracy: 0.9094 - Loss: 0.2560\n",
      "\n",
      "Batch 716/992 ━━━━━━━━━━━━━━━━━━━━ 06:42:25\n",
      "Accuracy: 0.9094 - Loss: 0.2562\n",
      "\n",
      "Batch 717/992 ━━━━━━━━━━━━━━━━━━━━ 06:42:35\n",
      "Accuracy: 0.9095 - Loss: 0.2560\n",
      "\n",
      "Batch 718/992 ━━━━━━━━━━━━━━━━━━━━ 06:42:45\n",
      "Accuracy: 0.9093 - Loss: 0.2563\n",
      "\n",
      "Batch 719/992 ━━━━━━━━━━━━━━━━━━━━ 06:42:55\n",
      "Accuracy: 0.9092 - Loss: 0.2564\n",
      "\n",
      "Batch 720/992 ━━━━━━━━━━━━━━━━━━━━ 06:43:05\n",
      "Accuracy: 0.9094 - Loss: 0.2562\n",
      "\n",
      "Batch 721/992 ━━━━━━━━━━━━━━━━━━━━ 06:43:15\n",
      "Accuracy: 0.9093 - Loss: 0.2563\n",
      "\n",
      "Batch 722/992 ━━━━━━━━━━━━━━━━━━━━ 06:43:26\n",
      "Accuracy: 0.9093 - Loss: 0.2564\n",
      "\n",
      "Batch 723/992 ━━━━━━━━━━━━━━━━━━━━ 06:43:36\n",
      "Accuracy: 0.9092 - Loss: 0.2562\n",
      "\n",
      "Batch 724/992 ━━━━━━━━━━━━━━━━━━━━ 06:43:46\n",
      "Accuracy: 0.9094 - Loss: 0.2560\n",
      "\n",
      "Batch 725/992 ━━━━━━━━━━━━━━━━━━━━ 06:43:56\n",
      "Accuracy: 0.9093 - Loss: 0.2560\n",
      "\n",
      "Batch 726/992 ━━━━━━━━━━━━━━━━━━━━ 06:44:06\n",
      "Accuracy: 0.9091 - Loss: 0.2561\n",
      "\n",
      "Batch 727/992 ━━━━━━━━━━━━━━━━━━━━ 06:44:17\n",
      "Accuracy: 0.9090 - Loss: 0.2563\n",
      "\n",
      "Batch 728/992 ━━━━━━━━━━━━━━━━━━━━ 06:44:27\n",
      "Accuracy: 0.9092 - Loss: 0.2561\n",
      "\n",
      "Batch 729/992 ━━━━━━━━━━━━━━━━━━━━ 06:44:37\n",
      "Accuracy: 0.9091 - Loss: 0.2560\n",
      "\n",
      "Batch 730/992 ━━━━━━━━━━━━━━━━━━━━ 06:44:47\n",
      "Accuracy: 0.9091 - Loss: 0.2561\n",
      "\n",
      "Batch 731/992 ━━━━━━━━━━━━━━━━━━━━ 06:44:57\n",
      "Accuracy: 0.9090 - Loss: 0.2562\n",
      "\n",
      "Batch 732/992 ━━━━━━━━━━━━━━━━━━━━ 06:45:08\n",
      "Accuracy: 0.9092 - Loss: 0.2562\n",
      "\n",
      "Batch 733/992 ━━━━━━━━━━━━━━━━━━━━ 06:45:18\n",
      "Accuracy: 0.9093 - Loss: 0.2559\n",
      "\n",
      "Batch 734/992 ━━━━━━━━━━━━━━━━━━━━ 06:45:28\n",
      "Accuracy: 0.9094 - Loss: 0.2557\n",
      "\n",
      "Batch 735/992 ━━━━━━━━━━━━━━━━━━━━ 06:45:39\n",
      "Accuracy: 0.9094 - Loss: 0.2559\n",
      "\n",
      "Batch 736/992 ━━━━━━━━━━━━━━━━━━━━ 06:45:49\n",
      "Accuracy: 0.9093 - Loss: 0.2560\n",
      "\n",
      "Batch 737/992 ━━━━━━━━━━━━━━━━━━━━ 06:45:59\n",
      "Accuracy: 0.9093 - Loss: 0.2563\n",
      "\n",
      "Batch 738/992 ━━━━━━━━━━━━━━━━━━━━ 06:46:09\n",
      "Accuracy: 0.9092 - Loss: 0.2562\n",
      "\n",
      "Batch 739/992 ━━━━━━━━━━━━━━━━━━━━ 06:46:19\n",
      "Accuracy: 0.9092 - Loss: 0.2561\n",
      "\n",
      "Batch 740/992 ━━━━━━━━━━━━━━━━━━━━ 06:46:30\n",
      "Accuracy: 0.9091 - Loss: 0.2561\n",
      "\n",
      "Batch 741/992 ━━━━━━━━━━━━━━━━━━━━ 06:46:40\n",
      "Accuracy: 0.9089 - Loss: 0.2563\n",
      "\n",
      "Batch 742/992 ━━━━━━━━━━━━━━━━━━━━ 06:46:50\n",
      "Accuracy: 0.9089 - Loss: 0.2563\n",
      "\n",
      "Batch 743/992 ━━━━━━━━━━━━━━━━━━━━ 06:47:00\n",
      "Accuracy: 0.9088 - Loss: 0.2563\n",
      "\n",
      "Batch 744/992 ━━━━━━━━━━━━━━━━━━━━ 06:47:11\n",
      "Accuracy: 0.9089 - Loss: 0.2561\n",
      "\n",
      "Batch 745/992 ━━━━━━━━━━━━━━━━━━━━ 06:47:21\n",
      "Accuracy: 0.9091 - Loss: 0.2560\n",
      "\n",
      "Batch 746/992 ━━━━━━━━━━━━━━━━━━━━ 06:47:31\n",
      "Accuracy: 0.9090 - Loss: 0.2559\n",
      "\n",
      "Batch 747/992 ━━━━━━━━━━━━━━━━━━━━ 06:47:41\n",
      "Accuracy: 0.9090 - Loss: 0.2560\n",
      "\n",
      "Batch 748/992 ━━━━━━━━━━━━━━━━━━━━ 06:47:51\n",
      "Accuracy: 0.9091 - Loss: 0.2557\n",
      "\n",
      "Batch 749/992 ━━━━━━━━━━━━━━━━━━━━ 06:48:01\n",
      "Accuracy: 0.9092 - Loss: 0.2555\n",
      "\n",
      "Batch 750/992 ━━━━━━━━━━━━━━━━━━━━ 06:48:12\n",
      "Accuracy: 0.9093 - Loss: 0.2554\n",
      "\n",
      "Batch 751/992 ━━━━━━━━━━━━━━━━━━━━ 06:48:22\n",
      "Accuracy: 0.9095 - Loss: 0.2551\n",
      "\n",
      "Batch 752/992 ━━━━━━━━━━━━━━━━━━━━ 06:48:32\n",
      "Accuracy: 0.9092 - Loss: 0.2554\n",
      "\n",
      "Batch 753/992 ━━━━━━━━━━━━━━━━━━━━ 06:48:42\n",
      "Accuracy: 0.9094 - Loss: 0.2551\n",
      "\n",
      "Batch 754/992 ━━━━━━━━━━━━━━━━━━━━ 06:48:52\n",
      "Accuracy: 0.9095 - Loss: 0.2551\n",
      "\n",
      "Batch 755/992 ━━━━━━━━━━━━━━━━━━━━ 06:49:02\n",
      "Accuracy: 0.9096 - Loss: 0.2549\n",
      "\n",
      "Batch 756/992 ━━━━━━━━━━━━━━━━━━━━ 06:49:13\n",
      "Accuracy: 0.9096 - Loss: 0.2548\n",
      "\n",
      "Batch 757/992 ━━━━━━━━━━━━━━━━━━━━ 06:49:23\n",
      "Accuracy: 0.9093 - Loss: 0.2551\n",
      "\n",
      "Batch 758/992 ━━━━━━━━━━━━━━━━━━━━ 06:49:33\n",
      "Accuracy: 0.9091 - Loss: 0.2554\n",
      "\n",
      "Batch 759/992 ━━━━━━━━━━━━━━━━━━━━ 06:49:43\n",
      "Accuracy: 0.9091 - Loss: 0.2552\n",
      "\n",
      "Batch 760/992 ━━━━━━━━━━━━━━━━━━━━ 06:49:53\n",
      "Accuracy: 0.9092 - Loss: 0.2550\n",
      "\n",
      "Batch 761/992 ━━━━━━━━━━━━━━━━━━━━ 06:50:04\n",
      "Accuracy: 0.9090 - Loss: 0.2552\n",
      "\n",
      "Batch 762/992 ━━━━━━━━━━━━━━━━━━━━ 06:50:14\n",
      "Accuracy: 0.9085 - Loss: 0.2557\n",
      "\n",
      "Batch 763/992 ━━━━━━━━━━━━━━━━━━━━ 06:50:24\n",
      "Accuracy: 0.9086 - Loss: 0.2554\n",
      "\n",
      "Batch 764/992 ━━━━━━━━━━━━━━━━━━━━ 06:50:34\n",
      "Accuracy: 0.9087 - Loss: 0.2553\n",
      "\n",
      "Batch 765/992 ━━━━━━━━━━━━━━━━━━━━ 06:50:44\n",
      "Accuracy: 0.9088 - Loss: 0.2550\n",
      "\n",
      "Batch 766/992 ━━━━━━━━━━━━━━━━━━━━ 06:50:54\n",
      "Accuracy: 0.9085 - Loss: 0.2556\n",
      "\n",
      "Batch 767/992 ━━━━━━━━━━━━━━━━━━━━ 06:51:05\n",
      "Accuracy: 0.9084 - Loss: 0.2557\n",
      "\n",
      "Batch 768/992 ━━━━━━━━━━━━━━━━━━━━ 06:51:15\n",
      "Accuracy: 0.9084 - Loss: 0.2556\n",
      "\n",
      "Batch 769/992 ━━━━━━━━━━━━━━━━━━━━ 06:51:25\n",
      "Accuracy: 0.9083 - Loss: 0.2556\n",
      "\n",
      "Batch 770/992 ━━━━━━━━━━━━━━━━━━━━ 06:51:35\n",
      "Accuracy: 0.9084 - Loss: 0.2554\n",
      "\n",
      "Batch 771/992 ━━━━━━━━━━━━━━━━━━━━ 06:51:45\n",
      "Accuracy: 0.9084 - Loss: 0.2553\n",
      "\n",
      "Batch 772/992 ━━━━━━━━━━━━━━━━━━━━ 06:51:55\n",
      "Accuracy: 0.9085 - Loss: 0.2554\n",
      "\n",
      "Batch 773/992 ━━━━━━━━━━━━━━━━━━━━ 06:52:06\n",
      "Accuracy: 0.9083 - Loss: 0.2555\n",
      "\n",
      "Batch 774/992 ━━━━━━━━━━━━━━━━━━━━ 06:52:16\n",
      "Accuracy: 0.9079 - Loss: 0.2560\n",
      "\n",
      "Batch 775/992 ━━━━━━━━━━━━━━━━━━━━ 06:52:26\n",
      "Accuracy: 0.9079 - Loss: 0.2560\n",
      "\n",
      "Batch 776/992 ━━━━━━━━━━━━━━━━━━━━ 06:52:36\n",
      "Accuracy: 0.9080 - Loss: 0.2560\n",
      "\n",
      "Batch 777/992 ━━━━━━━━━━━━━━━━━━━━ 06:52:46\n",
      "Accuracy: 0.9081 - Loss: 0.2558\n",
      "\n",
      "Batch 778/992 ━━━━━━━━━━━━━━━━━━━━ 06:52:56\n",
      "Accuracy: 0.9078 - Loss: 0.2562\n",
      "\n",
      "Batch 779/992 ━━━━━━━━━━━━━━━━━━━━ 06:53:07\n",
      "Accuracy: 0.9076 - Loss: 0.2565\n",
      "\n",
      "Batch 780/992 ━━━━━━━━━━━━━━━━━━━━ 06:53:17\n",
      "Accuracy: 0.9074 - Loss: 0.2569\n",
      "\n",
      "Batch 781/992 ━━━━━━━━━━━━━━━━━━━━ 06:53:27\n",
      "Accuracy: 0.9075 - Loss: 0.2566\n",
      "\n",
      "Batch 782/992 ━━━━━━━━━━━━━━━━━━━━ 06:53:37\n",
      "Accuracy: 0.9074 - Loss: 0.2566\n",
      "\n",
      "Batch 783/992 ━━━━━━━━━━━━━━━━━━━━ 06:53:47\n",
      "Accuracy: 0.9076 - Loss: 0.2563\n",
      "\n",
      "Batch 784/992 ━━━━━━━━━━━━━━━━━━━━ 06:53:57\n",
      "Accuracy: 0.9075 - Loss: 0.2568\n",
      "\n",
      "Batch 785/992 ━━━━━━━━━━━━━━━━━━━━ 06:54:08\n",
      "Accuracy: 0.9076 - Loss: 0.2565\n",
      "\n",
      "Batch 786/992 ━━━━━━━━━━━━━━━━━━━━ 06:54:18\n",
      "Accuracy: 0.9078 - Loss: 0.2562\n",
      "\n",
      "Batch 787/992 ━━━━━━━━━━━━━━━━━━━━ 06:54:28\n",
      "Accuracy: 0.9079 - Loss: 0.2561\n",
      "\n",
      "Batch 788/992 ━━━━━━━━━━━━━━━━━━━━ 06:54:38\n",
      "Accuracy: 0.9077 - Loss: 0.2566\n",
      "\n",
      "Batch 789/992 ━━━━━━━━━━━━━━━━━━━━ 06:54:48\n",
      "Accuracy: 0.9078 - Loss: 0.2564\n",
      "\n",
      "Batch 790/992 ━━━━━━━━━━━━━━━━━━━━ 06:54:58\n",
      "Accuracy: 0.9079 - Loss: 0.2563\n",
      "\n",
      "Batch 791/992 ━━━━━━━━━━━━━━━━━━━━ 06:55:08\n",
      "Accuracy: 0.9077 - Loss: 0.2564\n",
      "\n",
      "Batch 792/992 ━━━━━━━━━━━━━━━━━━━━ 06:55:19\n",
      "Accuracy: 0.9075 - Loss: 0.2569\n",
      "\n",
      "Batch 793/992 ━━━━━━━━━━━━━━━━━━━━ 06:55:29\n",
      "Accuracy: 0.9075 - Loss: 0.2570\n",
      "\n",
      "Batch 794/992 ━━━━━━━━━━━━━━━━━━━━ 06:55:39\n",
      "Accuracy: 0.9074 - Loss: 0.2571\n",
      "\n",
      "Batch 795/992 ━━━━━━━━━━━━━━━━━━━━ 06:55:49\n",
      "Accuracy: 0.9074 - Loss: 0.2570\n",
      "\n",
      "Batch 796/992 ━━━━━━━━━━━━━━━━━━━━ 06:55:59\n",
      "Accuracy: 0.9072 - Loss: 0.2575\n",
      "\n",
      "Batch 797/992 ━━━━━━━━━━━━━━━━━━━━ 06:56:09\n",
      "Accuracy: 0.9073 - Loss: 0.2572\n",
      "\n",
      "Batch 798/992 ━━━━━━━━━━━━━━━━━━━━ 06:56:20\n",
      "Accuracy: 0.9070 - Loss: 0.2579\n",
      "\n",
      "Batch 799/992 ━━━━━━━━━━━━━━━━━━━━ 06:56:30\n",
      "Accuracy: 0.9071 - Loss: 0.2577\n",
      "\n",
      "Batch 800/992 ━━━━━━━━━━━━━━━━━━━━ 06:56:40\n",
      "Accuracy: 0.9072 - Loss: 0.2577\n",
      "\n",
      "Batch 801/992 ━━━━━━━━━━━━━━━━━━━━ 06:56:50\n",
      "Accuracy: 0.9071 - Loss: 0.2575\n",
      "\n",
      "Batch 802/992 ━━━━━━━━━━━━━━━━━━━━ 06:57:00\n",
      "Accuracy: 0.9071 - Loss: 0.2576\n",
      "\n",
      "Batch 803/992 ━━━━━━━━━━━━━━━━━━━━ 06:57:11\n",
      "Accuracy: 0.9071 - Loss: 0.2580\n",
      "\n",
      "Batch 804/992 ━━━━━━━━━━━━━━━━━━━━ 06:57:21\n",
      "Accuracy: 0.9070 - Loss: 0.2580\n",
      "\n",
      "Batch 805/992 ━━━━━━━━━━━━━━━━━━━━ 06:57:31\n",
      "Accuracy: 0.9071 - Loss: 0.2578\n",
      "\n",
      "Batch 806/992 ━━━━━━━━━━━━━━━━━━━━ 06:57:41\n",
      "Accuracy: 0.9073 - Loss: 0.2575\n",
      "\n",
      "Batch 807/992 ━━━━━━━━━━━━━━━━━━━━ 06:57:51\n",
      "Accuracy: 0.9071 - Loss: 0.2579\n",
      "\n",
      "Batch 808/992 ━━━━━━━━━━━━━━━━━━━━ 06:58:01\n",
      "Accuracy: 0.9069 - Loss: 0.2579\n",
      "\n",
      "Batch 809/992 ━━━━━━━━━━━━━━━━━━━━ 06:58:12\n",
      "Accuracy: 0.9068 - Loss: 0.2580\n",
      "\n",
      "Batch 810/992 ━━━━━━━━━━━━━━━━━━━━ 06:58:22\n",
      "Accuracy: 0.9068 - Loss: 0.2584\n",
      "\n",
      "Batch 811/992 ━━━━━━━━━━━━━━━━━━━━ 06:58:32\n",
      "Accuracy: 0.9069 - Loss: 0.2582\n",
      "\n",
      "Batch 812/992 ━━━━━━━━━━━━━━━━━━━━ 06:58:42\n",
      "Accuracy: 0.9066 - Loss: 0.2596\n",
      "\n",
      "Batch 813/992 ━━━━━━━━━━━━━━━━━━━━ 06:58:53\n",
      "Accuracy: 0.9065 - Loss: 0.2598\n",
      "\n",
      "Batch 814/992 ━━━━━━━━━━━━━━━━━━━━ 06:59:03\n",
      "Accuracy: 0.9065 - Loss: 0.2598\n",
      "\n",
      "Batch 815/992 ━━━━━━━━━━━━━━━━━━━━ 06:59:13\n",
      "Accuracy: 0.9066 - Loss: 0.2595\n",
      "\n",
      "Batch 816/992 ━━━━━━━━━━━━━━━━━━━━ 06:59:23\n",
      "Accuracy: 0.9066 - Loss: 0.2598\n",
      "\n",
      "Batch 817/992 ━━━━━━━━━━━━━━━━━━━━ 06:59:33\n",
      "Accuracy: 0.9067 - Loss: 0.2597\n",
      "\n",
      "Batch 818/992 ━━━━━━━━━━━━━━━━━━━━ 06:59:43\n",
      "Accuracy: 0.9066 - Loss: 0.2598\n",
      "\n",
      "Batch 819/992 ━━━━━━━━━━━━━━━━━━━━ 06:59:54\n",
      "Accuracy: 0.9063 - Loss: 0.2605\n",
      "\n",
      "Batch 820/992 ━━━━━━━━━━━━━━━━━━━━ 07:00:04\n",
      "Accuracy: 0.9061 - Loss: 0.2608\n",
      "\n",
      "Batch 821/992 ━━━━━━━━━━━━━━━━━━━━ 07:00:14\n",
      "Accuracy: 0.9059 - Loss: 0.2617\n",
      "\n",
      "Batch 822/992 ━━━━━━━━━━━━━━━━━━━━ 07:00:24\n",
      "Accuracy: 0.9059 - Loss: 0.2615\n",
      "\n",
      "Batch 823/992 ━━━━━━━━━━━━━━━━━━━━ 07:00:34\n",
      "Accuracy: 0.9060 - Loss: 0.2613\n",
      "\n",
      "Batch 824/992 ━━━━━━━━━━━━━━━━━━━━ 07:00:44\n",
      "Accuracy: 0.9058 - Loss: 0.2614\n",
      "\n",
      "Batch 825/992 ━━━━━━━━━━━━━━━━━━━━ 07:00:54\n",
      "Accuracy: 0.9056 - Loss: 0.2616\n",
      "\n",
      "Batch 826/992 ━━━━━━━━━━━━━━━━━━━━ 07:01:04\n",
      "Accuracy: 0.9056 - Loss: 0.2615\n",
      "\n",
      "Batch 827/992 ━━━━━━━━━━━━━━━━━━━━ 07:01:15\n",
      "Accuracy: 0.9055 - Loss: 0.2613\n",
      "\n",
      "Batch 828/992 ━━━━━━━━━━━━━━━━━━━━ 07:01:26\n",
      "Accuracy: 0.9056 - Loss: 0.2612\n",
      "\n",
      "Batch 829/992 ━━━━━━━━━━━━━━━━━━━━ 07:01:36\n",
      "Accuracy: 0.9056 - Loss: 0.2611\n",
      "\n",
      "Batch 830/992 ━━━━━━━━━━━━━━━━━━━━ 07:01:47\n",
      "Accuracy: 0.9053 - Loss: 0.2614\n",
      "\n",
      "Batch 831/992 ━━━━━━━━━━━━━━━━━━━━ 07:01:57\n",
      "Accuracy: 0.9051 - Loss: 0.2614\n",
      "\n",
      "Batch 832/992 ━━━━━━━━━━━━━━━━━━━━ 07:02:07\n",
      "Accuracy: 0.9052 - Loss: 0.2613\n",
      "\n",
      "Batch 833/992 ━━━━━━━━━━━━━━━━━━━━ 07:02:17\n",
      "Accuracy: 0.9053 - Loss: 0.2611\n",
      "\n",
      "Batch 834/992 ━━━━━━━━━━━━━━━━━━━━ 07:02:27\n",
      "Accuracy: 0.9054 - Loss: 0.2609\n",
      "\n",
      "Batch 835/992 ━━━━━━━━━━━━━━━━━━━━ 07:02:37\n",
      "Accuracy: 0.9055 - Loss: 0.2607\n",
      "\n",
      "Batch 836/992 ━━━━━━━━━━━━━━━━━━━━ 07:02:48\n",
      "Accuracy: 0.9054 - Loss: 0.2610\n",
      "\n",
      "Batch 837/992 ━━━━━━━━━━━━━━━━━━━━ 07:02:58\n",
      "Accuracy: 0.9055 - Loss: 0.2607\n",
      "\n",
      "Batch 838/992 ━━━━━━━━━━━━━━━━━━━━ 07:03:08\n",
      "Accuracy: 0.9056 - Loss: 0.2604\n",
      "\n",
      "Batch 839/992 ━━━━━━━━━━━━━━━━━━━━ 07:03:18\n",
      "Accuracy: 0.9057 - Loss: 0.2604\n",
      "\n",
      "Batch 840/992 ━━━━━━━━━━━━━━━━━━━━ 07:03:28\n",
      "Accuracy: 0.9057 - Loss: 0.2604\n",
      "\n",
      "Batch 841/992 ━━━━━━━━━━━━━━━━━━━━ 07:03:39\n",
      "Accuracy: 0.9058 - Loss: 0.2602\n",
      "\n",
      "Batch 842/992 ━━━━━━━━━━━━━━━━━━━━ 07:03:49\n",
      "Accuracy: 0.9057 - Loss: 0.2609\n",
      "\n",
      "Batch 843/992 ━━━━━━━━━━━━━━━━━━━━ 07:03:59\n",
      "Accuracy: 0.9057 - Loss: 0.2610\n",
      "\n",
      "Batch 844/992 ━━━━━━━━━━━━━━━━━━━━ 07:04:09\n",
      "Accuracy: 0.9058 - Loss: 0.2608\n",
      "\n",
      "Batch 845/992 ━━━━━━━━━━━━━━━━━━━━ 07:04:19\n",
      "Accuracy: 0.9059 - Loss: 0.2606\n",
      "\n",
      "Batch 846/992 ━━━━━━━━━━━━━━━━━━━━ 07:04:29\n",
      "Accuracy: 0.9060 - Loss: 0.2605\n",
      "\n",
      "Batch 847/992 ━━━━━━━━━━━━━━━━━━━━ 07:04:40\n",
      "Accuracy: 0.9061 - Loss: 0.2603\n",
      "\n",
      "Batch 848/992 ━━━━━━━━━━━━━━━━━━━━ 07:04:50\n",
      "Accuracy: 0.9061 - Loss: 0.2607\n",
      "\n",
      "Batch 849/992 ━━━━━━━━━━━━━━━━━━━━ 07:05:00\n",
      "Accuracy: 0.9061 - Loss: 0.2608\n",
      "\n",
      "Batch 850/992 ━━━━━━━━━━━━━━━━━━━━ 07:05:10\n",
      "Accuracy: 0.9060 - Loss: 0.2611\n",
      "\n",
      "Batch 851/992 ━━━━━━━━━━━━━━━━━━━━ 07:05:21\n",
      "Accuracy: 0.9061 - Loss: 0.2610\n",
      "\n",
      "Batch 852/992 ━━━━━━━━━━━━━━━━━━━━ 07:05:31\n",
      "Accuracy: 0.9061 - Loss: 0.2610\n",
      "\n",
      "Batch 853/992 ━━━━━━━━━━━━━━━━━━━━ 07:05:41\n",
      "Accuracy: 0.9061 - Loss: 0.2610\n",
      "\n",
      "Batch 854/992 ━━━━━━━━━━━━━━━━━━━━ 07:05:51\n",
      "Accuracy: 0.9062 - Loss: 0.2607\n",
      "\n",
      "Batch 855/992 ━━━━━━━━━━━━━━━━━━━━ 07:06:01\n",
      "Accuracy: 0.9060 - Loss: 0.2612\n",
      "\n",
      "Batch 856/992 ━━━━━━━━━━━━━━━━━━━━ 07:06:12\n",
      "Accuracy: 0.9061 - Loss: 0.2611\n",
      "\n",
      "Batch 857/992 ━━━━━━━━━━━━━━━━━━━━ 07:06:22\n",
      "Accuracy: 0.9062 - Loss: 0.2611\n",
      "\n",
      "Batch 858/992 ━━━━━━━━━━━━━━━━━━━━ 07:06:32\n",
      "Accuracy: 0.9063 - Loss: 0.2609\n",
      "\n",
      "Batch 859/992 ━━━━━━━━━━━━━━━━━━━━ 07:06:42\n",
      "Accuracy: 0.9064 - Loss: 0.2607\n",
      "\n",
      "Batch 860/992 ━━━━━━━━━━━━━━━━━━━━ 07:06:52\n",
      "Accuracy: 0.9064 - Loss: 0.2606\n",
      "\n",
      "Batch 861/992 ━━━━━━━━━━━━━━━━━━━━ 07:07:03\n",
      "Accuracy: 0.9064 - Loss: 0.2605\n",
      "\n",
      "Batch 862/992 ━━━━━━━━━━━━━━━━━━━━ 07:07:13\n",
      "Accuracy: 0.9063 - Loss: 0.2607\n",
      "\n",
      "Batch 863/992 ━━━━━━━━━━━━━━━━━━━━ 07:07:23\n",
      "Accuracy: 0.9063 - Loss: 0.2610\n",
      "\n",
      "Batch 864/992 ━━━━━━━━━━━━━━━━━━━━ 07:07:33\n",
      "Accuracy: 0.9064 - Loss: 0.2608\n",
      "\n",
      "Batch 865/992 ━━━━━━━━━━━━━━━━━━━━ 07:07:44\n",
      "Accuracy: 0.9062 - Loss: 0.2609\n",
      "\n",
      "Batch 866/992 ━━━━━━━━━━━━━━━━━━━━ 07:07:54\n",
      "Accuracy: 0.9059 - Loss: 0.2615\n",
      "\n",
      "Batch 867/992 ━━━━━━━━━━━━━━━━━━━━ 07:08:04\n",
      "Accuracy: 0.9059 - Loss: 0.2616\n",
      "\n",
      "Batch 868/992 ━━━━━━━━━━━━━━━━━━━━ 07:08:14\n",
      "Accuracy: 0.9060 - Loss: 0.2616\n",
      "\n",
      "Batch 869/992 ━━━━━━━━━━━━━━━━━━━━ 07:08:24\n",
      "Accuracy: 0.9058 - Loss: 0.2619\n",
      "\n",
      "Batch 870/992 ━━━━━━━━━━━━━━━━━━━━ 07:08:34\n",
      "Accuracy: 0.9059 - Loss: 0.2617\n",
      "\n",
      "Batch 871/992 ━━━━━━━━━━━━━━━━━━━━ 07:08:45\n",
      "Accuracy: 0.9059 - Loss: 0.2617\n",
      "\n",
      "Batch 872/992 ━━━━━━━━━━━━━━━━━━━━ 07:08:55\n",
      "Accuracy: 0.9058 - Loss: 0.2617\n",
      "\n",
      "Batch 873/992 ━━━━━━━━━━━━━━━━━━━━ 07:09:05\n",
      "Accuracy: 0.9056 - Loss: 0.2619\n",
      "\n",
      "Batch 874/992 ━━━━━━━━━━━━━━━━━━━━ 07:09:16\n",
      "Accuracy: 0.9056 - Loss: 0.2621\n",
      "\n",
      "Batch 875/992 ━━━━━━━━━━━━━━━━━━━━ 07:09:26\n",
      "Accuracy: 0.9057 - Loss: 0.2619\n",
      "\n",
      "Batch 876/992 ━━━━━━━━━━━━━━━━━━━━ 07:09:36\n",
      "Accuracy: 0.9057 - Loss: 0.2621\n",
      "\n",
      "Batch 877/992 ━━━━━━━━━━━━━━━━━━━━ 07:09:46\n",
      "Accuracy: 0.9055 - Loss: 0.2624\n",
      "\n",
      "Batch 878/992 ━━━━━━━━━━━━━━━━━━━━ 07:09:56\n",
      "Accuracy: 0.9055 - Loss: 0.2624\n",
      "\n",
      "Batch 879/992 ━━━━━━━━━━━━━━━━━━━━ 07:10:06\n",
      "Accuracy: 0.9054 - Loss: 0.2623\n",
      "\n",
      "Batch 880/992 ━━━━━━━━━━━━━━━━━━━━ 07:10:17\n",
      "Accuracy: 0.9054 - Loss: 0.2624\n",
      "\n",
      "Batch 881/992 ━━━━━━━━━━━━━━━━━━━━ 07:10:27\n",
      "Accuracy: 0.9055 - Loss: 0.2621\n",
      "\n",
      "Batch 882/992 ━━━━━━━━━━━━━━━━━━━━ 07:10:37\n",
      "Accuracy: 0.9050 - Loss: 0.2626\n",
      "\n",
      "Batch 883/992 ━━━━━━━━━━━━━━━━━━━━ 07:10:47\n",
      "Accuracy: 0.9052 - Loss: 0.2623\n",
      "\n",
      "Batch 884/992 ━━━━━━━━━━━━━━━━━━━━ 07:10:57\n",
      "Accuracy: 0.9050 - Loss: 0.2625\n",
      "\n",
      "Batch 885/992 ━━━━━━━━━━━━━━━━━━━━ 07:11:07\n",
      "Accuracy: 0.9047 - Loss: 0.2632\n",
      "\n",
      "Batch 886/992 ━━━━━━━━━━━━━━━━━━━━ 07:11:18\n",
      "Accuracy: 0.9048 - Loss: 0.2630\n",
      "\n",
      "Batch 887/992 ━━━━━━━━━━━━━━━━━━━━ 07:11:28\n",
      "Accuracy: 0.9046 - Loss: 0.2633\n",
      "\n",
      "Batch 888/992 ━━━━━━━━━━━━━━━━━━━━ 07:11:39\n",
      "Accuracy: 0.9046 - Loss: 0.2634\n",
      "\n",
      "Batch 889/992 ━━━━━━━━━━━━━━━━━━━━ 07:11:49\n",
      "Accuracy: 0.9045 - Loss: 0.2633\n",
      "\n",
      "Batch 890/992 ━━━━━━━━━━━━━━━━━━━━ 07:11:59\n",
      "Accuracy: 0.9046 - Loss: 0.2630\n",
      "\n",
      "Batch 891/992 ━━━━━━━━━━━━━━━━━━━━ 07:12:09\n",
      "Accuracy: 0.9046 - Loss: 0.2633\n",
      "\n",
      "Batch 892/992 ━━━━━━━━━━━━━━━━━━━━ 07:12:20\n",
      "Accuracy: 0.9046 - Loss: 0.2634\n",
      "\n",
      "Batch 893/992 ━━━━━━━━━━━━━━━━━━━━ 07:12:30\n",
      "Accuracy: 0.9045 - Loss: 0.2635\n",
      "\n",
      "Batch 894/992 ━━━━━━━━━━━━━━━━━━━━ 07:12:40\n",
      "Accuracy: 0.9044 - Loss: 0.2639\n",
      "\n",
      "Batch 895/992 ━━━━━━━━━━━━━━━━━━━━ 07:12:50\n",
      "Accuracy: 0.9042 - Loss: 0.2642\n",
      "\n",
      "Batch 896/992 ━━━━━━━━━━━━━━━━━━━━ 07:13:00\n",
      "Accuracy: 0.9040 - Loss: 0.2647\n",
      "\n",
      "Batch 897/992 ━━━━━━━━━━━━━━━━━━━━ 07:13:10\n",
      "Accuracy: 0.9041 - Loss: 0.2644\n",
      "\n",
      "Batch 898/992 ━━━━━━━━━━━━━━━━━━━━ 07:13:21\n",
      "Accuracy: 0.9042 - Loss: 0.2644\n",
      "\n",
      "Batch 899/992 ━━━━━━━━━━━━━━━━━━━━ 07:13:31\n",
      "Accuracy: 0.9041 - Loss: 0.2646\n",
      "\n",
      "Batch 900/992 ━━━━━━━━━━━━━━━━━━━━ 07:13:41\n",
      "Accuracy: 0.9042 - Loss: 0.2644\n",
      "\n",
      "Batch 901/992 ━━━━━━━━━━━━━━━━━━━━ 07:13:51\n",
      "Accuracy: 0.9041 - Loss: 0.2646\n",
      "\n",
      "Batch 902/992 ━━━━━━━━━━━━━━━━━━━━ 07:14:01\n",
      "Accuracy: 0.9037 - Loss: 0.2654\n",
      "\n",
      "Batch 903/992 ━━━━━━━━━━━━━━━━━━━━ 07:14:11\n",
      "Accuracy: 0.9037 - Loss: 0.2658\n",
      "\n",
      "Batch 904/992 ━━━━━━━━━━━━━━━━━━━━ 07:14:21\n",
      "Accuracy: 0.9035 - Loss: 0.2660\n",
      "\n",
      "Batch 905/992 ━━━━━━━━━━━━━━━━━━━━ 07:14:31\n",
      "Accuracy: 0.9033 - Loss: 0.2662\n",
      "\n",
      "Batch 906/992 ━━━━━━━━━━━━━━━━━━━━ 07:14:41\n",
      "Accuracy: 0.9034 - Loss: 0.2660\n",
      "\n",
      "Batch 907/992 ━━━━━━━━━━━━━━━━━━━━ 07:14:52\n",
      "Accuracy: 0.9034 - Loss: 0.2663\n",
      "\n",
      "Batch 908/992 ━━━━━━━━━━━━━━━━━━━━ 07:15:02\n",
      "Accuracy: 0.9035 - Loss: 0.2661\n",
      "\n",
      "Batch 909/992 ━━━━━━━━━━━━━━━━━━━━ 07:15:12\n",
      "Accuracy: 0.9033 - Loss: 0.2664\n",
      "\n",
      "Batch 910/992 ━━━━━━━━━━━━━━━━━━━━ 07:15:22\n",
      "Accuracy: 0.9034 - Loss: 0.2661\n",
      "\n",
      "Batch 911/992 ━━━━━━━━━━━━━━━━━━━━ 07:15:32\n",
      "Accuracy: 0.9035 - Loss: 0.2659\n",
      "\n",
      "Batch 912/992 ━━━━━━━━━━━━━━━━━━━━ 07:15:42\n",
      "Accuracy: 0.9036 - Loss: 0.2656\n",
      "\n",
      "Batch 913/992 ━━━━━━━━━━━━━━━━━━━━ 07:15:52\n",
      "Accuracy: 0.9036 - Loss: 0.2657\n",
      "\n",
      "Batch 914/992 ━━━━━━━━━━━━━━━━━━━━ 07:16:02\n",
      "Accuracy: 0.9034 - Loss: 0.2663\n",
      "\n",
      "Batch 915/992 ━━━━━━━━━━━━━━━━━━━━ 07:16:12\n",
      "Accuracy: 0.9034 - Loss: 0.2662\n",
      "\n",
      "Batch 916/992 ━━━━━━━━━━━━━━━━━━━━ 07:16:22\n",
      "Accuracy: 0.9035 - Loss: 0.2660\n",
      "\n",
      "Batch 917/992 ━━━━━━━━━━━━━━━━━━━━ 07:16:32\n",
      "Accuracy: 0.9034 - Loss: 0.2665\n",
      "\n",
      "Batch 918/992 ━━━━━━━━━━━━━━━━━━━━ 07:16:42\n",
      "Accuracy: 0.9031 - Loss: 0.2669\n",
      "\n",
      "Batch 919/992 ━━━━━━━━━━━━━━━━━━━━ 07:16:53\n",
      "Accuracy: 0.9030 - Loss: 0.2671\n",
      "\n",
      "Batch 920/992 ━━━━━━━━━━━━━━━━━━━━ 07:17:03\n",
      "Accuracy: 0.9031 - Loss: 0.2669\n",
      "\n",
      "Batch 921/992 ━━━━━━━━━━━━━━━━━━━━ 07:17:13\n",
      "Accuracy: 0.9032 - Loss: 0.2666\n",
      "\n",
      "Batch 922/992 ━━━━━━━━━━━━━━━━━━━━ 07:17:24\n",
      "Accuracy: 0.9032 - Loss: 0.2669\n",
      "\n",
      "Batch 923/992 ━━━━━━━━━━━━━━━━━━━━ 07:17:34\n",
      "Accuracy: 0.9032 - Loss: 0.2670\n",
      "\n",
      "Batch 924/992 ━━━━━━━━━━━━━━━━━━━━ 07:17:44\n",
      "Accuracy: 0.9030 - Loss: 0.2672\n",
      "\n",
      "Batch 925/992 ━━━━━━━━━━━━━━━━━━━━ 07:17:55\n",
      "Accuracy: 0.9030 - Loss: 0.2676\n",
      "\n",
      "Batch 926/992 ━━━━━━━━━━━━━━━━━━━━ 07:18:05\n",
      "Accuracy: 0.9029 - Loss: 0.2676\n",
      "\n",
      "Batch 927/992 ━━━━━━━━━━━━━━━━━━━━ 07:18:15\n",
      "Accuracy: 0.9029 - Loss: 0.2676\n",
      "\n",
      "Batch 928/992 ━━━━━━━━━━━━━━━━━━━━ 07:18:25\n",
      "Accuracy: 0.9029 - Loss: 0.2676\n",
      "\n",
      "Batch 929/992 ━━━━━━━━━━━━━━━━━━━━ 07:18:35\n",
      "Accuracy: 0.9029 - Loss: 0.2678\n",
      "\n",
      "Batch 930/992 ━━━━━━━━━━━━━━━━━━━━ 07:18:45\n",
      "Accuracy: 0.9030 - Loss: 0.2677\n",
      "\n",
      "Batch 931/992 ━━━━━━━━━━━━━━━━━━━━ 07:18:55\n",
      "Accuracy: 0.9029 - Loss: 0.2679\n",
      "\n",
      "Batch 932/992 ━━━━━━━━━━━━━━━━━━━━ 07:19:05\n",
      "Accuracy: 0.9030 - Loss: 0.2677\n",
      "\n",
      "Batch 933/992 ━━━━━━━━━━━━━━━━━━━━ 07:19:15\n",
      "Accuracy: 0.9030 - Loss: 0.2680\n",
      "\n",
      "Batch 934/992 ━━━━━━━━━━━━━━━━━━━━ 07:19:25\n",
      "Accuracy: 0.9030 - Loss: 0.2684\n",
      "\n",
      "Batch 935/992 ━━━━━━━━━━━━━━━━━━━━ 07:19:36\n",
      "Accuracy: 0.9029 - Loss: 0.2685\n",
      "\n",
      "Batch 936/992 ━━━━━━━━━━━━━━━━━━━━ 07:19:46\n",
      "Accuracy: 0.9030 - Loss: 0.2682\n",
      "\n",
      "Batch 937/992 ━━━━━━━━━━━━━━━━━━━━ 07:19:56\n",
      "Accuracy: 0.9029 - Loss: 0.2685\n",
      "\n",
      "Batch 938/992 ━━━━━━━━━━━━━━━━━━━━ 07:20:06\n",
      "Accuracy: 0.9030 - Loss: 0.2684\n",
      "\n",
      "Batch 939/992 ━━━━━━━━━━━━━━━━━━━━ 07:20:16\n",
      "Accuracy: 0.9028 - Loss: 0.2684\n",
      "\n",
      "Batch 940/992 ━━━━━━━━━━━━━━━━━━━━ 07:20:26\n",
      "Accuracy: 0.9029 - Loss: 0.2682\n",
      "\n",
      "Batch 941/992 ━━━━━━━━━━━━━━━━━━━━ 07:20:36\n",
      "Accuracy: 0.9029 - Loss: 0.2681\n",
      "\n",
      "Batch 942/992 ━━━━━━━━━━━━━━━━━━━━ 07:20:46\n",
      "Accuracy: 0.9027 - Loss: 0.2683\n",
      "\n",
      "Batch 943/992 ━━━━━━━━━━━━━━━━━━━━ 07:20:56\n",
      "Accuracy: 0.9028 - Loss: 0.2680\n",
      "\n",
      "Batch 944/992 ━━━━━━━━━━━━━━━━━━━━ 07:21:06\n",
      "Accuracy: 0.9028 - Loss: 0.2680\n",
      "\n",
      "Batch 945/992 ━━━━━━━━━━━━━━━━━━━━ 07:21:17\n",
      "Accuracy: 0.9028 - Loss: 0.2682\n",
      "\n",
      "Batch 946/992 ━━━━━━━━━━━━━━━━━━━━ 07:21:27\n",
      "Accuracy: 0.9029 - Loss: 0.2680\n",
      "\n",
      "Batch 947/992 ━━━━━━━━━━━━━━━━━━━━ 07:21:37\n",
      "Accuracy: 0.9030 - Loss: 0.2678\n",
      "\n",
      "Batch 948/992 ━━━━━━━━━━━━━━━━━━━━ 07:21:47\n",
      "Accuracy: 0.9031 - Loss: 0.2678\n",
      "\n",
      "Batch 949/992 ━━━━━━━━━━━━━━━━━━━━ 07:21:57\n",
      "Accuracy: 0.9031 - Loss: 0.2677\n",
      "\n",
      "Batch 950/992 ━━━━━━━━━━━━━━━━━━━━ 07:22:07\n",
      "Accuracy: 0.9029 - Loss: 0.2678\n",
      "\n",
      "Batch 951/992 ━━━━━━━━━━━━━━━━━━━━ 07:22:17\n",
      "Accuracy: 0.9030 - Loss: 0.2676\n",
      "\n",
      "Batch 952/992 ━━━━━━━━━━━━━━━━━━━━ 07:22:27\n",
      "Accuracy: 0.9030 - Loss: 0.2677\n",
      "\n",
      "Batch 953/992 ━━━━━━━━━━━━━━━━━━━━ 07:22:37\n",
      "Accuracy: 0.9031 - Loss: 0.2675\n",
      "\n",
      "Batch 954/992 ━━━━━━━━━━━━━━━━━━━━ 07:22:47\n",
      "Accuracy: 0.9032 - Loss: 0.2673\n",
      "\n",
      "Batch 955/992 ━━━━━━━━━━━━━━━━━━━━ 07:22:57\n",
      "Accuracy: 0.9030 - Loss: 0.2675\n",
      "\n",
      "Batch 956/992 ━━━━━━━━━━━━━━━━━━━━ 07:23:08\n",
      "Accuracy: 0.9031 - Loss: 0.2673\n",
      "\n",
      "Batch 957/992 ━━━━━━━━━━━━━━━━━━━━ 07:23:18\n",
      "Accuracy: 0.9032 - Loss: 0.2671\n",
      "\n",
      "Batch 958/992 ━━━━━━━━━━━━━━━━━━━━ 07:23:28\n",
      "Accuracy: 0.9029 - Loss: 0.2677\n",
      "\n",
      "Batch 959/992 ━━━━━━━━━━━━━━━━━━━━ 07:23:38\n",
      "Accuracy: 0.9030 - Loss: 0.2676\n",
      "\n",
      "Batch 960/992 ━━━━━━━━━━━━━━━━━━━━ 07:23:48\n",
      "Accuracy: 0.9029 - Loss: 0.2681\n",
      "\n",
      "Batch 961/992 ━━━━━━━━━━━━━━━━━━━━ 07:23:58\n",
      "Accuracy: 0.9030 - Loss: 0.2679\n",
      "\n",
      "Batch 962/992 ━━━━━━━━━━━━━━━━━━━━ 07:24:08\n",
      "Accuracy: 0.9031 - Loss: 0.2678\n",
      "\n",
      "Batch 963/992 ━━━━━━━━━━━━━━━━━━━━ 07:24:18\n",
      "Accuracy: 0.9032 - Loss: 0.2676\n",
      "\n",
      "Batch 964/992 ━━━━━━━━━━━━━━━━━━━━ 07:24:28\n",
      "Accuracy: 0.9031 - Loss: 0.2676\n",
      "\n",
      "Batch 965/992 ━━━━━━━━━━━━━━━━━━━━ 07:24:39\n",
      "Accuracy: 0.9028 - Loss: 0.2682\n",
      "\n",
      "Batch 966/992 ━━━━━━━━━━━━━━━━━━━━ 07:24:49\n",
      "Accuracy: 0.9030 - Loss: 0.2680\n",
      "\n",
      "Batch 967/992 ━━━━━━━━━━━━━━━━━━━━ 07:25:00\n",
      "Accuracy: 0.9029 - Loss: 0.2680\n",
      "\n",
      "Batch 968/992 ━━━━━━━━━━━━━━━━━━━━ 07:25:10\n",
      "Accuracy: 0.9028 - Loss: 0.2681\n",
      "\n",
      "Batch 969/992 ━━━━━━━━━━━━━━━━━━━━ 07:25:20\n",
      "Accuracy: 0.9026 - Loss: 0.2682\n",
      "\n",
      "Batch 970/992 ━━━━━━━━━━━━━━━━━━━━ 07:25:30\n",
      "Accuracy: 0.9026 - Loss: 0.2683\n",
      "\n",
      "Batch 971/992 ━━━━━━━━━━━━━━━━━━━━ 07:25:41\n",
      "Accuracy: 0.9027 - Loss: 0.2682\n",
      "\n",
      "Batch 972/992 ━━━━━━━━━━━━━━━━━━━━ 07:25:51\n",
      "Accuracy: 0.9028 - Loss: 0.2680\n",
      "\n",
      "Batch 973/992 ━━━━━━━━━━━━━━━━━━━━ 07:26:01\n",
      "Accuracy: 0.9025 - Loss: 0.2683\n",
      "\n",
      "Batch 974/992 ━━━━━━━━━━━━━━━━━━━━ 07:26:11\n",
      "Accuracy: 0.9025 - Loss: 0.2683\n",
      "\n",
      "Batch 975/992 ━━━━━━━━━━━━━━━━━━━━ 07:26:22\n",
      "Accuracy: 0.9026 - Loss: 0.2681\n",
      "\n",
      "Batch 976/992 ━━━━━━━━━━━━━━━━━━━━ 07:26:32\n",
      "Accuracy: 0.9023 - Loss: 0.2687\n",
      "\n",
      "Batch 977/992 ━━━━━━━━━━━━━━━━━━━━ 07:26:42\n",
      "Accuracy: 0.9023 - Loss: 0.2686\n",
      "\n",
      "Batch 978/992 ━━━━━━━━━━━━━━━━━━━━ 07:26:52\n",
      "Accuracy: 0.9021 - Loss: 0.2688\n",
      "\n",
      "Batch 979/992 ━━━━━━━━━━━━━━━━━━━━ 07:27:02\n",
      "Accuracy: 0.9022 - Loss: 0.2686\n",
      "\n",
      "Batch 980/992 ━━━━━━━━━━━━━━━━━━━━ 07:27:13\n",
      "Accuracy: 0.9023 - Loss: 0.2685\n",
      "\n",
      "Batch 981/992 ━━━━━━━━━━━━━━━━━━━━ 07:27:23\n",
      "Accuracy: 0.9024 - Loss: 0.2683\n",
      "\n",
      "Batch 982/992 ━━━━━━━━━━━━━━━━━━━━ 07:27:33\n",
      "Accuracy: 0.9025 - Loss: 0.2681\n",
      "\n",
      "Batch 983/992 ━━━━━━━━━━━━━━━━━━━━ 07:27:43\n",
      "Accuracy: 0.9023 - Loss: 0.2692\n",
      "\n",
      "Batch 984/992 ━━━━━━━━━━━━━━━━━━━━ 07:27:53\n",
      "Accuracy: 0.9023 - Loss: 0.2692\n",
      "\n",
      "Batch 985/992 ━━━━━━━━━━━━━━━━━━━━ 07:28:03\n",
      "Accuracy: 0.9022 - Loss: 0.2697\n",
      "\n",
      "Batch 986/992 ━━━━━━━━━━━━━━━━━━━━ 07:28:13\n",
      "Accuracy: 0.9023 - Loss: 0.2694\n",
      "\n",
      "Batch 987/992 ━━━━━━━━━━━━━━━━━━━━ 07:28:23\n",
      "Accuracy: 0.9022 - Loss: 0.2693\n",
      "\n",
      "Batch 988/992 ━━━━━━━━━━━━━━━━━━━━ 07:28:34\n",
      "Accuracy: 0.9022 - Loss: 0.2693\n",
      "\n",
      "Batch 989/992 ━━━━━━━━━━━━━━━━━━━━ 07:28:44\n",
      "Accuracy: 0.9022 - Loss: 0.2693\n",
      "\n",
      "Batch 990/992 ━━━━━━━━━━━━━━━━━━━━ 07:28:54\n",
      "Accuracy: 0.9023 - Loss: 0.2691\n",
      "\n",
      "Batch 991/992 ━━━━━━━━━━━━━━━━━━━━ 07:29:04\n",
      "Accuracy: 0.9022 - Loss: 0.2690\n",
      "\n",
      "Batch 992/992 ━━━━━━━━━━━━━━━━━━━━ 07:29:14\n",
      "Accuracy: 0.9023 - Loss: 0.2689\n",
      "\n",
      "\n",
      "Epoch 6/10\n",
      "Batch 1/992 ━━━━━━━━━━━━━━━━━━━━ 07:45:44\n",
      "Accuracy: 0.8750 - Loss: 0.3971\n",
      "\n",
      "Batch 2/992 ━━━━━━━━━━━━━━━━━━━━ 07:45:54\n",
      "Accuracy: 0.8750 - Loss: 0.3405\n",
      "\n",
      "Batch 3/992 ━━━━━━━━━━━━━━━━━━━━ 07:46:04\n",
      "Accuracy: 0.8750 - Loss: 0.2939\n",
      "\n",
      "Batch 4/992 ━━━━━━━━━━━━━━━━━━━━ 07:46:14\n",
      "Accuracy: 0.9062 - Loss: 0.2313\n",
      "\n",
      "Batch 5/992 ━━━━━━━━━━━━━━━━━━━━ 07:46:24\n",
      "Accuracy: 0.9000 - Loss: 0.3293\n",
      "\n",
      "Batch 6/992 ━━━━━━━━━━━━━━━━━━━━ 07:46:35\n",
      "Accuracy: 0.8958 - Loss: 0.3650\n",
      "\n",
      "Batch 7/992 ━━━━━━━━━━━━━━━━━━━━ 07:46:45\n",
      "Accuracy: 0.8929 - Loss: 0.3714\n",
      "\n",
      "Batch 8/992 ━━━━━━━━━━━━━━━━━━━━ 07:46:56\n",
      "Accuracy: 0.8906 - Loss: 0.3518\n",
      "\n",
      "Batch 9/992 ━━━━━━━━━━━━━━━━━━━━ 07:47:06\n",
      "Accuracy: 0.9028 - Loss: 0.3201\n",
      "\n",
      "Batch 10/992 ━━━━━━━━━━━━━━━━━━━━ 07:47:16\n",
      "Accuracy: 0.8875 - Loss: 0.3387\n",
      "\n",
      "Batch 11/992 ━━━━━━━━━━━━━━━━━━━━ 07:47:26\n",
      "Accuracy: 0.8977 - Loss: 0.3155\n",
      "\n",
      "Batch 12/992 ━━━━━━━━━━━━━━━━━━━━ 07:47:36\n",
      "Accuracy: 0.8958 - Loss: 0.3110\n",
      "\n",
      "Batch 13/992 ━━━━━━━━━━━━━━━━━━━━ 07:47:46\n",
      "Accuracy: 0.8942 - Loss: 0.3064\n",
      "\n",
      "Batch 14/992 ━━━━━━━━━━━━━━━━━━━━ 07:47:56\n",
      "Accuracy: 0.9018 - Loss: 0.2895\n",
      "\n",
      "Batch 15/992 ━━━━━━━━━━━━━━━━━━━━ 07:48:06\n",
      "Accuracy: 0.9000 - Loss: 0.2903\n",
      "\n",
      "Batch 16/992 ━━━━━━━━━━━━━━━━━━━━ 07:48:16\n",
      "Accuracy: 0.9062 - Loss: 0.2759\n",
      "\n",
      "Batch 17/992 ━━━━━━━━━━━━━━━━━━━━ 07:48:26\n",
      "Accuracy: 0.8971 - Loss: 0.2883\n",
      "\n",
      "Batch 18/992 ━━━━━━━━━━━━━━━━━━━━ 07:48:37\n",
      "Accuracy: 0.9028 - Loss: 0.2754\n",
      "\n",
      "Batch 19/992 ━━━━━━━━━━━━━━━━━━━━ 07:48:47\n",
      "Accuracy: 0.8947 - Loss: 0.2837\n",
      "\n",
      "Batch 20/992 ━━━━━━━━━━━━━━━━━━━━ 07:48:57\n",
      "Accuracy: 0.9000 - Loss: 0.2717\n",
      "\n",
      "Batch 21/992 ━━━━━━━━━━━━━━━━━━━━ 07:49:07\n",
      "Accuracy: 0.9048 - Loss: 0.2649\n",
      "\n",
      "Batch 22/992 ━━━━━━━━━━━━━━━━━━━━ 07:49:17\n",
      "Accuracy: 0.8977 - Loss: 0.2719\n",
      "\n",
      "Batch 23/992 ━━━━━━━━━━━━━━━━━━━━ 07:49:27\n",
      "Accuracy: 0.8967 - Loss: 0.2852\n",
      "\n",
      "Batch 24/992 ━━━━━━━━━━━━━━━━━━━━ 07:49:37\n",
      "Accuracy: 0.8750 - Loss: 0.3184\n",
      "\n",
      "Batch 25/992 ━━━━━━━━━━━━━━━━━━━━ 07:49:47\n",
      "Accuracy: 0.8750 - Loss: 0.3240\n",
      "\n",
      "Batch 26/992 ━━━━━━━━━━━━━━━━━━━━ 07:49:57\n",
      "Accuracy: 0.8798 - Loss: 0.3200\n",
      "\n",
      "Batch 27/992 ━━━━━━━━━━━━━━━━━━━━ 07:50:07\n",
      "Accuracy: 0.8843 - Loss: 0.3104\n",
      "\n",
      "Batch 28/992 ━━━━━━━━━━━━━━━━━━━━ 07:50:17\n",
      "Accuracy: 0.8884 - Loss: 0.2998\n",
      "\n",
      "Batch 29/992 ━━━━━━━━━━━━━━━━━━━━ 07:50:27\n",
      "Accuracy: 0.8879 - Loss: 0.2959\n",
      "\n",
      "Batch 30/992 ━━━━━━━━━━━━━━━━━━━━ 07:50:38\n",
      "Accuracy: 0.8917 - Loss: 0.2888\n",
      "\n",
      "Batch 31/992 ━━━━━━━━━━━━━━━━━━━━ 07:50:48\n",
      "Accuracy: 0.8952 - Loss: 0.2835\n",
      "\n",
      "Batch 32/992 ━━━━━━━━━━━━━━━━━━━━ 07:50:58\n",
      "Accuracy: 0.8945 - Loss: 0.2850\n",
      "\n",
      "Batch 33/992 ━━━━━━━━━━━━━━━━━━━━ 07:51:08\n",
      "Accuracy: 0.8939 - Loss: 0.2981\n",
      "\n",
      "Batch 34/992 ━━━━━━━━━━━━━━━━━━━━ 07:51:18\n",
      "Accuracy: 0.8934 - Loss: 0.3037\n",
      "\n",
      "Batch 35/992 ━━━━━━━━━━━━━━━━━━━━ 07:51:28\n",
      "Accuracy: 0.8929 - Loss: 0.3002\n",
      "\n",
      "Batch 36/992 ━━━━━━━━━━━━━━━━━━━━ 07:51:38\n",
      "Accuracy: 0.8958 - Loss: 0.2971\n",
      "\n",
      "Batch 37/992 ━━━━━━━━━━━━━━━━━━━━ 07:51:48\n",
      "Accuracy: 0.8986 - Loss: 0.2914\n",
      "\n",
      "Batch 38/992 ━━━━━━━━━━━━━━━━━━━━ 07:51:58\n",
      "Accuracy: 0.9013 - Loss: 0.2878\n",
      "\n",
      "Batch 39/992 ━━━━━━━━━━━━━━━━━━━━ 07:52:09\n",
      "Accuracy: 0.9006 - Loss: 0.2839\n",
      "\n",
      "Batch 40/992 ━━━━━━━━━━━━━━━━━━━━ 07:52:19\n",
      "Accuracy: 0.9000 - Loss: 0.2843\n",
      "\n",
      "Batch 41/992 ━━━━━━━━━━━━━━━━━━━━ 07:52:29\n",
      "Accuracy: 0.9024 - Loss: 0.2798\n",
      "\n",
      "Batch 42/992 ━━━━━━━━━━━━━━━━━━━━ 07:52:39\n",
      "Accuracy: 0.9048 - Loss: 0.2766\n",
      "\n",
      "Batch 43/992 ━━━━━━━━━━━━━━━━━━━━ 07:52:49\n",
      "Accuracy: 0.9041 - Loss: 0.2761\n",
      "\n",
      "Batch 44/992 ━━━━━━━━━━━━━━━━━━━━ 07:52:59\n",
      "Accuracy: 0.9062 - Loss: 0.2715\n",
      "\n",
      "Batch 45/992 ━━━━━━━━━━━━━━━━━━━━ 07:53:09\n",
      "Accuracy: 0.9083 - Loss: 0.2666\n",
      "\n",
      "Batch 46/992 ━━━━━━━━━━━━━━━━━━━━ 07:53:19\n",
      "Accuracy: 0.9103 - Loss: 0.2628\n",
      "\n",
      "Batch 47/992 ━━━━━━━━━━━━━━━━━━━━ 07:53:29\n",
      "Accuracy: 0.9096 - Loss: 0.2646\n",
      "\n",
      "Batch 48/992 ━━━━━━━━━━━━━━━━━━━━ 07:53:40\n",
      "Accuracy: 0.9115 - Loss: 0.2604\n",
      "\n",
      "Batch 49/992 ━━━━━━━━━━━━━━━━━━━━ 07:53:50\n",
      "Accuracy: 0.9082 - Loss: 0.2643\n",
      "\n",
      "Batch 50/992 ━━━━━━━━━━━━━━━━━━━━ 07:54:00\n",
      "Accuracy: 0.9100 - Loss: 0.2615\n",
      "\n",
      "Batch 51/992 ━━━━━━━━━━━━━━━━━━━━ 07:54:11\n",
      "Accuracy: 0.9093 - Loss: 0.2666\n",
      "\n",
      "Batch 52/992 ━━━━━━━━━━━━━━━━━━━━ 07:54:21\n",
      "Accuracy: 0.9111 - Loss: 0.2636\n",
      "\n",
      "Batch 53/992 ━━━━━━━━━━━━━━━━━━━━ 07:54:31\n",
      "Accuracy: 0.9127 - Loss: 0.2592\n",
      "\n",
      "Batch 54/992 ━━━━━━━━━━━━━━━━━━━━ 07:54:41\n",
      "Accuracy: 0.9144 - Loss: 0.2555\n",
      "\n",
      "Batch 55/992 ━━━━━━━━━━━━━━━━━━━━ 07:54:51\n",
      "Accuracy: 0.9159 - Loss: 0.2529\n",
      "\n",
      "Batch 56/992 ━━━━━━━━━━━━━━━━━━━━ 07:55:01\n",
      "Accuracy: 0.9174 - Loss: 0.2489\n",
      "\n",
      "Batch 57/992 ━━━━━━━━━━━━━━━━━━━━ 07:55:11\n",
      "Accuracy: 0.9189 - Loss: 0.2478\n",
      "\n",
      "Batch 58/992 ━━━━━━━━━━━━━━━━━━━━ 07:55:21\n",
      "Accuracy: 0.9203 - Loss: 0.2453\n",
      "\n",
      "Batch 59/992 ━━━━━━━━━━━━━━━━━━━━ 07:55:31\n",
      "Accuracy: 0.9195 - Loss: 0.2439\n",
      "\n",
      "Batch 60/992 ━━━━━━━━━━━━━━━━━━━━ 07:55:42\n",
      "Accuracy: 0.9167 - Loss: 0.2480\n",
      "\n",
      "Batch 61/992 ━━━━━━━━━━━━━━━━━━━━ 07:55:52\n",
      "Accuracy: 0.9160 - Loss: 0.2475\n",
      "\n",
      "Batch 62/992 ━━━━━━━━━━━━━━━━━━━━ 07:56:02\n",
      "Accuracy: 0.9173 - Loss: 0.2438\n",
      "\n",
      "Batch 63/992 ━━━━━━━━━━━━━━━━━━━━ 07:56:13\n",
      "Accuracy: 0.9187 - Loss: 0.2402\n",
      "\n",
      "Batch 64/992 ━━━━━━━━━━━━━━━━━━━━ 07:56:23\n",
      "Accuracy: 0.9180 - Loss: 0.2401\n",
      "\n",
      "Batch 65/992 ━━━━━━━━━━━━━━━━━━━━ 07:56:33\n",
      "Accuracy: 0.9192 - Loss: 0.2371\n",
      "\n",
      "Batch 66/992 ━━━━━━━━━━━━━━━━━━━━ 07:56:43\n",
      "Accuracy: 0.9205 - Loss: 0.2340\n",
      "\n",
      "Batch 67/992 ━━━━━━━━━━━━━━━━━━━━ 07:56:53\n",
      "Accuracy: 0.9198 - Loss: 0.2325\n",
      "\n",
      "Batch 68/992 ━━━━━━━━━━━━━━━━━━━━ 07:57:03\n",
      "Accuracy: 0.9210 - Loss: 0.2297\n",
      "\n",
      "Batch 69/992 ━━━━━━━━━━━━━━━━━━━━ 07:57:14\n",
      "Accuracy: 0.9203 - Loss: 0.2292\n",
      "\n",
      "Batch 70/992 ━━━━━━━━━━━━━━━━━━━━ 07:57:24\n",
      "Accuracy: 0.9214 - Loss: 0.2263\n",
      "\n",
      "Batch 71/992 ━━━━━━━━━━━━━━━━━━━━ 07:57:34\n",
      "Accuracy: 0.9225 - Loss: 0.2247\n",
      "\n",
      "Batch 72/992 ━━━━━━━━━━━━━━━━━━━━ 07:57:44\n",
      "Accuracy: 0.9201 - Loss: 0.2331\n",
      "\n",
      "Batch 73/992 ━━━━━━━━━━━━━━━━━━━━ 07:57:54\n",
      "Accuracy: 0.9212 - Loss: 0.2318\n",
      "\n",
      "Batch 74/992 ━━━━━━━━━━━━━━━━━━━━ 07:58:04\n",
      "Accuracy: 0.9223 - Loss: 0.2289\n",
      "\n",
      "Batch 75/992 ━━━━━━━━━━━━━━━━━━━━ 07:58:15\n",
      "Accuracy: 0.9217 - Loss: 0.2293\n",
      "\n",
      "Batch 76/992 ━━━━━━━━━━━━━━━━━━━━ 07:58:25\n",
      "Accuracy: 0.9211 - Loss: 0.2301\n",
      "\n",
      "Batch 77/992 ━━━━━━━━━━━━━━━━━━━━ 07:58:35\n",
      "Accuracy: 0.9221 - Loss: 0.2274\n",
      "\n",
      "Batch 78/992 ━━━━━━━━━━━━━━━━━━━━ 07:58:46\n",
      "Accuracy: 0.9231 - Loss: 0.2255\n",
      "\n",
      "Batch 79/992 ━━━━━━━━━━━━━━━━━━━━ 07:58:56\n",
      "Accuracy: 0.9241 - Loss: 0.2240\n",
      "\n",
      "Batch 80/992 ━━━━━━━━━━━━━━━━━━━━ 07:59:06\n",
      "Accuracy: 0.9250 - Loss: 0.2218\n",
      "\n",
      "Batch 81/992 ━━━━━━━━━━━━━━━━━━━━ 07:59:16\n",
      "Accuracy: 0.9244 - Loss: 0.2229\n",
      "\n",
      "Batch 82/992 ━━━━━━━━━━━━━━━━━━━━ 07:59:26\n",
      "Accuracy: 0.9253 - Loss: 0.2208\n",
      "\n",
      "Batch 83/992 ━━━━━━━━━━━━━━━━━━━━ 07:59:36\n",
      "Accuracy: 0.9247 - Loss: 0.2210\n",
      "\n",
      "Batch 84/992 ━━━━━━━━━━━━━━━━━━━━ 07:59:46\n",
      "Accuracy: 0.9256 - Loss: 0.2200\n",
      "\n",
      "Batch 85/992 ━━━━━━━━━━━━━━━━━━━━ 07:59:57\n",
      "Accuracy: 0.9265 - Loss: 0.2178\n",
      "\n",
      "Batch 86/992 ━━━━━━━━━━━━━━━━━━━━ 08:00:07\n",
      "Accuracy: 0.9273 - Loss: 0.2160\n",
      "\n",
      "Batch 87/992 ━━━━━━━━━━━━━━━━━━━━ 08:00:17\n",
      "Accuracy: 0.9267 - Loss: 0.2189\n",
      "\n",
      "Batch 88/992 ━━━━━━━━━━━━━━━━━━━━ 08:00:27\n",
      "Accuracy: 0.9276 - Loss: 0.2173\n",
      "\n",
      "Batch 89/992 ━━━━━━━━━━━━━━━━━━━━ 08:00:37\n",
      "Accuracy: 0.9284 - Loss: 0.2166\n",
      "\n",
      "Batch 90/992 ━━━━━━━━━━━━━━━━━━━━ 08:00:47\n",
      "Accuracy: 0.9292 - Loss: 0.2148\n",
      "\n",
      "Batch 91/992 ━━━━━━━━━━━━━━━━━━━━ 08:00:57\n",
      "Accuracy: 0.9299 - Loss: 0.2131\n",
      "\n",
      "Batch 92/992 ━━━━━━━━━━━━━━━━━━━━ 08:01:07\n",
      "Accuracy: 0.9307 - Loss: 0.2111\n",
      "\n",
      "Batch 93/992 ━━━━━━━━━━━━━━━━━━━━ 08:01:18\n",
      "Accuracy: 0.9315 - Loss: 0.2089\n",
      "\n",
      "Batch 94/992 ━━━━━━━━━━━━━━━━━━━━ 08:01:28\n",
      "Accuracy: 0.9322 - Loss: 0.2070\n",
      "\n",
      "Batch 95/992 ━━━━━━━━━━━━━━━━━━━━ 08:01:38\n",
      "Accuracy: 0.9303 - Loss: 0.2110\n",
      "\n",
      "Batch 96/992 ━━━━━━━━━━━━━━━━━━━━ 08:01:49\n",
      "Accuracy: 0.9310 - Loss: 0.2090\n",
      "\n",
      "Batch 97/992 ━━━━━━━━━━━━━━━━━━━━ 08:01:59\n",
      "Accuracy: 0.9304 - Loss: 0.2087\n",
      "\n",
      "Batch 98/992 ━━━━━━━━━━━━━━━━━━━━ 08:02:09\n",
      "Accuracy: 0.9298 - Loss: 0.2084\n",
      "\n",
      "Batch 99/992 ━━━━━━━━━━━━━━━━━━━━ 08:02:19\n",
      "Accuracy: 0.9293 - Loss: 0.2095\n",
      "\n",
      "Batch 100/992 ━━━━━━━━━━━━━━━━━━━━ 08:02:29\n",
      "Accuracy: 0.9287 - Loss: 0.2097\n",
      "\n",
      "Batch 101/992 ━━━━━━━━━━━━━━━━━━━━ 08:02:39\n",
      "Accuracy: 0.9282 - Loss: 0.2094\n",
      "\n",
      "Batch 102/992 ━━━━━━━━━━━━━━━━━━━━ 08:02:49\n",
      "Accuracy: 0.9289 - Loss: 0.2076\n",
      "\n",
      "Batch 103/992 ━━━━━━━━━━━━━━━━━━━━ 08:02:59\n",
      "Accuracy: 0.9296 - Loss: 0.2064\n",
      "\n",
      "Batch 104/992 ━━━━━━━━━━━━━━━━━━━━ 08:03:09\n",
      "Accuracy: 0.9303 - Loss: 0.2048\n",
      "\n",
      "Batch 105/992 ━━━━━━━━━━━━━━━━━━━━ 08:03:19\n",
      "Accuracy: 0.9298 - Loss: 0.2051\n",
      "\n",
      "Batch 106/992 ━━━━━━━━━━━━━━━━━━━━ 08:03:29\n",
      "Accuracy: 0.9304 - Loss: 0.2033\n",
      "\n",
      "Batch 107/992 ━━━━━━━━━━━━━━━━━━━━ 08:03:40\n",
      "Accuracy: 0.9311 - Loss: 0.2034\n",
      "\n",
      "Batch 108/992 ━━━━━━━━━━━━━━━━━━━━ 08:03:50\n",
      "Accuracy: 0.9317 - Loss: 0.2016\n",
      "\n",
      "Batch 109/992 ━━━━━━━━━━━━━━━━━━━━ 08:04:00\n",
      "Accuracy: 0.9323 - Loss: 0.1999\n",
      "\n",
      "Batch 110/992 ━━━━━━━━━━━━━━━━━━━━ 08:04:10\n",
      "Accuracy: 0.9318 - Loss: 0.1995\n",
      "\n",
      "Batch 111/992 ━━━━━━━━━━━━━━━━━━━━ 08:04:20\n",
      "Accuracy: 0.9324 - Loss: 0.1999\n",
      "\n",
      "Batch 112/992 ━━━━━━━━━━━━━━━━━━━━ 08:04:30\n",
      "Accuracy: 0.9330 - Loss: 0.1984\n",
      "\n",
      "Batch 113/992 ━━━━━━━━━━━━━━━━━━━━ 08:04:40\n",
      "Accuracy: 0.9336 - Loss: 0.1971\n",
      "\n",
      "Batch 114/992 ━━━━━━━━━━━━━━━━━━━━ 08:04:50\n",
      "Accuracy: 0.9331 - Loss: 0.1976\n",
      "\n",
      "Batch 115/992 ━━━━━━━━━━━━━━━━━━━━ 08:05:00\n",
      "Accuracy: 0.9337 - Loss: 0.1961\n",
      "\n",
      "Batch 116/992 ━━━━━━━━━━━━━━━━━━━━ 08:05:11\n",
      "Accuracy: 0.9332 - Loss: 0.1983\n",
      "\n",
      "Batch 117/992 ━━━━━━━━━━━━━━━━━━━━ 08:05:21\n",
      "Accuracy: 0.9338 - Loss: 0.1969\n",
      "\n",
      "Batch 118/992 ━━━━━━━━━━━━━━━━━━━━ 08:05:31\n",
      "Accuracy: 0.9343 - Loss: 0.1961\n",
      "\n",
      "Batch 119/992 ━━━━━━━━━━━━━━━━━━━━ 08:05:41\n",
      "Accuracy: 0.9349 - Loss: 0.1952\n",
      "\n",
      "Batch 120/992 ━━━━━━━━━━━━━━━━━━━━ 08:05:51\n",
      "Accuracy: 0.9344 - Loss: 0.1951\n",
      "\n",
      "Batch 121/992 ━━━━━━━━━━━━━━━━━━━━ 08:06:01\n",
      "Accuracy: 0.9349 - Loss: 0.1942\n",
      "\n",
      "Batch 122/992 ━━━━━━━━━━━━━━━━━━━━ 08:06:11\n",
      "Accuracy: 0.9355 - Loss: 0.1933\n",
      "\n",
      "Batch 123/992 ━━━━━━━━━━━━━━━━━━━━ 08:06:22\n",
      "Accuracy: 0.9350 - Loss: 0.1935\n",
      "\n",
      "Batch 124/992 ━━━━━━━━━━━━━━━━━━━━ 08:06:32\n",
      "Accuracy: 0.9345 - Loss: 0.1937\n",
      "\n",
      "Batch 125/992 ━━━━━━━━━━━━━━━━━━━━ 08:06:42\n",
      "Accuracy: 0.9350 - Loss: 0.1924\n",
      "\n",
      "Batch 126/992 ━━━━━━━━━━━━━━━━━━━━ 08:06:52\n",
      "Accuracy: 0.9355 - Loss: 0.1918\n",
      "\n",
      "Batch 127/992 ━━━━━━━━━━━━━━━━━━━━ 08:07:02\n",
      "Accuracy: 0.9341 - Loss: 0.1926\n",
      "\n",
      "Batch 128/992 ━━━━━━━━━━━━━━━━━━━━ 08:07:12\n",
      "Accuracy: 0.9326 - Loss: 0.1937\n",
      "\n",
      "Batch 129/992 ━━━━━━━━━━━━━━━━━━━━ 08:07:22\n",
      "Accuracy: 0.9322 - Loss: 0.1937\n",
      "\n",
      "Batch 130/992 ━━━━━━━━━━━━━━━━━━━━ 08:07:32\n",
      "Accuracy: 0.9327 - Loss: 0.1928\n",
      "\n",
      "Batch 131/992 ━━━━━━━━━━━━━━━━━━━━ 08:07:43\n",
      "Accuracy: 0.9323 - Loss: 0.1941\n",
      "\n",
      "Batch 132/992 ━━━━━━━━━━━━━━━━━━━━ 08:07:53\n",
      "Accuracy: 0.9328 - Loss: 0.1937\n",
      "\n",
      "Batch 133/992 ━━━━━━━━━━━━━━━━━━━━ 08:08:03\n",
      "Accuracy: 0.9323 - Loss: 0.1948\n",
      "\n",
      "Batch 134/992 ━━━━━━━━━━━━━━━━━━━━ 08:08:13\n",
      "Accuracy: 0.9319 - Loss: 0.1945\n",
      "\n",
      "Batch 135/992 ━━━━━━━━━━━━━━━━━━━━ 08:08:24\n",
      "Accuracy: 0.9315 - Loss: 0.1961\n",
      "\n",
      "Batch 136/992 ━━━━━━━━━━━━━━━━━━━━ 08:08:34\n",
      "Accuracy: 0.9320 - Loss: 0.1956\n",
      "\n",
      "Batch 137/992 ━━━━━━━━━━━━━━━━━━━━ 08:08:44\n",
      "Accuracy: 0.9325 - Loss: 0.1948\n",
      "\n",
      "Batch 138/992 ━━━━━━━━━━━━━━━━━━━━ 08:08:54\n",
      "Accuracy: 0.9321 - Loss: 0.1950\n",
      "\n",
      "Batch 139/992 ━━━━━━━━━━━━━━━━━━━━ 08:09:04\n",
      "Accuracy: 0.9317 - Loss: 0.1950\n",
      "\n",
      "Batch 140/992 ━━━━━━━━━━━━━━━━━━━━ 08:09:15\n",
      "Accuracy: 0.9304 - Loss: 0.1971\n",
      "\n",
      "Batch 141/992 ━━━━━━━━━━━━━━━━━━━━ 08:09:25\n",
      "Accuracy: 0.9309 - Loss: 0.1958\n",
      "\n",
      "Batch 142/992 ━━━━━━━━━━━━━━━━━━━━ 08:09:35\n",
      "Accuracy: 0.9305 - Loss: 0.1967\n",
      "\n",
      "Batch 143/992 ━━━━━━━━━━━━━━━━━━━━ 08:09:45\n",
      "Accuracy: 0.9309 - Loss: 0.1955\n",
      "\n",
      "Batch 144/992 ━━━━━━━━━━━━━━━━━━━━ 08:09:55\n",
      "Accuracy: 0.9314 - Loss: 0.1949\n",
      "\n",
      "Batch 145/992 ━━━━━━━━━━━━━━━━━━━━ 08:10:05\n",
      "Accuracy: 0.9310 - Loss: 0.1951\n",
      "\n",
      "Batch 146/992 ━━━━━━━━━━━━━━━━━━━━ 08:10:16\n",
      "Accuracy: 0.9307 - Loss: 0.1965\n",
      "\n",
      "Batch 147/992 ━━━━━━━━━━━━━━━━━━━━ 08:10:26\n",
      "Accuracy: 0.9303 - Loss: 0.1961\n",
      "\n",
      "Batch 148/992 ━━━━━━━━━━━━━━━━━━━━ 08:10:36\n",
      "Accuracy: 0.9299 - Loss: 0.1964\n",
      "\n",
      "Batch 149/992 ━━━━━━━━━━━━━━━━━━━━ 08:10:46\n",
      "Accuracy: 0.9295 - Loss: 0.1973\n",
      "\n",
      "Batch 150/992 ━━━━━━━━━━━━━━━━━━━━ 08:10:56\n",
      "Accuracy: 0.9292 - Loss: 0.1976\n",
      "\n",
      "Batch 151/992 ━━━━━━━━━━━━━━━━━━━━ 08:11:06\n",
      "Accuracy: 0.9296 - Loss: 0.1970\n",
      "\n",
      "Batch 152/992 ━━━━━━━━━━━━━━━━━━━━ 08:11:17\n",
      "Accuracy: 0.9293 - Loss: 0.1965\n",
      "\n",
      "Batch 153/992 ━━━━━━━━━━━━━━━━━━━━ 08:11:27\n",
      "Accuracy: 0.9289 - Loss: 0.1979\n",
      "\n",
      "Batch 154/992 ━━━━━━━━━━━━━━━━━━━━ 08:11:37\n",
      "Accuracy: 0.9294 - Loss: 0.1976\n",
      "\n",
      "Batch 155/992 ━━━━━━━━━━━━━━━━━━━━ 08:11:47\n",
      "Accuracy: 0.9282 - Loss: 0.2024\n",
      "\n",
      "Batch 156/992 ━━━━━━━━━━━━━━━━━━━━ 08:11:57\n",
      "Accuracy: 0.9279 - Loss: 0.2024\n",
      "\n",
      "Batch 157/992 ━━━━━━━━━━━━━━━━━━━━ 08:12:08\n",
      "Accuracy: 0.9283 - Loss: 0.2015\n",
      "\n",
      "Batch 158/992 ━━━━━━━━━━━━━━━━━━━━ 08:12:18\n",
      "Accuracy: 0.9288 - Loss: 0.2006\n",
      "\n",
      "Batch 159/992 ━━━━━━━━━━━━━━━━━━━━ 08:12:28\n",
      "Accuracy: 0.9292 - Loss: 0.1996\n",
      "\n",
      "Batch 160/992 ━━━━━━━━━━━━━━━━━━━━ 08:12:38\n",
      "Accuracy: 0.9289 - Loss: 0.2001\n",
      "\n",
      "Batch 161/992 ━━━━━━━━━━━━━━━━━━━━ 08:12:48\n",
      "Accuracy: 0.9286 - Loss: 0.2017\n",
      "\n",
      "Batch 162/992 ━━━━━━━━━━━━━━━━━━━━ 08:12:58\n",
      "Accuracy: 0.9290 - Loss: 0.2010\n",
      "\n",
      "Batch 163/992 ━━━━━━━━━━━━━━━━━━━━ 08:13:08\n",
      "Accuracy: 0.9294 - Loss: 0.2007\n",
      "\n",
      "Batch 164/992 ━━━━━━━━━━━━━━━━━━━━ 08:13:19\n",
      "Accuracy: 0.9291 - Loss: 0.2020\n",
      "\n",
      "Batch 165/992 ━━━━━━━━━━━━━━━━━━━━ 08:13:29\n",
      "Accuracy: 0.9295 - Loss: 0.2016\n",
      "\n",
      "Batch 166/992 ━━━━━━━━━━━━━━━━━━━━ 08:13:39\n",
      "Accuracy: 0.9300 - Loss: 0.2012\n",
      "\n",
      "Batch 167/992 ━━━━━━━━━━━━━━━━━━━━ 08:13:49\n",
      "Accuracy: 0.9296 - Loss: 0.2010\n",
      "\n",
      "Batch 168/992 ━━━━━━━━━━━━━━━━━━━━ 08:13:59\n",
      "Accuracy: 0.9301 - Loss: 0.2004\n",
      "\n",
      "Batch 169/992 ━━━━━━━━━━━━━━━━━━━━ 08:14:09\n",
      "Accuracy: 0.9305 - Loss: 0.1994\n",
      "\n",
      "Batch 170/992 ━━━━━━━━━━━━━━━━━━━━ 08:14:20\n",
      "Accuracy: 0.9301 - Loss: 0.1995\n",
      "\n",
      "Batch 171/992 ━━━━━━━━━━━━━━━━━━━━ 08:14:30\n",
      "Accuracy: 0.9306 - Loss: 0.1990\n",
      "\n",
      "Batch 172/992 ━━━━━━━━━━━━━━━━━━━━ 08:14:40\n",
      "Accuracy: 0.9310 - Loss: 0.1990\n",
      "\n",
      "Batch 173/992 ━━━━━━━━━━━━━━━━━━━━ 08:14:50\n",
      "Accuracy: 0.9299 - Loss: 0.2002\n",
      "\n",
      "Batch 174/992 ━━━━━━━━━━━━━━━━━━━━ 08:15:00\n",
      "Accuracy: 0.9296 - Loss: 0.1999\n",
      "\n",
      "Batch 175/992 ━━━━━━━━━━━━━━━━━━━━ 08:15:10\n",
      "Accuracy: 0.9293 - Loss: 0.1996\n",
      "\n",
      "Batch 176/992 ━━━━━━━━━━━━━━━━━━━━ 08:15:21\n",
      "Accuracy: 0.9297 - Loss: 0.1989\n",
      "\n",
      "Batch 177/992 ━━━━━━━━━━━━━━━━━━━━ 08:15:31\n",
      "Accuracy: 0.9301 - Loss: 0.1983\n",
      "\n",
      "Batch 178/992 ━━━━━━━━━━━━━━━━━━━━ 08:15:41\n",
      "Accuracy: 0.9305 - Loss: 0.1975\n",
      "\n",
      "Batch 179/992 ━━━━━━━━━━━━━━━━━━━━ 08:15:51\n",
      "Accuracy: 0.9309 - Loss: 0.1966\n",
      "\n",
      "Batch 180/992 ━━━━━━━━━━━━━━━━━━━━ 08:16:01\n",
      "Accuracy: 0.9299 - Loss: 0.1981\n",
      "\n",
      "Batch 181/992 ━━━━━━━━━━━━━━━━━━━━ 08:16:11\n",
      "Accuracy: 0.9302 - Loss: 0.1974\n",
      "\n",
      "Batch 182/992 ━━━━━━━━━━━━━━━━━━━━ 08:16:21\n",
      "Accuracy: 0.9299 - Loss: 0.1980\n",
      "\n",
      "Batch 183/992 ━━━━━━━━━━━━━━━━━━━━ 08:16:31\n",
      "Accuracy: 0.9303 - Loss: 0.1972\n",
      "\n",
      "Batch 184/992 ━━━━━━━━━━━━━━━━━━━━ 08:16:41\n",
      "Accuracy: 0.9307 - Loss: 0.1976\n",
      "\n",
      "Batch 185/992 ━━━━━━━━━━━━━━━━━━━━ 08:16:52\n",
      "Accuracy: 0.9311 - Loss: 0.1973\n",
      "\n",
      "Batch 186/992 ━━━━━━━━━━━━━━━━━━━━ 08:17:02\n",
      "Accuracy: 0.9315 - Loss: 0.1973\n",
      "\n",
      "Batch 187/992 ━━━━━━━━━━━━━━━━━━━━ 08:17:12\n",
      "Accuracy: 0.9305 - Loss: 0.1996\n",
      "\n",
      "Batch 188/992 ━━━━━━━━━━━━━━━━━━━━ 08:17:22\n",
      "Accuracy: 0.9309 - Loss: 0.1991\n",
      "\n",
      "Batch 189/992 ━━━━━━━━━━━━━━━━━━━━ 08:17:32\n",
      "Accuracy: 0.9312 - Loss: 0.1982\n",
      "\n",
      "Batch 190/992 ━━━━━━━━━━━━━━━━━━━━ 08:17:42\n",
      "Accuracy: 0.9316 - Loss: 0.1975\n",
      "\n",
      "Batch 191/992 ━━━━━━━━━━━━━━━━━━━━ 08:17:52\n",
      "Accuracy: 0.9313 - Loss: 0.1986\n",
      "\n",
      "Batch 192/992 ━━━━━━━━━━━━━━━━━━━━ 08:18:03\n",
      "Accuracy: 0.9316 - Loss: 0.1980\n",
      "\n",
      "Batch 193/992 ━━━━━━━━━━━━━━━━━━━━ 08:18:13\n",
      "Accuracy: 0.9320 - Loss: 0.1979\n",
      "\n",
      "Batch 194/992 ━━━━━━━━━━━━━━━━━━━━ 08:18:23\n",
      "Accuracy: 0.9323 - Loss: 0.1973\n",
      "\n",
      "Batch 195/992 ━━━━━━━━━━━━━━━━━━━━ 08:18:33\n",
      "Accuracy: 0.9327 - Loss: 0.1969\n",
      "\n",
      "Batch 196/992 ━━━━━━━━━━━━━━━━━━━━ 08:18:43\n",
      "Accuracy: 0.9330 - Loss: 0.1959\n",
      "\n",
      "Batch 197/992 ━━━━━━━━━━━━━━━━━━━━ 08:18:53\n",
      "Accuracy: 0.9321 - Loss: 0.1981\n",
      "\n",
      "Batch 198/992 ━━━━━━━━━━━━━━━━━━━━ 08:19:03\n",
      "Accuracy: 0.9318 - Loss: 0.1978\n",
      "\n",
      "Batch 199/992 ━━━━━━━━━━━━━━━━━━━━ 08:19:13\n",
      "Accuracy: 0.9322 - Loss: 0.1974\n",
      "\n",
      "Batch 200/992 ━━━━━━━━━━━━━━━━━━━━ 08:19:23\n",
      "Accuracy: 0.9325 - Loss: 0.1966\n",
      "\n",
      "Batch 201/992 ━━━━━━━━━━━━━━━━━━━━ 08:19:34\n",
      "Accuracy: 0.9322 - Loss: 0.1975\n",
      "\n",
      "Batch 202/992 ━━━━━━━━━━━━━━━━━━━━ 08:19:44\n",
      "Accuracy: 0.9325 - Loss: 0.1966\n",
      "\n",
      "Batch 203/992 ━━━━━━━━━━━━━━━━━━━━ 08:19:54\n",
      "Accuracy: 0.9323 - Loss: 0.1970\n",
      "\n",
      "Batch 204/992 ━━━━━━━━━━━━━━━━━━━━ 08:20:04\n",
      "Accuracy: 0.9326 - Loss: 0.1962\n",
      "\n",
      "Batch 205/992 ━━━━━━━━━━━━━━━━━━━━ 08:20:14\n",
      "Accuracy: 0.9323 - Loss: 0.1967\n",
      "\n",
      "Batch 206/992 ━━━━━━━━━━━━━━━━━━━━ 08:20:24\n",
      "Accuracy: 0.9320 - Loss: 0.1967\n",
      "\n",
      "Batch 207/992 ━━━━━━━━━━━━━━━━━━━━ 08:20:34\n",
      "Accuracy: 0.9324 - Loss: 0.1965\n",
      "\n",
      "Batch 208/992 ━━━━━━━━━━━━━━━━━━━━ 08:20:44\n",
      "Accuracy: 0.9327 - Loss: 0.1959\n",
      "\n",
      "Batch 209/992 ━━━━━━━━━━━━━━━━━━━━ 08:20:55\n",
      "Accuracy: 0.9324 - Loss: 0.1960\n",
      "\n",
      "Batch 210/992 ━━━━━━━━━━━━━━━━━━━━ 08:21:05\n",
      "Accuracy: 0.9327 - Loss: 0.1959\n",
      "\n",
      "Batch 211/992 ━━━━━━━━━━━━━━━━━━━━ 08:21:15\n",
      "Accuracy: 0.9331 - Loss: 0.1961\n",
      "\n",
      "Batch 212/992 ━━━━━━━━━━━━━━━━━━━━ 08:21:25\n",
      "Accuracy: 0.9334 - Loss: 0.1954\n",
      "\n",
      "Batch 213/992 ━━━━━━━━━━━━━━━━━━━━ 08:21:35\n",
      "Accuracy: 0.9337 - Loss: 0.1946\n",
      "\n",
      "Batch 214/992 ━━━━━━━━━━━━━━━━━━━━ 08:21:46\n",
      "Accuracy: 0.9340 - Loss: 0.1947\n",
      "\n",
      "Batch 215/992 ━━━━━━━━━━━━━━━━━━━━ 08:21:56\n",
      "Accuracy: 0.9337 - Loss: 0.1946\n",
      "\n",
      "Batch 216/992 ━━━━━━━━━━━━━━━━━━━━ 08:22:06\n",
      "Accuracy: 0.9334 - Loss: 0.1947\n",
      "\n",
      "Batch 217/992 ━━━━━━━━━━━━━━━━━━━━ 08:22:17\n",
      "Accuracy: 0.9338 - Loss: 0.1943\n",
      "\n",
      "Batch 218/992 ━━━━━━━━━━━━━━━━━━━━ 08:22:27\n",
      "Accuracy: 0.9335 - Loss: 0.1946\n",
      "\n",
      "Batch 219/992 ━━━━━━━━━━━━━━━━━━━━ 08:22:37\n",
      "Accuracy: 0.9332 - Loss: 0.1944\n",
      "\n",
      "Batch 220/992 ━━━━━━━━━━━━━━━━━━━━ 08:22:47\n",
      "Accuracy: 0.9335 - Loss: 0.1942\n",
      "\n",
      "Batch 221/992 ━━━━━━━━━━━━━━━━━━━━ 08:22:57\n",
      "Accuracy: 0.9338 - Loss: 0.1941\n",
      "\n",
      "Batch 222/992 ━━━━━━━━━━━━━━━━━━━━ 08:23:07\n",
      "Accuracy: 0.9341 - Loss: 0.1932\n",
      "\n",
      "Batch 223/992 ━━━━━━━━━━━━━━━━━━━━ 08:23:17\n",
      "Accuracy: 0.9344 - Loss: 0.1925\n",
      "\n",
      "Batch 224/992 ━━━━━━━━━━━━━━━━━━━━ 08:23:27\n",
      "Accuracy: 0.9347 - Loss: 0.1919\n",
      "\n",
      "Batch 225/992 ━━━━━━━━━━━━━━━━━━━━ 08:23:38\n",
      "Accuracy: 0.9344 - Loss: 0.1917\n",
      "\n",
      "Batch 226/992 ━━━━━━━━━━━━━━━━━━━━ 08:23:48\n",
      "Accuracy: 0.9347 - Loss: 0.1914\n",
      "\n",
      "Batch 227/992 ━━━━━━━━━━━━━━━━━━━━ 08:23:58\n",
      "Accuracy: 0.9345 - Loss: 0.1920\n",
      "\n",
      "Batch 228/992 ━━━━━━━━━━━━━━━━━━━━ 08:24:09\n",
      "Accuracy: 0.9342 - Loss: 0.1936\n",
      "\n",
      "Batch 229/992 ━━━━━━━━━━━━━━━━━━━━ 08:24:19\n",
      "Accuracy: 0.9345 - Loss: 0.1932\n",
      "\n",
      "Batch 230/992 ━━━━━━━━━━━━━━━━━━━━ 08:24:29\n",
      "Accuracy: 0.9348 - Loss: 0.1930\n",
      "\n",
      "Batch 231/992 ━━━━━━━━━━━━━━━━━━━━ 08:24:39\n",
      "Accuracy: 0.9340 - Loss: 0.1970\n",
      "\n",
      "Batch 232/992 ━━━━━━━━━━━━━━━━━━━━ 08:24:49\n",
      "Accuracy: 0.9332 - Loss: 0.1986\n",
      "\n",
      "Batch 233/992 ━━━━━━━━━━━━━━━━━━━━ 08:24:59\n",
      "Accuracy: 0.9329 - Loss: 0.1986\n",
      "\n",
      "Batch 234/992 ━━━━━━━━━━━━━━━━━━━━ 08:25:09\n",
      "Accuracy: 0.9327 - Loss: 0.1986\n",
      "\n",
      "Batch 235/992 ━━━━━━━━━━━━━━━━━━━━ 08:25:20\n",
      "Accuracy: 0.9330 - Loss: 0.1978\n",
      "\n",
      "Batch 236/992 ━━━━━━━━━━━━━━━━━━━━ 08:25:30\n",
      "Accuracy: 0.9322 - Loss: 0.1978\n",
      "\n",
      "Batch 237/992 ━━━━━━━━━━━━━━━━━━━━ 08:25:40\n",
      "Accuracy: 0.9320 - Loss: 0.1985\n",
      "\n",
      "Batch 238/992 ━━━━━━━━━━━━━━━━━━━━ 08:25:50\n",
      "Accuracy: 0.9317 - Loss: 0.1992\n",
      "\n",
      "Batch 239/992 ━━━━━━━━━━━━━━━━━━━━ 08:26:00\n",
      "Accuracy: 0.9315 - Loss: 0.1994\n",
      "\n",
      "Batch 240/992 ━━━━━━━━━━━━━━━━━━━━ 08:26:10\n",
      "Accuracy: 0.9312 - Loss: 0.2003\n",
      "\n",
      "Batch 241/992 ━━━━━━━━━━━━━━━━━━━━ 08:26:21\n",
      "Accuracy: 0.9315 - Loss: 0.1997\n",
      "\n",
      "Batch 242/992 ━━━━━━━━━━━━━━━━━━━━ 08:26:31\n",
      "Accuracy: 0.9318 - Loss: 0.1992\n",
      "\n",
      "Batch 243/992 ━━━━━━━━━━━━━━━━━━━━ 08:26:41\n",
      "Accuracy: 0.9321 - Loss: 0.1987\n",
      "\n",
      "Batch 244/992 ━━━━━━━━━━━━━━━━━━━━ 08:26:51\n",
      "Accuracy: 0.9319 - Loss: 0.1989\n",
      "\n",
      "Batch 245/992 ━━━━━━━━━━━━━━━━━━━━ 08:27:01\n",
      "Accuracy: 0.9316 - Loss: 0.1990\n",
      "\n",
      "Batch 246/992 ━━━━━━━━━━━━━━━━━━━━ 08:27:11\n",
      "Accuracy: 0.9319 - Loss: 0.1987\n",
      "\n",
      "Batch 247/992 ━━━━━━━━━━━━━━━━━━━━ 08:27:22\n",
      "Accuracy: 0.9322 - Loss: 0.1982\n",
      "\n",
      "Batch 248/992 ━━━━━━━━━━━━━━━━━━━━ 08:27:32\n",
      "Accuracy: 0.9325 - Loss: 0.1977\n",
      "\n",
      "Batch 249/992 ━━━━━━━━━━━━━━━━━━━━ 08:27:42\n",
      "Accuracy: 0.9322 - Loss: 0.1975\n",
      "\n",
      "Batch 250/992 ━━━━━━━━━━━━━━━━━━━━ 08:27:52\n",
      "Accuracy: 0.9325 - Loss: 0.1969\n",
      "\n",
      "Batch 251/992 ━━━━━━━━━━━━━━━━━━━━ 08:28:03\n",
      "Accuracy: 0.9323 - Loss: 0.1971\n",
      "\n",
      "Batch 252/992 ━━━━━━━━━━━━━━━━━━━━ 08:28:13\n",
      "Accuracy: 0.9320 - Loss: 0.1977\n",
      "\n",
      "Batch 253/992 ━━━━━━━━━━━━━━━━━━━━ 08:28:23\n",
      "Accuracy: 0.9318 - Loss: 0.1979\n",
      "\n",
      "Batch 254/992 ━━━━━━━━━━━━━━━━━━━━ 08:28:33\n",
      "Accuracy: 0.9316 - Loss: 0.1987\n",
      "\n",
      "Batch 255/992 ━━━━━━━━━━━━━━━━━━━━ 08:28:43\n",
      "Accuracy: 0.9309 - Loss: 0.1996\n",
      "\n",
      "Batch 256/992 ━━━━━━━━━━━━━━━━━━━━ 08:28:54\n",
      "Accuracy: 0.9312 - Loss: 0.1990\n",
      "\n",
      "Batch 257/992 ━━━━━━━━━━━━━━━━━━━━ 08:29:04\n",
      "Accuracy: 0.9314 - Loss: 0.1986\n",
      "\n",
      "Batch 258/992 ━━━━━━━━━━━━━━━━━━━━ 08:29:14\n",
      "Accuracy: 0.9312 - Loss: 0.1990\n",
      "\n",
      "Batch 259/992 ━━━━━━━━━━━━━━━━━━━━ 08:29:24\n",
      "Accuracy: 0.9315 - Loss: 0.1986\n",
      "\n",
      "Batch 260/992 ━━━━━━━━━━━━━━━━━━━━ 08:29:34\n",
      "Accuracy: 0.9317 - Loss: 0.1979\n",
      "\n",
      "Batch 261/992 ━━━━━━━━━━━━━━━━━━━━ 08:29:44\n",
      "Accuracy: 0.9320 - Loss: 0.1977\n",
      "\n",
      "Batch 262/992 ━━━━━━━━━━━━━━━━━━━━ 08:29:55\n",
      "Accuracy: 0.9323 - Loss: 0.1972\n",
      "\n",
      "Batch 263/992 ━━━━━━━━━━━━━━━━━━━━ 08:30:05\n",
      "Accuracy: 0.9320 - Loss: 0.1972\n",
      "\n",
      "Batch 264/992 ━━━━━━━━━━━━━━━━━━━━ 08:30:15\n",
      "Accuracy: 0.9323 - Loss: 0.1968\n",
      "\n",
      "Batch 265/992 ━━━━━━━━━━━━━━━━━━━━ 08:30:25\n",
      "Accuracy: 0.9325 - Loss: 0.1961\n",
      "\n",
      "Batch 266/992 ━━━━━━━━━━━━━━━━━━━━ 08:30:35\n",
      "Accuracy: 0.9323 - Loss: 0.1965\n",
      "\n",
      "Batch 267/992 ━━━━━━━━━━━━━━━━━━━━ 08:30:45\n",
      "Accuracy: 0.9326 - Loss: 0.1965\n",
      "\n",
      "Batch 268/992 ━━━━━━━━━━━━━━━━━━━━ 08:30:55\n",
      "Accuracy: 0.9328 - Loss: 0.1960\n",
      "\n",
      "Batch 269/992 ━━━━━━━━━━━━━━━━━━━━ 08:31:06\n",
      "Accuracy: 0.9326 - Loss: 0.1957\n",
      "\n",
      "Batch 270/992 ━━━━━━━━━━━━━━━━━━━━ 08:31:16\n",
      "Accuracy: 0.9324 - Loss: 0.1963\n",
      "\n",
      "Batch 271/992 ━━━━━━━━━━━━━━━━━━━━ 08:31:26\n",
      "Accuracy: 0.9322 - Loss: 0.1969\n",
      "\n",
      "Batch 272/992 ━━━━━━━━━━━━━━━━━━━━ 08:31:36\n",
      "Accuracy: 0.9320 - Loss: 0.1973\n",
      "\n",
      "Batch 273/992 ━━━━━━━━━━━━━━━━━━━━ 08:31:46\n",
      "Accuracy: 0.9318 - Loss: 0.1985\n",
      "\n",
      "Batch 274/992 ━━━━━━━━━━━━━━━━━━━━ 08:31:56\n",
      "Accuracy: 0.9316 - Loss: 0.1988\n",
      "\n",
      "Batch 275/992 ━━━━━━━━━━━━━━━━━━━━ 08:32:06\n",
      "Accuracy: 0.9309 - Loss: 0.2008\n",
      "\n",
      "Batch 276/992 ━━━━━━━━━━━━━━━━━━━━ 08:32:17\n",
      "Accuracy: 0.9312 - Loss: 0.2001\n",
      "\n",
      "Batch 277/992 ━━━━━━━━━━━━━━━━━━━━ 08:32:27\n",
      "Accuracy: 0.9314 - Loss: 0.1997\n",
      "\n",
      "Batch 278/992 ━━━━━━━━━━━━━━━━━━━━ 08:32:37\n",
      "Accuracy: 0.9312 - Loss: 0.1999\n",
      "\n",
      "Batch 279/992 ━━━━━━━━━━━━━━━━━━━━ 08:32:47\n",
      "Accuracy: 0.9315 - Loss: 0.1992\n",
      "\n",
      "Batch 280/992 ━━━━━━━━━━━━━━━━━━━━ 08:32:57\n",
      "Accuracy: 0.9317 - Loss: 0.1987\n",
      "\n",
      "Batch 281/992 ━━━━━━━━━━━━━━━━━━━━ 08:33:07\n",
      "Accuracy: 0.9319 - Loss: 0.1983\n",
      "\n",
      "Batch 282/992 ━━━━━━━━━━━━━━━━━━━━ 08:33:17\n",
      "Accuracy: 0.9322 - Loss: 0.1978\n",
      "\n",
      "Batch 283/992 ━━━━━━━━━━━━━━━━━━━━ 08:33:27\n",
      "Accuracy: 0.9324 - Loss: 0.1973\n",
      "\n",
      "Batch 284/992 ━━━━━━━━━━━━━━━━━━━━ 08:33:37\n",
      "Accuracy: 0.9322 - Loss: 0.1975\n",
      "\n",
      "Batch 285/992 ━━━━━━━━━━━━━━━━━━━━ 08:33:48\n",
      "Accuracy: 0.9325 - Loss: 0.1969\n",
      "\n",
      "Batch 286/992 ━━━━━━━━━━━━━━━━━━━━ 08:33:58\n",
      "Accuracy: 0.9323 - Loss: 0.1976\n",
      "\n",
      "Batch 287/992 ━━━━━━━━━━━━━━━━━━━━ 08:34:08\n",
      "Accuracy: 0.9316 - Loss: 0.1989\n",
      "\n",
      "Batch 288/992 ━━━━━━━━━━━━━━━━━━━━ 08:34:18\n",
      "Accuracy: 0.9310 - Loss: 0.2007\n",
      "\n",
      "Batch 289/992 ━━━━━━━━━━━━━━━━━━━━ 08:34:28\n",
      "Accuracy: 0.9312 - Loss: 0.2001\n",
      "\n",
      "Batch 290/992 ━━━━━━━━━━━━━━━━━━━━ 08:34:38\n",
      "Accuracy: 0.9310 - Loss: 0.2001\n",
      "\n",
      "Batch 291/992 ━━━━━━━━━━━━━━━━━━━━ 08:34:48\n",
      "Accuracy: 0.9313 - Loss: 0.2000\n",
      "\n",
      "Batch 292/992 ━━━━━━━━━━━━━━━━━━━━ 08:34:58\n",
      "Accuracy: 0.9315 - Loss: 0.2000\n",
      "\n",
      "Batch 293/992 ━━━━━━━━━━━━━━━━━━━━ 08:35:09\n",
      "Accuracy: 0.9309 - Loss: 0.2013\n",
      "\n",
      "Batch 294/992 ━━━━━━━━━━━━━━━━━━━━ 08:35:19\n",
      "Accuracy: 0.9311 - Loss: 0.2011\n",
      "\n",
      "Batch 295/992 ━━━━━━━━━━━━━━━━━━━━ 08:35:29\n",
      "Accuracy: 0.9309 - Loss: 0.2019\n",
      "\n",
      "Batch 296/992 ━━━━━━━━━━━━━━━━━━━━ 08:35:39\n",
      "Accuracy: 0.9312 - Loss: 0.2015\n",
      "\n",
      "Batch 297/992 ━━━━━━━━━━━━━━━━━━━━ 08:35:49\n",
      "Accuracy: 0.9306 - Loss: 0.2019\n",
      "\n",
      "Batch 298/992 ━━━━━━━━━━━━━━━━━━━━ 08:36:00\n",
      "Accuracy: 0.9299 - Loss: 0.2021\n",
      "\n",
      "Batch 299/992 ━━━━━━━━━━━━━━━━━━━━ 08:36:10\n",
      "Accuracy: 0.9298 - Loss: 0.2021\n",
      "\n",
      "Batch 300/992 ━━━━━━━━━━━━━━━━━━━━ 08:36:21\n",
      "Accuracy: 0.9300 - Loss: 0.2018\n",
      "\n",
      "Batch 301/992 ━━━━━━━━━━━━━━━━━━━━ 08:36:31\n",
      "Accuracy: 0.9298 - Loss: 0.2020\n",
      "\n",
      "Batch 302/992 ━━━━━━━━━━━━━━━━━━━━ 08:36:41\n",
      "Accuracy: 0.9292 - Loss: 0.2026\n",
      "\n",
      "Batch 303/992 ━━━━━━━━━━━━━━━━━━━━ 08:36:51\n",
      "Accuracy: 0.9290 - Loss: 0.2030\n",
      "\n",
      "Batch 304/992 ━━━━━━━━━━━━━━━━━━━━ 08:37:01\n",
      "Accuracy: 0.9289 - Loss: 0.2035\n",
      "\n",
      "Batch 305/992 ━━━━━━━━━━━━━━━━━━━━ 08:37:11\n",
      "Accuracy: 0.9283 - Loss: 0.2044\n",
      "\n",
      "Batch 306/992 ━━━━━━━━━━━━━━━━━━━━ 08:37:21\n",
      "Accuracy: 0.9285 - Loss: 0.2044\n",
      "\n",
      "Batch 307/992 ━━━━━━━━━━━━━━━━━━━━ 08:37:31\n",
      "Accuracy: 0.9287 - Loss: 0.2042\n",
      "\n",
      "Batch 308/992 ━━━━━━━━━━━━━━━━━━━━ 08:37:42\n",
      "Accuracy: 0.9286 - Loss: 0.2044\n",
      "\n",
      "Batch 309/992 ━━━━━━━━━━━━━━━━━━━━ 08:37:52\n",
      "Accuracy: 0.9288 - Loss: 0.2041\n",
      "\n",
      "Batch 310/992 ━━━━━━━━━━━━━━━━━━━━ 08:38:02\n",
      "Accuracy: 0.9290 - Loss: 0.2037\n",
      "\n",
      "Batch 311/992 ━━━━━━━━━━━━━━━━━━━━ 08:38:12\n",
      "Accuracy: 0.9293 - Loss: 0.2032\n",
      "\n",
      "Batch 312/992 ━━━━━━━━━━━━━━━━━━━━ 08:38:22\n",
      "Accuracy: 0.9287 - Loss: 0.2038\n",
      "\n",
      "Batch 313/992 ━━━━━━━━━━━━━━━━━━━━ 08:38:32\n",
      "Accuracy: 0.9285 - Loss: 0.2041\n",
      "\n",
      "Batch 314/992 ━━━━━━━━━━━━━━━━━━━━ 08:38:42\n",
      "Accuracy: 0.9283 - Loss: 0.2046\n",
      "\n",
      "Batch 315/992 ━━━━━━━━━━━━━━━━━━━━ 08:38:53\n",
      "Accuracy: 0.9286 - Loss: 0.2042\n",
      "\n",
      "Batch 316/992 ━━━━━━━━━━━━━━━━━━━━ 08:39:03\n",
      "Accuracy: 0.9288 - Loss: 0.2036\n",
      "\n",
      "Batch 317/992 ━━━━━━━━━━━━━━━━━━━━ 08:39:13\n",
      "Accuracy: 0.9290 - Loss: 0.2031\n",
      "\n",
      "Batch 318/992 ━━━━━━━━━━━━━━━━━━━━ 08:39:23\n",
      "Accuracy: 0.9289 - Loss: 0.2031\n",
      "\n",
      "Batch 319/992 ━━━━━━━━━━━━━━━━━━━━ 08:39:33\n",
      "Accuracy: 0.9291 - Loss: 0.2026\n",
      "\n",
      "Batch 320/992 ━━━━━━━━━━━━━━━━━━━━ 08:39:44\n",
      "Accuracy: 0.9289 - Loss: 0.2026\n",
      "\n",
      "Batch 321/992 ━━━━━━━━━━━━━━━━━━━━ 08:39:54\n",
      "Accuracy: 0.9287 - Loss: 0.2032\n",
      "\n",
      "Batch 322/992 ━━━━━━━━━━━━━━━━━━━━ 08:40:04\n",
      "Accuracy: 0.9286 - Loss: 0.2035\n",
      "\n",
      "Batch 323/992 ━━━━━━━━━━━━━━━━━━━━ 08:40:14\n",
      "Accuracy: 0.9288 - Loss: 0.2031\n",
      "\n",
      "Batch 324/992 ━━━━━━━━━━━━━━━━━━━━ 08:40:25\n",
      "Accuracy: 0.9290 - Loss: 0.2025\n",
      "\n",
      "Batch 325/992 ━━━━━━━━━━━━━━━━━━━━ 08:40:35\n",
      "Accuracy: 0.9292 - Loss: 0.2027\n",
      "\n",
      "Batch 326/992 ━━━━━━━━━━━━━━━━━━━━ 08:40:45\n",
      "Accuracy: 0.9294 - Loss: 0.2023\n",
      "\n",
      "Batch 327/992 ━━━━━━━━━━━━━━━━━━━━ 08:40:55\n",
      "Accuracy: 0.9293 - Loss: 0.2034\n",
      "\n",
      "Batch 328/992 ━━━━━━━━━━━━━━━━━━━━ 08:41:05\n",
      "Accuracy: 0.9291 - Loss: 0.2042\n",
      "\n",
      "Batch 329/992 ━━━━━━━━━━━━━━━━━━━━ 08:41:15\n",
      "Accuracy: 0.9293 - Loss: 0.2037\n",
      "\n",
      "Batch 330/992 ━━━━━━━━━━━━━━━━━━━━ 08:41:25\n",
      "Accuracy: 0.9295 - Loss: 0.2033\n",
      "\n",
      "Batch 331/992 ━━━━━━━━━━━━━━━━━━━━ 08:41:35\n",
      "Accuracy: 0.9298 - Loss: 0.2031\n",
      "\n",
      "Batch 332/992 ━━━━━━━━━━━━━━━━━━━━ 08:41:46\n",
      "Accuracy: 0.9296 - Loss: 0.2031\n",
      "\n",
      "Batch 333/992 ━━━━━━━━━━━━━━━━━━━━ 08:41:56\n",
      "Accuracy: 0.9298 - Loss: 0.2031\n",
      "\n",
      "Batch 334/992 ━━━━━━━━━━━━━━━━━━━━ 08:42:06\n",
      "Accuracy: 0.9296 - Loss: 0.2029\n",
      "\n",
      "Batch 335/992 ━━━━━━━━━━━━━━━━━━━━ 08:42:16\n",
      "Accuracy: 0.9299 - Loss: 0.2025\n",
      "\n",
      "Batch 336/992 ━━━━━━━━━━━━━━━━━━━━ 08:42:26\n",
      "Accuracy: 0.9297 - Loss: 0.2035\n",
      "\n",
      "Batch 337/992 ━━━━━━━━━━━━━━━━━━━━ 08:42:37\n",
      "Accuracy: 0.9299 - Loss: 0.2031\n",
      "\n",
      "Batch 338/992 ━━━━━━━━━━━━━━━━━━━━ 08:42:47\n",
      "Accuracy: 0.9301 - Loss: 0.2026\n",
      "\n",
      "Batch 339/992 ━━━━━━━━━━━━━━━━━━━━ 08:42:57\n",
      "Accuracy: 0.9296 - Loss: 0.2036\n",
      "\n",
      "Batch 340/992 ━━━━━━━━━━━━━━━━━━━━ 08:43:08\n",
      "Accuracy: 0.9290 - Loss: 0.2041\n",
      "\n",
      "Batch 341/992 ━━━━━━━━━━━━━━━━━━━━ 08:43:18\n",
      "Accuracy: 0.9289 - Loss: 0.2041\n",
      "\n",
      "Batch 342/992 ━━━━━━━━━━━━━━━━━━━━ 08:43:28\n",
      "Accuracy: 0.9287 - Loss: 0.2047\n",
      "\n",
      "Batch 343/992 ━━━━━━━━━━━━━━━━━━━━ 08:43:38\n",
      "Accuracy: 0.9286 - Loss: 0.2049\n",
      "\n",
      "Batch 344/992 ━━━━━━━━━━━━━━━━━━━━ 08:43:48\n",
      "Accuracy: 0.9288 - Loss: 0.2046\n",
      "\n",
      "Batch 345/992 ━━━━━━━━━━━━━━━━━━━━ 08:43:58\n",
      "Accuracy: 0.9286 - Loss: 0.2044\n",
      "\n",
      "Batch 346/992 ━━━━━━━━━━━━━━━━━━━━ 08:44:08\n",
      "Accuracy: 0.9288 - Loss: 0.2039\n",
      "\n",
      "Batch 347/992 ━━━━━━━━━━━━━━━━━━━━ 08:44:19\n",
      "Accuracy: 0.9287 - Loss: 0.2039\n",
      "\n",
      "Batch 348/992 ━━━━━━━━━━━━━━━━━━━━ 08:44:29\n",
      "Accuracy: 0.9285 - Loss: 0.2039\n",
      "\n",
      "Batch 349/992 ━━━━━━━━━━━━━━━━━━━━ 08:44:39\n",
      "Accuracy: 0.9287 - Loss: 0.2037\n",
      "\n",
      "Batch 350/992 ━━━━━━━━━━━━━━━━━━━━ 08:44:49\n",
      "Accuracy: 0.9286 - Loss: 0.2037\n",
      "\n",
      "Batch 351/992 ━━━━━━━━━━━━━━━━━━━━ 08:44:59\n",
      "Accuracy: 0.9284 - Loss: 0.2037\n",
      "\n",
      "Batch 352/992 ━━━━━━━━━━━━━━━━━━━━ 08:45:09\n",
      "Accuracy: 0.9286 - Loss: 0.2035\n",
      "\n",
      "Batch 353/992 ━━━━━━━━━━━━━━━━━━━━ 08:45:19\n",
      "Accuracy: 0.9288 - Loss: 0.2030\n",
      "\n",
      "Batch 354/992 ━━━━━━━━━━━━━━━━━━━━ 08:45:29\n",
      "Accuracy: 0.9283 - Loss: 0.2035\n",
      "\n",
      "Batch 355/992 ━━━━━━━━━━━━━━━━━━━━ 08:45:39\n",
      "Accuracy: 0.9285 - Loss: 0.2032\n",
      "\n",
      "Batch 356/992 ━━━━━━━━━━━━━━━━━━━━ 08:45:49\n",
      "Accuracy: 0.9287 - Loss: 0.2029\n",
      "\n",
      "Batch 357/992 ━━━━━━━━━━━━━━━━━━━━ 08:46:00\n",
      "Accuracy: 0.9282 - Loss: 0.2047\n",
      "\n",
      "Batch 358/992 ━━━━━━━━━━━━━━━━━━━━ 08:46:10\n",
      "Accuracy: 0.9281 - Loss: 0.2047\n",
      "\n",
      "Batch 359/992 ━━━━━━━━━━━━━━━━━━━━ 08:46:20\n",
      "Accuracy: 0.9272 - Loss: 0.2064\n",
      "\n",
      "Batch 360/992 ━━━━━━━━━━━━━━━━━━━━ 08:46:30\n",
      "Accuracy: 0.9274 - Loss: 0.2059\n",
      "\n",
      "Batch 361/992 ━━━━━━━━━━━━━━━━━━━━ 08:46:40\n",
      "Accuracy: 0.9276 - Loss: 0.2055\n",
      "\n",
      "Batch 362/992 ━━━━━━━━━━━━━━━━━━━━ 08:46:51\n",
      "Accuracy: 0.9271 - Loss: 0.2067\n",
      "\n",
      "Batch 363/992 ━━━━━━━━━━━━━━━━━━━━ 08:47:01\n",
      "Accuracy: 0.9270 - Loss: 0.2069\n",
      "\n",
      "Batch 364/992 ━━━━━━━━━━━━━━━━━━━━ 08:47:11\n",
      "Accuracy: 0.9272 - Loss: 0.2063\n",
      "\n",
      "Batch 365/992 ━━━━━━━━━━━━━━━━━━━━ 08:47:21\n",
      "Accuracy: 0.9274 - Loss: 0.2060\n",
      "\n",
      "Batch 366/992 ━━━━━━━━━━━━━━━━━━━━ 08:47:31\n",
      "Accuracy: 0.9276 - Loss: 0.2056\n",
      "\n",
      "Batch 367/992 ━━━━━━━━━━━━━━━━━━━━ 08:47:41\n",
      "Accuracy: 0.9271 - Loss: 0.2058\n",
      "\n",
      "Batch 368/992 ━━━━━━━━━━━━━━━━━━━━ 08:47:51\n",
      "Accuracy: 0.9270 - Loss: 0.2060\n",
      "\n",
      "Batch 369/992 ━━━━━━━━━━━━━━━━━━━━ 08:48:01\n",
      "Accuracy: 0.9272 - Loss: 0.2056\n",
      "\n",
      "Batch 370/992 ━━━━━━━━━━━━━━━━━━━━ 08:48:12\n",
      "Accuracy: 0.9270 - Loss: 0.2059\n",
      "\n",
      "Batch 371/992 ━━━━━━━━━━━━━━━━━━━━ 08:48:22\n",
      "Accuracy: 0.9272 - Loss: 0.2055\n",
      "\n",
      "Batch 372/992 ━━━━━━━━━━━━━━━━━━━━ 08:48:32\n",
      "Accuracy: 0.9274 - Loss: 0.2051\n",
      "\n",
      "Batch 373/992 ━━━━━━━━━━━━━━━━━━━━ 08:48:42\n",
      "Accuracy: 0.9269 - Loss: 0.2056\n",
      "\n",
      "Batch 374/992 ━━━━━━━━━━━━━━━━━━━━ 08:48:52\n",
      "Accuracy: 0.9261 - Loss: 0.2074\n",
      "\n",
      "Batch 375/992 ━━━━━━━━━━━━━━━━━━━━ 08:49:02\n",
      "Accuracy: 0.9263 - Loss: 0.2075\n",
      "\n",
      "Batch 376/992 ━━━━━━━━━━━━━━━━━━━━ 08:49:12\n",
      "Accuracy: 0.9265 - Loss: 0.2072\n",
      "\n",
      "Batch 377/992 ━━━━━━━━━━━━━━━━━━━━ 08:49:23\n",
      "Accuracy: 0.9264 - Loss: 0.2077\n",
      "\n",
      "Batch 378/992 ━━━━━━━━━━━━━━━━━━━━ 08:49:33\n",
      "Accuracy: 0.9266 - Loss: 0.2073\n",
      "\n",
      "Batch 379/992 ━━━━━━━━━━━━━━━━━━━━ 08:49:43\n",
      "Accuracy: 0.9268 - Loss: 0.2072\n",
      "\n",
      "Batch 380/992 ━━━━━━━━━━━━━━━━━━━━ 08:49:53\n",
      "Accuracy: 0.9270 - Loss: 0.2068\n",
      "\n",
      "Batch 381/992 ━━━━━━━━━━━━━━━━━━━━ 08:50:03\n",
      "Accuracy: 0.9268 - Loss: 0.2070\n",
      "\n",
      "Batch 382/992 ━━━━━━━━━━━━━━━━━━━━ 08:50:14\n",
      "Accuracy: 0.9270 - Loss: 0.2068\n",
      "\n",
      "Batch 383/992 ━━━━━━━━━━━━━━━━━━━━ 08:50:24\n",
      "Accuracy: 0.9272 - Loss: 0.2066\n",
      "\n",
      "Batch 384/992 ━━━━━━━━━━━━━━━━━━━━ 08:50:34\n",
      "Accuracy: 0.9271 - Loss: 0.2070\n",
      "\n",
      "Batch 385/992 ━━━━━━━━━━━━━━━━━━━━ 08:50:44\n",
      "Accuracy: 0.9273 - Loss: 0.2068\n",
      "\n",
      "Batch 386/992 ━━━━━━━━━━━━━━━━━━━━ 08:50:55\n",
      "Accuracy: 0.9268 - Loss: 0.2074\n",
      "\n",
      "Batch 387/992 ━━━━━━━━━━━━━━━━━━━━ 08:51:05\n",
      "Accuracy: 0.9267 - Loss: 0.2074\n",
      "\n",
      "Batch 388/992 ━━━━━━━━━━━━━━━━━━━━ 08:51:15\n",
      "Accuracy: 0.9269 - Loss: 0.2071\n",
      "\n",
      "Batch 389/992 ━━━━━━━━━━━━━━━━━━━━ 08:51:25\n",
      "Accuracy: 0.9271 - Loss: 0.2067\n",
      "\n",
      "Batch 390/992 ━━━━━━━━━━━━━━━━━━━━ 08:51:35\n",
      "Accuracy: 0.9272 - Loss: 0.2064\n",
      "\n",
      "Batch 391/992 ━━━━━━━━━━━━━━━━━━━━ 08:51:45\n",
      "Accuracy: 0.9274 - Loss: 0.2062\n",
      "\n",
      "Batch 392/992 ━━━━━━━━━━━━━━━━━━━━ 08:51:55\n",
      "Accuracy: 0.9276 - Loss: 0.2058\n",
      "\n",
      "Batch 393/992 ━━━━━━━━━━━━━━━━━━━━ 08:52:05\n",
      "Accuracy: 0.9278 - Loss: 0.2053\n",
      "\n",
      "Batch 394/992 ━━━━━━━━━━━━━━━━━━━━ 08:52:16\n",
      "Accuracy: 0.9273 - Loss: 0.2067\n",
      "\n",
      "Batch 395/992 ━━━━━━━━━━━━━━━━━━━━ 08:52:26\n",
      "Accuracy: 0.9275 - Loss: 0.2064\n",
      "\n",
      "Batch 396/992 ━━━━━━━━━━━━━━━━━━━━ 08:52:36\n",
      "Accuracy: 0.9274 - Loss: 0.2068\n",
      "\n",
      "Batch 397/992 ━━━━━━━━━━━━━━━━━━━━ 08:52:47\n",
      "Accuracy: 0.9276 - Loss: 0.2067\n",
      "\n",
      "Batch 398/992 ━━━━━━━━━━━━━━━━━━━━ 08:52:57\n",
      "Accuracy: 0.9278 - Loss: 0.2063\n",
      "\n",
      "Batch 399/992 ━━━━━━━━━━━━━━━━━━━━ 08:53:07\n",
      "Accuracy: 0.9279 - Loss: 0.2059\n",
      "\n",
      "Batch 400/992 ━━━━━━━━━━━━━━━━━━━━ 08:53:17\n",
      "Accuracy: 0.9281 - Loss: 0.2055\n",
      "\n",
      "Batch 401/992 ━━━━━━━━━━━━━━━━━━━━ 08:53:28\n",
      "Accuracy: 0.9283 - Loss: 0.2053\n",
      "\n",
      "Batch 402/992 ━━━━━━━━━━━━━━━━━━━━ 08:53:38\n",
      "Accuracy: 0.9285 - Loss: 0.2051\n",
      "\n",
      "Batch 403/992 ━━━━━━━━━━━━━━━━━━━━ 08:53:48\n",
      "Accuracy: 0.9287 - Loss: 0.2049\n",
      "\n",
      "Batch 404/992 ━━━━━━━━━━━━━━━━━━━━ 08:53:58\n",
      "Accuracy: 0.9285 - Loss: 0.2056\n",
      "\n",
      "Batch 405/992 ━━━━━━━━━━━━━━━━━━━━ 08:54:08\n",
      "Accuracy: 0.9287 - Loss: 0.2054\n",
      "\n",
      "Batch 406/992 ━━━━━━━━━━━━━━━━━━━━ 08:54:18\n",
      "Accuracy: 0.9286 - Loss: 0.2058\n",
      "\n",
      "Batch 407/992 ━━━━━━━━━━━━━━━━━━━━ 08:54:28\n",
      "Accuracy: 0.9287 - Loss: 0.2055\n",
      "\n",
      "Batch 408/992 ━━━━━━━━━━━━━━━━━━━━ 08:54:38\n",
      "Accuracy: 0.9289 - Loss: 0.2051\n",
      "\n",
      "Batch 409/992 ━━━━━━━━━━━━━━━━━━━━ 08:54:48\n",
      "Accuracy: 0.9288 - Loss: 0.2062\n",
      "\n",
      "Batch 410/992 ━━━━━━━━━━━━━━━━━━━━ 08:54:59\n",
      "Accuracy: 0.9287 - Loss: 0.2078\n",
      "\n",
      "Batch 411/992 ━━━━━━━━━━━━━━━━━━━━ 08:55:09\n",
      "Accuracy: 0.9285 - Loss: 0.2078\n",
      "\n",
      "Batch 412/992 ━━━━━━━━━━━━━━━━━━━━ 08:55:19\n",
      "Accuracy: 0.9284 - Loss: 0.2082\n",
      "\n",
      "Batch 413/992 ━━━━━━━━━━━━━━━━━━━━ 08:55:30\n",
      "Accuracy: 0.9286 - Loss: 0.2080\n",
      "\n",
      "Batch 414/992 ━━━━━━━━━━━━━━━━━━━━ 08:55:40\n",
      "Accuracy: 0.9287 - Loss: 0.2075\n",
      "\n",
      "Batch 415/992 ━━━━━━━━━━━━━━━━━━━━ 08:55:50\n",
      "Accuracy: 0.9289 - Loss: 0.2070\n",
      "\n",
      "Batch 416/992 ━━━━━━━━━━━━━━━━━━━━ 08:56:00\n",
      "Accuracy: 0.9291 - Loss: 0.2069\n",
      "\n",
      "Batch 417/992 ━━━━━━━━━━━━━━━━━━━━ 08:56:11\n",
      "Accuracy: 0.9287 - Loss: 0.2072\n",
      "\n",
      "Batch 418/992 ━━━━━━━━━━━━━━━━━━━━ 08:56:21\n",
      "Accuracy: 0.9288 - Loss: 0.2069\n",
      "\n",
      "Batch 419/992 ━━━━━━━━━━━━━━━━━━━━ 08:56:31\n",
      "Accuracy: 0.9290 - Loss: 0.2065\n",
      "\n",
      "Batch 420/992 ━━━━━━━━━━━━━━━━━━━━ 08:56:41\n",
      "Accuracy: 0.9292 - Loss: 0.2061\n",
      "\n",
      "Batch 421/992 ━━━━━━━━━━━━━━━━━━━━ 08:56:51\n",
      "Accuracy: 0.9290 - Loss: 0.2062\n",
      "\n",
      "Batch 422/992 ━━━━━━━━━━━━━━━━━━━━ 08:57:02\n",
      "Accuracy: 0.9292 - Loss: 0.2057\n",
      "\n",
      "Batch 423/992 ━━━━━━━━━━━━━━━━━━━━ 08:57:12\n",
      "Accuracy: 0.9288 - Loss: 0.2064\n",
      "\n",
      "Batch 424/992 ━━━━━━━━━━━━━━━━━━━━ 08:57:22\n",
      "Accuracy: 0.9290 - Loss: 0.2061\n",
      "\n",
      "Batch 425/992 ━━━━━━━━━━━━━━━━━━━━ 08:57:32\n",
      "Accuracy: 0.9291 - Loss: 0.2057\n",
      "\n",
      "Batch 426/992 ━━━━━━━━━━━━━━━━━━━━ 08:57:42\n",
      "Accuracy: 0.9293 - Loss: 0.2056\n",
      "\n",
      "Batch 427/992 ━━━━━━━━━━━━━━━━━━━━ 08:57:52\n",
      "Accuracy: 0.9292 - Loss: 0.2058\n",
      "\n",
      "Batch 428/992 ━━━━━━━━━━━━━━━━━━━━ 08:58:03\n",
      "Accuracy: 0.9287 - Loss: 0.2060\n",
      "\n",
      "Batch 429/992 ━━━━━━━━━━━━━━━━━━━━ 08:58:14\n",
      "Accuracy: 0.9289 - Loss: 0.2057\n",
      "\n",
      "Batch 430/992 ━━━━━━━━━━━━━━━━━━━━ 08:58:24\n",
      "Accuracy: 0.9288 - Loss: 0.2056\n",
      "\n",
      "Batch 431/992 ━━━━━━━━━━━━━━━━━━━━ 08:58:34\n",
      "Accuracy: 0.9287 - Loss: 0.2056\n",
      "\n",
      "Batch 432/992 ━━━━━━━━━━━━━━━━━━━━ 08:58:46\n",
      "Accuracy: 0.9288 - Loss: 0.2052\n",
      "\n",
      "Batch 433/992 ━━━━━━━━━━━━━━━━━━━━ 08:58:59\n",
      "Accuracy: 0.9287 - Loss: 0.2053\n",
      "\n",
      "Batch 434/992 ━━━━━━━━━━━━━━━━━━━━ 08:59:10\n",
      "Accuracy: 0.9286 - Loss: 0.2058\n",
      "\n",
      "Batch 435/992 ━━━━━━━━━━━━━━━━━━━━ 08:59:22\n",
      "Accuracy: 0.9287 - Loss: 0.2055\n",
      "\n",
      "Batch 436/992 ━━━━━━━━━━━━━━━━━━━━ 08:59:34\n",
      "Accuracy: 0.9289 - Loss: 0.2052\n",
      "\n",
      "Batch 437/992 ━━━━━━━━━━━━━━━━━━━━ 08:59:45\n",
      "Accuracy: 0.9288 - Loss: 0.2056\n",
      "\n",
      "Batch 438/992 ━━━━━━━━━━━━━━━━━━━━ 08:59:56\n",
      "Accuracy: 0.9289 - Loss: 0.2055\n",
      "\n",
      "Batch 439/992 ━━━━━━━━━━━━━━━━━━━━ 09:00:07\n",
      "Accuracy: 0.9291 - Loss: 0.2054\n",
      "\n",
      "Batch 440/992 ━━━━━━━━━━━━━━━━━━━━ 09:00:17\n",
      "Accuracy: 0.9293 - Loss: 0.2051\n",
      "\n",
      "Batch 441/992 ━━━━━━━━━━━━━━━━━━━━ 09:00:28\n",
      "Accuracy: 0.9291 - Loss: 0.2051\n",
      "\n",
      "Batch 442/992 ━━━━━━━━━━━━━━━━━━━━ 09:00:38\n",
      "Accuracy: 0.9293 - Loss: 0.2048\n",
      "\n",
      "Batch 443/992 ━━━━━━━━━━━━━━━━━━━━ 09:00:49\n",
      "Accuracy: 0.9295 - Loss: 0.2046\n",
      "\n",
      "Batch 444/992 ━━━━━━━━━━━━━━━━━━━━ 09:00:59\n",
      "Accuracy: 0.9296 - Loss: 0.2041\n",
      "\n",
      "Batch 445/992 ━━━━━━━━━━━━━━━━━━━━ 09:01:10\n",
      "Accuracy: 0.9295 - Loss: 0.2043\n",
      "\n",
      "Batch 446/992 ━━━━━━━━━━━━━━━━━━━━ 09:01:20\n",
      "Accuracy: 0.9297 - Loss: 0.2039\n",
      "\n",
      "Batch 447/992 ━━━━━━━━━━━━━━━━━━━━ 09:01:30\n",
      "Accuracy: 0.9295 - Loss: 0.2039\n",
      "\n",
      "Batch 448/992 ━━━━━━━━━━━━━━━━━━━━ 09:01:41\n",
      "Accuracy: 0.9294 - Loss: 0.2039\n",
      "\n",
      "Batch 449/992 ━━━━━━━━━━━━━━━━━━━━ 09:01:52\n",
      "Accuracy: 0.9296 - Loss: 0.2035\n",
      "\n",
      "Batch 450/992 ━━━━━━━━━━━━━━━━━━━━ 09:02:03\n",
      "Accuracy: 0.9297 - Loss: 0.2032\n",
      "\n",
      "Batch 451/992 ━━━━━━━━━━━━━━━━━━━━ 09:02:13\n",
      "Accuracy: 0.9296 - Loss: 0.2033\n",
      "\n",
      "Batch 452/992 ━━━━━━━━━━━━━━━━━━━━ 09:02:24\n",
      "Accuracy: 0.9292 - Loss: 0.2043\n",
      "\n",
      "Batch 453/992 ━━━━━━━━━━━━━━━━━━━━ 09:02:34\n",
      "Accuracy: 0.9291 - Loss: 0.2046\n",
      "\n",
      "Batch 454/992 ━━━━━━━━━━━━━━━━━━━━ 09:02:44\n",
      "Accuracy: 0.9292 - Loss: 0.2042\n",
      "\n",
      "Batch 455/992 ━━━━━━━━━━━━━━━━━━━━ 09:02:55\n",
      "Accuracy: 0.9294 - Loss: 0.2041\n",
      "\n",
      "Batch 456/992 ━━━━━━━━━━━━━━━━━━━━ 09:03:05\n",
      "Accuracy: 0.9296 - Loss: 0.2039\n",
      "\n",
      "Batch 457/992 ━━━━━━━━━━━━━━━━━━━━ 09:03:16\n",
      "Accuracy: 0.9297 - Loss: 0.2038\n",
      "\n",
      "Batch 458/992 ━━━━━━━━━━━━━━━━━━━━ 09:03:26\n",
      "Accuracy: 0.9299 - Loss: 0.2034\n",
      "\n",
      "Batch 459/992 ━━━━━━━━━━━━━━━━━━━━ 09:03:37\n",
      "Accuracy: 0.9297 - Loss: 0.2037\n",
      "\n",
      "Batch 460/992 ━━━━━━━━━━━━━━━━━━━━ 09:03:48\n",
      "Accuracy: 0.9299 - Loss: 0.2035\n",
      "\n",
      "Batch 461/992 ━━━━━━━━━━━━━━━━━━━━ 09:03:59\n",
      "Accuracy: 0.9300 - Loss: 0.2034\n",
      "\n",
      "Batch 462/992 ━━━━━━━━━━━━━━━━━━━━ 09:04:09\n",
      "Accuracy: 0.9302 - Loss: 0.2032\n",
      "\n",
      "Batch 463/992 ━━━━━━━━━━━━━━━━━━━━ 09:04:19\n",
      "Accuracy: 0.9303 - Loss: 0.2029\n",
      "\n",
      "Batch 464/992 ━━━━━━━━━━━━━━━━━━━━ 09:04:30\n",
      "Accuracy: 0.9305 - Loss: 0.2024\n",
      "\n",
      "Batch 465/992 ━━━━━━━━━━━━━━━━━━━━ 09:04:40\n",
      "Accuracy: 0.9306 - Loss: 0.2023\n",
      "\n",
      "Batch 466/992 ━━━━━━━━━━━━━━━━━━━━ 09:04:51\n",
      "Accuracy: 0.9308 - Loss: 0.2020\n",
      "\n",
      "Batch 467/992 ━━━━━━━━━━━━━━━━━━━━ 09:05:03\n",
      "Accuracy: 0.9309 - Loss: 0.2017\n",
      "\n",
      "Batch 468/992 ━━━━━━━━━━━━━━━━━━━━ 09:05:15\n",
      "Accuracy: 0.9311 - Loss: 0.2015\n",
      "\n",
      "Batch 469/992 ━━━━━━━━━━━━━━━━━━━━ 09:05:26\n",
      "Accuracy: 0.9312 - Loss: 0.2011\n",
      "\n",
      "Batch 470/992 ━━━━━━━━━━━━━━━━━━━━ 09:05:36\n",
      "Accuracy: 0.9314 - Loss: 0.2010\n",
      "\n",
      "Batch 471/992 ━━━━━━━━━━━━━━━━━━━━ 09:05:47\n",
      "Accuracy: 0.9310 - Loss: 0.2012\n",
      "\n",
      "Batch 472/992 ━━━━━━━━━━━━━━━━━━━━ 09:05:58\n",
      "Accuracy: 0.9311 - Loss: 0.2008\n",
      "\n",
      "Batch 473/992 ━━━━━━━━━━━━━━━━━━━━ 09:06:08\n",
      "Accuracy: 0.9313 - Loss: 0.2005\n",
      "\n",
      "Batch 474/992 ━━━━━━━━━━━━━━━━━━━━ 09:06:19\n",
      "Accuracy: 0.9314 - Loss: 0.2001\n",
      "\n",
      "Batch 475/992 ━━━━━━━━━━━━━━━━━━━━ 09:06:29\n",
      "Accuracy: 0.9316 - Loss: 0.1999\n",
      "\n",
      "Batch 476/992 ━━━━━━━━━━━━━━━━━━━━ 09:06:40\n",
      "Accuracy: 0.9317 - Loss: 0.1996\n",
      "\n",
      "Batch 477/992 ━━━━━━━━━━━━━━━━━━━━ 09:06:50\n",
      "Accuracy: 0.9316 - Loss: 0.1996\n",
      "\n",
      "Batch 478/992 ━━━━━━━━━━━━━━━━━━━━ 09:07:01\n",
      "Accuracy: 0.9315 - Loss: 0.1997\n",
      "\n",
      "Batch 479/992 ━━━━━━━━━━━━━━━━━━━━ 09:07:12\n",
      "Accuracy: 0.9314 - Loss: 0.1997\n",
      "\n",
      "Batch 480/992 ━━━━━━━━━━━━━━━━━━━━ 09:07:22\n",
      "Accuracy: 0.9315 - Loss: 0.1995\n",
      "\n",
      "Batch 481/992 ━━━━━━━━━━━━━━━━━━━━ 09:07:33\n",
      "Accuracy: 0.9317 - Loss: 0.1991\n",
      "\n",
      "Batch 482/992 ━━━━━━━━━━━━━━━━━━━━ 09:07:43\n",
      "Accuracy: 0.9315 - Loss: 0.1992\n",
      "\n",
      "Batch 483/992 ━━━━━━━━━━━━━━━━━━━━ 09:07:54\n",
      "Accuracy: 0.9317 - Loss: 0.1988\n",
      "\n",
      "Batch 484/992 ━━━━━━━━━━━━━━━━━━━━ 09:08:05\n",
      "Accuracy: 0.9316 - Loss: 0.1991\n",
      "\n",
      "Batch 485/992 ━━━━━━━━━━━━━━━━━━━━ 09:08:15\n",
      "Accuracy: 0.9317 - Loss: 0.1988\n",
      "\n",
      "Batch 486/992 ━━━━━━━━━━━━━━━━━━━━ 09:08:26\n",
      "Accuracy: 0.9318 - Loss: 0.1985\n",
      "\n",
      "Batch 487/992 ━━━━━━━━━━━━━━━━━━━━ 09:08:36\n",
      "Accuracy: 0.9317 - Loss: 0.1990\n",
      "\n",
      "Batch 488/992 ━━━━━━━━━━━━━━━━━━━━ 09:08:47\n",
      "Accuracy: 0.9319 - Loss: 0.1988\n",
      "\n",
      "Batch 489/992 ━━━━━━━━━━━━━━━━━━━━ 09:08:57\n",
      "Accuracy: 0.9320 - Loss: 0.1985\n",
      "\n",
      "Batch 490/992 ━━━━━━━━━━━━━━━━━━━━ 09:09:08\n",
      "Accuracy: 0.9319 - Loss: 0.1995\n",
      "\n",
      "Batch 491/992 ━━━━━━━━━━━━━━━━━━━━ 09:09:18\n",
      "Accuracy: 0.9320 - Loss: 0.1992\n",
      "\n",
      "Batch 492/992 ━━━━━━━━━━━━━━━━━━━━ 09:09:29\n",
      "Accuracy: 0.9322 - Loss: 0.1988\n",
      "\n",
      "Batch 493/992 ━━━━━━━━━━━━━━━━━━━━ 09:09:40\n",
      "Accuracy: 0.9320 - Loss: 0.1987\n",
      "\n",
      "Batch 494/992 ━━━━━━━━━━━━━━━━━━━━ 09:09:51\n",
      "Accuracy: 0.9322 - Loss: 0.1983\n",
      "\n",
      "Batch 495/992 ━━━━━━━━━━━━━━━━━━━━ 09:10:02\n",
      "Accuracy: 0.9316 - Loss: 0.1989\n",
      "\n",
      "Batch 496/992 ━━━━━━━━━━━━━━━━━━━━ 09:10:12\n",
      "Accuracy: 0.9317 - Loss: 0.1990\n",
      "\n",
      "Batch 497/992 ━━━━━━━━━━━━━━━━━━━━ 09:10:23\n",
      "Accuracy: 0.9316 - Loss: 0.1991\n",
      "\n",
      "Batch 498/992 ━━━━━━━━━━━━━━━━━━━━ 09:10:33\n",
      "Accuracy: 0.9317 - Loss: 0.1992\n",
      "\n",
      "Batch 499/992 ━━━━━━━━━━━━━━━━━━━━ 09:10:44\n",
      "Accuracy: 0.9319 - Loss: 0.1989\n",
      "\n",
      "Batch 500/992 ━━━━━━━━━━━━━━━━━━━━ 09:10:54\n",
      "Accuracy: 0.9320 - Loss: 0.1988\n",
      "\n",
      "Batch 501/992 ━━━━━━━━━━━━━━━━━━━━ 09:11:05\n",
      "Accuracy: 0.9319 - Loss: 0.1990\n",
      "\n",
      "Batch 502/992 ━━━━━━━━━━━━━━━━━━━━ 09:11:15\n",
      "Accuracy: 0.9318 - Loss: 0.1991\n",
      "\n",
      "Batch 503/992 ━━━━━━━━━━━━━━━━━━━━ 09:11:26\n",
      "Accuracy: 0.9319 - Loss: 0.1988\n",
      "\n",
      "Batch 504/992 ━━━━━━━━━━━━━━━━━━━━ 09:11:36\n",
      "Accuracy: 0.9318 - Loss: 0.1987\n",
      "\n",
      "Batch 505/992 ━━━━━━━━━━━━━━━━━━━━ 09:11:47\n",
      "Accuracy: 0.9317 - Loss: 0.1987\n",
      "\n",
      "Batch 506/992 ━━━━━━━━━━━━━━━━━━━━ 09:11:58\n",
      "Accuracy: 0.9318 - Loss: 0.1985\n",
      "\n",
      "Batch 507/992 ━━━━━━━━━━━━━━━━━━━━ 09:12:09\n",
      "Accuracy: 0.9320 - Loss: 0.1983\n",
      "\n",
      "Batch 508/992 ━━━━━━━━━━━━━━━━━━━━ 09:12:19\n",
      "Accuracy: 0.9321 - Loss: 0.1980\n",
      "\n",
      "Batch 509/992 ━━━━━━━━━━━━━━━━━━━━ 09:12:30\n",
      "Accuracy: 0.9322 - Loss: 0.1977\n",
      "\n",
      "Batch 510/992 ━━━━━━━━━━━━━━━━━━━━ 09:12:41\n",
      "Accuracy: 0.9324 - Loss: 0.1974\n",
      "\n",
      "Batch 511/992 ━━━━━━━━━━━━━━━━━━━━ 09:12:51\n",
      "Accuracy: 0.9320 - Loss: 0.1986\n",
      "\n",
      "Batch 512/992 ━━━━━━━━━━━━━━━━━━━━ 09:13:02\n",
      "Accuracy: 0.9321 - Loss: 0.1985\n",
      "\n",
      "Batch 513/992 ━━━━━━━━━━━━━━━━━━━━ 09:13:12\n",
      "Accuracy: 0.9320 - Loss: 0.1987\n",
      "\n",
      "Batch 514/992 ━━━━━━━━━━━━━━━━━━━━ 09:13:23\n",
      "Accuracy: 0.9317 - Loss: 0.1990\n",
      "\n",
      "Batch 515/992 ━━━━━━━━━━━━━━━━━━━━ 09:13:35\n",
      "Accuracy: 0.9318 - Loss: 0.1987\n",
      "\n",
      "Batch 516/992 ━━━━━━━━━━━━━━━━━━━━ 09:13:46\n",
      "Accuracy: 0.9317 - Loss: 0.1987\n",
      "\n",
      "Batch 517/992 ━━━━━━━━━━━━━━━━━━━━ 09:13:57\n",
      "Accuracy: 0.9318 - Loss: 0.1983\n",
      "\n",
      "Batch 518/992 ━━━━━━━━━━━━━━━━━━━━ 09:14:08\n",
      "Accuracy: 0.9317 - Loss: 0.1984\n",
      "\n",
      "Batch 519/992 ━━━━━━━━━━━━━━━━━━━━ 09:14:19\n",
      "Accuracy: 0.9318 - Loss: 0.1981\n",
      "\n",
      "Batch 520/992 ━━━━━━━━━━━━━━━━━━━━ 09:14:30\n",
      "Accuracy: 0.9320 - Loss: 0.1978\n",
      "\n",
      "Batch 521/992 ━━━━━━━━━━━━━━━━━━━━ 09:14:43\n",
      "Accuracy: 0.9321 - Loss: 0.1975\n",
      "\n",
      "Batch 522/992 ━━━━━━━━━━━━━━━━━━━━ 09:14:54\n",
      "Accuracy: 0.9322 - Loss: 0.1974\n",
      "\n",
      "Batch 523/992 ━━━━━━━━━━━━━━━━━━━━ 09:15:04\n",
      "Accuracy: 0.9324 - Loss: 0.1972\n",
      "\n",
      "Batch 524/992 ━━━━━━━━━━━━━━━━━━━━ 09:15:15\n",
      "Accuracy: 0.9323 - Loss: 0.1975\n",
      "\n",
      "Batch 525/992 ━━━━━━━━━━━━━━━━━━━━ 09:15:25\n",
      "Accuracy: 0.9321 - Loss: 0.1980\n",
      "\n",
      "Batch 526/992 ━━━━━━━━━━━━━━━━━━━━ 09:15:37\n",
      "Accuracy: 0.9320 - Loss: 0.1980\n",
      "\n",
      "Batch 527/992 ━━━━━━━━━━━━━━━━━━━━ 09:15:52\n",
      "Accuracy: 0.9322 - Loss: 0.1977\n",
      "\n",
      "Batch 528/992 ━━━━━━━━━━━━━━━━━━━━ 09:16:04\n",
      "Accuracy: 0.9321 - Loss: 0.1982\n",
      "\n",
      "Batch 529/992 ━━━━━━━━━━━━━━━━━━━━ 09:16:15\n",
      "Accuracy: 0.9319 - Loss: 0.1982\n",
      "\n",
      "Batch 530/992 ━━━━━━━━━━━━━━━━━━━━ 09:16:27\n",
      "Accuracy: 0.9321 - Loss: 0.1980\n",
      "\n",
      "Batch 531/992 ━━━━━━━━━━━━━━━━━━━━ 09:16:39\n",
      "Accuracy: 0.9315 - Loss: 0.1987\n",
      "\n",
      "Batch 532/992 ━━━━━━━━━━━━━━━━━━━━ 09:16:50\n",
      "Accuracy: 0.9316 - Loss: 0.1985\n",
      "\n",
      "Batch 533/992 ━━━━━━━━━━━━━━━━━━━━ 09:17:02\n",
      "Accuracy: 0.9315 - Loss: 0.1985\n",
      "\n",
      "Batch 534/992 ━━━━━━━━━━━━━━━━━━━━ 09:17:13\n",
      "Accuracy: 0.9314 - Loss: 0.1988\n",
      "\n",
      "Batch 535/992 ━━━━━━━━━━━━━━━━━━━━ 09:17:24\n",
      "Accuracy: 0.9315 - Loss: 0.1985\n",
      "\n",
      "Batch 536/992 ━━━━━━━━━━━━━━━━━━━━ 09:17:39\n",
      "Accuracy: 0.9314 - Loss: 0.1990\n",
      "\n",
      "Batch 537/992 ━━━━━━━━━━━━━━━━━━━━ 09:17:51\n",
      "Accuracy: 0.9311 - Loss: 0.2005\n",
      "\n",
      "Batch 538/992 ━━━━━━━━━━━━━━━━━━━━ 09:18:03\n",
      "Accuracy: 0.9312 - Loss: 0.2004\n",
      "\n",
      "Batch 539/992 ━━━━━━━━━━━━━━━━━━━━ 09:18:15\n",
      "Accuracy: 0.9314 - Loss: 0.2001\n",
      "\n",
      "Batch 540/992 ━━━━━━━━━━━━━━━━━━━━ 09:18:26\n",
      "Accuracy: 0.9315 - Loss: 0.1998\n",
      "\n",
      "Batch 541/992 ━━━━━━━━━━━━━━━━━━━━ 09:18:39\n",
      "Accuracy: 0.9316 - Loss: 0.1995\n",
      "\n",
      "Batch 542/992 ━━━━━━━━━━━━━━━━━━━━ 09:18:52\n",
      "Accuracy: 0.9317 - Loss: 0.1994\n",
      "\n",
      "Batch 543/992 ━━━━━━━━━━━━━━━━━━━━ 09:19:05\n",
      "Accuracy: 0.9316 - Loss: 0.2004\n",
      "\n",
      "Batch 544/992 ━━━━━━━━━━━━━━━━━━━━ 09:19:17\n",
      "Accuracy: 0.9318 - Loss: 0.2003\n",
      "\n",
      "Batch 545/992 ━━━━━━━━━━━━━━━━━━━━ 09:19:29\n",
      "Accuracy: 0.9314 - Loss: 0.2009\n",
      "\n",
      "Batch 546/992 ━━━━━━━━━━━━━━━━━━━━ 09:19:43\n",
      "Accuracy: 0.9315 - Loss: 0.2006\n",
      "\n",
      "Batch 547/992 ━━━━━━━━━━━━━━━━━━━━ 09:19:55\n",
      "Accuracy: 0.9317 - Loss: 0.2004\n",
      "\n",
      "Batch 548/992 ━━━━━━━━━━━━━━━━━━━━ 09:20:06\n",
      "Accuracy: 0.9318 - Loss: 0.2001\n",
      "\n",
      "Batch 549/992 ━━━━━━━━━━━━━━━━━━━━ 09:20:18\n",
      "Accuracy: 0.9317 - Loss: 0.2002\n",
      "\n",
      "Batch 550/992 ━━━━━━━━━━━━━━━━━━━━ 09:20:29\n",
      "Accuracy: 0.9316 - Loss: 0.2002\n",
      "\n",
      "Batch 551/992 ━━━━━━━━━━━━━━━━━━━━ 09:20:41\n",
      "Accuracy: 0.9317 - Loss: 0.2000\n",
      "\n",
      "Batch 552/992 ━━━━━━━━━━━━━━━━━━━━ 09:20:58\n",
      "Accuracy: 0.9318 - Loss: 0.1998\n",
      "\n",
      "Batch 553/992 ━━━━━━━━━━━━━━━━━━━━ 09:21:11\n",
      "Accuracy: 0.9308 - Loss: 0.2009\n",
      "\n",
      "Batch 554/992 ━━━━━━━━━━━━━━━━━━━━ 09:21:26\n",
      "Accuracy: 0.9310 - Loss: 0.2007\n",
      "\n",
      "Batch 555/992 ━━━━━━━━━━━━━━━━━━━━ 09:21:40\n",
      "Accuracy: 0.9306 - Loss: 0.2013\n",
      "\n",
      "Batch 556/992 ━━━━━━━━━━━━━━━━━━━━ 09:21:54\n",
      "Accuracy: 0.9308 - Loss: 0.2010\n",
      "\n",
      "Batch 557/992 ━━━━━━━━━━━━━━━━━━━━ 09:22:06\n",
      "Accuracy: 0.9309 - Loss: 0.2007\n",
      "\n",
      "Batch 558/992 ━━━━━━━━━━━━━━━━━━━━ 09:22:18\n",
      "Accuracy: 0.9306 - Loss: 0.2014\n",
      "\n",
      "Batch 559/992 ━━━━━━━━━━━━━━━━━━━━ 09:22:28\n",
      "Accuracy: 0.9307 - Loss: 0.2013\n",
      "\n",
      "Batch 560/992 ━━━━━━━━━━━━━━━━━━━━ 09:22:40\n",
      "Accuracy: 0.9306 - Loss: 0.2013\n",
      "\n",
      "Batch 561/992 ━━━━━━━━━━━━━━━━━━━━ 09:22:52\n",
      "Accuracy: 0.9303 - Loss: 0.2019\n",
      "\n",
      "Batch 562/992 ━━━━━━━━━━━━━━━━━━━━ 09:23:03\n",
      "Accuracy: 0.9302 - Loss: 0.2018\n",
      "\n",
      "Batch 563/992 ━━━━━━━━━━━━━━━━━━━━ 09:23:15\n",
      "Accuracy: 0.9298 - Loss: 0.2020\n",
      "\n",
      "Batch 564/992 ━━━━━━━━━━━━━━━━━━━━ 09:23:26\n",
      "Accuracy: 0.9295 - Loss: 0.2024\n",
      "\n",
      "Batch 565/992 ━━━━━━━━━━━━━━━━━━━━ 09:23:39\n",
      "Accuracy: 0.9296 - Loss: 0.2023\n",
      "\n",
      "Batch 566/992 ━━━━━━━━━━━━━━━━━━━━ 09:23:54\n",
      "Accuracy: 0.9295 - Loss: 0.2025\n",
      "\n",
      "Batch 567/992 ━━━━━━━━━━━━━━━━━━━━ 09:24:06\n",
      "Accuracy: 0.9295 - Loss: 0.2025\n",
      "\n",
      "Batch 568/992 ━━━━━━━━━━━━━━━━━━━━ 09:24:17\n",
      "Accuracy: 0.9294 - Loss: 0.2032\n",
      "\n",
      "Batch 569/992 ━━━━━━━━━━━━━━━━━━━━ 09:24:29\n",
      "Accuracy: 0.9295 - Loss: 0.2031\n",
      "\n",
      "Batch 570/992 ━━━━━━━━━━━━━━━━━━━━ 09:24:40\n",
      "Accuracy: 0.9292 - Loss: 0.2040\n",
      "\n",
      "Batch 571/992 ━━━━━━━━━━━━━━━━━━━━ 09:24:52\n",
      "Accuracy: 0.9293 - Loss: 0.2038\n",
      "\n",
      "Batch 572/992 ━━━━━━━━━━━━━━━━━━━━ 09:25:02\n",
      "Accuracy: 0.9292 - Loss: 0.2038\n",
      "\n",
      "Batch 573/992 ━━━━━━━━━━━━━━━━━━━━ 09:25:13\n",
      "Accuracy: 0.9291 - Loss: 0.2041\n",
      "\n",
      "Batch 574/992 ━━━━━━━━━━━━━━━━━━━━ 09:25:24\n",
      "Accuracy: 0.9290 - Loss: 0.2039\n",
      "\n",
      "Batch 575/992 ━━━━━━━━━━━━━━━━━━━━ 09:25:35\n",
      "Accuracy: 0.9289 - Loss: 0.2042\n",
      "\n",
      "Batch 576/992 ━━━━━━━━━━━━━━━━━━━━ 09:25:46\n",
      "Accuracy: 0.9290 - Loss: 0.2041\n",
      "\n",
      "Batch 577/992 ━━━━━━━━━━━━━━━━━━━━ 09:25:57\n",
      "Accuracy: 0.9292 - Loss: 0.2038\n",
      "\n",
      "Batch 578/992 ━━━━━━━━━━━━━━━━━━━━ 09:26:08\n",
      "Accuracy: 0.9293 - Loss: 0.2035\n",
      "\n",
      "Batch 579/992 ━━━━━━━━━━━━━━━━━━━━ 09:26:18\n",
      "Accuracy: 0.9292 - Loss: 0.2038\n",
      "\n",
      "Batch 580/992 ━━━━━━━━━━━━━━━━━━━━ 09:26:29\n",
      "Accuracy: 0.9289 - Loss: 0.2042\n",
      "\n",
      "Batch 581/992 ━━━━━━━━━━━━━━━━━━━━ 09:26:39\n",
      "Accuracy: 0.9290 - Loss: 0.2041\n",
      "\n",
      "Batch 582/992 ━━━━━━━━━━━━━━━━━━━━ 09:26:50\n",
      "Accuracy: 0.9289 - Loss: 0.2042\n",
      "\n",
      "Batch 583/992 ━━━━━━━━━━━━━━━━━━━━ 09:27:01\n",
      "Accuracy: 0.9290 - Loss: 0.2041\n",
      "\n",
      "Batch 584/992 ━━━━━━━━━━━━━━━━━━━━ 09:27:11\n",
      "Accuracy: 0.9289 - Loss: 0.2041\n",
      "\n",
      "Batch 585/992 ━━━━━━━━━━━━━━━━━━━━ 09:27:22\n",
      "Accuracy: 0.9286 - Loss: 0.2047\n",
      "\n",
      "Batch 586/992 ━━━━━━━━━━━━━━━━━━━━ 09:27:33\n",
      "Accuracy: 0.9288 - Loss: 0.2044\n",
      "\n",
      "Batch 587/992 ━━━━━━━━━━━━━━━━━━━━ 09:27:44\n",
      "Accuracy: 0.9287 - Loss: 0.2046\n",
      "\n",
      "Batch 588/992 ━━━━━━━━━━━━━━━━━━━━ 09:27:55\n",
      "Accuracy: 0.9288 - Loss: 0.2043\n",
      "\n",
      "Batch 589/992 ━━━━━━━━━━━━━━━━━━━━ 09:28:07\n",
      "Accuracy: 0.9289 - Loss: 0.2041\n",
      "\n",
      "Batch 590/992 ━━━━━━━━━━━━━━━━━━━━ 09:28:18\n",
      "Accuracy: 0.9290 - Loss: 0.2039\n",
      "\n",
      "Batch 591/992 ━━━━━━━━━━━━━━━━━━━━ 09:28:28\n",
      "Accuracy: 0.9287 - Loss: 0.2043\n",
      "\n",
      "Batch 592/992 ━━━━━━━━━━━━━━━━━━━━ 09:28:39\n",
      "Accuracy: 0.9286 - Loss: 0.2045\n",
      "\n",
      "Batch 593/992 ━━━━━━━━━━━━━━━━━━━━ 09:28:50\n",
      "Accuracy: 0.9283 - Loss: 0.2050\n",
      "\n",
      "Batch 594/992 ━━━━━━━━━━━━━━━━━━━━ 09:29:01\n",
      "Accuracy: 0.9285 - Loss: 0.2047\n",
      "\n",
      "Batch 595/992 ━━━━━━━━━━━━━━━━━━━━ 09:29:12\n",
      "Accuracy: 0.9284 - Loss: 0.2046\n",
      "\n",
      "Batch 596/992 ━━━━━━━━━━━━━━━━━━━━ 09:29:24\n",
      "Accuracy: 0.9285 - Loss: 0.2045\n",
      "\n",
      "Batch 597/992 ━━━━━━━━━━━━━━━━━━━━ 09:29:35\n",
      "Accuracy: 0.9284 - Loss: 0.2047\n",
      "\n",
      "Batch 598/992 ━━━━━━━━━━━━━━━━━━━━ 09:29:45\n",
      "Accuracy: 0.9281 - Loss: 0.2053\n",
      "\n",
      "Batch 599/992 ━━━━━━━━━━━━━━━━━━━━ 09:29:55\n",
      "Accuracy: 0.9282 - Loss: 0.2051\n",
      "\n",
      "Batch 600/992 ━━━━━━━━━━━━━━━━━━━━ 09:30:06\n",
      "Accuracy: 0.9277 - Loss: 0.2056\n",
      "\n",
      "Batch 601/992 ━━━━━━━━━━━━━━━━━━━━ 09:30:17\n",
      "Accuracy: 0.9270 - Loss: 0.2075\n",
      "\n",
      "Batch 602/992 ━━━━━━━━━━━━━━━━━━━━ 09:30:27\n",
      "Accuracy: 0.9269 - Loss: 0.2075\n",
      "\n",
      "Batch 603/992 ━━━━━━━━━━━━━━━━━━━━ 09:30:38\n",
      "Accuracy: 0.9266 - Loss: 0.2081\n",
      "\n",
      "Batch 604/992 ━━━━━━━━━━━━━━━━━━━━ 09:30:48\n",
      "Accuracy: 0.9263 - Loss: 0.2084\n",
      "\n",
      "Batch 605/992 ━━━━━━━━━━━━━━━━━━━━ 09:30:59\n",
      "Accuracy: 0.9262 - Loss: 0.2085\n",
      "\n",
      "Batch 606/992 ━━━━━━━━━━━━━━━━━━━━ 09:31:10\n",
      "Accuracy: 0.9264 - Loss: 0.2084\n",
      "\n",
      "Batch 607/992 ━━━━━━━━━━━━━━━━━━━━ 09:31:20\n",
      "Accuracy: 0.9259 - Loss: 0.2089\n",
      "\n",
      "Batch 608/992 ━━━━━━━━━━━━━━━━━━━━ 09:31:30\n",
      "Accuracy: 0.9260 - Loss: 0.2087\n",
      "\n",
      "Batch 609/992 ━━━━━━━━━━━━━━━━━━━━ 09:31:41\n",
      "Accuracy: 0.9261 - Loss: 0.2086\n",
      "\n",
      "Batch 610/992 ━━━━━━━━━━━━━━━━━━━━ 09:31:51\n",
      "Accuracy: 0.9258 - Loss: 0.2091\n",
      "\n",
      "Batch 611/992 ━━━━━━━━━━━━━━━━━━━━ 09:32:03\n",
      "Accuracy: 0.9255 - Loss: 0.2094\n",
      "\n",
      "Batch 612/992 ━━━━━━━━━━━━━━━━━━━━ 09:32:14\n",
      "Accuracy: 0.9252 - Loss: 0.2095\n",
      "\n",
      "Batch 613/992 ━━━━━━━━━━━━━━━━━━━━ 09:32:25\n",
      "Accuracy: 0.9250 - Loss: 0.2102\n",
      "\n",
      "Batch 614/992 ━━━━━━━━━━━━━━━━━━━━ 09:32:36\n",
      "Accuracy: 0.9251 - Loss: 0.2099\n",
      "\n",
      "Batch 615/992 ━━━━━━━━━━━━━━━━━━━━ 09:32:46\n",
      "Accuracy: 0.9250 - Loss: 0.2101\n",
      "\n",
      "Batch 616/992 ━━━━━━━━━━━━━━━━━━━━ 09:32:57\n",
      "Accuracy: 0.9249 - Loss: 0.2105\n",
      "\n",
      "Batch 617/992 ━━━━━━━━━━━━━━━━━━━━ 09:33:07\n",
      "Accuracy: 0.9250 - Loss: 0.2103\n",
      "\n",
      "Batch 618/992 ━━━━━━━━━━━━━━━━━━━━ 09:33:17\n",
      "Accuracy: 0.9252 - Loss: 0.2101\n",
      "\n",
      "Batch 619/992 ━━━━━━━━━━━━━━━━━━━━ 09:33:28\n",
      "Accuracy: 0.9253 - Loss: 0.2100\n",
      "\n",
      "Batch 620/992 ━━━━━━━━━━━━━━━━━━━━ 09:33:39\n",
      "Accuracy: 0.9252 - Loss: 0.2102\n",
      "\n",
      "Batch 621/992 ━━━━━━━━━━━━━━━━━━━━ 09:33:49\n",
      "Accuracy: 0.9251 - Loss: 0.2103\n",
      "\n",
      "Batch 622/992 ━━━━━━━━━━━━━━━━━━━━ 09:33:59\n",
      "Accuracy: 0.9248 - Loss: 0.2117\n",
      "\n",
      "Batch 623/992 ━━━━━━━━━━━━━━━━━━━━ 09:34:09\n",
      "Accuracy: 0.9246 - Loss: 0.2120\n",
      "\n",
      "Batch 624/992 ━━━━━━━━━━━━━━━━━━━━ 09:34:19\n",
      "Accuracy: 0.9247 - Loss: 0.2119\n",
      "\n",
      "Batch 625/992 ━━━━━━━━━━━━━━━━━━━━ 09:34:30\n",
      "Accuracy: 0.9246 - Loss: 0.2118\n",
      "\n",
      "Batch 626/992 ━━━━━━━━━━━━━━━━━━━━ 09:34:40\n",
      "Accuracy: 0.9243 - Loss: 0.2124\n",
      "\n",
      "Batch 627/992 ━━━━━━━━━━━━━━━━━━━━ 09:34:51\n",
      "Accuracy: 0.9238 - Loss: 0.2130\n",
      "\n",
      "Batch 628/992 ━━━━━━━━━━━━━━━━━━━━ 09:35:03\n",
      "Accuracy: 0.9238 - Loss: 0.2134\n",
      "\n",
      "Batch 629/992 ━━━━━━━━━━━━━━━━━━━━ 09:35:13\n",
      "Accuracy: 0.9239 - Loss: 0.2131\n",
      "\n",
      "Batch 630/992 ━━━━━━━━━━━━━━━━━━━━ 09:35:24\n",
      "Accuracy: 0.9234 - Loss: 0.2136\n",
      "\n",
      "Batch 631/992 ━━━━━━━━━━━━━━━━━━━━ 09:35:34\n",
      "Accuracy: 0.9235 - Loss: 0.2133\n",
      "\n",
      "Batch 632/992 ━━━━━━━━━━━━━━━━━━━━ 09:35:44\n",
      "Accuracy: 0.9235 - Loss: 0.2133\n",
      "\n",
      "Batch 633/992 ━━━━━━━━━━━━━━━━━━━━ 09:35:54\n",
      "Accuracy: 0.9236 - Loss: 0.2131\n",
      "\n",
      "Batch 634/992 ━━━━━━━━━━━━━━━━━━━━ 09:36:04\n",
      "Accuracy: 0.9237 - Loss: 0.2128\n",
      "\n",
      "Batch 635/992 ━━━━━━━━━━━━━━━━━━━━ 09:36:14\n",
      "Accuracy: 0.9236 - Loss: 0.2132\n",
      "\n",
      "Batch 636/992 ━━━━━━━━━━━━━━━━━━━━ 09:36:25\n",
      "Accuracy: 0.9237 - Loss: 0.2131\n",
      "\n",
      "Batch 637/992 ━━━━━━━━━━━━━━━━━━━━ 09:36:35\n",
      "Accuracy: 0.9239 - Loss: 0.2128\n",
      "\n",
      "Batch 638/992 ━━━━━━━━━━━━━━━━━━━━ 09:36:45\n",
      "Accuracy: 0.9240 - Loss: 0.2127\n",
      "\n",
      "Batch 639/992 ━━━━━━━━━━━━━━━━━━━━ 09:36:55\n",
      "Accuracy: 0.9239 - Loss: 0.2127\n",
      "\n",
      "Batch 640/992 ━━━━━━━━━━━━━━━━━━━━ 09:37:06\n",
      "Accuracy: 0.9240 - Loss: 0.2127\n",
      "\n",
      "Batch 641/992 ━━━━━━━━━━━━━━━━━━━━ 09:37:16\n",
      "Accuracy: 0.9238 - Loss: 0.2134\n",
      "\n",
      "Batch 642/992 ━━━━━━━━━━━━━━━━━━━━ 09:37:26\n",
      "Accuracy: 0.9239 - Loss: 0.2131\n",
      "\n",
      "Batch 643/992 ━━━━━━━━━━━━━━━━━━━━ 09:37:36\n",
      "Accuracy: 0.9240 - Loss: 0.2130\n",
      "\n",
      "Batch 644/992 ━━━━━━━━━━━━━━━━━━━━ 09:37:46\n",
      "Accuracy: 0.9241 - Loss: 0.2127\n",
      "\n",
      "Batch 645/992 ━━━━━━━━━━━━━━━━━━━━ 09:37:57\n",
      "Accuracy: 0.9238 - Loss: 0.2132\n",
      "\n",
      "Batch 646/992 ━━━━━━━━━━━━━━━━━━━━ 09:38:07\n",
      "Accuracy: 0.9240 - Loss: 0.2130\n",
      "\n",
      "Batch 647/992 ━━━━━━━━━━━━━━━━━━━━ 09:38:17\n",
      "Accuracy: 0.9239 - Loss: 0.2132\n",
      "\n",
      "Batch 648/992 ━━━━━━━━━━━━━━━━━━━━ 09:38:27\n",
      "Accuracy: 0.9240 - Loss: 0.2131\n",
      "\n",
      "Batch 649/992 ━━━━━━━━━━━━━━━━━━━━ 09:38:37\n",
      "Accuracy: 0.9239 - Loss: 0.2131\n",
      "\n",
      "Batch 650/992 ━━━━━━━━━━━━━━━━━━━━ 09:38:48\n",
      "Accuracy: 0.9240 - Loss: 0.2130\n",
      "\n",
      "Batch 651/992 ━━━━━━━━━━━━━━━━━━━━ 09:38:58\n",
      "Accuracy: 0.9242 - Loss: 0.2127\n",
      "\n",
      "Batch 652/992 ━━━━━━━━━━━━━━━━━━━━ 09:39:08\n",
      "Accuracy: 0.9243 - Loss: 0.2126\n",
      "\n",
      "Batch 653/992 ━━━━━━━━━━━━━━━━━━━━ 09:39:18\n",
      "Accuracy: 0.9242 - Loss: 0.2128\n",
      "\n",
      "Batch 654/992 ━━━━━━━━━━━━━━━━━━━━ 09:39:28\n",
      "Accuracy: 0.9239 - Loss: 0.2134\n",
      "\n",
      "Batch 655/992 ━━━━━━━━━━━━━━━━━━━━ 09:39:39\n",
      "Accuracy: 0.9239 - Loss: 0.2137\n",
      "\n",
      "Batch 656/992 ━━━━━━━━━━━━━━━━━━━━ 09:39:49\n",
      "Accuracy: 0.9240 - Loss: 0.2135\n",
      "\n",
      "Batch 657/992 ━━━━━━━━━━━━━━━━━━━━ 09:39:59\n",
      "Accuracy: 0.9241 - Loss: 0.2133\n",
      "\n",
      "Batch 658/992 ━━━━━━━━━━━━━━━━━━━━ 09:40:09\n",
      "Accuracy: 0.9242 - Loss: 0.2131\n",
      "\n",
      "Batch 659/992 ━━━━━━━━━━━━━━━━━━━━ 09:40:19\n",
      "Accuracy: 0.9243 - Loss: 0.2129\n",
      "\n",
      "Batch 660/992 ━━━━━━━━━━━━━━━━━━━━ 09:40:29\n",
      "Accuracy: 0.9244 - Loss: 0.2127\n",
      "\n",
      "Batch 661/992 ━━━━━━━━━━━━━━━━━━━━ 09:40:40\n",
      "Accuracy: 0.9244 - Loss: 0.2128\n",
      "\n",
      "Batch 662/992 ━━━━━━━━━━━━━━━━━━━━ 09:40:50\n",
      "Accuracy: 0.9245 - Loss: 0.2126\n",
      "\n",
      "Batch 663/992 ━━━━━━━━━━━━━━━━━━━━ 09:41:00\n",
      "Accuracy: 0.9246 - Loss: 0.2124\n",
      "\n",
      "Batch 664/992 ━━━━━━━━━━━━━━━━━━━━ 09:41:10\n",
      "Accuracy: 0.9245 - Loss: 0.2123\n",
      "\n",
      "Batch 665/992 ━━━━━━━━━━━━━━━━━━━━ 09:41:20\n",
      "Accuracy: 0.9246 - Loss: 0.2120\n",
      "\n",
      "Batch 666/992 ━━━━━━━━━━━━━━━━━━━━ 09:41:31\n",
      "Accuracy: 0.9247 - Loss: 0.2121\n",
      "\n",
      "Batch 667/992 ━━━━━━━━━━━━━━━━━━━━ 09:41:41\n",
      "Accuracy: 0.9249 - Loss: 0.2118\n",
      "\n",
      "Batch 668/992 ━━━━━━━━━━━━━━━━━━━━ 09:41:51\n",
      "Accuracy: 0.9250 - Loss: 0.2117\n",
      "\n",
      "Batch 669/992 ━━━━━━━━━━━━━━━━━━━━ 09:42:01\n",
      "Accuracy: 0.9249 - Loss: 0.2117\n",
      "\n",
      "Batch 670/992 ━━━━━━━━━━━━━━━━━━━━ 09:42:11\n",
      "Accuracy: 0.9250 - Loss: 0.2116\n",
      "\n",
      "Batch 671/992 ━━━━━━━━━━━━━━━━━━━━ 09:42:21\n",
      "Accuracy: 0.9251 - Loss: 0.2113\n",
      "\n",
      "Batch 672/992 ━━━━━━━━━━━━━━━━━━━━ 09:42:32\n",
      "Accuracy: 0.9252 - Loss: 0.2111\n",
      "\n",
      "Batch 673/992 ━━━━━━━━━━━━━━━━━━━━ 09:42:42\n",
      "Accuracy: 0.9253 - Loss: 0.2109\n",
      "\n",
      "Batch 674/992 ━━━━━━━━━━━━━━━━━━━━ 09:42:52\n",
      "Accuracy: 0.9253 - Loss: 0.2111\n",
      "\n",
      "Batch 675/992 ━━━━━━━━━━━━━━━━━━━━ 09:43:02\n",
      "Accuracy: 0.9254 - Loss: 0.2110\n",
      "\n",
      "Batch 676/992 ━━━━━━━━━━━━━━━━━━━━ 09:43:12\n",
      "Accuracy: 0.9251 - Loss: 0.2115\n",
      "\n",
      "Batch 677/992 ━━━━━━━━━━━━━━━━━━━━ 09:43:22\n",
      "Accuracy: 0.9250 - Loss: 0.2117\n",
      "\n",
      "Batch 678/992 ━━━━━━━━━━━━━━━━━━━━ 09:43:33\n",
      "Accuracy: 0.9251 - Loss: 0.2116\n",
      "\n",
      "Batch 679/992 ━━━━━━━━━━━━━━━━━━━━ 09:43:43\n",
      "Accuracy: 0.9251 - Loss: 0.2117\n",
      "\n",
      "Batch 680/992 ━━━━━━━━━━━━━━━━━━━━ 09:43:53\n",
      "Accuracy: 0.9252 - Loss: 0.2115\n",
      "\n",
      "Batch 681/992 ━━━━━━━━━━━━━━━━━━━━ 09:44:03\n",
      "Accuracy: 0.9253 - Loss: 0.2114\n",
      "\n",
      "Batch 682/992 ━━━━━━━━━━━━━━━━━━━━ 09:44:13\n",
      "Accuracy: 0.9250 - Loss: 0.2119\n",
      "\n",
      "Batch 683/992 ━━━━━━━━━━━━━━━━━━━━ 09:44:23\n",
      "Accuracy: 0.9248 - Loss: 0.2126\n",
      "\n",
      "Batch 684/992 ━━━━━━━━━━━━━━━━━━━━ 09:44:33\n",
      "Accuracy: 0.9247 - Loss: 0.2127\n",
      "\n",
      "Batch 685/992 ━━━━━━━━━━━━━━━━━━━━ 09:44:44\n",
      "Accuracy: 0.9245 - Loss: 0.2136\n",
      "\n",
      "Batch 686/992 ━━━━━━━━━━━━━━━━━━━━ 09:44:54\n",
      "Accuracy: 0.9246 - Loss: 0.2133\n",
      "\n",
      "Batch 687/992 ━━━━━━━━━━━━━━━━━━━━ 09:45:04\n",
      "Accuracy: 0.9245 - Loss: 0.2140\n",
      "\n",
      "Batch 688/992 ━━━━━━━━━━━━━━━━━━━━ 09:45:14\n",
      "Accuracy: 0.9244 - Loss: 0.2147\n",
      "\n",
      "Batch 689/992 ━━━━━━━━━━━━━━━━━━━━ 09:45:24\n",
      "Accuracy: 0.9242 - Loss: 0.2148\n",
      "\n",
      "Batch 690/992 ━━━━━━━━━━━━━━━━━━━━ 09:45:34\n",
      "Accuracy: 0.9241 - Loss: 0.2147\n",
      "\n",
      "Batch 691/992 ━━━━━━━━━━━━━━━━━━━━ 09:45:44\n",
      "Accuracy: 0.9240 - Loss: 0.2146\n",
      "\n",
      "Batch 692/992 ━━━━━━━━━━━━━━━━━━━━ 09:45:54\n",
      "Accuracy: 0.9238 - Loss: 0.2151\n",
      "\n",
      "Batch 693/992 ━━━━━━━━━━━━━━━━━━━━ 09:46:05\n",
      "Accuracy: 0.9237 - Loss: 0.2153\n",
      "\n",
      "Batch 694/992 ━━━━━━━━━━━━━━━━━━━━ 09:46:15\n",
      "Accuracy: 0.9236 - Loss: 0.2154\n",
      "\n",
      "Batch 695/992 ━━━━━━━━━━━━━━━━━━━━ 09:46:25\n",
      "Accuracy: 0.9236 - Loss: 0.2156\n",
      "\n",
      "Batch 696/992 ━━━━━━━━━━━━━━━━━━━━ 09:46:35\n",
      "Accuracy: 0.9237 - Loss: 0.2155\n",
      "\n",
      "Batch 697/992 ━━━━━━━━━━━━━━━━━━━━ 09:46:45\n",
      "Accuracy: 0.9236 - Loss: 0.2158\n",
      "\n",
      "Batch 698/992 ━━━━━━━━━━━━━━━━━━━━ 09:46:56\n",
      "Accuracy: 0.9237 - Loss: 0.2156\n",
      "\n",
      "Batch 699/992 ━━━━━━━━━━━━━━━━━━━━ 09:47:06\n",
      "Accuracy: 0.9235 - Loss: 0.2168\n",
      "\n",
      "Batch 700/992 ━━━━━━━━━━━━━━━━━━━━ 09:47:16\n",
      "Accuracy: 0.9234 - Loss: 0.2168\n",
      "\n",
      "Batch 701/992 ━━━━━━━━━━━━━━━━━━━━ 09:47:26\n",
      "Accuracy: 0.9233 - Loss: 0.2170\n",
      "\n",
      "Batch 702/992 ━━━━━━━━━━━━━━━━━━━━ 09:47:36\n",
      "Accuracy: 0.9234 - Loss: 0.2169\n",
      "\n",
      "Batch 703/992 ━━━━━━━━━━━━━━━━━━━━ 09:47:46\n",
      "Accuracy: 0.9235 - Loss: 0.2168\n",
      "\n",
      "Batch 704/992 ━━━━━━━━━━━━━━━━━━━━ 09:47:57\n",
      "Accuracy: 0.9233 - Loss: 0.2171\n",
      "\n",
      "Batch 705/992 ━━━━━━━━━━━━━━━━━━━━ 09:48:07\n",
      "Accuracy: 0.9232 - Loss: 0.2174\n",
      "\n",
      "Batch 706/992 ━━━━━━━━━━━━━━━━━━━━ 09:48:17\n",
      "Accuracy: 0.9233 - Loss: 0.2173\n",
      "\n",
      "Batch 707/992 ━━━━━━━━━━━━━━━━━━━━ 09:48:27\n",
      "Accuracy: 0.9234 - Loss: 0.2172\n",
      "\n",
      "Batch 708/992 ━━━━━━━━━━━━━━━━━━━━ 09:48:37\n",
      "Accuracy: 0.9236 - Loss: 0.2171\n",
      "\n",
      "Batch 709/992 ━━━━━━━━━━━━━━━━━━━━ 09:48:47\n",
      "Accuracy: 0.9235 - Loss: 0.2171\n",
      "\n",
      "Batch 710/992 ━━━━━━━━━━━━━━━━━━━━ 09:48:57\n",
      "Accuracy: 0.9236 - Loss: 0.2172\n",
      "\n",
      "Batch 711/992 ━━━━━━━━━━━━━━━━━━━━ 09:49:08\n",
      "Accuracy: 0.9237 - Loss: 0.2170\n",
      "\n",
      "Batch 712/992 ━━━━━━━━━━━━━━━━━━━━ 09:49:18\n",
      "Accuracy: 0.9236 - Loss: 0.2172\n",
      "\n",
      "Batch 713/992 ━━━━━━━━━━━━━━━━━━━━ 09:49:28\n",
      "Accuracy: 0.9237 - Loss: 0.2169\n",
      "\n",
      "Batch 714/992 ━━━━━━━━━━━━━━━━━━━━ 09:49:39\n",
      "Accuracy: 0.9235 - Loss: 0.2175\n",
      "\n",
      "Batch 715/992 ━━━━━━━━━━━━━━━━━━━━ 09:49:54\n",
      "Accuracy: 0.9236 - Loss: 0.2174\n",
      "\n",
      "Batch 716/992 ━━━━━━━━━━━━━━━━━━━━ 09:50:06\n",
      "Accuracy: 0.9237 - Loss: 0.2171\n",
      "\n",
      "Batch 717/992 ━━━━━━━━━━━━━━━━━━━━ 09:50:19\n",
      "Accuracy: 0.9236 - Loss: 0.2175\n",
      "\n",
      "Batch 718/992 ━━━━━━━━━━━━━━━━━━━━ 09:50:30\n",
      "Accuracy: 0.9237 - Loss: 0.2174\n",
      "\n",
      "Batch 719/992 ━━━━━━━━━━━━━━━━━━━━ 09:50:42\n",
      "Accuracy: 0.9237 - Loss: 0.2172\n",
      "\n",
      "Batch 720/992 ━━━━━━━━━━━━━━━━━━━━ 09:50:54\n",
      "Accuracy: 0.9236 - Loss: 0.2174\n",
      "\n",
      "Batch 721/992 ━━━━━━━━━━━━━━━━━━━━ 09:51:05\n",
      "Accuracy: 0.9237 - Loss: 0.2171\n",
      "\n",
      "Batch 722/992 ━━━━━━━━━━━━━━━━━━━━ 09:51:17\n",
      "Accuracy: 0.9238 - Loss: 0.2170\n",
      "\n",
      "Batch 723/992 ━━━━━━━━━━━━━━━━━━━━ 09:51:28\n",
      "Accuracy: 0.9239 - Loss: 0.2169\n",
      "\n",
      "Batch 724/992 ━━━━━━━━━━━━━━━━━━━━ 09:51:39\n",
      "Accuracy: 0.9240 - Loss: 0.2166\n",
      "\n",
      "Batch 725/992 ━━━━━━━━━━━━━━━━━━━━ 09:51:50\n",
      "Accuracy: 0.9241 - Loss: 0.2165\n",
      "\n",
      "Batch 726/992 ━━━━━━━━━━━━━━━━━━━━ 09:52:01\n",
      "Accuracy: 0.9242 - Loss: 0.2164\n",
      "\n",
      "Batch 727/992 ━━━━━━━━━━━━━━━━━━━━ 09:52:12\n",
      "Accuracy: 0.9243 - Loss: 0.2163\n",
      "\n",
      "Batch 728/992 ━━━━━━━━━━━━━━━━━━━━ 09:52:23\n",
      "Accuracy: 0.9243 - Loss: 0.2164\n",
      "\n",
      "Batch 729/992 ━━━━━━━━━━━━━━━━━━━━ 09:52:35\n",
      "Accuracy: 0.9244 - Loss: 0.2162\n",
      "\n",
      "Batch 730/992 ━━━━━━━━━━━━━━━━━━━━ 09:52:45\n",
      "Accuracy: 0.9245 - Loss: 0.2161\n",
      "\n",
      "Batch 731/992 ━━━━━━━━━━━━━━━━━━━━ 09:52:56\n",
      "Accuracy: 0.9246 - Loss: 0.2159\n",
      "\n",
      "Batch 732/992 ━━━━━━━━━━━━━━━━━━━━ 09:53:07\n",
      "Accuracy: 0.9245 - Loss: 0.2160\n",
      "\n",
      "Batch 733/992 ━━━━━━━━━━━━━━━━━━━━ 09:53:17\n",
      "Accuracy: 0.9246 - Loss: 0.2158\n",
      "\n",
      "Batch 734/992 ━━━━━━━━━━━━━━━━━━━━ 09:53:29\n",
      "Accuracy: 0.9247 - Loss: 0.2155\n",
      "\n",
      "Batch 735/992 ━━━━━━━━━━━━━━━━━━━━ 09:53:39\n",
      "Accuracy: 0.9248 - Loss: 0.2154\n",
      "\n",
      "Batch 736/992 ━━━━━━━━━━━━━━━━━━━━ 09:53:49\n",
      "Accuracy: 0.9246 - Loss: 0.2156\n",
      "\n",
      "Batch 737/992 ━━━━━━━━━━━━━━━━━━━━ 09:53:59\n",
      "Accuracy: 0.9245 - Loss: 0.2156\n",
      "\n",
      "Batch 738/992 ━━━━━━━━━━━━━━━━━━━━ 09:54:10\n",
      "Accuracy: 0.9245 - Loss: 0.2155\n",
      "\n",
      "Batch 739/992 ━━━━━━━━━━━━━━━━━━━━ 09:54:22\n",
      "Accuracy: 0.9246 - Loss: 0.2154\n",
      "\n",
      "Batch 740/992 ━━━━━━━━━━━━━━━━━━━━ 09:54:34\n",
      "Accuracy: 0.9243 - Loss: 0.2156\n",
      "\n",
      "Batch 741/992 ━━━━━━━━━━━━━━━━━━━━ 09:54:47\n",
      "Accuracy: 0.9244 - Loss: 0.2154\n",
      "\n",
      "Batch 742/992 ━━━━━━━━━━━━━━━━━━━━ 09:54:58\n",
      "Accuracy: 0.9245 - Loss: 0.2152\n",
      "\n",
      "Batch 743/992 ━━━━━━━━━━━━━━━━━━━━ 09:55:11\n",
      "Accuracy: 0.9245 - Loss: 0.2152\n",
      "\n",
      "Batch 744/992 ━━━━━━━━━━━━━━━━━━━━ 09:55:26\n",
      "Accuracy: 0.9246 - Loss: 0.2149\n",
      "\n",
      "Batch 745/992 ━━━━━━━━━━━━━━━━━━━━ 09:55:41\n",
      "Accuracy: 0.9245 - Loss: 0.2150\n",
      "\n",
      "Batch 746/992 ━━━━━━━━━━━━━━━━━━━━ 09:55:55\n",
      "Accuracy: 0.9246 - Loss: 0.2149\n",
      "\n",
      "Batch 747/992 ━━━━━━━━━━━━━━━━━━━━ 09:56:08\n",
      "Accuracy: 0.9245 - Loss: 0.2149\n",
      "\n",
      "Batch 748/992 ━━━━━━━━━━━━━━━━━━━━ 09:56:24\n",
      "Accuracy: 0.9246 - Loss: 0.2146\n",
      "\n",
      "Batch 749/992 ━━━━━━━━━━━━━━━━━━━━ 09:56:36\n",
      "Accuracy: 0.9246 - Loss: 0.2149\n",
      "\n",
      "Batch 750/992 ━━━━━━━━━━━━━━━━━━━━ 09:56:49\n",
      "Accuracy: 0.9247 - Loss: 0.2147\n",
      "\n",
      "Batch 751/992 ━━━━━━━━━━━━━━━━━━━━ 09:57:04\n",
      "Accuracy: 0.9248 - Loss: 0.2147\n",
      "\n",
      "Batch 752/992 ━━━━━━━━━━━━━━━━━━━━ 09:57:18\n",
      "Accuracy: 0.9249 - Loss: 0.2146\n",
      "\n",
      "Batch 753/992 ━━━━━━━━━━━━━━━━━━━━ 09:57:29\n",
      "Accuracy: 0.9250 - Loss: 0.2143\n",
      "\n",
      "Batch 754/992 ━━━━━━━━━━━━━━━━━━━━ 09:57:42\n",
      "Accuracy: 0.9249 - Loss: 0.2144\n",
      "\n",
      "Batch 755/992 ━━━━━━━━━━━━━━━━━━━━ 09:57:53\n",
      "Accuracy: 0.9248 - Loss: 0.2145\n",
      "\n",
      "Batch 756/992 ━━━━━━━━━━━━━━━━━━━━ 09:58:04\n",
      "Accuracy: 0.9249 - Loss: 0.2143\n",
      "\n",
      "Batch 757/992 ━━━━━━━━━━━━━━━━━━━━ 09:58:15\n",
      "Accuracy: 0.9249 - Loss: 0.2142\n",
      "\n",
      "Batch 758/992 ━━━━━━━━━━━━━━━━━━━━ 09:58:30\n",
      "Accuracy: 0.9246 - Loss: 0.2148\n",
      "\n",
      "Batch 759/992 ━━━━━━━━━━━━━━━━━━━━ 09:58:44\n",
      "Accuracy: 0.9247 - Loss: 0.2146\n",
      "\n",
      "Batch 760/992 ━━━━━━━━━━━━━━━━━━━━ 09:58:58\n",
      "Accuracy: 0.9248 - Loss: 0.2143\n",
      "\n",
      "Batch 761/992 ━━━━━━━━━━━━━━━━━━━━ 09:59:11\n",
      "Accuracy: 0.9249 - Loss: 0.2141\n",
      "\n",
      "Batch 762/992 ━━━━━━━━━━━━━━━━━━━━ 09:59:25\n",
      "Accuracy: 0.9250 - Loss: 0.2139\n",
      "\n",
      "Batch 763/992 ━━━━━━━━━━━━━━━━━━━━ 09:59:39\n",
      "Accuracy: 0.9251 - Loss: 0.2139\n",
      "\n",
      "Batch 764/992 ━━━━━━━━━━━━━━━━━━━━ 09:59:53\n",
      "Accuracy: 0.9251 - Loss: 0.2141\n",
      "\n",
      "Batch 765/992 ━━━━━━━━━━━━━━━━━━━━ 10:00:07\n",
      "Accuracy: 0.9252 - Loss: 0.2140\n",
      "\n",
      "Batch 766/992 ━━━━━━━━━━━━━━━━━━━━ 10:00:21\n",
      "Accuracy: 0.9253 - Loss: 0.2137\n",
      "\n",
      "Batch 767/992 ━━━━━━━━━━━━━━━━━━━━ 10:00:34\n",
      "Accuracy: 0.9254 - Loss: 0.2135\n",
      "\n",
      "Batch 768/992 ━━━━━━━━━━━━━━━━━━━━ 10:00:46\n",
      "Accuracy: 0.9255 - Loss: 0.2134\n",
      "\n",
      "Batch 769/992 ━━━━━━━━━━━━━━━━━━━━ 10:00:56\n",
      "Accuracy: 0.9256 - Loss: 0.2133\n",
      "\n",
      "Batch 770/992 ━━━━━━━━━━━━━━━━━━━━ 10:01:07\n",
      "Accuracy: 0.9255 - Loss: 0.2139\n",
      "\n",
      "Batch 771/992 ━━━━━━━━━━━━━━━━━━━━ 10:01:17\n",
      "Accuracy: 0.9256 - Loss: 0.2137\n",
      "\n",
      "Batch 772/992 ━━━━━━━━━━━━━━━━━━━━ 10:01:27\n",
      "Accuracy: 0.9257 - Loss: 0.2135\n",
      "\n",
      "Batch 773/992 ━━━━━━━━━━━━━━━━━━━━ 10:01:37\n",
      "Accuracy: 0.9255 - Loss: 0.2138\n",
      "\n",
      "Batch 774/992 ━━━━━━━━━━━━━━━━━━━━ 10:01:47\n",
      "Accuracy: 0.9255 - Loss: 0.2137\n",
      "\n",
      "Batch 775/992 ━━━━━━━━━━━━━━━━━━━━ 10:01:57\n",
      "Accuracy: 0.9256 - Loss: 0.2135\n",
      "\n",
      "Batch 776/992 ━━━━━━━━━━━━━━━━━━━━ 10:02:07\n",
      "Accuracy: 0.9257 - Loss: 0.2132\n",
      "\n",
      "Batch 777/992 ━━━━━━━━━━━━━━━━━━━━ 10:02:17\n",
      "Accuracy: 0.9258 - Loss: 0.2129\n",
      "\n",
      "Batch 778/992 ━━━━━━━━━━━━━━━━━━━━ 10:02:28\n",
      "Accuracy: 0.9259 - Loss: 0.2128\n",
      "\n",
      "Batch 779/992 ━━━━━━━━━━━━━━━━━━━━ 10:02:38\n",
      "Accuracy: 0.9260 - Loss: 0.2127\n",
      "\n",
      "Batch 780/992 ━━━━━━━━━━━━━━━━━━━━ 10:02:48\n",
      "Accuracy: 0.9260 - Loss: 0.2128\n",
      "\n",
      "Batch 781/992 ━━━━━━━━━━━━━━━━━━━━ 10:02:58\n",
      "Accuracy: 0.9254 - Loss: 0.2140\n",
      "\n",
      "Batch 782/992 ━━━━━━━━━━━━━━━━━━━━ 10:03:08\n",
      "Accuracy: 0.9255 - Loss: 0.2139\n",
      "\n",
      "Batch 783/992 ━━━━━━━━━━━━━━━━━━━━ 10:03:21\n",
      "Accuracy: 0.9256 - Loss: 0.2137\n",
      "\n",
      "Batch 784/992 ━━━━━━━━━━━━━━━━━━━━ 10:03:36\n",
      "Accuracy: 0.9257 - Loss: 0.2135\n",
      "\n",
      "Batch 785/992 ━━━━━━━━━━━━━━━━━━━━ 10:03:50\n",
      "Accuracy: 0.9258 - Loss: 0.2133\n",
      "\n",
      "Batch 786/992 ━━━━━━━━━━━━━━━━━━━━ 10:04:00\n",
      "Accuracy: 0.9257 - Loss: 0.2133\n",
      "\n",
      "Batch 787/992 ━━━━━━━━━━━━━━━━━━━━ 10:04:11\n",
      "Accuracy: 0.9255 - Loss: 0.2144\n",
      "\n",
      "Batch 788/992 ━━━━━━━━━━━━━━━━━━━━ 10:04:23\n",
      "Accuracy: 0.9254 - Loss: 0.2144\n",
      "\n",
      "Batch 789/992 ━━━━━━━━━━━━━━━━━━━━ 10:04:36\n",
      "Accuracy: 0.9255 - Loss: 0.2141\n",
      "\n",
      "Batch 790/992 ━━━━━━━━━━━━━━━━━━━━ 10:04:48\n",
      "Accuracy: 0.9255 - Loss: 0.2141\n",
      "\n",
      "Batch 791/992 ━━━━━━━━━━━━━━━━━━━━ 10:05:01\n",
      "Accuracy: 0.9254 - Loss: 0.2142\n",
      "\n",
      "Batch 792/992 ━━━━━━━━━━━━━━━━━━━━ 10:05:14\n",
      "Accuracy: 0.9252 - Loss: 0.2145\n",
      "\n",
      "Batch 793/992 ━━━━━━━━━━━━━━━━━━━━ 10:05:28\n",
      "Accuracy: 0.9253 - Loss: 0.2142\n",
      "\n",
      "Batch 794/992 ━━━━━━━━━━━━━━━━━━━━ 10:05:42\n",
      "Accuracy: 0.9252 - Loss: 0.2143\n",
      "\n",
      "Batch 795/992 ━━━━━━━━━━━━━━━━━━━━ 10:05:57\n",
      "Accuracy: 0.9253 - Loss: 0.2142\n",
      "\n",
      "Batch 796/992 ━━━━━━━━━━━━━━━━━━━━ 10:06:08\n",
      "Accuracy: 0.9254 - Loss: 0.2141\n",
      "\n",
      "Batch 797/992 ━━━━━━━━━━━━━━━━━━━━ 10:06:23\n",
      "Accuracy: 0.9255 - Loss: 0.2140\n",
      "\n",
      "Batch 798/992 ━━━━━━━━━━━━━━━━━━━━ 10:06:38\n",
      "Accuracy: 0.9256 - Loss: 0.2139\n",
      "\n",
      "Batch 799/992 ━━━━━━━━━━━━━━━━━━━━ 10:06:53\n",
      "Accuracy: 0.9257 - Loss: 0.2137\n",
      "\n",
      "Batch 800/992 ━━━━━━━━━━━━━━━━━━━━ 10:07:08\n",
      "Accuracy: 0.9258 - Loss: 0.2137\n",
      "\n",
      "Batch 801/992 ━━━━━━━━━━━━━━━━━━━━ 10:07:22\n",
      "Accuracy: 0.9257 - Loss: 0.2139\n",
      "\n",
      "Batch 802/992 ━━━━━━━━━━━━━━━━━━━━ 10:07:35\n",
      "Accuracy: 0.9257 - Loss: 0.2139\n",
      "\n",
      "Batch 803/992 ━━━━━━━━━━━━━━━━━━━━ 10:07:46\n",
      "Accuracy: 0.9254 - Loss: 0.2141\n",
      "\n",
      "Batch 804/992 ━━━━━━━━━━━━━━━━━━━━ 10:08:01\n",
      "Accuracy: 0.9255 - Loss: 0.2139\n",
      "\n",
      "Batch 805/992 ━━━━━━━━━━━━━━━━━━━━ 10:08:16\n",
      "Accuracy: 0.9256 - Loss: 0.2137\n",
      "\n",
      "Batch 806/992 ━━━━━━━━━━━━━━━━━━━━ 10:08:31\n",
      "Accuracy: 0.9256 - Loss: 0.2140\n",
      "\n",
      "Batch 807/992 ━━━━━━━━━━━━━━━━━━━━ 10:08:45\n",
      "Accuracy: 0.9253 - Loss: 0.2141\n",
      "\n",
      "Batch 808/992 ━━━━━━━━━━━━━━━━━━━━ 10:09:00\n",
      "Accuracy: 0.9253 - Loss: 0.2148\n",
      "\n",
      "Batch 809/992 ━━━━━━━━━━━━━━━━━━━━ 10:09:15\n",
      "Accuracy: 0.9252 - Loss: 0.2148\n",
      "\n",
      "Batch 810/992 ━━━━━━━━━━━━━━━━━━━━ 10:09:29\n",
      "Accuracy: 0.9253 - Loss: 0.2146\n",
      "\n",
      "Batch 811/992 ━━━━━━━━━━━━━━━━━━━━ 10:09:42\n",
      "Accuracy: 0.9254 - Loss: 0.2144\n",
      "\n",
      "Batch 812/992 ━━━━━━━━━━━━━━━━━━━━ 10:09:53\n",
      "Accuracy: 0.9255 - Loss: 0.2142\n",
      "\n",
      "Batch 813/992 ━━━━━━━━━━━━━━━━━━━━ 10:10:05\n",
      "Accuracy: 0.9254 - Loss: 0.2143\n",
      "\n",
      "Batch 814/992 ━━━━━━━━━━━━━━━━━━━━ 10:10:16\n",
      "Accuracy: 0.9255 - Loss: 0.2142\n",
      "\n",
      "Batch 815/992 ━━━━━━━━━━━━━━━━━━━━ 10:10:28\n",
      "Accuracy: 0.9256 - Loss: 0.2141\n",
      "\n",
      "Batch 816/992 ━━━━━━━━━━━━━━━━━━━━ 10:10:39\n",
      "Accuracy: 0.9257 - Loss: 0.2139\n",
      "\n",
      "Batch 817/992 ━━━━━━━━━━━━━━━━━━━━ 10:10:50\n",
      "Accuracy: 0.9256 - Loss: 0.2140\n",
      "\n",
      "Batch 818/992 ━━━━━━━━━━━━━━━━━━━━ 10:11:01\n",
      "Accuracy: 0.9256 - Loss: 0.2142\n",
      "\n",
      "Batch 819/992 ━━━━━━━━━━━━━━━━━━━━ 10:11:12\n",
      "Accuracy: 0.9257 - Loss: 0.2140\n",
      "\n",
      "Batch 820/992 ━━━━━━━━━━━━━━━━━━━━ 10:11:27\n",
      "Accuracy: 0.9258 - Loss: 0.2140\n",
      "\n",
      "Batch 821/992 ━━━━━━━━━━━━━━━━━━━━ 10:11:40\n",
      "Accuracy: 0.9257 - Loss: 0.2142\n",
      "\n",
      "Batch 822/992 ━━━━━━━━━━━━━━━━━━━━ 10:11:50\n",
      "Accuracy: 0.9258 - Loss: 0.2140\n",
      "\n",
      "Batch 823/992 ━━━━━━━━━━━━━━━━━━━━ 10:12:02\n",
      "Accuracy: 0.9257 - Loss: 0.2141\n",
      "\n",
      "Batch 824/992 ━━━━━━━━━━━━━━━━━━━━ 10:12:18\n",
      "Accuracy: 0.9258 - Loss: 0.2140\n",
      "\n",
      "Batch 825/992 ━━━━━━━━━━━━━━━━━━━━ 10:12:31\n",
      "Accuracy: 0.9258 - Loss: 0.2142\n",
      "\n",
      "Batch 826/992 ━━━━━━━━━━━━━━━━━━━━ 10:12:41\n",
      "Accuracy: 0.9258 - Loss: 0.2141\n",
      "\n",
      "Batch 827/992 ━━━━━━━━━━━━━━━━━━━━ 10:12:52\n",
      "Accuracy: 0.9256 - Loss: 0.2144\n",
      "\n",
      "Batch 828/992 ━━━━━━━━━━━━━━━━━━━━ 10:13:06\n",
      "Accuracy: 0.9257 - Loss: 0.2142\n",
      "\n",
      "Batch 829/992 ━━━━━━━━━━━━━━━━━━━━ 10:13:18\n",
      "Accuracy: 0.9258 - Loss: 0.2141\n",
      "\n",
      "Batch 830/992 ━━━━━━━━━━━━━━━━━━━━ 10:13:29\n",
      "Accuracy: 0.9258 - Loss: 0.2146\n",
      "\n",
      "Batch 831/992 ━━━━━━━━━━━━━━━━━━━━ 10:13:40\n",
      "Accuracy: 0.9255 - Loss: 0.2149\n",
      "\n",
      "Batch 832/992 ━━━━━━━━━━━━━━━━━━━━ 10:13:53\n",
      "Accuracy: 0.9253 - Loss: 0.2156\n",
      "\n",
      "Batch 833/992 ━━━━━━━━━━━━━━━━━━━━ 10:14:04\n",
      "Accuracy: 0.9254 - Loss: 0.2154\n",
      "\n",
      "Batch 834/992 ━━━━━━━━━━━━━━━━━━━━ 10:14:17\n",
      "Accuracy: 0.9255 - Loss: 0.2152\n",
      "\n",
      "Batch 835/992 ━━━━━━━━━━━━━━━━━━━━ 10:14:31\n",
      "Accuracy: 0.9256 - Loss: 0.2151\n",
      "\n",
      "Batch 836/992 ━━━━━━━━━━━━━━━━━━━━ 10:14:44\n",
      "Accuracy: 0.9257 - Loss: 0.2149\n",
      "\n",
      "Batch 837/992 ━━━━━━━━━━━━━━━━━━━━ 10:14:57\n",
      "Accuracy: 0.9258 - Loss: 0.2147\n",
      "\n",
      "Batch 838/992 ━━━━━━━━━━━━━━━━━━━━ 10:15:09\n",
      "Accuracy: 0.9257 - Loss: 0.2147\n",
      "\n",
      "Batch 839/992 ━━━━━━━━━━━━━━━━━━━━ 10:15:22\n",
      "Accuracy: 0.9257 - Loss: 0.2146\n",
      "\n",
      "Batch 840/992 ━━━━━━━━━━━━━━━━━━━━ 10:15:32\n",
      "Accuracy: 0.9257 - Loss: 0.2145\n",
      "\n",
      "Batch 841/992 ━━━━━━━━━━━━━━━━━━━━ 10:15:43\n",
      "Accuracy: 0.9257 - Loss: 0.2146\n",
      "\n",
      "Batch 842/992 ━━━━━━━━━━━━━━━━━━━━ 10:16:00\n",
      "Accuracy: 0.9255 - Loss: 0.2150\n",
      "\n",
      "Batch 843/992 ━━━━━━━━━━━━━━━━━━━━ 10:16:12\n",
      "Accuracy: 0.9254 - Loss: 0.2150\n",
      "\n",
      "Batch 844/992 ━━━━━━━━━━━━━━━━━━━━ 10:16:22\n",
      "Accuracy: 0.9252 - Loss: 0.2154\n",
      "\n",
      "Batch 845/992 ━━━━━━━━━━━━━━━━━━━━ 10:16:34\n",
      "Accuracy: 0.9253 - Loss: 0.2152\n",
      "\n",
      "Batch 846/992 ━━━━━━━━━━━━━━━━━━━━ 10:16:46\n",
      "Accuracy: 0.9254 - Loss: 0.2150\n",
      "\n",
      "Batch 847/992 ━━━━━━━━━━━━━━━━━━━━ 10:16:57\n",
      "Accuracy: 0.9255 - Loss: 0.2149\n",
      "\n",
      "Batch 848/992 ━━━━━━━━━━━━━━━━━━━━ 10:17:09\n",
      "Accuracy: 0.9256 - Loss: 0.2147\n",
      "\n",
      "Batch 849/992 ━━━━━━━━━━━━━━━━━━━━ 10:17:19\n",
      "Accuracy: 0.9256 - Loss: 0.2146\n",
      "\n",
      "Batch 850/992 ━━━━━━━━━━━━━━━━━━━━ 10:17:30\n",
      "Accuracy: 0.9257 - Loss: 0.2144\n",
      "\n",
      "Batch 851/992 ━━━━━━━━━━━━━━━━━━━━ 10:17:41\n",
      "Accuracy: 0.9258 - Loss: 0.2143\n",
      "\n",
      "Batch 852/992 ━━━━━━━━━━━━━━━━━━━━ 10:17:51\n",
      "Accuracy: 0.9259 - Loss: 0.2140\n",
      "\n",
      "Batch 853/992 ━━━━━━━━━━━━━━━━━━━━ 10:18:02\n",
      "Accuracy: 0.9257 - Loss: 0.2145\n",
      "\n",
      "Batch 854/992 ━━━━━━━━━━━━━━━━━━━━ 10:18:12\n",
      "Accuracy: 0.9256 - Loss: 0.2148\n",
      "\n",
      "Batch 855/992 ━━━━━━━━━━━━━━━━━━━━ 10:18:24\n",
      "Accuracy: 0.9257 - Loss: 0.2147\n",
      "\n",
      "Batch 856/992 ━━━━━━━━━━━━━━━━━━━━ 10:18:35\n",
      "Accuracy: 0.9258 - Loss: 0.2146\n",
      "\n",
      "Batch 857/992 ━━━━━━━━━━━━━━━━━━━━ 10:18:46\n",
      "Accuracy: 0.9259 - Loss: 0.2145\n",
      "\n",
      "Batch 858/992 ━━━━━━━━━━━━━━━━━━━━ 10:18:57\n",
      "Accuracy: 0.9257 - Loss: 0.2147\n",
      "\n",
      "Batch 859/992 ━━━━━━━━━━━━━━━━━━━━ 10:19:07\n",
      "Accuracy: 0.9258 - Loss: 0.2145\n",
      "\n",
      "Batch 860/992 ━━━━━━━━━━━━━━━━━━━━ 10:19:17\n",
      "Accuracy: 0.9256 - Loss: 0.2148\n",
      "\n",
      "Batch 861/992 ━━━━━━━━━━━━━━━━━━━━ 10:19:27\n",
      "Accuracy: 0.9254 - Loss: 0.2152\n",
      "\n",
      "Batch 862/992 ━━━━━━━━━━━━━━━━━━━━ 10:19:38\n",
      "Accuracy: 0.9252 - Loss: 0.2154\n",
      "\n",
      "Batch 863/992 ━━━━━━━━━━━━━━━━━━━━ 10:19:48\n",
      "Accuracy: 0.9253 - Loss: 0.2152\n",
      "\n",
      "Batch 864/992 ━━━━━━━━━━━━━━━━━━━━ 10:19:58\n",
      "Accuracy: 0.9253 - Loss: 0.2151\n",
      "\n",
      "Batch 865/992 ━━━━━━━━━━━━━━━━━━━━ 10:20:09\n",
      "Accuracy: 0.9254 - Loss: 0.2149\n",
      "\n",
      "Batch 866/992 ━━━━━━━━━━━━━━━━━━━━ 10:20:19\n",
      "Accuracy: 0.9252 - Loss: 0.2151\n",
      "\n",
      "Batch 867/992 ━━━━━━━━━━━━━━━━━━━━ 10:20:30\n",
      "Accuracy: 0.9253 - Loss: 0.2149\n",
      "\n",
      "Batch 868/992 ━━━━━━━━━━━━━━━━━━━━ 10:20:40\n",
      "Accuracy: 0.9253 - Loss: 0.2149\n",
      "\n",
      "Batch 869/992 ━━━━━━━━━━━━━━━━━━━━ 10:20:51\n",
      "Accuracy: 0.9251 - Loss: 0.2151\n",
      "\n",
      "Batch 870/992 ━━━━━━━━━━━━━━━━━━━━ 10:21:05\n",
      "Accuracy: 0.9251 - Loss: 0.2149\n",
      "\n",
      "Batch 871/992 ━━━━━━━━━━━━━━━━━━━━ 10:21:20\n",
      "Accuracy: 0.9252 - Loss: 0.2147\n",
      "\n",
      "Batch 872/992 ━━━━━━━━━━━━━━━━━━━━ 10:21:30\n",
      "Accuracy: 0.9253 - Loss: 0.2145\n",
      "\n",
      "Batch 873/992 ━━━━━━━━━━━━━━━━━━━━ 10:21:40\n",
      "Accuracy: 0.9254 - Loss: 0.2144\n",
      "\n",
      "Batch 874/992 ━━━━━━━━━━━━━━━━━━━━ 10:21:51\n",
      "Accuracy: 0.9252 - Loss: 0.2146\n",
      "\n",
      "Batch 875/992 ━━━━━━━━━━━━━━━━━━━━ 10:22:02\n",
      "Accuracy: 0.9253 - Loss: 0.2144\n",
      "\n",
      "Batch 876/992 ━━━━━━━━━━━━━━━━━━━━ 10:22:12\n",
      "Accuracy: 0.9252 - Loss: 0.2145\n",
      "\n",
      "Batch 877/992 ━━━━━━━━━━━━━━━━━━━━ 10:22:22\n",
      "Accuracy: 0.9253 - Loss: 0.2144\n",
      "\n",
      "Batch 878/992 ━━━━━━━━━━━━━━━━━━━━ 10:22:32\n",
      "Accuracy: 0.9253 - Loss: 0.2146\n",
      "\n",
      "Batch 879/992 ━━━━━━━━━━━━━━━━━━━━ 10:22:42\n",
      "Accuracy: 0.9253 - Loss: 0.2144\n",
      "\n",
      "Batch 880/992 ━━━━━━━━━━━━━━━━━━━━ 10:22:52\n",
      "Accuracy: 0.9254 - Loss: 0.2142\n",
      "\n",
      "Batch 881/992 ━━━━━━━━━━━━━━━━━━━━ 10:23:02\n",
      "Accuracy: 0.9254 - Loss: 0.2141\n",
      "\n",
      "Batch 882/992 ━━━━━━━━━━━━━━━━━━━━ 10:23:15\n",
      "Accuracy: 0.9255 - Loss: 0.2139\n",
      "\n",
      "Batch 883/992 ━━━━━━━━━━━━━━━━━━━━ 10:23:27\n",
      "Accuracy: 0.9254 - Loss: 0.2141\n",
      "\n",
      "Batch 884/992 ━━━━━━━━━━━━━━━━━━━━ 10:23:39\n",
      "Accuracy: 0.9255 - Loss: 0.2139\n",
      "\n",
      "Batch 885/992 ━━━━━━━━━━━━━━━━━━━━ 10:23:53\n",
      "Accuracy: 0.9256 - Loss: 0.2137\n",
      "\n",
      "Batch 886/992 ━━━━━━━━━━━━━━━━━━━━ 10:24:06\n",
      "Accuracy: 0.9255 - Loss: 0.2138\n",
      "\n",
      "Batch 887/992 ━━━━━━━━━━━━━━━━━━━━ 10:24:18\n",
      "Accuracy: 0.9256 - Loss: 0.2137\n",
      "\n",
      "Batch 888/992 ━━━━━━━━━━━━━━━━━━━━ 10:24:29\n",
      "Accuracy: 0.9254 - Loss: 0.2139\n",
      "\n",
      "Batch 889/992 ━━━━━━━━━━━━━━━━━━━━ 10:24:39\n",
      "Accuracy: 0.9255 - Loss: 0.2138\n",
      "\n",
      "Batch 890/992 ━━━━━━━━━━━━━━━━━━━━ 10:24:51\n",
      "Accuracy: 0.9256 - Loss: 0.2138\n",
      "\n",
      "Batch 891/992 ━━━━━━━━━━━━━━━━━━━━ 10:25:02\n",
      "Accuracy: 0.9255 - Loss: 0.2137\n",
      "\n",
      "Batch 892/992 ━━━━━━━━━━━━━━━━━━━━ 10:25:12\n",
      "Accuracy: 0.9256 - Loss: 0.2136\n",
      "\n",
      "Batch 893/992 ━━━━━━━━━━━━━━━━━━━━ 10:25:26\n",
      "Accuracy: 0.9257 - Loss: 0.2134\n",
      "\n",
      "Batch 894/992 ━━━━━━━━━━━━━━━━━━━━ 10:25:39\n",
      "Accuracy: 0.9258 - Loss: 0.2134\n",
      "\n",
      "Batch 895/992 ━━━━━━━━━━━━━━━━━━━━ 10:25:53\n",
      "Accuracy: 0.9257 - Loss: 0.2135\n",
      "\n",
      "Batch 896/992 ━━━━━━━━━━━━━━━━━━━━ 10:26:03\n",
      "Accuracy: 0.9258 - Loss: 0.2133\n",
      "\n",
      "Batch 897/992 ━━━━━━━━━━━━━━━━━━━━ 10:26:15\n",
      "Accuracy: 0.9256 - Loss: 0.2135\n",
      "\n",
      "Batch 898/992 ━━━━━━━━━━━━━━━━━━━━ 10:26:26\n",
      "Accuracy: 0.9257 - Loss: 0.2132\n",
      "\n",
      "Batch 899/992 ━━━━━━━━━━━━━━━━━━━━ 10:26:40\n",
      "Accuracy: 0.9256 - Loss: 0.2133\n",
      "\n",
      "Batch 900/992 ━━━━━━━━━━━━━━━━━━━━ 10:26:54\n",
      "Accuracy: 0.9253 - Loss: 0.2137\n",
      "\n",
      "Batch 901/992 ━━━━━━━━━━━━━━━━━━━━ 10:27:06\n",
      "Accuracy: 0.9254 - Loss: 0.2135\n",
      "\n",
      "Batch 902/992 ━━━━━━━━━━━━━━━━━━━━ 10:27:18\n",
      "Accuracy: 0.9253 - Loss: 0.2135\n",
      "\n",
      "Batch 903/992 ━━━━━━━━━━━━━━━━━━━━ 10:27:31\n",
      "Accuracy: 0.9252 - Loss: 0.2137\n",
      "\n",
      "Batch 904/992 ━━━━━━━━━━━━━━━━━━━━ 10:27:45\n",
      "Accuracy: 0.9251 - Loss: 0.2143\n",
      "\n",
      "Batch 905/992 ━━━━━━━━━━━━━━━━━━━━ 10:27:59\n",
      "Accuracy: 0.9251 - Loss: 0.2142\n",
      "\n",
      "Batch 906/992 ━━━━━━━━━━━━━━━━━━━━ 10:28:13\n",
      "Accuracy: 0.9251 - Loss: 0.2141\n",
      "\n",
      "Batch 907/992 ━━━━━━━━━━━━━━━━━━━━ 10:28:24\n",
      "Accuracy: 0.9250 - Loss: 0.2147\n",
      "\n",
      "Batch 908/992 ━━━━━━━━━━━━━━━━━━━━ 10:28:35\n",
      "Accuracy: 0.9250 - Loss: 0.2147\n",
      "\n",
      "Batch 909/992 ━━━━━━━━━━━━━━━━━━━━ 10:28:47\n",
      "Accuracy: 0.9249 - Loss: 0.2149\n",
      "\n",
      "Batch 910/992 ━━━━━━━━━━━━━━━━━━━━ 10:28:58\n",
      "Accuracy: 0.9250 - Loss: 0.2147\n",
      "\n",
      "Batch 911/992 ━━━━━━━━━━━━━━━━━━━━ 10:29:11\n",
      "Accuracy: 0.9251 - Loss: 0.2146\n",
      "\n",
      "Batch 912/992 ━━━━━━━━━━━━━━━━━━━━ 10:29:25\n",
      "Accuracy: 0.9250 - Loss: 0.2146\n",
      "\n",
      "Batch 913/992 ━━━━━━━━━━━━━━━━━━━━ 10:29:39\n",
      "Accuracy: 0.9250 - Loss: 0.2146\n",
      "\n",
      "Batch 914/992 ━━━━━━━━━━━━━━━━━━━━ 10:29:53\n",
      "Accuracy: 0.9251 - Loss: 0.2144\n",
      "\n",
      "Batch 915/992 ━━━━━━━━━━━━━━━━━━━━ 10:30:05\n",
      "Accuracy: 0.9251 - Loss: 0.2143\n",
      "\n",
      "Batch 916/992 ━━━━━━━━━━━━━━━━━━━━ 10:30:15\n",
      "Accuracy: 0.9252 - Loss: 0.2142\n",
      "\n",
      "Batch 917/992 ━━━━━━━━━━━━━━━━━━━━ 10:30:25\n",
      "Accuracy: 0.9253 - Loss: 0.2141\n",
      "\n",
      "Batch 918/992 ━━━━━━━━━━━━━━━━━━━━ 10:30:36\n",
      "Accuracy: 0.9252 - Loss: 0.2142\n",
      "\n",
      "Batch 919/992 ━━━━━━━━━━━━━━━━━━━━ 10:30:46\n",
      "Accuracy: 0.9252 - Loss: 0.2143\n",
      "\n",
      "Batch 920/992 ━━━━━━━━━━━━━━━━━━━━ 10:30:56\n",
      "Accuracy: 0.9250 - Loss: 0.2144\n",
      "\n",
      "Batch 921/992 ━━━━━━━━━━━━━━━━━━━━ 10:31:06\n",
      "Accuracy: 0.9251 - Loss: 0.2142\n",
      "\n",
      "Batch 922/992 ━━━━━━━━━━━━━━━━━━━━ 10:31:17\n",
      "Accuracy: 0.9252 - Loss: 0.2140\n",
      "\n",
      "Batch 923/992 ━━━━━━━━━━━━━━━━━━━━ 10:31:27\n",
      "Accuracy: 0.9252 - Loss: 0.2138\n",
      "\n",
      "Batch 924/992 ━━━━━━━━━━━━━━━━━━━━ 10:31:37\n",
      "Accuracy: 0.9252 - Loss: 0.2139\n",
      "\n",
      "Batch 925/992 ━━━━━━━━━━━━━━━━━━━━ 10:31:48\n",
      "Accuracy: 0.9253 - Loss: 0.2138\n",
      "\n",
      "Batch 926/992 ━━━━━━━━━━━━━━━━━━━━ 10:31:58\n",
      "Accuracy: 0.9254 - Loss: 0.2137\n",
      "\n",
      "Batch 927/992 ━━━━━━━━━━━━━━━━━━━━ 10:32:11\n",
      "Accuracy: 0.9252 - Loss: 0.2140\n",
      "\n",
      "Batch 928/992 ━━━━━━━━━━━━━━━━━━━━ 10:32:25\n",
      "Accuracy: 0.9248 - Loss: 0.2148\n",
      "\n",
      "Batch 929/992 ━━━━━━━━━━━━━━━━━━━━ 10:32:38\n",
      "Accuracy: 0.9248 - Loss: 0.2148\n",
      "\n",
      "Batch 930/992 ━━━━━━━━━━━━━━━━━━━━ 10:32:51\n",
      "Accuracy: 0.9246 - Loss: 0.2150\n",
      "\n",
      "Batch 931/992 ━━━━━━━━━━━━━━━━━━━━ 10:33:05\n",
      "Accuracy: 0.9247 - Loss: 0.2149\n",
      "\n",
      "Batch 932/992 ━━━━━━━━━━━━━━━━━━━━ 10:33:17\n",
      "Accuracy: 0.9248 - Loss: 0.2147\n",
      "\n",
      "Batch 933/992 ━━━━━━━━━━━━━━━━━━━━ 10:33:30\n",
      "Accuracy: 0.9247 - Loss: 0.2149\n",
      "\n",
      "Batch 934/992 ━━━━━━━━━━━━━━━━━━━━ 10:33:44\n",
      "Accuracy: 0.9248 - Loss: 0.2149\n",
      "\n",
      "Batch 935/992 ━━━━━━━━━━━━━━━━━━━━ 10:33:55\n",
      "Accuracy: 0.9246 - Loss: 0.2152\n",
      "\n",
      "Batch 936/992 ━━━━━━━━━━━━━━━━━━━━ 10:34:07\n",
      "Accuracy: 0.9245 - Loss: 0.2153\n",
      "\n",
      "Batch 937/992 ━━━━━━━━━━━━━━━━━━━━ 10:34:20\n",
      "Accuracy: 0.9246 - Loss: 0.2152\n",
      "\n",
      "Batch 938/992 ━━━━━━━━━━━━━━━━━━━━ 10:34:32\n",
      "Accuracy: 0.9247 - Loss: 0.2150\n",
      "\n",
      "Batch 939/992 ━━━━━━━━━━━━━━━━━━━━ 10:34:45\n",
      "Accuracy: 0.9248 - Loss: 0.2149\n",
      "\n",
      "Batch 940/992 ━━━━━━━━━━━━━━━━━━━━ 10:34:56\n",
      "Accuracy: 0.9247 - Loss: 0.2151\n",
      "\n",
      "Batch 941/992 ━━━━━━━━━━━━━━━━━━━━ 10:35:07\n",
      "Accuracy: 0.9248 - Loss: 0.2150\n",
      "\n",
      "Batch 942/992 ━━━━━━━━━━━━━━━━━━━━ 10:35:18\n",
      "Accuracy: 0.9249 - Loss: 0.2149\n",
      "\n",
      "Batch 943/992 ━━━━━━━━━━━━━━━━━━━━ 10:35:30\n",
      "Accuracy: 0.9248 - Loss: 0.2150\n",
      "\n",
      "Batch 944/992 ━━━━━━━━━━━━━━━━━━━━ 10:35:43\n",
      "Accuracy: 0.9249 - Loss: 0.2148\n",
      "\n",
      "Batch 945/992 ━━━━━━━━━━━━━━━━━━━━ 10:35:53\n",
      "Accuracy: 0.9247 - Loss: 0.2151\n",
      "\n",
      "Batch 946/992 ━━━━━━━━━━━━━━━━━━━━ 10:36:04\n",
      "Accuracy: 0.9247 - Loss: 0.2150\n",
      "\n",
      "Batch 947/992 ━━━━━━━━━━━━━━━━━━━━ 10:36:15\n",
      "Accuracy: 0.9248 - Loss: 0.2150\n",
      "\n",
      "Batch 948/992 ━━━━━━━━━━━━━━━━━━━━ 10:36:26\n",
      "Accuracy: 0.9247 - Loss: 0.2150\n",
      "\n",
      "Batch 949/992 ━━━━━━━━━━━━━━━━━━━━ 10:36:37\n",
      "Accuracy: 0.9247 - Loss: 0.2151\n",
      "\n",
      "Batch 950/992 ━━━━━━━━━━━━━━━━━━━━ 10:36:51\n",
      "Accuracy: 0.9246 - Loss: 0.2150\n",
      "\n",
      "Batch 951/992 ━━━━━━━━━━━━━━━━━━━━ 10:37:04\n",
      "Accuracy: 0.9247 - Loss: 0.2149\n",
      "\n",
      "Batch 952/992 ━━━━━━━━━━━━━━━━━━━━ 10:37:15\n",
      "Accuracy: 0.9246 - Loss: 0.2148\n",
      "\n",
      "Batch 953/992 ━━━━━━━━━━━━━━━━━━━━ 10:37:26\n",
      "Accuracy: 0.9247 - Loss: 0.2147\n",
      "\n",
      "Batch 954/992 ━━━━━━━━━━━━━━━━━━━━ 10:37:38\n",
      "Accuracy: 0.9248 - Loss: 0.2146\n",
      "\n",
      "Batch 955/992 ━━━━━━━━━━━━━━━━━━━━ 10:37:49\n",
      "Accuracy: 0.9249 - Loss: 0.2146\n",
      "\n",
      "Batch 956/992 ━━━━━━━━━━━━━━━━━━━━ 10:38:01\n",
      "Accuracy: 0.9249 - Loss: 0.2146\n",
      "\n",
      "Batch 957/992 ━━━━━━━━━━━━━━━━━━━━ 10:38:16\n",
      "Accuracy: 0.9250 - Loss: 0.2148\n",
      "\n",
      "Batch 958/992 ━━━━━━━━━━━━━━━━━━━━ 10:38:29\n",
      "Accuracy: 0.9250 - Loss: 0.2149\n",
      "\n",
      "Batch 959/992 ━━━━━━━━━━━━━━━━━━━━ 10:38:40\n",
      "Accuracy: 0.9249 - Loss: 0.2149\n",
      "\n",
      "Batch 960/992 ━━━━━━━━━━━━━━━━━━━━ 10:38:50\n",
      "Accuracy: 0.9250 - Loss: 0.2148\n",
      "\n",
      "Batch 961/992 ━━━━━━━━━━━━━━━━━━━━ 10:39:02\n",
      "Accuracy: 0.9251 - Loss: 0.2146\n",
      "\n",
      "Batch 962/992 ━━━━━━━━━━━━━━━━━━━━ 10:39:13\n",
      "Accuracy: 0.9252 - Loss: 0.2144\n",
      "\n",
      "Batch 963/992 ━━━━━━━━━━━━━━━━━━━━ 10:39:25\n",
      "Accuracy: 0.9252 - Loss: 0.2143\n",
      "\n",
      "Batch 964/992 ━━━━━━━━━━━━━━━━━━━━ 10:39:38\n",
      "Accuracy: 0.9253 - Loss: 0.2141\n",
      "\n",
      "Batch 965/992 ━━━━━━━━━━━━━━━━━━━━ 10:39:49\n",
      "Accuracy: 0.9254 - Loss: 0.2140\n",
      "\n",
      "Batch 966/992 ━━━━━━━━━━━━━━━━━━━━ 10:40:00\n",
      "Accuracy: 0.9253 - Loss: 0.2140\n",
      "\n",
      "Batch 967/992 ━━━━━━━━━━━━━━━━━━━━ 10:40:11\n",
      "Accuracy: 0.9254 - Loss: 0.2138\n",
      "\n",
      "Batch 968/992 ━━━━━━━━━━━━━━━━━━━━ 10:40:23\n",
      "Accuracy: 0.9254 - Loss: 0.2140\n",
      "\n",
      "Batch 969/992 ━━━━━━━━━━━━━━━━━━━━ 10:40:34\n",
      "Accuracy: 0.9254 - Loss: 0.2138\n",
      "\n",
      "Batch 970/992 ━━━━━━━━━━━━━━━━━━━━ 10:40:45\n",
      "Accuracy: 0.9253 - Loss: 0.2139\n",
      "\n",
      "Batch 971/992 ━━━━━━━━━━━━━━━━━━━━ 10:40:55\n",
      "Accuracy: 0.9252 - Loss: 0.2141\n",
      "\n",
      "Batch 972/992 ━━━━━━━━━━━━━━━━━━━━ 10:41:07\n",
      "Accuracy: 0.9253 - Loss: 0.2140\n",
      "\n",
      "Batch 973/992 ━━━━━━━━━━━━━━━━━━━━ 10:41:21\n",
      "Accuracy: 0.9252 - Loss: 0.2143\n",
      "\n",
      "Batch 974/992 ━━━━━━━━━━━━━━━━━━━━ 10:41:32\n",
      "Accuracy: 0.9253 - Loss: 0.2142\n",
      "\n",
      "Batch 975/992 ━━━━━━━━━━━━━━━━━━━━ 10:41:42\n",
      "Accuracy: 0.9254 - Loss: 0.2142\n",
      "\n",
      "Batch 976/992 ━━━━━━━━━━━━━━━━━━━━ 10:41:52\n",
      "Accuracy: 0.9255 - Loss: 0.2140\n",
      "\n",
      "Batch 977/992 ━━━━━━━━━━━━━━━━━━━━ 10:42:02\n",
      "Accuracy: 0.9255 - Loss: 0.2138\n",
      "\n",
      "Batch 978/992 ━━━━━━━━━━━━━━━━━━━━ 10:42:12\n",
      "Accuracy: 0.9254 - Loss: 0.2147\n",
      "\n",
      "Batch 979/992 ━━━━━━━━━━━━━━━━━━━━ 10:42:23\n",
      "Accuracy: 0.9254 - Loss: 0.2145\n",
      "\n",
      "Batch 980/992 ━━━━━━━━━━━━━━━━━━━━ 10:42:33\n",
      "Accuracy: 0.9251 - Loss: 0.2152\n",
      "\n",
      "Batch 981/992 ━━━━━━━━━━━━━━━━━━━━ 10:42:43\n",
      "Accuracy: 0.9251 - Loss: 0.2156\n",
      "\n",
      "Batch 982/992 ━━━━━━━━━━━━━━━━━━━━ 10:42:53\n",
      "Accuracy: 0.9252 - Loss: 0.2154\n",
      "\n",
      "Batch 983/992 ━━━━━━━━━━━━━━━━━━━━ 10:43:03\n",
      "Accuracy: 0.9251 - Loss: 0.2154\n",
      "\n",
      "Batch 984/992 ━━━━━━━━━━━━━━━━━━━━ 10:43:13\n",
      "Accuracy: 0.9251 - Loss: 0.2155\n",
      "\n",
      "Batch 985/992 ━━━━━━━━━━━━━━━━━━━━ 10:43:23\n",
      "Accuracy: 0.9251 - Loss: 0.2154\n",
      "\n",
      "Batch 986/992 ━━━━━━━━━━━━━━━━━━━━ 10:43:33\n",
      "Accuracy: 0.9251 - Loss: 0.2154\n",
      "\n",
      "Batch 987/992 ━━━━━━━━━━━━━━━━━━━━ 10:43:43\n",
      "Accuracy: 0.9250 - Loss: 0.2154\n",
      "\n",
      "Batch 988/992 ━━━━━━━━━━━━━━━━━━━━ 10:43:54\n",
      "Accuracy: 0.9250 - Loss: 0.2153\n",
      "\n",
      "Batch 989/992 ━━━━━━━━━━━━━━━━━━━━ 10:44:04\n",
      "Accuracy: 0.9249 - Loss: 0.2154\n",
      "\n",
      "Batch 990/992 ━━━━━━━━━━━━━━━━━━━━ 10:44:14\n",
      "Accuracy: 0.9249 - Loss: 0.2154\n",
      "\n",
      "Batch 991/992 ━━━━━━━━━━━━━━━━━━━━ 10:44:24\n",
      "Accuracy: 0.9249 - Loss: 0.2154\n",
      "\n",
      "Batch 992/992 ━━━━━━━━━━━━━━━━━━━━ 10:44:34\n",
      "Accuracy: 0.9250 - Loss: 0.2153\n",
      "\n",
      "\n",
      "Epoch 7/10\n",
      "Batch 1/992 ━━━━━━━━━━━━━━━━━━━━ 11:02:25\n",
      "Accuracy: 1.0000 - Loss: 0.0911\n",
      "\n",
      "Batch 2/992 ━━━━━━━━━━━━━━━━━━━━ 11:02:36\n",
      "Accuracy: 1.0000 - Loss: 0.0987\n",
      "\n",
      "Batch 3/992 ━━━━━━━━━━━━━━━━━━━━ 11:02:46\n",
      "Accuracy: 0.9583 - Loss: 0.1346\n",
      "\n",
      "Batch 4/992 ━━━━━━━━━━━━━━━━━━━━ 11:02:57\n",
      "Accuracy: 0.9375 - Loss: 0.1892\n",
      "\n",
      "Batch 5/992 ━━━━━━━━━━━━━━━━━━━━ 11:03:07\n",
      "Accuracy: 0.9000 - Loss: 0.2220\n",
      "\n",
      "Batch 6/992 ━━━━━━━━━━━━━━━━━━━━ 11:03:18\n",
      "Accuracy: 0.9167 - Loss: 0.1989\n",
      "\n",
      "Batch 7/992 ━━━━━━━━━━━━━━━━━━━━ 11:03:31\n",
      "Accuracy: 0.9286 - Loss: 0.1886\n",
      "\n",
      "Batch 8/992 ━━━━━━━━━━━━━━━━━━━━ 11:03:42\n",
      "Accuracy: 0.9375 - Loss: 0.1913\n",
      "\n",
      "Batch 9/992 ━━━━━━━━━━━━━━━━━━━━ 11:03:53\n",
      "Accuracy: 0.9167 - Loss: 0.2031\n",
      "\n",
      "Batch 10/992 ━━━━━━━━━━━━━━━━━━━━ 11:04:04\n",
      "Accuracy: 0.9125 - Loss: 0.2186\n",
      "\n",
      "Batch 11/992 ━━━━━━━━━━━━━━━━━━━━ 11:04:14\n",
      "Accuracy: 0.9091 - Loss: 0.2454\n",
      "\n",
      "Batch 12/992 ━━━━━━━━━━━━━━━━━━━━ 11:04:25\n",
      "Accuracy: 0.9062 - Loss: 0.2398\n",
      "\n",
      "Batch 13/992 ━━━━━━━━━━━━━━━━━━━━ 11:04:36\n",
      "Accuracy: 0.9038 - Loss: 0.2429\n",
      "\n",
      "Batch 14/992 ━━━━━━━━━━━━━━━━━━━━ 11:04:47\n",
      "Accuracy: 0.9107 - Loss: 0.2260\n",
      "\n",
      "Batch 15/992 ━━━━━━━━━━━━━━━━━━━━ 11:04:57\n",
      "Accuracy: 0.9167 - Loss: 0.2183\n",
      "\n",
      "Batch 16/992 ━━━━━━━━━━━━━━━━━━━━ 11:05:08\n",
      "Accuracy: 0.9141 - Loss: 0.2225\n",
      "\n",
      "Batch 17/992 ━━━━━━━━━━━━━━━━━━━━ 11:05:19\n",
      "Accuracy: 0.9191 - Loss: 0.2097\n",
      "\n",
      "Batch 18/992 ━━━━━━━━━━━━━━━━━━━━ 11:05:29\n",
      "Accuracy: 0.9236 - Loss: 0.2012\n",
      "\n",
      "Batch 19/992 ━━━━━━━━━━━━━━━━━━━━ 11:05:40\n",
      "Accuracy: 0.9211 - Loss: 0.2120\n",
      "\n",
      "Batch 20/992 ━━━━━━━━━━━━━━━━━━━━ 11:05:50\n",
      "Accuracy: 0.9250 - Loss: 0.2060\n",
      "\n",
      "Batch 21/992 ━━━━━━━━━━━━━━━━━━━━ 11:06:01\n",
      "Accuracy: 0.9226 - Loss: 0.2146\n",
      "\n",
      "Batch 22/992 ━━━━━━━━━━━━━━━━━━━━ 11:06:11\n",
      "Accuracy: 0.9261 - Loss: 0.2077\n",
      "\n",
      "Batch 23/992 ━━━━━━━━━━━━━━━━━━━━ 11:06:22\n",
      "Accuracy: 0.9293 - Loss: 0.1996\n",
      "\n",
      "Batch 24/992 ━━━━━━━━━━━━━━━━━━━━ 11:06:32\n",
      "Accuracy: 0.9271 - Loss: 0.2095\n",
      "\n",
      "Batch 25/992 ━━━━━━━━━━━━━━━━━━━━ 11:06:43\n",
      "Accuracy: 0.9250 - Loss: 0.2055\n",
      "\n",
      "Batch 26/992 ━━━━━━━━━━━━━━━━━━━━ 11:06:53\n",
      "Accuracy: 0.9183 - Loss: 0.2129\n",
      "\n",
      "Batch 27/992 ━━━━━━━━━━━━━━━━━━━━ 11:07:04\n",
      "Accuracy: 0.9213 - Loss: 0.2074\n",
      "\n",
      "Batch 28/992 ━━━━━━━━━━━━━━━━━━━━ 11:07:14\n",
      "Accuracy: 0.9241 - Loss: 0.2041\n",
      "\n",
      "Batch 29/992 ━━━━━━━━━━━━━━━━━━━━ 11:07:24\n",
      "Accuracy: 0.9224 - Loss: 0.2071\n",
      "\n",
      "Batch 30/992 ━━━━━━━━━━━━━━━━━━━━ 11:07:35\n",
      "Accuracy: 0.9250 - Loss: 0.2049\n",
      "\n",
      "Batch 31/992 ━━━━━━━━━━━━━━━━━━━━ 11:07:45\n",
      "Accuracy: 0.9274 - Loss: 0.1988\n",
      "\n",
      "Batch 32/992 ━━━━━━━━━━━━━━━━━━━━ 11:07:56\n",
      "Accuracy: 0.9297 - Loss: 0.1938\n",
      "\n",
      "Batch 33/992 ━━━━━━━━━━━━━━━━━━━━ 11:08:07\n",
      "Accuracy: 0.9318 - Loss: 0.1886\n",
      "\n",
      "Batch 34/992 ━━━━━━━━━━━━━━━━━━━━ 11:08:18\n",
      "Accuracy: 0.9301 - Loss: 0.1902\n",
      "\n",
      "Batch 35/992 ━━━━━━━━━━━━━━━━━━━━ 11:08:28\n",
      "Accuracy: 0.9321 - Loss: 0.1850\n",
      "\n",
      "Batch 36/992 ━━━━━━━━━━━━━━━━━━━━ 11:08:39\n",
      "Accuracy: 0.9340 - Loss: 0.1835\n",
      "\n",
      "Batch 37/992 ━━━━━━━━━━━━━━━━━━━━ 11:08:49\n",
      "Accuracy: 0.9324 - Loss: 0.1821\n",
      "\n",
      "Batch 38/992 ━━━━━━━━━━━━━━━━━━━━ 11:09:00\n",
      "Accuracy: 0.9309 - Loss: 0.1866\n",
      "\n",
      "Batch 39/992 ━━━━━━━━━━━━━━━━━━━━ 11:09:11\n",
      "Accuracy: 0.9295 - Loss: 0.1894\n",
      "\n",
      "Batch 40/992 ━━━━━━━━━━━━━━━━━━━━ 11:09:21\n",
      "Accuracy: 0.9312 - Loss: 0.1860\n",
      "\n",
      "Batch 41/992 ━━━━━━━━━━━━━━━━━━━━ 11:09:32\n",
      "Accuracy: 0.9329 - Loss: 0.1829\n",
      "\n",
      "Batch 42/992 ━━━━━━━━━━━━━━━━━━━━ 11:09:43\n",
      "Accuracy: 0.9345 - Loss: 0.1798\n",
      "\n",
      "Batch 43/992 ━━━━━━━━━━━━━━━━━━━━ 11:09:53\n",
      "Accuracy: 0.9360 - Loss: 0.1767\n",
      "\n",
      "Batch 44/992 ━━━━━━━━━━━━━━━━━━━━ 11:10:04\n",
      "Accuracy: 0.9375 - Loss: 0.1731\n",
      "\n",
      "Batch 45/992 ━━━━━━━━━━━━━━━━━━━━ 11:10:14\n",
      "Accuracy: 0.9389 - Loss: 0.1706\n",
      "\n",
      "Batch 46/992 ━━━━━━━━━━━━━━━━━━━━ 11:10:25\n",
      "Accuracy: 0.9375 - Loss: 0.1746\n",
      "\n",
      "Batch 47/992 ━━━━━━━━━━━━━━━━━━━━ 11:10:35\n",
      "Accuracy: 0.9388 - Loss: 0.1730\n",
      "\n",
      "Batch 48/992 ━━━━━━━━━━━━━━━━━━━━ 11:10:46\n",
      "Accuracy: 0.9349 - Loss: 0.1849\n",
      "\n",
      "Batch 49/992 ━━━━━━━━━━━━━━━━━━━━ 11:10:57\n",
      "Accuracy: 0.9362 - Loss: 0.1818\n",
      "\n",
      "Batch 50/992 ━━━━━━━━━━━━━━━━━━━━ 11:11:07\n",
      "Accuracy: 0.9375 - Loss: 0.1787\n",
      "\n",
      "Batch 51/992 ━━━━━━━━━━━━━━━━━━━━ 11:11:18\n",
      "Accuracy: 0.9387 - Loss: 0.1771\n",
      "\n",
      "Batch 52/992 ━━━━━━━━━━━━━━━━━━━━ 11:11:29\n",
      "Accuracy: 0.9399 - Loss: 0.1744\n",
      "\n",
      "Batch 53/992 ━━━━━━━━━━━━━━━━━━━━ 11:11:40\n",
      "Accuracy: 0.9410 - Loss: 0.1724\n",
      "\n",
      "Batch 54/992 ━━━━━━━━━━━━━━━━━━━━ 11:11:51\n",
      "Accuracy: 0.9421 - Loss: 0.1709\n",
      "\n",
      "Batch 55/992 ━━━━━━━━━━━━━━━━━━━━ 11:12:02\n",
      "Accuracy: 0.9432 - Loss: 0.1680\n",
      "\n",
      "Batch 56/992 ━━━━━━━━━━━━━━━━━━━━ 11:12:13\n",
      "Accuracy: 0.9442 - Loss: 0.1666\n",
      "\n",
      "Batch 57/992 ━━━━━━━━━━━━━━━━━━━━ 11:12:25\n",
      "Accuracy: 0.9452 - Loss: 0.1646\n",
      "\n",
      "Batch 58/992 ━━━━━━━━━━━━━━━━━━━━ 11:12:41\n",
      "Accuracy: 0.9461 - Loss: 0.1620\n",
      "\n",
      "Batch 59/992 ━━━━━━━━━━━━━━━━━━━━ 11:12:53\n",
      "Accuracy: 0.9470 - Loss: 0.1602\n",
      "\n",
      "Batch 60/992 ━━━━━━━━━━━━━━━━━━━━ 11:13:04\n",
      "Accuracy: 0.9479 - Loss: 0.1594\n",
      "\n",
      "Batch 61/992 ━━━━━━━━━━━━━━━━━━━━ 11:13:15\n",
      "Accuracy: 0.9447 - Loss: 0.1690\n",
      "\n",
      "Batch 62/992 ━━━━━━━━━━━━━━━━━━━━ 11:13:28\n",
      "Accuracy: 0.9435 - Loss: 0.1738\n",
      "\n",
      "Batch 63/992 ━━━━━━━━━━━━━━━━━━━━ 11:13:44\n",
      "Accuracy: 0.9444 - Loss: 0.1716\n",
      "\n",
      "Batch 64/992 ━━━━━━━━━━━━━━━━━━━━ 11:13:58\n",
      "Accuracy: 0.9453 - Loss: 0.1693\n",
      "\n",
      "Batch 65/992 ━━━━━━━━━━━━━━━━━━━━ 11:14:13\n",
      "Accuracy: 0.9462 - Loss: 0.1674\n",
      "\n",
      "Batch 66/992 ━━━━━━━━━━━━━━━━━━━━ 11:14:28\n",
      "Accuracy: 0.9451 - Loss: 0.1693\n",
      "\n",
      "Batch 67/992 ━━━━━━━━━━━━━━━━━━━━ 11:14:42\n",
      "Accuracy: 0.9459 - Loss: 0.1679\n",
      "\n",
      "Batch 68/992 ━━━━━━━━━━━━━━━━━━━━ 11:14:56\n",
      "Accuracy: 0.9449 - Loss: 0.1703\n",
      "\n",
      "Batch 69/992 ━━━━━━━━━━━━━━━━━━━━ 11:15:11\n",
      "Accuracy: 0.9457 - Loss: 0.1688\n",
      "\n",
      "Batch 70/992 ━━━━━━━━━━━━━━━━━━━━ 11:15:22\n",
      "Accuracy: 0.9464 - Loss: 0.1676\n",
      "\n",
      "Batch 71/992 ━━━━━━━━━━━━━━━━━━━━ 11:15:38\n",
      "Accuracy: 0.9437 - Loss: 0.1694\n",
      "\n",
      "Batch 72/992 ━━━━━━━━━━━━━━━━━━━━ 11:15:51\n",
      "Accuracy: 0.9444 - Loss: 0.1676\n",
      "\n",
      "Batch 73/992 ━━━━━━━━━━━━━━━━━━━━ 11:16:03\n",
      "Accuracy: 0.9452 - Loss: 0.1664\n",
      "\n",
      "Batch 74/992 ━━━━━━━━━━━━━━━━━━━━ 11:16:14\n",
      "Accuracy: 0.9459 - Loss: 0.1656\n",
      "\n",
      "Batch 75/992 ━━━━━━━━━━━━━━━━━━━━ 11:16:27\n",
      "Accuracy: 0.9467 - Loss: 0.1637\n",
      "\n",
      "Batch 76/992 ━━━━━━━━━━━━━━━━━━━━ 11:16:40\n",
      "Accuracy: 0.9474 - Loss: 0.1638\n",
      "\n",
      "Batch 77/992 ━━━━━━━━━━━━━━━━━━━━ 11:16:56\n",
      "Accuracy: 0.9481 - Loss: 0.1628\n",
      "\n",
      "Batch 78/992 ━━━━━━━━━━━━━━━━━━━━ 11:17:09\n",
      "Accuracy: 0.9455 - Loss: 0.1657\n",
      "\n",
      "Batch 79/992 ━━━━━━━━━━━━━━━━━━━━ 11:17:19\n",
      "Accuracy: 0.9446 - Loss: 0.1656\n",
      "\n",
      "Batch 80/992 ━━━━━━━━━━━━━━━━━━━━ 11:17:29\n",
      "Accuracy: 0.9453 - Loss: 0.1641\n",
      "\n",
      "Batch 81/992 ━━━━━━━━━━━━━━━━━━━━ 11:17:39\n",
      "Accuracy: 0.9460 - Loss: 0.1629\n",
      "\n",
      "Batch 82/992 ━━━━━━━━━━━━━━━━━━━━ 11:17:49\n",
      "Accuracy: 0.9466 - Loss: 0.1615\n",
      "\n",
      "Batch 83/992 ━━━━━━━━━━━━━━━━━━━━ 11:18:00\n",
      "Accuracy: 0.9473 - Loss: 0.1607\n",
      "\n",
      "Batch 84/992 ━━━━━━━━━━━━━━━━━━━━ 11:18:11\n",
      "Accuracy: 0.9479 - Loss: 0.1599\n",
      "\n",
      "Batch 85/992 ━━━━━━━━━━━━━━━━━━━━ 11:18:25\n",
      "Accuracy: 0.9485 - Loss: 0.1587\n",
      "\n",
      "Batch 86/992 ━━━━━━━━━━━━━━━━━━━━ 11:18:39\n",
      "Accuracy: 0.9477 - Loss: 0.1596\n",
      "\n",
      "Batch 87/992 ━━━━━━━━━━━━━━━━━━━━ 11:18:51\n",
      "Accuracy: 0.9468 - Loss: 0.1631\n",
      "\n",
      "Batch 88/992 ━━━━━━━━━━━━━━━━━━━━ 11:19:02\n",
      "Accuracy: 0.9474 - Loss: 0.1623\n",
      "\n",
      "Batch 89/992 ━━━━━━━━━━━━━━━━━━━━ 11:19:13\n",
      "Accuracy: 0.9466 - Loss: 0.1671\n",
      "\n",
      "Batch 90/992 ━━━━━━━━━━━━━━━━━━━━ 11:19:24\n",
      "Accuracy: 0.9458 - Loss: 0.1678\n",
      "\n",
      "Batch 91/992 ━━━━━━━━━━━━━━━━━━━━ 11:19:36\n",
      "Accuracy: 0.9464 - Loss: 0.1668\n",
      "\n",
      "Batch 92/992 ━━━━━━━━━━━━━━━━━━━━ 11:19:47\n",
      "Accuracy: 0.9470 - Loss: 0.1652\n",
      "\n",
      "Batch 93/992 ━━━━━━━━━━━━━━━━━━━━ 11:19:58\n",
      "Accuracy: 0.9462 - Loss: 0.1652\n",
      "\n",
      "Batch 94/992 ━━━━━━━━━━━━━━━━━━━━ 11:20:09\n",
      "Accuracy: 0.9468 - Loss: 0.1639\n",
      "\n",
      "Batch 95/992 ━━━━━━━━━━━━━━━━━━━━ 11:20:19\n",
      "Accuracy: 0.9461 - Loss: 0.1648\n",
      "\n",
      "Batch 96/992 ━━━━━━━━━━━━━━━━━━━━ 11:20:32\n",
      "Accuracy: 0.9466 - Loss: 0.1632\n",
      "\n",
      "Batch 97/992 ━━━━━━━━━━━━━━━━━━━━ 11:20:43\n",
      "Accuracy: 0.9472 - Loss: 0.1618\n",
      "\n",
      "Batch 98/992 ━━━━━━━━━━━━━━━━━━━━ 11:20:55\n",
      "Accuracy: 0.9464 - Loss: 0.1641\n",
      "\n",
      "Batch 99/992 ━━━━━━━━━━━━━━━━━━━━ 11:21:07\n",
      "Accuracy: 0.9470 - Loss: 0.1629\n",
      "\n",
      "Batch 100/992 ━━━━━━━━━━━━━━━━━━━━ 11:21:19\n",
      "Accuracy: 0.9475 - Loss: 0.1621\n",
      "\n",
      "Batch 101/992 ━━━━━━━━━━━━━━━━━━━━ 11:21:30\n",
      "Accuracy: 0.9480 - Loss: 0.1610\n",
      "\n",
      "Batch 102/992 ━━━━━━━━━━━━━━━━━━━━ 11:21:42\n",
      "Accuracy: 0.9485 - Loss: 0.1614\n",
      "\n",
      "Batch 103/992 ━━━━━━━━━━━━━━━━━━━━ 11:21:52\n",
      "Accuracy: 0.9454 - Loss: 0.1647\n",
      "\n",
      "Batch 104/992 ━━━━━━━━━━━━━━━━━━━━ 11:22:04\n",
      "Accuracy: 0.9435 - Loss: 0.1685\n",
      "\n",
      "Batch 105/992 ━━━━━━━━━━━━━━━━━━━━ 11:22:15\n",
      "Accuracy: 0.9429 - Loss: 0.1694\n",
      "\n",
      "Batch 106/992 ━━━━━━━━━━━━━━━━━━━━ 11:22:28\n",
      "Accuracy: 0.9434 - Loss: 0.1680\n",
      "\n",
      "Batch 107/992 ━━━━━━━━━━━━━━━━━━━━ 11:22:39\n",
      "Accuracy: 0.9439 - Loss: 0.1673\n",
      "\n",
      "Batch 108/992 ━━━━━━━━━━━━━━━━━━━━ 11:22:52\n",
      "Accuracy: 0.9433 - Loss: 0.1685\n",
      "\n",
      "Batch 109/992 ━━━━━━━━━━━━━━━━━━━━ 11:23:05\n",
      "Accuracy: 0.9415 - Loss: 0.1721\n",
      "\n",
      "Batch 110/992 ━━━━━━━━━━━━━━━━━━━━ 11:23:19\n",
      "Accuracy: 0.9409 - Loss: 0.1734\n",
      "\n",
      "Batch 111/992 ━━━━━━━━━━━━━━━━━━━━ 11:23:31\n",
      "Accuracy: 0.9414 - Loss: 0.1723\n",
      "\n",
      "Batch 112/992 ━━━━━━━━━━━━━━━━━━━━ 11:23:44\n",
      "Accuracy: 0.9420 - Loss: 0.1716\n",
      "\n",
      "Batch 113/992 ━━━━━━━━━━━━━━━━━━━━ 11:23:57\n",
      "Accuracy: 0.9425 - Loss: 0.1704\n",
      "\n",
      "Batch 114/992 ━━━━━━━━━━━━━━━━━━━━ 11:24:10\n",
      "Accuracy: 0.9430 - Loss: 0.1690\n",
      "\n",
      "Batch 115/992 ━━━━━━━━━━━━━━━━━━━━ 11:24:24\n",
      "Accuracy: 0.9435 - Loss: 0.1688\n",
      "\n",
      "Batch 116/992 ━━━━━━━━━━━━━━━━━━━━ 11:24:37\n",
      "Accuracy: 0.9440 - Loss: 0.1677\n",
      "\n",
      "Batch 117/992 ━━━━━━━━━━━━━━━━━━━━ 11:24:51\n",
      "Accuracy: 0.9444 - Loss: 0.1669\n",
      "\n",
      "Batch 118/992 ━━━━━━━━━━━━━━━━━━━━ 11:25:05\n",
      "Accuracy: 0.9449 - Loss: 0.1665\n",
      "\n",
      "Batch 119/992 ━━━━━━━━━━━━━━━━━━━━ 11:25:21\n",
      "Accuracy: 0.9454 - Loss: 0.1660\n",
      "\n",
      "Batch 120/992 ━━━━━━━━━━━━━━━━━━━━ 11:25:36\n",
      "Accuracy: 0.9438 - Loss: 0.1678\n",
      "\n",
      "Batch 121/992 ━━━━━━━━━━━━━━━━━━━━ 11:25:51\n",
      "Accuracy: 0.9442 - Loss: 0.1666\n",
      "\n",
      "Batch 122/992 ━━━━━━━━━━━━━━━━━━━━ 11:26:05\n",
      "Accuracy: 0.9447 - Loss: 0.1654\n",
      "\n",
      "Batch 123/992 ━━━━━━━━━━━━━━━━━━━━ 11:26:20\n",
      "Accuracy: 0.9451 - Loss: 0.1647\n",
      "\n",
      "Batch 124/992 ━━━━━━━━━━━━━━━━━━━━ 11:26:35\n",
      "Accuracy: 0.9456 - Loss: 0.1641\n",
      "\n",
      "Batch 125/992 ━━━━━━━━━━━━━━━━━━━━ 11:26:49\n",
      "Accuracy: 0.9460 - Loss: 0.1631\n",
      "\n",
      "Batch 126/992 ━━━━━━━━━━━━━━━━━━━━ 11:27:04\n",
      "Accuracy: 0.9464 - Loss: 0.1625\n",
      "\n",
      "Batch 127/992 ━━━━━━━━━━━━━━━━━━━━ 11:27:18\n",
      "Accuracy: 0.9469 - Loss: 0.1613\n",
      "\n",
      "Batch 128/992 ━━━━━━━━━━━━━━━━━━━━ 11:27:33\n",
      "Accuracy: 0.9463 - Loss: 0.1630\n",
      "\n",
      "Batch 129/992 ━━━━━━━━━━━━━━━━━━━━ 11:27:47\n",
      "Accuracy: 0.9467 - Loss: 0.1627\n",
      "\n",
      "Batch 130/992 ━━━━━━━━━━━━━━━━━━━━ 11:28:02\n",
      "Accuracy: 0.9471 - Loss: 0.1619\n",
      "\n",
      "Batch 131/992 ━━━━━━━━━━━━━━━━━━━━ 11:28:16\n",
      "Accuracy: 0.9475 - Loss: 0.1611\n",
      "\n",
      "Batch 132/992 ━━━━━━━━━━━━━━━━━━━━ 11:28:29\n",
      "Accuracy: 0.9479 - Loss: 0.1602\n",
      "\n",
      "Batch 133/992 ━━━━━━━━━━━━━━━━━━━━ 11:28:40\n",
      "Accuracy: 0.9483 - Loss: 0.1595\n",
      "\n",
      "Batch 134/992 ━━━━━━━━━━━━━━━━━━━━ 11:28:52\n",
      "Accuracy: 0.9487 - Loss: 0.1590\n",
      "\n",
      "Batch 135/992 ━━━━━━━━━━━━━━━━━━━━ 11:29:04\n",
      "Accuracy: 0.9491 - Loss: 0.1579\n",
      "\n",
      "Batch 136/992 ━━━━━━━━━━━━━━━━━━━━ 11:29:15\n",
      "Accuracy: 0.9494 - Loss: 0.1569\n",
      "\n",
      "Batch 137/992 ━━━━━━━━━━━━━━━━━━━━ 11:29:26\n",
      "Accuracy: 0.9498 - Loss: 0.1563\n",
      "\n",
      "Batch 138/992 ━━━━━━━━━━━━━━━━━━━━ 11:29:37\n",
      "Accuracy: 0.9493 - Loss: 0.1570\n",
      "\n",
      "Batch 139/992 ━━━━━━━━━━━━━━━━━━━━ 11:29:49\n",
      "Accuracy: 0.9496 - Loss: 0.1561\n",
      "\n",
      "Batch 140/992 ━━━━━━━━━━━━━━━━━━━━ 11:30:00\n",
      "Accuracy: 0.9491 - Loss: 0.1564\n",
      "\n",
      "Batch 141/992 ━━━━━━━━━━━━━━━━━━━━ 11:30:11\n",
      "Accuracy: 0.9495 - Loss: 0.1560\n",
      "\n",
      "Batch 142/992 ━━━━━━━━━━━━━━━━━━━━ 11:30:22\n",
      "Accuracy: 0.9498 - Loss: 0.1552\n",
      "\n",
      "Batch 143/992 ━━━━━━━━━━━━━━━━━━━━ 11:30:33\n",
      "Accuracy: 0.9502 - Loss: 0.1548\n",
      "\n",
      "Batch 144/992 ━━━━━━━━━━━━━━━━━━━━ 11:30:44\n",
      "Accuracy: 0.9505 - Loss: 0.1539\n",
      "\n",
      "Batch 145/992 ━━━━━━━━━━━━━━━━━━━━ 11:30:55\n",
      "Accuracy: 0.9509 - Loss: 0.1530\n",
      "\n",
      "Batch 146/992 ━━━━━━━━━━━━━━━━━━━━ 11:31:05\n",
      "Accuracy: 0.9512 - Loss: 0.1520\n",
      "\n",
      "Batch 147/992 ━━━━━━━━━━━━━━━━━━━━ 11:31:16\n",
      "Accuracy: 0.9515 - Loss: 0.1512\n",
      "\n",
      "Batch 148/992 ━━━━━━━━━━━━━━━━━━━━ 11:31:27\n",
      "Accuracy: 0.9519 - Loss: 0.1502\n",
      "\n",
      "Batch 149/992 ━━━━━━━━━━━━━━━━━━━━ 11:31:38\n",
      "Accuracy: 0.9505 - Loss: 0.1519\n",
      "\n",
      "Batch 150/992 ━━━━━━━━━━━━━━━━━━━━ 11:31:49\n",
      "Accuracy: 0.9500 - Loss: 0.1537\n",
      "\n",
      "Batch 151/992 ━━━━━━━━━━━━━━━━━━━━ 11:32:01\n",
      "Accuracy: 0.9503 - Loss: 0.1529\n",
      "\n",
      "Batch 152/992 ━━━━━━━━━━━━━━━━━━━━ 11:32:13\n",
      "Accuracy: 0.9498 - Loss: 0.1543\n",
      "\n",
      "Batch 153/992 ━━━━━━━━━━━━━━━━━━━━ 11:32:25\n",
      "Accuracy: 0.9502 - Loss: 0.1533\n",
      "\n",
      "Batch 154/992 ━━━━━━━━━━━━━━━━━━━━ 11:32:36\n",
      "Accuracy: 0.9505 - Loss: 0.1525\n",
      "\n",
      "Batch 155/992 ━━━━━━━━━━━━━━━━━━━━ 11:32:47\n",
      "Accuracy: 0.9508 - Loss: 0.1522\n",
      "\n",
      "Batch 156/992 ━━━━━━━━━━━━━━━━━━━━ 11:32:58\n",
      "Accuracy: 0.9511 - Loss: 0.1515\n",
      "\n",
      "Batch 157/992 ━━━━━━━━━━━━━━━━━━━━ 11:33:09\n",
      "Accuracy: 0.9514 - Loss: 0.1507\n",
      "\n",
      "Batch 158/992 ━━━━━━━━━━━━━━━━━━━━ 11:33:20\n",
      "Accuracy: 0.9509 - Loss: 0.1510\n",
      "\n",
      "Batch 159/992 ━━━━━━━━━━━━━━━━━━━━ 11:33:31\n",
      "Accuracy: 0.9505 - Loss: 0.1516\n",
      "\n",
      "Batch 160/992 ━━━━━━━━━━━━━━━━━━━━ 11:33:42\n",
      "Accuracy: 0.9500 - Loss: 0.1521\n",
      "\n",
      "Batch 161/992 ━━━━━━━━━━━━━━━━━━━━ 11:33:54\n",
      "Accuracy: 0.9503 - Loss: 0.1516\n",
      "\n",
      "Batch 162/992 ━━━━━━━━━━━━━━━━━━━━ 11:34:07\n",
      "Accuracy: 0.9506 - Loss: 0.1507\n",
      "\n",
      "Batch 163/992 ━━━━━━━━━━━━━━━━━━━━ 11:34:18\n",
      "Accuracy: 0.9502 - Loss: 0.1505\n",
      "\n",
      "Batch 164/992 ━━━━━━━━━━━━━━━━━━━━ 11:34:29\n",
      "Accuracy: 0.9505 - Loss: 0.1508\n",
      "\n",
      "Batch 165/992 ━━━━━━━━━━━━━━━━━━━━ 11:34:41\n",
      "Accuracy: 0.9508 - Loss: 0.1501\n",
      "\n",
      "Batch 166/992 ━━━━━━━━━━━━━━━━━━━━ 11:34:52\n",
      "Accuracy: 0.9511 - Loss: 0.1492\n",
      "\n",
      "Batch 167/992 ━━━━━━━━━━━━━━━━━━━━ 11:35:03\n",
      "Accuracy: 0.9513 - Loss: 0.1488\n",
      "\n",
      "Batch 168/992 ━━━━━━━━━━━━━━━━━━━━ 11:35:14\n",
      "Accuracy: 0.9501 - Loss: 0.1502\n",
      "\n",
      "Batch 169/992 ━━━━━━━━━━━━━━━━━━━━ 11:35:25\n",
      "Accuracy: 0.9504 - Loss: 0.1494\n",
      "\n",
      "Batch 170/992 ━━━━━━━━━━━━━━━━━━━━ 11:35:37\n",
      "Accuracy: 0.9493 - Loss: 0.1570\n",
      "\n",
      "Batch 171/992 ━━━━━━━━━━━━━━━━━━━━ 11:35:49\n",
      "Accuracy: 0.9496 - Loss: 0.1563\n",
      "\n",
      "Batch 172/992 ━━━━━━━━━━━━━━━━━━━━ 11:36:00\n",
      "Accuracy: 0.9499 - Loss: 0.1564\n",
      "\n",
      "Batch 173/992 ━━━━━━━━━━━━━━━━━━━━ 11:36:12\n",
      "Accuracy: 0.9501 - Loss: 0.1565\n",
      "\n",
      "Batch 174/992 ━━━━━━━━━━━━━━━━━━━━ 11:36:23\n",
      "Accuracy: 0.9504 - Loss: 0.1568\n",
      "\n",
      "Batch 175/992 ━━━━━━━━━━━━━━━━━━━━ 11:36:34\n",
      "Accuracy: 0.9507 - Loss: 0.1561\n",
      "\n",
      "Batch 176/992 ━━━━━━━━━━━━━━━━━━━━ 11:36:45\n",
      "Accuracy: 0.9510 - Loss: 0.1558\n",
      "\n",
      "Batch 177/992 ━━━━━━━━━━━━━━━━━━━━ 11:36:57\n",
      "Accuracy: 0.9506 - Loss: 0.1561\n",
      "\n",
      "Batch 178/992 ━━━━━━━━━━━━━━━━━━━━ 11:37:10\n",
      "Accuracy: 0.9508 - Loss: 0.1553\n",
      "\n",
      "Batch 179/992 ━━━━━━━━━━━━━━━━━━━━ 11:37:21\n",
      "Accuracy: 0.9511 - Loss: 0.1547\n",
      "\n",
      "Batch 180/992 ━━━━━━━━━━━━━━━━━━━━ 11:37:33\n",
      "Accuracy: 0.9514 - Loss: 0.1539\n",
      "\n",
      "Batch 181/992 ━━━━━━━━━━━━━━━━━━━━ 11:37:45\n",
      "Accuracy: 0.9517 - Loss: 0.1535\n",
      "\n",
      "Batch 182/992 ━━━━━━━━━━━━━━━━━━━━ 11:37:56\n",
      "Accuracy: 0.9519 - Loss: 0.1534\n",
      "\n",
      "Batch 183/992 ━━━━━━━━━━━━━━━━━━━━ 11:38:08\n",
      "Accuracy: 0.9522 - Loss: 0.1526\n",
      "\n",
      "Batch 184/992 ━━━━━━━━━━━━━━━━━━━━ 11:38:19\n",
      "Accuracy: 0.9518 - Loss: 0.1532\n",
      "\n",
      "Batch 185/992 ━━━━━━━━━━━━━━━━━━━━ 11:38:31\n",
      "Accuracy: 0.9514 - Loss: 0.1540\n",
      "\n",
      "Batch 186/992 ━━━━━━━━━━━━━━━━━━━━ 11:38:42\n",
      "Accuracy: 0.9509 - Loss: 0.1543\n",
      "\n",
      "Batch 187/992 ━━━━━━━━━━━━━━━━━━━━ 11:38:54\n",
      "Accuracy: 0.9512 - Loss: 0.1540\n",
      "\n",
      "Batch 188/992 ━━━━━━━━━━━━━━━━━━━━ 11:39:05\n",
      "Accuracy: 0.9508 - Loss: 0.1548\n",
      "\n",
      "Batch 189/992 ━━━━━━━━━━━━━━━━━━━━ 11:39:17\n",
      "Accuracy: 0.9511 - Loss: 0.1542\n",
      "\n",
      "Batch 190/992 ━━━━━━━━━━━━━━━━━━━━ 11:39:28\n",
      "Accuracy: 0.9507 - Loss: 0.1545\n",
      "\n",
      "Batch 191/992 ━━━━━━━━━━━━━━━━━━━━ 11:39:40\n",
      "Accuracy: 0.9509 - Loss: 0.1538\n",
      "\n",
      "Batch 192/992 ━━━━━━━━━━━━━━━━━━━━ 11:39:52\n",
      "Accuracy: 0.9512 - Loss: 0.1532\n",
      "\n",
      "Batch 193/992 ━━━━━━━━━━━━━━━━━━━━ 11:40:04\n",
      "Accuracy: 0.9514 - Loss: 0.1526\n",
      "\n",
      "Batch 194/992 ━━━━━━━━━━━━━━━━━━━━ 11:40:15\n",
      "Accuracy: 0.9517 - Loss: 0.1521\n",
      "\n",
      "Batch 195/992 ━━━━━━━━━━━━━━━━━━━━ 11:40:26\n",
      "Accuracy: 0.9519 - Loss: 0.1521\n",
      "\n",
      "Batch 196/992 ━━━━━━━━━━━━━━━━━━━━ 11:40:37\n",
      "Accuracy: 0.9522 - Loss: 0.1513\n",
      "\n",
      "Batch 197/992 ━━━━━━━━━━━━━━━━━━━━ 11:40:48\n",
      "Accuracy: 0.9524 - Loss: 0.1511\n",
      "\n",
      "Batch 198/992 ━━━━━━━━━━━━━━━━━━━━ 11:40:59\n",
      "Accuracy: 0.9527 - Loss: 0.1506\n",
      "\n",
      "Batch 199/992 ━━━━━━━━━━━━━━━━━━━━ 11:41:11\n",
      "Accuracy: 0.9516 - Loss: 0.1534\n",
      "\n",
      "Batch 200/992 ━━━━━━━━━━━━━━━━━━━━ 11:41:23\n",
      "Accuracy: 0.9519 - Loss: 0.1527\n",
      "\n",
      "Batch 201/992 ━━━━━━━━━━━━━━━━━━━━ 11:41:34\n",
      "Accuracy: 0.9521 - Loss: 0.1525\n",
      "\n",
      "Batch 202/992 ━━━━━━━━━━━━━━━━━━━━ 11:41:46\n",
      "Accuracy: 0.9524 - Loss: 0.1518\n",
      "\n",
      "Batch 203/992 ━━━━━━━━━━━━━━━━━━━━ 11:41:56\n",
      "Accuracy: 0.9526 - Loss: 0.1511\n",
      "\n",
      "Batch 204/992 ━━━━━━━━━━━━━━━━━━━━ 11:42:06\n",
      "Accuracy: 0.9528 - Loss: 0.1507\n",
      "\n",
      "Batch 205/992 ━━━━━━━━━━━━━━━━━━━━ 11:42:17\n",
      "Accuracy: 0.9524 - Loss: 0.1508\n",
      "\n",
      "Batch 206/992 ━━━━━━━━━━━━━━━━━━━━ 11:42:28\n",
      "Accuracy: 0.9521 - Loss: 0.1520\n",
      "\n",
      "Batch 207/992 ━━━━━━━━━━━━━━━━━━━━ 11:42:40\n",
      "Accuracy: 0.9511 - Loss: 0.1539\n",
      "\n",
      "Batch 208/992 ━━━━━━━━━━━━━━━━━━━━ 11:42:52\n",
      "Accuracy: 0.9513 - Loss: 0.1533\n",
      "\n",
      "Batch 209/992 ━━━━━━━━━━━━━━━━━━━━ 11:43:02\n",
      "Accuracy: 0.9516 - Loss: 0.1528\n",
      "\n",
      "Batch 210/992 ━━━━━━━━━━━━━━━━━━━━ 11:43:12\n",
      "Accuracy: 0.9518 - Loss: 0.1527\n",
      "\n",
      "Batch 211/992 ━━━━━━━━━━━━━━━━━━━━ 11:43:22\n",
      "Accuracy: 0.9520 - Loss: 0.1522\n",
      "\n",
      "Batch 212/992 ━━━━━━━━━━━━━━━━━━━━ 11:43:33\n",
      "Accuracy: 0.9517 - Loss: 0.1525\n",
      "\n",
      "Batch 213/992 ━━━━━━━━━━━━━━━━━━━━ 11:43:43\n",
      "Accuracy: 0.9513 - Loss: 0.1528\n",
      "\n",
      "Batch 214/992 ━━━━━━━━━━━━━━━━━━━━ 11:43:54\n",
      "Accuracy: 0.9515 - Loss: 0.1522\n",
      "\n",
      "Batch 215/992 ━━━━━━━━━━━━━━━━━━━━ 11:44:04\n",
      "Accuracy: 0.9517 - Loss: 0.1517\n",
      "\n",
      "Batch 216/992 ━━━━━━━━━━━━━━━━━━━━ 11:44:14\n",
      "Accuracy: 0.9520 - Loss: 0.1516\n",
      "\n",
      "Batch 217/992 ━━━━━━━━━━━━━━━━━━━━ 11:44:25\n",
      "Accuracy: 0.9516 - Loss: 0.1524\n",
      "\n",
      "Batch 218/992 ━━━━━━━━━━━━━━━━━━━━ 11:44:36\n",
      "Accuracy: 0.9513 - Loss: 0.1525\n",
      "\n",
      "Batch 219/992 ━━━━━━━━━━━━━━━━━━━━ 11:44:47\n",
      "Accuracy: 0.9509 - Loss: 0.1524\n",
      "\n",
      "Batch 220/992 ━━━━━━━━━━━━━━━━━━━━ 11:44:59\n",
      "Accuracy: 0.9511 - Loss: 0.1519\n",
      "\n",
      "Batch 221/992 ━━━━━━━━━━━━━━━━━━━━ 11:45:09\n",
      "Accuracy: 0.9514 - Loss: 0.1514\n",
      "\n",
      "Batch 222/992 ━━━━━━━━━━━━━━━━━━━━ 11:45:19\n",
      "Accuracy: 0.9516 - Loss: 0.1513\n",
      "\n",
      "Batch 223/992 ━━━━━━━━━━━━━━━━━━━━ 11:45:30\n",
      "Accuracy: 0.9518 - Loss: 0.1507\n",
      "\n",
      "Batch 224/992 ━━━━━━━━━━━━━━━━━━━━ 11:45:40\n",
      "Accuracy: 0.9515 - Loss: 0.1528\n",
      "\n",
      "Batch 225/992 ━━━━━━━━━━━━━━━━━━━━ 11:45:51\n",
      "Accuracy: 0.9517 - Loss: 0.1524\n",
      "\n",
      "Batch 226/992 ━━━━━━━━━━━━━━━━━━━━ 11:46:01\n",
      "Accuracy: 0.9519 - Loss: 0.1519\n",
      "\n",
      "Batch 227/992 ━━━━━━━━━━━━━━━━━━━━ 11:46:11\n",
      "Accuracy: 0.9515 - Loss: 0.1523\n",
      "\n",
      "Batch 228/992 ━━━━━━━━━━━━━━━━━━━━ 11:46:21\n",
      "Accuracy: 0.9512 - Loss: 0.1531\n",
      "\n",
      "Batch 229/992 ━━━━━━━━━━━━━━━━━━━━ 11:46:31\n",
      "Accuracy: 0.9514 - Loss: 0.1527\n",
      "\n",
      "Batch 230/992 ━━━━━━━━━━━━━━━━━━━━ 11:46:41\n",
      "Accuracy: 0.9516 - Loss: 0.1522\n",
      "\n",
      "Batch 231/992 ━━━━━━━━━━━━━━━━━━━━ 11:46:52\n",
      "Accuracy: 0.9518 - Loss: 0.1516\n",
      "\n",
      "Batch 232/992 ━━━━━━━━━━━━━━━━━━━━ 11:47:02\n",
      "Accuracy: 0.9520 - Loss: 0.1511\n",
      "\n",
      "Batch 233/992 ━━━━━━━━━━━━━━━━━━━━ 11:47:12\n",
      "Accuracy: 0.9523 - Loss: 0.1506\n",
      "\n",
      "Batch 234/992 ━━━━━━━━━━━━━━━━━━━━ 11:47:22\n",
      "Accuracy: 0.9525 - Loss: 0.1501\n",
      "\n",
      "Batch 235/992 ━━━━━━━━━━━━━━━━━━━━ 11:47:33\n",
      "Accuracy: 0.9521 - Loss: 0.1508\n",
      "\n",
      "Batch 236/992 ━━━━━━━━━━━━━━━━━━━━ 11:47:43\n",
      "Accuracy: 0.9523 - Loss: 0.1503\n",
      "\n",
      "Batch 237/992 ━━━━━━━━━━━━━━━━━━━━ 11:47:54\n",
      "Accuracy: 0.9525 - Loss: 0.1497\n",
      "\n",
      "Batch 238/992 ━━━━━━━━━━━━━━━━━━━━ 11:48:04\n",
      "Accuracy: 0.9527 - Loss: 0.1492\n",
      "\n",
      "Batch 239/992 ━━━━━━━━━━━━━━━━━━━━ 11:48:14\n",
      "Accuracy: 0.9529 - Loss: 0.1489\n",
      "\n",
      "Batch 240/992 ━━━━━━━━━━━━━━━━━━━━ 11:48:24\n",
      "Accuracy: 0.9531 - Loss: 0.1485\n",
      "\n",
      "Batch 241/992 ━━━━━━━━━━━━━━━━━━━━ 11:48:34\n",
      "Accuracy: 0.9533 - Loss: 0.1482\n",
      "\n",
      "Batch 242/992 ━━━━━━━━━━━━━━━━━━━━ 11:48:45\n",
      "Accuracy: 0.9535 - Loss: 0.1477\n",
      "\n",
      "Batch 243/992 ━━━━━━━━━━━━━━━━━━━━ 11:48:55\n",
      "Accuracy: 0.9532 - Loss: 0.1481\n",
      "\n",
      "Batch 244/992 ━━━━━━━━━━━━━━━━━━━━ 11:49:05\n",
      "Accuracy: 0.9534 - Loss: 0.1478\n",
      "\n",
      "Batch 245/992 ━━━━━━━━━━━━━━━━━━━━ 11:49:15\n",
      "Accuracy: 0.9536 - Loss: 0.1472\n",
      "\n",
      "Batch 246/992 ━━━━━━━━━━━━━━━━━━━━ 11:49:25\n",
      "Accuracy: 0.9538 - Loss: 0.1467\n",
      "\n",
      "Batch 247/992 ━━━━━━━━━━━━━━━━━━━━ 11:49:36\n",
      "Accuracy: 0.9524 - Loss: 0.1506\n",
      "\n",
      "Batch 248/992 ━━━━━━━━━━━━━━━━━━━━ 11:49:46\n",
      "Accuracy: 0.9526 - Loss: 0.1502\n",
      "\n",
      "Batch 249/992 ━━━━━━━━━━━━━━━━━━━━ 11:49:56\n",
      "Accuracy: 0.9528 - Loss: 0.1503\n",
      "\n",
      "Batch 250/992 ━━━━━━━━━━━━━━━━━━━━ 11:50:07\n",
      "Accuracy: 0.9530 - Loss: 0.1499\n",
      "\n",
      "Batch 251/992 ━━━━━━━━━━━━━━━━━━━━ 11:50:17\n",
      "Accuracy: 0.9532 - Loss: 0.1497\n",
      "\n",
      "Batch 252/992 ━━━━━━━━━━━━━━━━━━━━ 11:50:27\n",
      "Accuracy: 0.9534 - Loss: 0.1491\n",
      "\n",
      "Batch 253/992 ━━━━━━━━━━━━━━━━━━━━ 11:50:37\n",
      "Accuracy: 0.9536 - Loss: 0.1490\n",
      "\n",
      "Batch 254/992 ━━━━━━━━━━━━━━━━━━━━ 11:50:47\n",
      "Accuracy: 0.9532 - Loss: 0.1494\n",
      "\n",
      "Batch 255/992 ━━━━━━━━━━━━━━━━━━━━ 11:50:57\n",
      "Accuracy: 0.9529 - Loss: 0.1495\n",
      "\n",
      "Batch 256/992 ━━━━━━━━━━━━━━━━━━━━ 11:51:07\n",
      "Accuracy: 0.9526 - Loss: 0.1499\n",
      "\n",
      "Batch 257/992 ━━━━━━━━━━━━━━━━━━━━ 11:51:18\n",
      "Accuracy: 0.9528 - Loss: 0.1496\n",
      "\n",
      "Batch 258/992 ━━━━━━━━━━━━━━━━━━━━ 11:51:28\n",
      "Accuracy: 0.9525 - Loss: 0.1527\n",
      "\n",
      "Batch 259/992 ━━━━━━━━━━━━━━━━━━━━ 11:51:38\n",
      "Accuracy: 0.9527 - Loss: 0.1524\n",
      "\n",
      "Batch 260/992 ━━━━━━━━━━━━━━━━━━━━ 11:51:49\n",
      "Accuracy: 0.9524 - Loss: 0.1528\n",
      "\n",
      "Batch 261/992 ━━━━━━━━━━━━━━━━━━━━ 11:51:59\n",
      "Accuracy: 0.9526 - Loss: 0.1524\n",
      "\n",
      "Batch 262/992 ━━━━━━━━━━━━━━━━━━━━ 11:52:09\n",
      "Accuracy: 0.9528 - Loss: 0.1523\n",
      "\n",
      "Batch 263/992 ━━━━━━━━━━━━━━━━━━━━ 11:52:19\n",
      "Accuracy: 0.9529 - Loss: 0.1519\n",
      "\n",
      "Batch 264/992 ━━━━━━━━━━━━━━━━━━━━ 11:52:29\n",
      "Accuracy: 0.9531 - Loss: 0.1515\n",
      "\n",
      "Batch 265/992 ━━━━━━━━━━━━━━━━━━━━ 11:52:40\n",
      "Accuracy: 0.9533 - Loss: 0.1510\n",
      "\n",
      "Batch 266/992 ━━━━━━━━━━━━━━━━━━━━ 11:52:50\n",
      "Accuracy: 0.9535 - Loss: 0.1509\n",
      "\n",
      "Batch 267/992 ━━━━━━━━━━━━━━━━━━━━ 11:53:00\n",
      "Accuracy: 0.9532 - Loss: 0.1510\n",
      "\n",
      "Batch 268/992 ━━━━━━━━━━━━━━━━━━━━ 11:53:10\n",
      "Accuracy: 0.9529 - Loss: 0.1516\n",
      "\n",
      "Batch 269/992 ━━━━━━━━━━━━━━━━━━━━ 11:53:20\n",
      "Accuracy: 0.9526 - Loss: 0.1518\n",
      "\n",
      "Batch 270/992 ━━━━━━━━━━━━━━━━━━━━ 11:53:30\n",
      "Accuracy: 0.9528 - Loss: 0.1513\n",
      "\n",
      "Batch 271/992 ━━━━━━━━━━━━━━━━━━━━ 11:53:41\n",
      "Accuracy: 0.9525 - Loss: 0.1519\n",
      "\n",
      "Batch 272/992 ━━━━━━━━━━━━━━━━━━━━ 11:53:51\n",
      "Accuracy: 0.9522 - Loss: 0.1523\n",
      "\n",
      "Batch 273/992 ━━━━━━━━━━━━━━━━━━━━ 11:54:02\n",
      "Accuracy: 0.9524 - Loss: 0.1520\n",
      "\n",
      "Batch 274/992 ━━━━━━━━━━━━━━━━━━━━ 11:54:13\n",
      "Accuracy: 0.9526 - Loss: 0.1515\n",
      "\n",
      "Batch 275/992 ━━━━━━━━━━━━━━━━━━━━ 11:54:25\n",
      "Accuracy: 0.9527 - Loss: 0.1511\n",
      "\n",
      "Batch 276/992 ━━━━━━━━━━━━━━━━━━━━ 11:54:37\n",
      "Accuracy: 0.9529 - Loss: 0.1508\n",
      "\n",
      "Batch 277/992 ━━━━━━━━━━━━━━━━━━━━ 11:54:48\n",
      "Accuracy: 0.9526 - Loss: 0.1512\n",
      "\n",
      "Batch 278/992 ━━━━━━━━━━━━━━━━━━━━ 11:54:59\n",
      "Accuracy: 0.9528 - Loss: 0.1509\n",
      "\n",
      "Batch 279/992 ━━━━━━━━━━━━━━━━━━━━ 11:55:11\n",
      "Accuracy: 0.9530 - Loss: 0.1505\n",
      "\n",
      "Batch 280/992 ━━━━━━━━━━━━━━━━━━━━ 11:55:22\n",
      "Accuracy: 0.9527 - Loss: 0.1508\n",
      "\n",
      "Batch 281/992 ━━━━━━━━━━━━━━━━━━━━ 11:55:34\n",
      "Accuracy: 0.9528 - Loss: 0.1503\n",
      "\n",
      "Batch 282/992 ━━━━━━━━━━━━━━━━━━━━ 11:55:46\n",
      "Accuracy: 0.9530 - Loss: 0.1499\n",
      "\n",
      "Batch 283/992 ━━━━━━━━━━━━━━━━━━━━ 11:55:58\n",
      "Accuracy: 0.9532 - Loss: 0.1495\n",
      "\n",
      "Batch 284/992 ━━━━━━━━━━━━━━━━━━━━ 11:56:11\n",
      "Accuracy: 0.9529 - Loss: 0.1502\n",
      "\n",
      "Batch 285/992 ━━━━━━━━━━━━━━━━━━━━ 11:56:22\n",
      "Accuracy: 0.9518 - Loss: 0.1513\n",
      "\n",
      "Batch 286/992 ━━━━━━━━━━━━━━━━━━━━ 11:56:33\n",
      "Accuracy: 0.9519 - Loss: 0.1511\n",
      "\n",
      "Batch 287/992 ━━━━━━━━━━━━━━━━━━━━ 11:56:43\n",
      "Accuracy: 0.9517 - Loss: 0.1522\n",
      "\n",
      "Batch 288/992 ━━━━━━━━━━━━━━━━━━━━ 11:56:56\n",
      "Accuracy: 0.9518 - Loss: 0.1520\n",
      "\n",
      "Batch 289/992 ━━━━━━━━━━━━━━━━━━━━ 11:57:08\n",
      "Accuracy: 0.9520 - Loss: 0.1519\n",
      "\n",
      "Batch 290/992 ━━━━━━━━━━━━━━━━━━━━ 11:57:21\n",
      "Accuracy: 0.9513 - Loss: 0.1533\n",
      "\n",
      "Batch 291/992 ━━━━━━━━━━━━━━━━━━━━ 11:57:32\n",
      "Accuracy: 0.9506 - Loss: 0.1552\n",
      "\n",
      "Batch 292/992 ━━━━━━━━━━━━━━━━━━━━ 11:57:43\n",
      "Accuracy: 0.9508 - Loss: 0.1547\n",
      "\n",
      "Batch 293/992 ━━━━━━━━━━━━━━━━━━━━ 11:57:54\n",
      "Accuracy: 0.9509 - Loss: 0.1542\n",
      "\n",
      "Batch 294/992 ━━━━━━━━━━━━━━━━━━━━ 11:58:06\n",
      "Accuracy: 0.9511 - Loss: 0.1538\n",
      "\n",
      "Batch 295/992 ━━━━━━━━━━━━━━━━━━━━ 11:58:19\n",
      "Accuracy: 0.9513 - Loss: 0.1534\n",
      "\n",
      "Batch 296/992 ━━━━━━━━━━━━━━━━━━━━ 11:58:30\n",
      "Accuracy: 0.9514 - Loss: 0.1533\n",
      "\n",
      "Batch 297/992 ━━━━━━━━━━━━━━━━━━━━ 11:58:43\n",
      "Accuracy: 0.9508 - Loss: 0.1541\n",
      "\n",
      "Batch 298/992 ━━━━━━━━━━━━━━━━━━━━ 11:58:54\n",
      "Accuracy: 0.9509 - Loss: 0.1536\n",
      "\n",
      "Batch 299/992 ━━━━━━━━━━━━━━━━━━━━ 11:59:05\n",
      "Accuracy: 0.9511 - Loss: 0.1531\n",
      "\n",
      "Batch 300/992 ━━━━━━━━━━━━━━━━━━━━ 11:59:16\n",
      "Accuracy: 0.9513 - Loss: 0.1532\n",
      "\n",
      "Batch 301/992 ━━━━━━━━━━━━━━━━━━━━ 11:59:27\n",
      "Accuracy: 0.9514 - Loss: 0.1527\n",
      "\n",
      "Batch 302/992 ━━━━━━━━━━━━━━━━━━━━ 11:59:38\n",
      "Accuracy: 0.9512 - Loss: 0.1528\n",
      "\n",
      "Batch 303/992 ━━━━━━━━━━━━━━━━━━━━ 11:59:50\n",
      "Accuracy: 0.9513 - Loss: 0.1527\n",
      "\n",
      "Batch 304/992 ━━━━━━━━━━━━━━━━━━━━ 12:00:02\n",
      "Accuracy: 0.9515 - Loss: 0.1524\n",
      "\n",
      "Batch 305/992 ━━━━━━━━━━━━━━━━━━━━ 12:00:13\n",
      "Accuracy: 0.9516 - Loss: 0.1520\n",
      "\n",
      "Batch 306/992 ━━━━━━━━━━━━━━━━━━━━ 12:00:24\n",
      "Accuracy: 0.9518 - Loss: 0.1516\n",
      "\n",
      "Batch 307/992 ━━━━━━━━━━━━━━━━━━━━ 12:00:36\n",
      "Accuracy: 0.9520 - Loss: 0.1512\n",
      "\n",
      "Batch 308/992 ━━━━━━━━━━━━━━━━━━━━ 12:00:47\n",
      "Accuracy: 0.9521 - Loss: 0.1509\n",
      "\n",
      "Batch 309/992 ━━━━━━━━━━━━━━━━━━━━ 12:00:58\n",
      "Accuracy: 0.9519 - Loss: 0.1512\n",
      "\n",
      "Batch 310/992 ━━━━━━━━━━━━━━━━━━━━ 12:01:09\n",
      "Accuracy: 0.9516 - Loss: 0.1512\n",
      "\n",
      "Batch 311/992 ━━━━━━━━━━━━━━━━━━━━ 12:01:20\n",
      "Accuracy: 0.9514 - Loss: 0.1515\n",
      "\n",
      "Batch 312/992 ━━━━━━━━━━━━━━━━━━━━ 12:01:32\n",
      "Accuracy: 0.9511 - Loss: 0.1515\n",
      "\n",
      "Batch 313/992 ━━━━━━━━━━━━━━━━━━━━ 12:01:42\n",
      "Accuracy: 0.9513 - Loss: 0.1510\n",
      "\n",
      "Batch 314/992 ━━━━━━━━━━━━━━━━━━━━ 12:01:54\n",
      "Accuracy: 0.9514 - Loss: 0.1507\n",
      "\n",
      "Batch 315/992 ━━━━━━━━━━━━━━━━━━━━ 12:02:05\n",
      "Accuracy: 0.9512 - Loss: 0.1507\n",
      "\n",
      "Batch 316/992 ━━━━━━━━━━━━━━━━━━━━ 12:02:15\n",
      "Accuracy: 0.9513 - Loss: 0.1503\n",
      "\n",
      "Batch 317/992 ━━━━━━━━━━━━━━━━━━━━ 12:02:26\n",
      "Accuracy: 0.9515 - Loss: 0.1499\n",
      "\n",
      "Batch 318/992 ━━━━━━━━━━━━━━━━━━━━ 12:02:36\n",
      "Accuracy: 0.9513 - Loss: 0.1502\n",
      "\n",
      "Batch 319/992 ━━━━━━━━━━━━━━━━━━━━ 12:02:47\n",
      "Accuracy: 0.9514 - Loss: 0.1499\n",
      "\n",
      "Batch 320/992 ━━━━━━━━━━━━━━━━━━━━ 12:02:59\n",
      "Accuracy: 0.9516 - Loss: 0.1496\n",
      "\n",
      "Batch 321/992 ━━━━━━━━━━━━━━━━━━━━ 12:03:11\n",
      "Accuracy: 0.9513 - Loss: 0.1498\n",
      "\n",
      "Batch 322/992 ━━━━━━━━━━━━━━━━━━━━ 12:03:21\n",
      "Accuracy: 0.9511 - Loss: 0.1498\n",
      "\n",
      "Batch 323/992 ━━━━━━━━━━━━━━━━━━━━ 12:03:32\n",
      "Accuracy: 0.9512 - Loss: 0.1493\n",
      "\n",
      "Batch 324/992 ━━━━━━━━━━━━━━━━━━━━ 12:03:44\n",
      "Accuracy: 0.9514 - Loss: 0.1488\n",
      "\n",
      "Batch 325/992 ━━━━━━━━━━━━━━━━━━━━ 12:03:59\n",
      "Accuracy: 0.9512 - Loss: 0.1492\n",
      "\n",
      "Batch 326/992 ━━━━━━━━━━━━━━━━━━━━ 12:04:14\n",
      "Accuracy: 0.9509 - Loss: 0.1493\n",
      "\n",
      "Batch 327/992 ━━━━━━━━━━━━━━━━━━━━ 12:04:26\n",
      "Accuracy: 0.9507 - Loss: 0.1504\n",
      "\n",
      "Batch 328/992 ━━━━━━━━━━━━━━━━━━━━ 12:04:37\n",
      "Accuracy: 0.9505 - Loss: 0.1504\n",
      "\n",
      "Batch 329/992 ━━━━━━━━━━━━━━━━━━━━ 12:04:49\n",
      "Accuracy: 0.9506 - Loss: 0.1501\n",
      "\n",
      "Batch 330/992 ━━━━━━━━━━━━━━━━━━━━ 12:05:01\n",
      "Accuracy: 0.9508 - Loss: 0.1497\n",
      "\n",
      "Batch 331/992 ━━━━━━━━━━━━━━━━━━━━ 12:05:12\n",
      "Accuracy: 0.9509 - Loss: 0.1493\n",
      "\n",
      "Batch 332/992 ━━━━━━━━━━━━━━━━━━━━ 12:05:23\n",
      "Accuracy: 0.9511 - Loss: 0.1491\n",
      "\n",
      "Batch 333/992 ━━━━━━━━━━━━━━━━━━━━ 12:05:35\n",
      "Accuracy: 0.9508 - Loss: 0.1496\n",
      "\n",
      "Batch 334/992 ━━━━━━━━━━━━━━━━━━━━ 12:05:45\n",
      "Accuracy: 0.9510 - Loss: 0.1494\n",
      "\n",
      "Batch 335/992 ━━━━━━━━━━━━━━━━━━━━ 12:05:56\n",
      "Accuracy: 0.9511 - Loss: 0.1490\n",
      "\n",
      "Batch 336/992 ━━━━━━━━━━━━━━━━━━━━ 12:06:07\n",
      "Accuracy: 0.9513 - Loss: 0.1486\n",
      "\n",
      "Batch 337/992 ━━━━━━━━━━━━━━━━━━━━ 12:06:19\n",
      "Accuracy: 0.9514 - Loss: 0.1484\n",
      "\n",
      "Batch 338/992 ━━━━━━━━━━━━━━━━━━━━ 12:06:30\n",
      "Accuracy: 0.9516 - Loss: 0.1480\n",
      "\n",
      "Batch 339/992 ━━━━━━━━━━━━━━━━━━━━ 12:06:41\n",
      "Accuracy: 0.9517 - Loss: 0.1476\n",
      "\n",
      "Batch 340/992 ━━━━━━━━━━━━━━━━━━━━ 12:06:51\n",
      "Accuracy: 0.9515 - Loss: 0.1482\n",
      "\n",
      "Batch 341/992 ━━━━━━━━━━━━━━━━━━━━ 12:07:02\n",
      "Accuracy: 0.9512 - Loss: 0.1485\n",
      "\n",
      "Batch 342/992 ━━━━━━━━━━━━━━━━━━━━ 12:07:14\n",
      "Accuracy: 0.9514 - Loss: 0.1483\n",
      "\n",
      "Batch 343/992 ━━━━━━━━━━━━━━━━━━━━ 12:07:26\n",
      "Accuracy: 0.9515 - Loss: 0.1481\n",
      "\n",
      "Batch 344/992 ━━━━━━━━━━━━━━━━━━━━ 12:07:38\n",
      "Accuracy: 0.9517 - Loss: 0.1478\n",
      "\n",
      "Batch 345/992 ━━━━━━━━━━━━━━━━━━━━ 12:07:51\n",
      "Accuracy: 0.9518 - Loss: 0.1476\n",
      "\n",
      "Batch 346/992 ━━━━━━━━━━━━━━━━━━━━ 12:08:06\n",
      "Accuracy: 0.9516 - Loss: 0.1481\n",
      "\n",
      "Batch 347/992 ━━━━━━━━━━━━━━━━━━━━ 12:08:18\n",
      "Accuracy: 0.9517 - Loss: 0.1478\n",
      "\n",
      "Batch 348/992 ━━━━━━━━━━━━━━━━━━━━ 12:08:29\n",
      "Accuracy: 0.9515 - Loss: 0.1479\n",
      "\n",
      "Batch 349/992 ━━━━━━━━━━━━━━━━━━━━ 12:08:41\n",
      "Accuracy: 0.9516 - Loss: 0.1476\n",
      "\n",
      "Batch 350/992 ━━━━━━━━━━━━━━━━━━━━ 12:08:52\n",
      "Accuracy: 0.9518 - Loss: 0.1472\n",
      "\n",
      "Batch 351/992 ━━━━━━━━━━━━━━━━━━━━ 12:09:09\n",
      "Accuracy: 0.9516 - Loss: 0.1482\n",
      "\n",
      "Batch 352/992 ━━━━━━━━━━━━━━━━━━━━ 12:09:20\n",
      "Accuracy: 0.9513 - Loss: 0.1482\n",
      "\n",
      "Batch 353/992 ━━━━━━━━━━━━━━━━━━━━ 12:09:31\n",
      "Accuracy: 0.9515 - Loss: 0.1482\n",
      "\n",
      "Batch 354/992 ━━━━━━━━━━━━━━━━━━━━ 12:09:42\n",
      "Accuracy: 0.9516 - Loss: 0.1479\n",
      "\n",
      "Batch 355/992 ━━━━━━━━━━━━━━━━━━━━ 12:09:53\n",
      "Accuracy: 0.9518 - Loss: 0.1476\n",
      "\n",
      "Batch 356/992 ━━━━━━━━━━━━━━━━━━━━ 12:10:03\n",
      "Accuracy: 0.9519 - Loss: 0.1473\n",
      "\n",
      "Batch 357/992 ━━━━━━━━━━━━━━━━━━━━ 12:10:14\n",
      "Accuracy: 0.9520 - Loss: 0.1469\n",
      "\n",
      "Batch 358/992 ━━━━━━━━━━━━━━━━━━━━ 12:10:26\n",
      "Accuracy: 0.9522 - Loss: 0.1467\n",
      "\n",
      "Batch 359/992 ━━━━━━━━━━━━━━━━━━━━ 12:10:36\n",
      "Accuracy: 0.9523 - Loss: 0.1464\n",
      "\n",
      "Batch 360/992 ━━━━━━━━━━━━━━━━━━━━ 12:10:46\n",
      "Accuracy: 0.9524 - Loss: 0.1460\n",
      "\n",
      "Batch 361/992 ━━━━━━━━━━━━━━━━━━━━ 12:10:56\n",
      "Accuracy: 0.9519 - Loss: 0.1467\n",
      "\n",
      "Batch 362/992 ━━━━━━━━━━━━━━━━━━━━ 12:11:07\n",
      "Accuracy: 0.9520 - Loss: 0.1464\n",
      "\n",
      "Batch 363/992 ━━━━━━━━━━━━━━━━━━━━ 12:11:18\n",
      "Accuracy: 0.9518 - Loss: 0.1465\n",
      "\n",
      "Batch 364/992 ━━━━━━━━━━━━━━━━━━━━ 12:11:29\n",
      "Accuracy: 0.9519 - Loss: 0.1461\n",
      "\n",
      "Batch 365/992 ━━━━━━━━━━━━━━━━━━━━ 12:11:41\n",
      "Accuracy: 0.9514 - Loss: 0.1471\n",
      "\n",
      "Batch 366/992 ━━━━━━━━━━━━━━━━━━━━ 12:11:52\n",
      "Accuracy: 0.9512 - Loss: 0.1471\n",
      "\n",
      "Batch 367/992 ━━━━━━━━━━━━━━━━━━━━ 12:12:05\n",
      "Accuracy: 0.9513 - Loss: 0.1469\n",
      "\n",
      "Batch 368/992 ━━━━━━━━━━━━━━━━━━━━ 12:12:17\n",
      "Accuracy: 0.9514 - Loss: 0.1470\n",
      "\n",
      "Batch 369/992 ━━━━━━━━━━━━━━━━━━━━ 12:12:28\n",
      "Accuracy: 0.9516 - Loss: 0.1467\n",
      "\n",
      "Batch 370/992 ━━━━━━━━━━━━━━━━━━━━ 12:12:39\n",
      "Accuracy: 0.9517 - Loss: 0.1464\n",
      "\n",
      "Batch 371/992 ━━━━━━━━━━━━━━━━━━━━ 12:12:50\n",
      "Accuracy: 0.9518 - Loss: 0.1461\n",
      "\n",
      "Batch 372/992 ━━━━━━━━━━━━━━━━━━━━ 12:13:01\n",
      "Accuracy: 0.9516 - Loss: 0.1467\n",
      "\n",
      "Batch 373/992 ━━━━━━━━━━━━━━━━━━━━ 12:13:12\n",
      "Accuracy: 0.9511 - Loss: 0.1471\n",
      "\n",
      "Batch 374/992 ━━━━━━━━━━━━━━━━━━━━ 12:13:22\n",
      "Accuracy: 0.9512 - Loss: 0.1469\n",
      "\n",
      "Batch 375/992 ━━━━━━━━━━━━━━━━━━━━ 12:13:33\n",
      "Accuracy: 0.9507 - Loss: 0.1481\n",
      "\n",
      "Batch 376/992 ━━━━━━━━━━━━━━━━━━━━ 12:13:43\n",
      "Accuracy: 0.9505 - Loss: 0.1493\n",
      "\n",
      "Batch 377/992 ━━━━━━━━━━━━━━━━━━━━ 12:13:55\n",
      "Accuracy: 0.9506 - Loss: 0.1491\n",
      "\n",
      "Batch 378/992 ━━━━━━━━━━━━━━━━━━━━ 12:14:06\n",
      "Accuracy: 0.9507 - Loss: 0.1490\n",
      "\n",
      "Batch 379/992 ━━━━━━━━━━━━━━━━━━━━ 12:14:17\n",
      "Accuracy: 0.9509 - Loss: 0.1489\n",
      "\n",
      "Batch 380/992 ━━━━━━━━━━━━━━━━━━━━ 12:14:27\n",
      "Accuracy: 0.9510 - Loss: 0.1488\n",
      "\n",
      "Batch 381/992 ━━━━━━━━━━━━━━━━━━━━ 12:14:38\n",
      "Accuracy: 0.9511 - Loss: 0.1484\n",
      "\n",
      "Batch 382/992 ━━━━━━━━━━━━━━━━━━━━ 12:14:48\n",
      "Accuracy: 0.9512 - Loss: 0.1484\n",
      "\n",
      "Batch 383/992 ━━━━━━━━━━━━━━━━━━━━ 12:14:59\n",
      "Accuracy: 0.9514 - Loss: 0.1481\n",
      "\n",
      "Batch 384/992 ━━━━━━━━━━━━━━━━━━━━ 12:15:11\n",
      "Accuracy: 0.9515 - Loss: 0.1479\n",
      "\n",
      "Batch 385/992 ━━━━━━━━━━━━━━━━━━━━ 12:15:22\n",
      "Accuracy: 0.9516 - Loss: 0.1477\n",
      "\n",
      "Batch 386/992 ━━━━━━━━━━━━━━━━━━━━ 12:15:35\n",
      "Accuracy: 0.9514 - Loss: 0.1480\n",
      "\n",
      "Batch 387/992 ━━━━━━━━━━━━━━━━━━━━ 12:15:46\n",
      "Accuracy: 0.9512 - Loss: 0.1485\n",
      "\n",
      "Batch 388/992 ━━━━━━━━━━━━━━━━━━━━ 12:15:57\n",
      "Accuracy: 0.9514 - Loss: 0.1481\n",
      "\n",
      "Batch 389/992 ━━━━━━━━━━━━━━━━━━━━ 12:16:08\n",
      "Accuracy: 0.9508 - Loss: 0.1491\n",
      "\n",
      "Batch 390/992 ━━━━━━━━━━━━━━━━━━━━ 12:16:18\n",
      "Accuracy: 0.9506 - Loss: 0.1511\n",
      "\n",
      "Batch 391/992 ━━━━━━━━━━━━━━━━━━━━ 12:16:29\n",
      "Accuracy: 0.9508 - Loss: 0.1507\n",
      "\n",
      "Batch 392/992 ━━━━━━━━━━━━━━━━━━━━ 12:16:40\n",
      "Accuracy: 0.9506 - Loss: 0.1509\n",
      "\n",
      "Batch 393/992 ━━━━━━━━━━━━━━━━━━━━ 12:16:52\n",
      "Accuracy: 0.9504 - Loss: 0.1510\n",
      "\n",
      "Batch 394/992 ━━━━━━━━━━━━━━━━━━━━ 12:17:02\n",
      "Accuracy: 0.9505 - Loss: 0.1509\n",
      "\n",
      "Batch 395/992 ━━━━━━━━━━━━━━━━━━━━ 12:17:13\n",
      "Accuracy: 0.9503 - Loss: 0.1509\n",
      "\n",
      "Batch 396/992 ━━━━━━━━━━━━━━━━━━━━ 12:17:23\n",
      "Accuracy: 0.9501 - Loss: 0.1511\n",
      "\n",
      "Batch 397/992 ━━━━━━━━━━━━━━━━━━━━ 12:17:34\n",
      "Accuracy: 0.9499 - Loss: 0.1518\n",
      "\n",
      "Batch 398/992 ━━━━━━━━━━━━━━━━━━━━ 12:17:45\n",
      "Accuracy: 0.9501 - Loss: 0.1518\n",
      "\n",
      "Batch 399/992 ━━━━━━━━━━━━━━━━━━━━ 12:17:58\n",
      "Accuracy: 0.9499 - Loss: 0.1519\n",
      "\n",
      "Batch 400/992 ━━━━━━━━━━━━━━━━━━━━ 12:18:09\n",
      "Accuracy: 0.9500 - Loss: 0.1516\n",
      "\n",
      "Batch 401/992 ━━━━━━━━━━━━━━━━━━━━ 12:18:20\n",
      "Accuracy: 0.9498 - Loss: 0.1519\n",
      "\n",
      "Batch 402/992 ━━━━━━━━━━━━━━━━━━━━ 12:18:31\n",
      "Accuracy: 0.9499 - Loss: 0.1516\n",
      "\n",
      "Batch 403/992 ━━━━━━━━━━━━━━━━━━━━ 12:18:43\n",
      "Accuracy: 0.9501 - Loss: 0.1513\n",
      "\n",
      "Batch 404/992 ━━━━━━━━━━━━━━━━━━━━ 12:18:54\n",
      "Accuracy: 0.9499 - Loss: 0.1515\n",
      "\n",
      "Batch 405/992 ━━━━━━━━━━━━━━━━━━━━ 12:19:06\n",
      "Accuracy: 0.9500 - Loss: 0.1513\n",
      "\n",
      "Batch 406/992 ━━━━━━━━━━━━━━━━━━━━ 12:19:16\n",
      "Accuracy: 0.9498 - Loss: 0.1514\n",
      "\n",
      "Batch 407/992 ━━━━━━━━━━━━━━━━━━━━ 12:19:27\n",
      "Accuracy: 0.9499 - Loss: 0.1512\n",
      "\n",
      "Batch 408/992 ━━━━━━━━━━━━━━━━━━━━ 12:19:37\n",
      "Accuracy: 0.9498 - Loss: 0.1512\n",
      "\n",
      "Batch 409/992 ━━━━━━━━━━━━━━━━━━━━ 12:19:48\n",
      "Accuracy: 0.9499 - Loss: 0.1510\n",
      "\n",
      "Batch 410/992 ━━━━━━━━━━━━━━━━━━━━ 12:19:59\n",
      "Accuracy: 0.9497 - Loss: 0.1510\n",
      "\n",
      "Batch 411/992 ━━━━━━━━━━━━━━━━━━━━ 12:20:10\n",
      "Accuracy: 0.9498 - Loss: 0.1509\n",
      "\n",
      "Batch 412/992 ━━━━━━━━━━━━━━━━━━━━ 12:20:20\n",
      "Accuracy: 0.9499 - Loss: 0.1505\n",
      "\n",
      "Batch 413/992 ━━━━━━━━━━━━━━━━━━━━ 12:20:31\n",
      "Accuracy: 0.9492 - Loss: 0.1513\n",
      "\n",
      "Batch 414/992 ━━━━━━━━━━━━━━━━━━━━ 12:20:41\n",
      "Accuracy: 0.9490 - Loss: 0.1514\n",
      "\n",
      "Batch 415/992 ━━━━━━━━━━━━━━━━━━━━ 12:20:52\n",
      "Accuracy: 0.9491 - Loss: 0.1512\n",
      "\n",
      "Batch 416/992 ━━━━━━━━━━━━━━━━━━━━ 12:21:02\n",
      "Accuracy: 0.9492 - Loss: 0.1510\n",
      "\n",
      "Batch 417/992 ━━━━━━━━━━━━━━━━━━━━ 12:21:12\n",
      "Accuracy: 0.9484 - Loss: 0.1524\n",
      "\n",
      "Batch 418/992 ━━━━━━━━━━━━━━━━━━━━ 12:21:23\n",
      "Accuracy: 0.9483 - Loss: 0.1527\n",
      "\n",
      "Batch 419/992 ━━━━━━━━━━━━━━━━━━━━ 12:21:34\n",
      "Accuracy: 0.9484 - Loss: 0.1524\n",
      "\n",
      "Batch 420/992 ━━━━━━━━━━━━━━━━━━━━ 12:21:44\n",
      "Accuracy: 0.9485 - Loss: 0.1524\n",
      "\n",
      "Batch 421/992 ━━━━━━━━━━━━━━━━━━━━ 12:21:55\n",
      "Accuracy: 0.9486 - Loss: 0.1521\n",
      "\n",
      "Batch 422/992 ━━━━━━━━━━━━━━━━━━━━ 12:22:06\n",
      "Accuracy: 0.9488 - Loss: 0.1519\n",
      "\n",
      "Batch 423/992 ━━━━━━━━━━━━━━━━━━━━ 12:22:16\n",
      "Accuracy: 0.9486 - Loss: 0.1525\n",
      "\n",
      "Batch 424/992 ━━━━━━━━━━━━━━━━━━━━ 12:22:27\n",
      "Accuracy: 0.9487 - Loss: 0.1521\n",
      "\n",
      "Batch 425/992 ━━━━━━━━━━━━━━━━━━━━ 12:22:37\n",
      "Accuracy: 0.9485 - Loss: 0.1522\n",
      "\n",
      "Batch 426/992 ━━━━━━━━━━━━━━━━━━━━ 12:22:48\n",
      "Accuracy: 0.9487 - Loss: 0.1520\n",
      "\n",
      "Batch 427/992 ━━━━━━━━━━━━━━━━━━━━ 12:22:58\n",
      "Accuracy: 0.9488 - Loss: 0.1517\n",
      "\n",
      "Batch 428/992 ━━━━━━━━━━━━━━━━━━━━ 12:23:08\n",
      "Accuracy: 0.9489 - Loss: 0.1516\n",
      "\n",
      "Batch 429/992 ━━━━━━━━━━━━━━━━━━━━ 12:23:19\n",
      "Accuracy: 0.9490 - Loss: 0.1514\n",
      "\n",
      "Batch 430/992 ━━━━━━━━━━━━━━━━━━━━ 12:23:29\n",
      "Accuracy: 0.9491 - Loss: 0.1511\n",
      "\n",
      "Batch 431/992 ━━━━━━━━━━━━━━━━━━━━ 12:23:39\n",
      "Accuracy: 0.9492 - Loss: 0.1509\n",
      "\n",
      "Batch 432/992 ━━━━━━━━━━━━━━━━━━━━ 12:23:50\n",
      "Accuracy: 0.9494 - Loss: 0.1508\n",
      "\n",
      "Batch 433/992 ━━━━━━━━━━━━━━━━━━━━ 12:24:01\n",
      "Accuracy: 0.9489 - Loss: 0.1513\n",
      "\n",
      "Batch 434/992 ━━━━━━━━━━━━━━━━━━━━ 12:24:13\n",
      "Accuracy: 0.9490 - Loss: 0.1510\n",
      "\n",
      "Batch 435/992 ━━━━━━━━━━━━━━━━━━━━ 12:24:24\n",
      "Accuracy: 0.9491 - Loss: 0.1509\n",
      "\n",
      "Batch 436/992 ━━━━━━━━━━━━━━━━━━━━ 12:24:34\n",
      "Accuracy: 0.9490 - Loss: 0.1514\n",
      "\n",
      "Batch 437/992 ━━━━━━━━━━━━━━━━━━━━ 12:24:45\n",
      "Accuracy: 0.9491 - Loss: 0.1511\n",
      "\n",
      "Batch 438/992 ━━━━━━━━━━━━━━━━━━━━ 12:24:55\n",
      "Accuracy: 0.9492 - Loss: 0.1508\n",
      "\n",
      "Batch 439/992 ━━━━━━━━━━━━━━━━━━━━ 12:25:06\n",
      "Accuracy: 0.9490 - Loss: 0.1516\n",
      "\n",
      "Batch 440/992 ━━━━━━━━━━━━━━━━━━━━ 12:25:17\n",
      "Accuracy: 0.9491 - Loss: 0.1514\n",
      "\n",
      "Batch 441/992 ━━━━━━━━━━━━━━━━━━━━ 12:25:27\n",
      "Accuracy: 0.9487 - Loss: 0.1520\n",
      "\n",
      "Batch 442/992 ━━━━━━━━━━━━━━━━━━━━ 12:25:38\n",
      "Accuracy: 0.9488 - Loss: 0.1518\n",
      "\n",
      "Batch 443/992 ━━━━━━━━━━━━━━━━━━━━ 12:25:48\n",
      "Accuracy: 0.9489 - Loss: 0.1515\n",
      "\n",
      "Batch 444/992 ━━━━━━━━━━━━━━━━━━━━ 12:25:59\n",
      "Accuracy: 0.9490 - Loss: 0.1513\n",
      "\n",
      "Batch 445/992 ━━━━━━━━━━━━━━━━━━━━ 12:26:10\n",
      "Accuracy: 0.9489 - Loss: 0.1516\n",
      "\n",
      "Batch 446/992 ━━━━━━━━━━━━━━━━━━━━ 12:26:22\n",
      "Accuracy: 0.9490 - Loss: 0.1514\n",
      "\n",
      "Batch 447/992 ━━━━━━━━━━━━━━━━━━━━ 12:26:33\n",
      "Accuracy: 0.9491 - Loss: 0.1511\n",
      "\n",
      "Batch 448/992 ━━━━━━━━━━━━━━━━━━━━ 12:26:44\n",
      "Accuracy: 0.9492 - Loss: 0.1509\n",
      "\n",
      "Batch 449/992 ━━━━━━━━━━━━━━━━━━━━ 12:26:55\n",
      "Accuracy: 0.9493 - Loss: 0.1508\n",
      "\n",
      "Batch 450/992 ━━━━━━━━━━━━━━━━━━━━ 12:27:05\n",
      "Accuracy: 0.9494 - Loss: 0.1505\n",
      "\n",
      "Batch 451/992 ━━━━━━━━━━━━━━━━━━━━ 12:27:16\n",
      "Accuracy: 0.9496 - Loss: 0.1505\n",
      "\n",
      "Batch 452/992 ━━━━━━━━━━━━━━━━━━━━ 12:27:26\n",
      "Accuracy: 0.9497 - Loss: 0.1502\n",
      "\n",
      "Batch 453/992 ━━━━━━━━━━━━━━━━━━━━ 12:27:37\n",
      "Accuracy: 0.9498 - Loss: 0.1501\n",
      "\n",
      "Batch 454/992 ━━━━━━━━━━━━━━━━━━━━ 12:27:47\n",
      "Accuracy: 0.9499 - Loss: 0.1498\n",
      "\n",
      "Batch 455/992 ━━━━━━━━━━━━━━━━━━━━ 12:27:59\n",
      "Accuracy: 0.9500 - Loss: 0.1496\n",
      "\n",
      "Batch 456/992 ━━━━━━━━━━━━━━━━━━━━ 12:28:10\n",
      "Accuracy: 0.9498 - Loss: 0.1497\n",
      "\n",
      "Batch 457/992 ━━━━━━━━━━━━━━━━━━━━ 12:28:22\n",
      "Accuracy: 0.9499 - Loss: 0.1495\n",
      "\n",
      "Batch 458/992 ━━━━━━━━━━━━━━━━━━━━ 12:28:34\n",
      "Accuracy: 0.9501 - Loss: 0.1492\n",
      "\n",
      "Batch 459/992 ━━━━━━━━━━━━━━━━━━━━ 12:28:46\n",
      "Accuracy: 0.9502 - Loss: 0.1491\n",
      "\n",
      "Batch 460/992 ━━━━━━━━━━━━━━━━━━━━ 12:28:57\n",
      "Accuracy: 0.9503 - Loss: 0.1488\n",
      "\n",
      "Batch 461/992 ━━━━━━━━━━━━━━━━━━━━ 12:29:09\n",
      "Accuracy: 0.9504 - Loss: 0.1486\n",
      "\n",
      "Batch 462/992 ━━━━━━━━━━━━━━━━━━━━ 12:29:22\n",
      "Accuracy: 0.9505 - Loss: 0.1485\n",
      "\n",
      "Batch 463/992 ━━━━━━━━━━━━━━━━━━━━ 12:29:32\n",
      "Accuracy: 0.9503 - Loss: 0.1485\n",
      "\n",
      "Batch 464/992 ━━━━━━━━━━━━━━━━━━━━ 12:29:42\n",
      "Accuracy: 0.9504 - Loss: 0.1484\n",
      "\n",
      "Batch 465/992 ━━━━━━━━━━━━━━━━━━━━ 12:29:54\n",
      "Accuracy: 0.9503 - Loss: 0.1485\n",
      "\n",
      "Batch 466/992 ━━━━━━━━━━━━━━━━━━━━ 12:30:06\n",
      "Accuracy: 0.9504 - Loss: 0.1484\n",
      "\n",
      "Batch 467/992 ━━━━━━━━━━━━━━━━━━━━ 12:30:16\n",
      "Accuracy: 0.9505 - Loss: 0.1483\n",
      "\n",
      "Batch 468/992 ━━━━━━━━━━━━━━━━━━━━ 12:30:27\n",
      "Accuracy: 0.9506 - Loss: 0.1481\n",
      "\n",
      "Batch 469/992 ━━━━━━━━━━━━━━━━━━━━ 12:30:38\n",
      "Accuracy: 0.9507 - Loss: 0.1481\n",
      "\n",
      "Batch 470/992 ━━━━━━━━━━━━━━━━━━━━ 12:30:48\n",
      "Accuracy: 0.9508 - Loss: 0.1480\n",
      "\n",
      "Batch 471/992 ━━━━━━━━━━━━━━━━━━━━ 12:30:59\n",
      "Accuracy: 0.9509 - Loss: 0.1478\n",
      "\n",
      "Batch 472/992 ━━━━━━━━━━━━━━━━━━━━ 12:31:10\n",
      "Accuracy: 0.9510 - Loss: 0.1477\n",
      "\n",
      "Batch 473/992 ━━━━━━━━━━━━━━━━━━━━ 12:31:20\n",
      "Accuracy: 0.9511 - Loss: 0.1475\n",
      "\n",
      "Batch 474/992 ━━━━━━━━━━━━━━━━━━━━ 12:31:32\n",
      "Accuracy: 0.9509 - Loss: 0.1482\n",
      "\n",
      "Batch 475/992 ━━━━━━━━━━━━━━━━━━━━ 12:31:42\n",
      "Accuracy: 0.9511 - Loss: 0.1480\n",
      "\n",
      "Batch 476/992 ━━━━━━━━━━━━━━━━━━━━ 12:31:53\n",
      "Accuracy: 0.9509 - Loss: 0.1492\n",
      "\n",
      "Batch 477/992 ━━━━━━━━━━━━━━━━━━━━ 12:32:04\n",
      "Accuracy: 0.9510 - Loss: 0.1490\n",
      "\n",
      "Batch 478/992 ━━━━━━━━━━━━━━━━━━━━ 12:32:15\n",
      "Accuracy: 0.9511 - Loss: 0.1488\n",
      "\n",
      "Batch 479/992 ━━━━━━━━━━━━━━━━━━━━ 12:32:25\n",
      "Accuracy: 0.9512 - Loss: 0.1486\n",
      "\n",
      "Batch 480/992 ━━━━━━━━━━━━━━━━━━━━ 12:32:36\n",
      "Accuracy: 0.9513 - Loss: 0.1486\n",
      "\n",
      "Batch 481/992 ━━━━━━━━━━━━━━━━━━━━ 12:32:47\n",
      "Accuracy: 0.9514 - Loss: 0.1484\n",
      "\n",
      "Batch 482/992 ━━━━━━━━━━━━━━━━━━━━ 12:32:57\n",
      "Accuracy: 0.9512 - Loss: 0.1486\n",
      "\n",
      "Batch 483/992 ━━━━━━━━━━━━━━━━━━━━ 12:33:08\n",
      "Accuracy: 0.9511 - Loss: 0.1486\n",
      "\n",
      "Batch 484/992 ━━━━━━━━━━━━━━━━━━━━ 12:33:18\n",
      "Accuracy: 0.9509 - Loss: 0.1486\n",
      "\n",
      "Batch 485/992 ━━━━━━━━━━━━━━━━━━━━ 12:33:28\n",
      "Accuracy: 0.9508 - Loss: 0.1486\n",
      "\n",
      "Batch 486/992 ━━━━━━━━━━━━━━━━━━━━ 12:33:38\n",
      "Accuracy: 0.9509 - Loss: 0.1484\n",
      "\n",
      "Batch 487/992 ━━━━━━━━━━━━━━━━━━━━ 12:33:50\n",
      "Accuracy: 0.9510 - Loss: 0.1481\n",
      "\n",
      "Batch 488/992 ━━━━━━━━━━━━━━━━━━━━ 12:34:03\n",
      "Accuracy: 0.9511 - Loss: 0.1478\n",
      "\n",
      "Batch 489/992 ━━━━━━━━━━━━━━━━━━━━ 12:34:14\n",
      "Accuracy: 0.9507 - Loss: 0.1488\n",
      "\n",
      "Batch 490/992 ━━━━━━━━━━━━━━━━━━━━ 12:34:25\n",
      "Accuracy: 0.9508 - Loss: 0.1488\n",
      "\n",
      "Batch 491/992 ━━━━━━━━━━━━━━━━━━━━ 12:34:37\n",
      "Accuracy: 0.9509 - Loss: 0.1486\n",
      "\n",
      "Batch 492/992 ━━━━━━━━━━━━━━━━━━━━ 12:34:52\n",
      "Accuracy: 0.9507 - Loss: 0.1487\n",
      "\n",
      "Batch 493/992 ━━━━━━━━━━━━━━━━━━━━ 12:35:06\n",
      "Accuracy: 0.9508 - Loss: 0.1485\n",
      "\n",
      "Batch 494/992 ━━━━━━━━━━━━━━━━━━━━ 12:35:19\n",
      "Accuracy: 0.9507 - Loss: 0.1487\n",
      "\n",
      "Batch 495/992 ━━━━━━━━━━━━━━━━━━━━ 12:35:31\n",
      "Accuracy: 0.9508 - Loss: 0.1484\n",
      "\n",
      "Batch 496/992 ━━━━━━━━━━━━━━━━━━━━ 12:35:44\n",
      "Accuracy: 0.9509 - Loss: 0.1483\n",
      "\n",
      "Batch 497/992 ━━━━━━━━━━━━━━━━━━━━ 12:35:55\n",
      "Accuracy: 0.9510 - Loss: 0.1480\n",
      "\n",
      "Batch 498/992 ━━━━━━━━━━━━━━━━━━━━ 12:36:08\n",
      "Accuracy: 0.9511 - Loss: 0.1478\n",
      "\n",
      "Batch 499/992 ━━━━━━━━━━━━━━━━━━━━ 12:36:23\n",
      "Accuracy: 0.9509 - Loss: 0.1479\n",
      "\n",
      "Batch 500/992 ━━━━━━━━━━━━━━━━━━━━ 12:36:36\n",
      "Accuracy: 0.9510 - Loss: 0.1477\n",
      "\n",
      "Batch 501/992 ━━━━━━━━━━━━━━━━━━━━ 12:36:48\n",
      "Accuracy: 0.9511 - Loss: 0.1475\n",
      "\n",
      "Batch 502/992 ━━━━━━━━━━━━━━━━━━━━ 12:36:59\n",
      "Accuracy: 0.9509 - Loss: 0.1477\n",
      "\n",
      "Batch 503/992 ━━━━━━━━━━━━━━━━━━━━ 12:37:12\n",
      "Accuracy: 0.9510 - Loss: 0.1475\n",
      "\n",
      "Batch 504/992 ━━━━━━━━━━━━━━━━━━━━ 12:37:26\n",
      "Accuracy: 0.9506 - Loss: 0.1482\n",
      "\n",
      "Batch 505/992 ━━━━━━━━━━━━━━━━━━━━ 12:37:39\n",
      "Accuracy: 0.9505 - Loss: 0.1487\n",
      "\n",
      "Batch 506/992 ━━━━━━━━━━━━━━━━━━━━ 12:37:51\n",
      "Accuracy: 0.9503 - Loss: 0.1486\n",
      "\n",
      "Batch 507/992 ━━━━━━━━━━━━━━━━━━━━ 12:38:05\n",
      "Accuracy: 0.9502 - Loss: 0.1490\n",
      "\n",
      "Batch 508/992 ━━━━━━━━━━━━━━━━━━━━ 12:38:19\n",
      "Accuracy: 0.9503 - Loss: 0.1489\n",
      "\n",
      "Batch 509/992 ━━━━━━━━━━━━━━━━━━━━ 12:38:31\n",
      "Accuracy: 0.9501 - Loss: 0.1491\n",
      "\n",
      "Batch 510/992 ━━━━━━━━━━━━━━━━━━━━ 12:38:42\n",
      "Accuracy: 0.9502 - Loss: 0.1491\n",
      "\n",
      "Batch 511/992 ━━━━━━━━━━━━━━━━━━━━ 12:38:53\n",
      "Accuracy: 0.9503 - Loss: 0.1491\n",
      "\n",
      "Batch 512/992 ━━━━━━━━━━━━━━━━━━━━ 12:39:05\n",
      "Accuracy: 0.9504 - Loss: 0.1488\n",
      "\n",
      "Batch 513/992 ━━━━━━━━━━━━━━━━━━━━ 12:39:17\n",
      "Accuracy: 0.9505 - Loss: 0.1488\n",
      "\n",
      "Batch 514/992 ━━━━━━━━━━━━━━━━━━━━ 12:39:27\n",
      "Accuracy: 0.9506 - Loss: 0.1486\n",
      "\n",
      "Batch 515/992 ━━━━━━━━━━━━━━━━━━━━ 12:39:37\n",
      "Accuracy: 0.9507 - Loss: 0.1484\n",
      "\n",
      "Batch 516/992 ━━━━━━━━━━━━━━━━━━━━ 12:39:48\n",
      "Accuracy: 0.9506 - Loss: 0.1484\n",
      "\n",
      "Batch 517/992 ━━━━━━━━━━━━━━━━━━━━ 12:39:58\n",
      "Accuracy: 0.9502 - Loss: 0.1485\n",
      "\n",
      "Batch 518/992 ━━━━━━━━━━━━━━━━━━━━ 12:40:08\n",
      "Accuracy: 0.9503 - Loss: 0.1484\n",
      "\n",
      "Batch 519/992 ━━━━━━━━━━━━━━━━━━━━ 12:40:18\n",
      "Accuracy: 0.9504 - Loss: 0.1483\n",
      "\n",
      "Batch 520/992 ━━━━━━━━━━━━━━━━━━━━ 12:40:28\n",
      "Accuracy: 0.9505 - Loss: 0.1481\n",
      "\n",
      "Batch 521/992 ━━━━━━━━━━━━━━━━━━━━ 12:40:38\n",
      "Accuracy: 0.9506 - Loss: 0.1482\n",
      "\n",
      "Batch 522/992 ━━━━━━━━━━━━━━━━━━━━ 12:40:49\n",
      "Accuracy: 0.9507 - Loss: 0.1479\n",
      "\n",
      "Batch 523/992 ━━━━━━━━━━━━━━━━━━━━ 12:40:59\n",
      "Accuracy: 0.9505 - Loss: 0.1480\n",
      "\n",
      "Batch 524/992 ━━━━━━━━━━━━━━━━━━━━ 12:41:09\n",
      "Accuracy: 0.9506 - Loss: 0.1478\n",
      "\n",
      "Batch 525/992 ━━━━━━━━━━━━━━━━━━━━ 12:41:19\n",
      "Accuracy: 0.9507 - Loss: 0.1476\n",
      "\n",
      "Batch 526/992 ━━━━━━━━━━━━━━━━━━━━ 12:41:29\n",
      "Accuracy: 0.9508 - Loss: 0.1475\n",
      "\n",
      "Batch 527/992 ━━━━━━━━━━━━━━━━━━━━ 12:41:39\n",
      "Accuracy: 0.9507 - Loss: 0.1482\n",
      "\n",
      "Batch 528/992 ━━━━━━━━━━━━━━━━━━━━ 12:41:49\n",
      "Accuracy: 0.9508 - Loss: 0.1480\n",
      "\n",
      "Batch 529/992 ━━━━━━━━━━━━━━━━━━━━ 12:41:59\n",
      "Accuracy: 0.9509 - Loss: 0.1478\n",
      "\n",
      "Batch 530/992 ━━━━━━━━━━━━━━━━━━━━ 12:42:09\n",
      "Accuracy: 0.9507 - Loss: 0.1481\n",
      "\n",
      "Batch 531/992 ━━━━━━━━━━━━━━━━━━━━ 12:42:20\n",
      "Accuracy: 0.9508 - Loss: 0.1480\n",
      "\n",
      "Batch 532/992 ━━━━━━━━━━━━━━━━━━━━ 12:42:30\n",
      "Accuracy: 0.9509 - Loss: 0.1479\n",
      "\n",
      "Batch 533/992 ━━━━━━━━━━━━━━━━━━━━ 12:42:40\n",
      "Accuracy: 0.9508 - Loss: 0.1481\n",
      "\n",
      "Batch 534/992 ━━━━━━━━━━━━━━━━━━━━ 12:42:50\n",
      "Accuracy: 0.9508 - Loss: 0.1478\n",
      "\n",
      "Batch 535/992 ━━━━━━━━━━━━━━━━━━━━ 12:43:00\n",
      "Accuracy: 0.9509 - Loss: 0.1478\n",
      "\n",
      "Batch 536/992 ━━━━━━━━━━━━━━━━━━━━ 12:43:11\n",
      "Accuracy: 0.9508 - Loss: 0.1479\n",
      "\n",
      "Batch 537/992 ━━━━━━━━━━━━━━━━━━━━ 12:43:21\n",
      "Accuracy: 0.9507 - Loss: 0.1482\n",
      "\n",
      "Batch 538/992 ━━━━━━━━━━━━━━━━━━━━ 12:43:31\n",
      "Accuracy: 0.9507 - Loss: 0.1480\n",
      "\n",
      "Batch 539/992 ━━━━━━━━━━━━━━━━━━━━ 12:43:41\n",
      "Accuracy: 0.9508 - Loss: 0.1479\n",
      "\n",
      "Batch 540/992 ━━━━━━━━━━━━━━━━━━━━ 12:43:51\n",
      "Accuracy: 0.9509 - Loss: 0.1478\n",
      "\n",
      "Batch 541/992 ━━━━━━━━━━━━━━━━━━━━ 12:44:02\n",
      "Accuracy: 0.9510 - Loss: 0.1475\n",
      "\n",
      "Batch 542/992 ━━━━━━━━━━━━━━━━━━━━ 12:44:12\n",
      "Accuracy: 0.9509 - Loss: 0.1478\n",
      "\n",
      "Batch 543/992 ━━━━━━━━━━━━━━━━━━━━ 12:44:22\n",
      "Accuracy: 0.9510 - Loss: 0.1477\n",
      "\n",
      "Batch 544/992 ━━━━━━━━━━━━━━━━━━━━ 12:44:32\n",
      "Accuracy: 0.9511 - Loss: 0.1476\n",
      "\n",
      "Batch 545/992 ━━━━━━━━━━━━━━━━━━━━ 12:44:42\n",
      "Accuracy: 0.9509 - Loss: 0.1478\n",
      "\n",
      "Batch 546/992 ━━━━━━━━━━━━━━━━━━━━ 12:44:52\n",
      "Accuracy: 0.9510 - Loss: 0.1476\n",
      "\n",
      "Batch 547/992 ━━━━━━━━━━━━━━━━━━━━ 12:45:03\n",
      "Accuracy: 0.9511 - Loss: 0.1474\n",
      "\n",
      "Batch 548/992 ━━━━━━━━━━━━━━━━━━━━ 12:45:13\n",
      "Accuracy: 0.9512 - Loss: 0.1471\n",
      "\n",
      "Batch 549/992 ━━━━━━━━━━━━━━━━━━━━ 12:45:23\n",
      "Accuracy: 0.9513 - Loss: 0.1469\n",
      "\n",
      "Batch 550/992 ━━━━━━━━━━━━━━━━━━━━ 12:45:33\n",
      "Accuracy: 0.9514 - Loss: 0.1467\n",
      "\n",
      "Batch 551/992 ━━━━━━━━━━━━━━━━━━━━ 12:45:43\n",
      "Accuracy: 0.9515 - Loss: 0.1465\n",
      "\n",
      "Batch 552/992 ━━━━━━━━━━━━━━━━━━━━ 12:45:53\n",
      "Accuracy: 0.9515 - Loss: 0.1463\n",
      "\n",
      "Batch 553/992 ━━━━━━━━━━━━━━━━━━━━ 12:46:04\n",
      "Accuracy: 0.9514 - Loss: 0.1468\n",
      "\n",
      "Batch 554/992 ━━━━━━━━━━━━━━━━━━━━ 12:46:14\n",
      "Accuracy: 0.9513 - Loss: 0.1469\n",
      "\n",
      "Batch 555/992 ━━━━━━━━━━━━━━━━━━━━ 12:46:24\n",
      "Accuracy: 0.9514 - Loss: 0.1466\n",
      "\n",
      "Batch 556/992 ━━━━━━━━━━━━━━━━━━━━ 12:46:34\n",
      "Accuracy: 0.9514 - Loss: 0.1465\n",
      "\n",
      "Batch 557/992 ━━━━━━━━━━━━━━━━━━━━ 12:46:44\n",
      "Accuracy: 0.9513 - Loss: 0.1464\n",
      "\n",
      "Batch 558/992 ━━━━━━━━━━━━━━━━━━━━ 12:46:55\n",
      "Accuracy: 0.9512 - Loss: 0.1466\n",
      "\n",
      "Batch 559/992 ━━━━━━━━━━━━━━━━━━━━ 12:47:06\n",
      "Accuracy: 0.9510 - Loss: 0.1469\n",
      "\n",
      "Batch 560/992 ━━━━━━━━━━━━━━━━━━━━ 12:47:16\n",
      "Accuracy: 0.9511 - Loss: 0.1467\n",
      "\n",
      "Batch 561/992 ━━━━━━━━━━━━━━━━━━━━ 12:47:26\n",
      "Accuracy: 0.9510 - Loss: 0.1467\n",
      "\n",
      "Batch 562/992 ━━━━━━━━━━━━━━━━━━━━ 12:47:39\n",
      "Accuracy: 0.9508 - Loss: 0.1467\n",
      "\n",
      "Batch 563/992 ━━━━━━━━━━━━━━━━━━━━ 12:47:54\n",
      "Accuracy: 0.9505 - Loss: 0.1473\n",
      "\n",
      "Batch 564/992 ━━━━━━━━━━━━━━━━━━━━ 12:48:06\n",
      "Accuracy: 0.9504 - Loss: 0.1474\n",
      "\n",
      "Batch 565/992 ━━━━━━━━━━━━━━━━━━━━ 12:48:17\n",
      "Accuracy: 0.9504 - Loss: 0.1473\n",
      "\n",
      "Batch 566/992 ━━━━━━━━━━━━━━━━━━━━ 12:48:28\n",
      "Accuracy: 0.9503 - Loss: 0.1475\n",
      "\n",
      "Batch 567/992 ━━━━━━━━━━━━━━━━━━━━ 12:48:39\n",
      "Accuracy: 0.9502 - Loss: 0.1476\n",
      "\n",
      "Batch 568/992 ━━━━━━━━━━━━━━━━━━━━ 12:48:49\n",
      "Accuracy: 0.9503 - Loss: 0.1474\n",
      "\n",
      "Batch 569/992 ━━━━━━━━━━━━━━━━━━━━ 12:49:00\n",
      "Accuracy: 0.9504 - Loss: 0.1472\n",
      "\n",
      "Batch 570/992 ━━━━━━━━━━━━━━━━━━━━ 12:49:11\n",
      "Accuracy: 0.9502 - Loss: 0.1471\n",
      "\n",
      "Batch 571/992 ━━━━━━━━━━━━━━━━━━━━ 12:49:22\n",
      "Accuracy: 0.9503 - Loss: 0.1470\n",
      "\n",
      "Batch 572/992 ━━━━━━━━━━━━━━━━━━━━ 12:49:32\n",
      "Accuracy: 0.9504 - Loss: 0.1470\n",
      "\n",
      "Batch 573/992 ━━━━━━━━━━━━━━━━━━━━ 12:49:45\n",
      "Accuracy: 0.9503 - Loss: 0.1471\n",
      "\n",
      "Batch 574/992 ━━━━━━━━━━━━━━━━━━━━ 12:49:56\n",
      "Accuracy: 0.9503 - Loss: 0.1469\n",
      "\n",
      "Batch 575/992 ━━━━━━━━━━━━━━━━━━━━ 12:50:06\n",
      "Accuracy: 0.9504 - Loss: 0.1467\n",
      "\n",
      "Batch 576/992 ━━━━━━━━━━━━━━━━━━━━ 12:50:17\n",
      "Accuracy: 0.9503 - Loss: 0.1469\n",
      "\n",
      "Batch 577/992 ━━━━━━━━━━━━━━━━━━━━ 12:50:28\n",
      "Accuracy: 0.9504 - Loss: 0.1468\n",
      "\n",
      "Batch 578/992 ━━━━━━━━━━━━━━━━━━━━ 12:50:38\n",
      "Accuracy: 0.9505 - Loss: 0.1467\n",
      "\n",
      "Batch 579/992 ━━━━━━━━━━━━━━━━━━━━ 12:50:49\n",
      "Accuracy: 0.9506 - Loss: 0.1465\n",
      "\n",
      "Batch 580/992 ━━━━━━━━━━━━━━━━━━━━ 12:51:00\n",
      "Accuracy: 0.9504 - Loss: 0.1469\n",
      "\n",
      "Batch 581/992 ━━━━━━━━━━━━━━━━━━━━ 12:51:11\n",
      "Accuracy: 0.9503 - Loss: 0.1470\n",
      "\n",
      "Batch 582/992 ━━━━━━━━━━━━━━━━━━━━ 12:51:21\n",
      "Accuracy: 0.9502 - Loss: 0.1476\n",
      "\n",
      "Batch 583/992 ━━━━━━━━━━━━━━━━━━━━ 12:51:32\n",
      "Accuracy: 0.9500 - Loss: 0.1482\n",
      "\n",
      "Batch 584/992 ━━━━━━━━━━━━━━━━━━━━ 12:51:43\n",
      "Accuracy: 0.9495 - Loss: 0.1493\n",
      "\n",
      "Batch 585/992 ━━━━━━━━━━━━━━━━━━━━ 12:51:53\n",
      "Accuracy: 0.9494 - Loss: 0.1497\n",
      "\n",
      "Batch 586/992 ━━━━━━━━━━━━━━━━━━━━ 12:52:05\n",
      "Accuracy: 0.9494 - Loss: 0.1495\n",
      "\n",
      "Batch 587/992 ━━━━━━━━━━━━━━━━━━━━ 12:52:17\n",
      "Accuracy: 0.9493 - Loss: 0.1497\n",
      "\n",
      "Batch 588/992 ━━━━━━━━━━━━━━━━━━━━ 12:52:28\n",
      "Accuracy: 0.9492 - Loss: 0.1497\n",
      "\n",
      "Batch 589/992 ━━━━━━━━━━━━━━━━━━━━ 12:52:39\n",
      "Accuracy: 0.9493 - Loss: 0.1496\n",
      "\n",
      "Batch 590/992 ━━━━━━━━━━━━━━━━━━━━ 12:52:50\n",
      "Accuracy: 0.9492 - Loss: 0.1498\n",
      "\n",
      "Batch 591/992 ━━━━━━━━━━━━━━━━━━━━ 12:53:01\n",
      "Accuracy: 0.9492 - Loss: 0.1496\n",
      "\n",
      "Batch 592/992 ━━━━━━━━━━━━━━━━━━━━ 12:53:11\n",
      "Accuracy: 0.9493 - Loss: 0.1494\n",
      "\n",
      "Batch 593/992 ━━━━━━━━━━━━━━━━━━━━ 12:53:22\n",
      "Accuracy: 0.9492 - Loss: 0.1496\n",
      "\n",
      "Batch 594/992 ━━━━━━━━━━━━━━━━━━━━ 12:53:33\n",
      "Accuracy: 0.9493 - Loss: 0.1495\n",
      "\n",
      "Batch 595/992 ━━━━━━━━━━━━━━━━━━━━ 12:53:44\n",
      "Accuracy: 0.9489 - Loss: 0.1501\n",
      "\n",
      "Batch 596/992 ━━━━━━━━━━━━━━━━━━━━ 12:53:55\n",
      "Accuracy: 0.9490 - Loss: 0.1499\n",
      "\n",
      "Batch 597/992 ━━━━━━━━━━━━━━━━━━━━ 12:54:05\n",
      "Accuracy: 0.9491 - Loss: 0.1499\n",
      "\n",
      "Batch 598/992 ━━━━━━━━━━━━━━━━━━━━ 12:54:16\n",
      "Accuracy: 0.9490 - Loss: 0.1499\n",
      "\n",
      "Batch 599/992 ━━━━━━━━━━━━━━━━━━━━ 12:54:27\n",
      "Accuracy: 0.9491 - Loss: 0.1497\n",
      "\n",
      "Batch 600/992 ━━━━━━━━━━━━━━━━━━━━ 12:54:37\n",
      "Accuracy: 0.9492 - Loss: 0.1496\n",
      "\n",
      "Batch 601/992 ━━━━━━━━━━━━━━━━━━━━ 12:54:48\n",
      "Accuracy: 0.9490 - Loss: 0.1502\n",
      "\n",
      "Batch 602/992 ━━━━━━━━━━━━━━━━━━━━ 12:54:59\n",
      "Accuracy: 0.9491 - Loss: 0.1502\n",
      "\n",
      "Batch 603/992 ━━━━━━━━━━━━━━━━━━━━ 12:55:10\n",
      "Accuracy: 0.9490 - Loss: 0.1503\n",
      "\n",
      "Batch 604/992 ━━━━━━━━━━━━━━━━━━━━ 12:55:21\n",
      "Accuracy: 0.9491 - Loss: 0.1503\n",
      "\n",
      "Batch 605/992 ━━━━━━━━━━━━━━━━━━━━ 12:55:35\n",
      "Accuracy: 0.9492 - Loss: 0.1501\n",
      "\n",
      "Batch 606/992 ━━━━━━━━━━━━━━━━━━━━ 12:55:47\n",
      "Accuracy: 0.9493 - Loss: 0.1499\n",
      "\n",
      "Batch 607/992 ━━━━━━━━━━━━━━━━━━━━ 12:55:57\n",
      "Accuracy: 0.9493 - Loss: 0.1498\n",
      "\n",
      "Batch 608/992 ━━━━━━━━━━━━━━━━━━━━ 12:56:09\n",
      "Accuracy: 0.9492 - Loss: 0.1498\n",
      "\n",
      "Batch 609/992 ━━━━━━━━━━━━━━━━━━━━ 12:56:20\n",
      "Accuracy: 0.9493 - Loss: 0.1496\n",
      "\n",
      "Batch 610/992 ━━━━━━━━━━━━━━━━━━━━ 12:56:31\n",
      "Accuracy: 0.9492 - Loss: 0.1498\n",
      "\n",
      "Batch 611/992 ━━━━━━━━━━━━━━━━━━━━ 12:56:41\n",
      "Accuracy: 0.9491 - Loss: 0.1499\n",
      "\n",
      "Batch 612/992 ━━━━━━━━━━━━━━━━━━━━ 12:56:52\n",
      "Accuracy: 0.9491 - Loss: 0.1498\n",
      "\n",
      "Batch 613/992 ━━━━━━━━━━━━━━━━━━━━ 12:57:03\n",
      "Accuracy: 0.9490 - Loss: 0.1500\n",
      "\n",
      "Batch 614/992 ━━━━━━━━━━━━━━━━━━━━ 12:57:13\n",
      "Accuracy: 0.9489 - Loss: 0.1502\n",
      "\n",
      "Batch 615/992 ━━━━━━━━━━━━━━━━━━━━ 12:57:24\n",
      "Accuracy: 0.9488 - Loss: 0.1503\n",
      "\n",
      "Batch 616/992 ━━━━━━━━━━━━━━━━━━━━ 12:57:34\n",
      "Accuracy: 0.9487 - Loss: 0.1505\n",
      "\n",
      "Batch 617/992 ━━━━━━━━━━━━━━━━━━━━ 12:57:45\n",
      "Accuracy: 0.9487 - Loss: 0.1503\n",
      "\n",
      "Batch 618/992 ━━━━━━━━━━━━━━━━━━━━ 12:57:57\n",
      "Accuracy: 0.9488 - Loss: 0.1501\n",
      "\n",
      "Batch 619/992 ━━━━━━━━━━━━━━━━━━━━ 12:58:08\n",
      "Accuracy: 0.9489 - Loss: 0.1499\n",
      "\n",
      "Batch 620/992 ━━━━━━━━━━━━━━━━━━━━ 12:58:18\n",
      "Accuracy: 0.9490 - Loss: 0.1497\n",
      "\n",
      "Batch 621/992 ━━━━━━━━━━━━━━━━━━━━ 12:58:29\n",
      "Accuracy: 0.9487 - Loss: 0.1503\n",
      "\n",
      "Batch 622/992 ━━━━━━━━━━━━━━━━━━━━ 12:58:40\n",
      "Accuracy: 0.9486 - Loss: 0.1505\n",
      "\n",
      "Batch 623/992 ━━━━━━━━━━━━━━━━━━━━ 12:58:51\n",
      "Accuracy: 0.9486 - Loss: 0.1505\n",
      "\n",
      "Batch 624/992 ━━━━━━━━━━━━━━━━━━━━ 12:59:01\n",
      "Accuracy: 0.9487 - Loss: 0.1503\n",
      "\n",
      "Batch 625/992 ━━━━━━━━━━━━━━━━━━━━ 12:59:12\n",
      "Accuracy: 0.9488 - Loss: 0.1501\n",
      "\n",
      "Batch 626/992 ━━━━━━━━━━━━━━━━━━━━ 12:59:25\n",
      "Accuracy: 0.9489 - Loss: 0.1499\n",
      "\n",
      "Batch 627/992 ━━━━━━━━━━━━━━━━━━━━ 12:59:36\n",
      "Accuracy: 0.9488 - Loss: 0.1506\n",
      "\n",
      "Batch 628/992 ━━━━━━━━━━━━━━━━━━━━ 12:59:48\n",
      "Accuracy: 0.9486 - Loss: 0.1508\n",
      "\n",
      "Batch 629/992 ━━━━━━━━━━━━━━━━━━━━ 12:59:58\n",
      "Accuracy: 0.9487 - Loss: 0.1507\n",
      "\n",
      "Batch 630/992 ━━━━━━━━━━━━━━━━━━━━ 13:00:09\n",
      "Accuracy: 0.9488 - Loss: 0.1506\n",
      "\n",
      "Batch 631/992 ━━━━━━━━━━━━━━━━━━━━ 13:00:20\n",
      "Accuracy: 0.9489 - Loss: 0.1506\n",
      "\n",
      "Batch 632/992 ━━━━━━━━━━━━━━━━━━━━ 13:00:31\n",
      "Accuracy: 0.9490 - Loss: 0.1504\n",
      "\n",
      "Batch 633/992 ━━━━━━━━━━━━━━━━━━━━ 13:00:42\n",
      "Accuracy: 0.9491 - Loss: 0.1503\n",
      "\n",
      "Batch 634/992 ━━━━━━━━━━━━━━━━━━━━ 13:00:53\n",
      "Accuracy: 0.9491 - Loss: 0.1502\n",
      "\n",
      "Batch 635/992 ━━━━━━━━━━━━━━━━━━━━ 13:01:03\n",
      "Accuracy: 0.9492 - Loss: 0.1499\n",
      "\n",
      "Batch 636/992 ━━━━━━━━━━━━━━━━━━━━ 13:01:14\n",
      "Accuracy: 0.9491 - Loss: 0.1504\n",
      "\n",
      "Batch 637/992 ━━━━━━━━━━━━━━━━━━━━ 13:01:25\n",
      "Accuracy: 0.9492 - Loss: 0.1504\n",
      "\n",
      "Batch 638/992 ━━━━━━━━━━━━━━━━━━━━ 13:01:36\n",
      "Accuracy: 0.9493 - Loss: 0.1503\n",
      "\n",
      "Batch 639/992 ━━━━━━━━━━━━━━━━━━━━ 13:01:47\n",
      "Accuracy: 0.9493 - Loss: 0.1503\n",
      "\n",
      "Batch 640/992 ━━━━━━━━━━━━━━━━━━━━ 13:02:00\n",
      "Accuracy: 0.9494 - Loss: 0.1501\n",
      "\n",
      "Batch 641/992 ━━━━━━━━━━━━━━━━━━━━ 13:02:11\n",
      "Accuracy: 0.9495 - Loss: 0.1503\n",
      "\n",
      "Batch 642/992 ━━━━━━━━━━━━━━━━━━━━ 13:02:21\n",
      "Accuracy: 0.9496 - Loss: 0.1503\n",
      "\n",
      "Batch 643/992 ━━━━━━━━━━━━━━━━━━━━ 13:02:32\n",
      "Accuracy: 0.9497 - Loss: 0.1501\n",
      "\n",
      "Batch 644/992 ━━━━━━━━━━━━━━━━━━━━ 13:02:43\n",
      "Accuracy: 0.9497 - Loss: 0.1499\n",
      "\n",
      "Batch 645/992 ━━━━━━━━━━━━━━━━━━━━ 13:02:53\n",
      "Accuracy: 0.9498 - Loss: 0.1498\n",
      "\n",
      "Batch 646/992 ━━━━━━━━━━━━━━━━━━━━ 13:03:03\n",
      "Accuracy: 0.9499 - Loss: 0.1499\n",
      "\n",
      "Batch 647/992 ━━━━━━━━━━━━━━━━━━━━ 13:03:14\n",
      "Accuracy: 0.9500 - Loss: 0.1498\n",
      "\n",
      "Batch 648/992 ━━━━━━━━━━━━━━━━━━━━ 13:03:25\n",
      "Accuracy: 0.9500 - Loss: 0.1497\n",
      "\n",
      "Batch 649/992 ━━━━━━━━━━━━━━━━━━━━ 13:03:37\n",
      "Accuracy: 0.9501 - Loss: 0.1496\n",
      "\n",
      "Batch 650/992 ━━━━━━━━━━━━━━━━━━━━ 13:03:48\n",
      "Accuracy: 0.9502 - Loss: 0.1494\n",
      "\n",
      "Batch 651/992 ━━━━━━━━━━━━━━━━━━━━ 13:03:59\n",
      "Accuracy: 0.9501 - Loss: 0.1498\n",
      "\n",
      "Batch 652/992 ━━━━━━━━━━━━━━━━━━━━ 13:04:10\n",
      "Accuracy: 0.9500 - Loss: 0.1500\n",
      "\n",
      "Batch 653/992 ━━━━━━━━━━━━━━━━━━━━ 13:04:21\n",
      "Accuracy: 0.9498 - Loss: 0.1500\n",
      "\n",
      "Batch 654/992 ━━━━━━━━━━━━━━━━━━━━ 13:04:31\n",
      "Accuracy: 0.9499 - Loss: 0.1498\n",
      "\n",
      "Batch 655/992 ━━━━━━━━━━━━━━━━━━━━ 13:04:42\n",
      "Accuracy: 0.9500 - Loss: 0.1495\n",
      "\n",
      "Batch 656/992 ━━━━━━━━━━━━━━━━━━━━ 13:04:53\n",
      "Accuracy: 0.9501 - Loss: 0.1495\n",
      "\n",
      "Batch 657/992 ━━━━━━━━━━━━━━━━━━━━ 13:05:04\n",
      "Accuracy: 0.9502 - Loss: 0.1493\n",
      "\n",
      "Batch 658/992 ━━━━━━━━━━━━━━━━━━━━ 13:05:15\n",
      "Accuracy: 0.9500 - Loss: 0.1493\n",
      "\n",
      "Batch 659/992 ━━━━━━━━━━━━━━━━━━━━ 13:05:25\n",
      "Accuracy: 0.9501 - Loss: 0.1491\n",
      "\n",
      "Batch 660/992 ━━━━━━━━━━━━━━━━━━━━ 13:05:36\n",
      "Accuracy: 0.9502 - Loss: 0.1489\n",
      "\n",
      "Batch 661/992 ━━━━━━━━━━━━━━━━━━━━ 13:05:46\n",
      "Accuracy: 0.9501 - Loss: 0.1494\n",
      "\n",
      "Batch 662/992 ━━━━━━━━━━━━━━━━━━━━ 13:05:57\n",
      "Accuracy: 0.9498 - Loss: 0.1499\n",
      "\n",
      "Batch 663/992 ━━━━━━━━━━━━━━━━━━━━ 13:06:08\n",
      "Accuracy: 0.9498 - Loss: 0.1499\n",
      "\n",
      "Batch 664/992 ━━━━━━━━━━━━━━━━━━━━ 13:06:19\n",
      "Accuracy: 0.9499 - Loss: 0.1497\n",
      "\n",
      "Batch 665/992 ━━━━━━━━━━━━━━━━━━━━ 13:06:30\n",
      "Accuracy: 0.9500 - Loss: 0.1495\n",
      "\n",
      "Batch 666/992 ━━━━━━━━━━━━━━━━━━━━ 13:06:40\n",
      "Accuracy: 0.9501 - Loss: 0.1493\n",
      "\n",
      "Batch 667/992 ━━━━━━━━━━━━━━━━━━━━ 13:06:50\n",
      "Accuracy: 0.9501 - Loss: 0.1493\n",
      "\n",
      "Batch 668/992 ━━━━━━━━━━━━━━━━━━━━ 13:07:01\n",
      "Accuracy: 0.9500 - Loss: 0.1498\n",
      "\n",
      "Batch 669/992 ━━━━━━━━━━━━━━━━━━━━ 13:07:12\n",
      "Accuracy: 0.9501 - Loss: 0.1497\n",
      "\n",
      "Batch 670/992 ━━━━━━━━━━━━━━━━━━━━ 13:07:22\n",
      "Accuracy: 0.9500 - Loss: 0.1497\n",
      "\n",
      "Batch 671/992 ━━━━━━━━━━━━━━━━━━━━ 13:07:33\n",
      "Accuracy: 0.9499 - Loss: 0.1499\n",
      "\n",
      "Batch 672/992 ━━━━━━━━━━━━━━━━━━━━ 13:07:43\n",
      "Accuracy: 0.9498 - Loss: 0.1501\n",
      "\n",
      "Batch 673/992 ━━━━━━━━━━━━━━━━━━━━ 13:07:54\n",
      "Accuracy: 0.9499 - Loss: 0.1499\n",
      "\n",
      "Batch 674/992 ━━━━━━━━━━━━━━━━━━━━ 13:08:07\n",
      "Accuracy: 0.9499 - Loss: 0.1497\n",
      "\n",
      "Batch 675/992 ━━━━━━━━━━━━━━━━━━━━ 13:08:18\n",
      "Accuracy: 0.9500 - Loss: 0.1496\n",
      "\n",
      "Batch 676/992 ━━━━━━━━━━━━━━━━━━━━ 13:08:28\n",
      "Accuracy: 0.9501 - Loss: 0.1493\n",
      "\n",
      "Batch 677/992 ━━━━━━━━━━━━━━━━━━━━ 13:08:38\n",
      "Accuracy: 0.9501 - Loss: 0.1492\n",
      "\n",
      "Batch 678/992 ━━━━━━━━━━━━━━━━━━━━ 13:08:48\n",
      "Accuracy: 0.9500 - Loss: 0.1494\n",
      "\n",
      "Batch 679/992 ━━━━━━━━━━━━━━━━━━━━ 13:09:00\n",
      "Accuracy: 0.9499 - Loss: 0.1496\n",
      "\n",
      "Batch 680/992 ━━━━━━━━━━━━━━━━━━━━ 13:09:12\n",
      "Accuracy: 0.9500 - Loss: 0.1495\n",
      "\n",
      "Batch 681/992 ━━━━━━━━━━━━━━━━━━━━ 13:09:24\n",
      "Accuracy: 0.9499 - Loss: 0.1494\n",
      "\n",
      "Batch 682/992 ━━━━━━━━━━━━━━━━━━━━ 13:09:36\n",
      "Accuracy: 0.9500 - Loss: 0.1494\n",
      "\n",
      "Batch 683/992 ━━━━━━━━━━━━━━━━━━━━ 13:09:50\n",
      "Accuracy: 0.9499 - Loss: 0.1494\n",
      "\n",
      "Batch 684/992 ━━━━━━━━━━━━━━━━━━━━ 13:10:02\n",
      "Accuracy: 0.9499 - Loss: 0.1492\n",
      "\n",
      "Batch 685/992 ━━━━━━━━━━━━━━━━━━━━ 13:10:13\n",
      "Accuracy: 0.9498 - Loss: 0.1495\n",
      "\n",
      "Batch 686/992 ━━━━━━━━━━━━━━━━━━━━ 13:10:23\n",
      "Accuracy: 0.9497 - Loss: 0.1495\n",
      "\n",
      "Batch 687/992 ━━━━━━━━━━━━━━━━━━━━ 13:10:34\n",
      "Accuracy: 0.9494 - Loss: 0.1498\n",
      "\n",
      "Batch 688/992 ━━━━━━━━━━━━━━━━━━━━ 13:10:44\n",
      "Accuracy: 0.9495 - Loss: 0.1497\n",
      "\n",
      "Batch 689/992 ━━━━━━━━━━━━━━━━━━━━ 13:10:55\n",
      "Accuracy: 0.9496 - Loss: 0.1496\n",
      "\n",
      "Batch 690/992 ━━━━━━━━━━━━━━━━━━━━ 13:11:05\n",
      "Accuracy: 0.9496 - Loss: 0.1494\n",
      "\n",
      "Batch 691/992 ━━━━━━━━━━━━━━━━━━━━ 13:11:15\n",
      "Accuracy: 0.9495 - Loss: 0.1494\n",
      "\n",
      "Batch 692/992 ━━━━━━━━━━━━━━━━━━━━ 13:11:26\n",
      "Accuracy: 0.9496 - Loss: 0.1493\n",
      "\n",
      "Batch 693/992 ━━━━━━━━━━━━━━━━━━━━ 13:11:36\n",
      "Accuracy: 0.9495 - Loss: 0.1495\n",
      "\n",
      "Batch 694/992 ━━━━━━━━━━━━━━━━━━━━ 13:11:47\n",
      "Accuracy: 0.9494 - Loss: 0.1496\n",
      "\n",
      "Batch 695/992 ━━━━━━━━━━━━━━━━━━━━ 13:11:57\n",
      "Accuracy: 0.9495 - Loss: 0.1494\n",
      "\n",
      "Batch 696/992 ━━━━━━━━━━━━━━━━━━━━ 13:12:08\n",
      "Accuracy: 0.9495 - Loss: 0.1492\n",
      "\n",
      "Batch 697/992 ━━━━━━━━━━━━━━━━━━━━ 13:12:19\n",
      "Accuracy: 0.9496 - Loss: 0.1491\n",
      "\n",
      "Batch 698/992 ━━━━━━━━━━━━━━━━━━━━ 13:12:29\n",
      "Accuracy: 0.9497 - Loss: 0.1490\n",
      "\n",
      "Batch 699/992 ━━━━━━━━━━━━━━━━━━━━ 13:12:39\n",
      "Accuracy: 0.9497 - Loss: 0.1488\n",
      "\n",
      "Batch 700/992 ━━━━━━━━━━━━━━━━━━━━ 13:12:50\n",
      "Accuracy: 0.9496 - Loss: 0.1491\n",
      "\n",
      "Batch 701/992 ━━━━━━━━━━━━━━━━━━━━ 13:13:00\n",
      "Accuracy: 0.9497 - Loss: 0.1490\n",
      "\n",
      "Batch 702/992 ━━━━━━━━━━━━━━━━━━━━ 13:13:10\n",
      "Accuracy: 0.9498 - Loss: 0.1489\n",
      "\n",
      "Batch 703/992 ━━━━━━━━━━━━━━━━━━━━ 13:13:21\n",
      "Accuracy: 0.9499 - Loss: 0.1487\n",
      "\n",
      "Batch 704/992 ━━━━━━━━━━━━━━━━━━━━ 13:13:31\n",
      "Accuracy: 0.9498 - Loss: 0.1489\n",
      "\n",
      "Batch 705/992 ━━━━━━━━━━━━━━━━━━━━ 13:13:42\n",
      "Accuracy: 0.9493 - Loss: 0.1497\n",
      "\n",
      "Batch 706/992 ━━━━━━━━━━━━━━━━━━━━ 13:13:52\n",
      "Accuracy: 0.9490 - Loss: 0.1505\n",
      "\n",
      "Batch 707/992 ━━━━━━━━━━━━━━━━━━━━ 13:14:03\n",
      "Accuracy: 0.9489 - Loss: 0.1505\n",
      "\n",
      "Batch 708/992 ━━━━━━━━━━━━━━━━━━━━ 13:14:14\n",
      "Accuracy: 0.9490 - Loss: 0.1503\n",
      "\n",
      "Batch 709/992 ━━━━━━━━━━━━━━━━━━━━ 13:14:24\n",
      "Accuracy: 0.9490 - Loss: 0.1502\n",
      "\n",
      "Batch 710/992 ━━━━━━━━━━━━━━━━━━━━ 13:14:35\n",
      "Accuracy: 0.9489 - Loss: 0.1505\n",
      "\n",
      "Batch 711/992 ━━━━━━━━━━━━━━━━━━━━ 13:14:45\n",
      "Accuracy: 0.9490 - Loss: 0.1503\n",
      "\n",
      "Batch 712/992 ━━━━━━━━━━━━━━━━━━━━ 13:14:55\n",
      "Accuracy: 0.9487 - Loss: 0.1511\n",
      "\n",
      "Batch 713/992 ━━━━━━━━━━━━━━━━━━━━ 13:15:06\n",
      "Accuracy: 0.9488 - Loss: 0.1510\n",
      "\n",
      "Batch 714/992 ━━━━━━━━━━━━━━━━━━━━ 13:15:16\n",
      "Accuracy: 0.9489 - Loss: 0.1509\n",
      "\n",
      "Batch 715/992 ━━━━━━━━━━━━━━━━━━━━ 13:15:27\n",
      "Accuracy: 0.9490 - Loss: 0.1507\n",
      "\n",
      "Batch 716/992 ━━━━━━━━━━━━━━━━━━━━ 13:15:37\n",
      "Accuracy: 0.9487 - Loss: 0.1514\n",
      "\n",
      "Batch 717/992 ━━━━━━━━━━━━━━━━━━━━ 13:15:47\n",
      "Accuracy: 0.9486 - Loss: 0.1515\n",
      "\n",
      "Batch 718/992 ━━━━━━━━━━━━━━━━━━━━ 13:15:58\n",
      "Accuracy: 0.9486 - Loss: 0.1515\n",
      "\n",
      "Batch 719/992 ━━━━━━━━━━━━━━━━━━━━ 13:16:09\n",
      "Accuracy: 0.9485 - Loss: 0.1519\n",
      "\n",
      "Batch 720/992 ━━━━━━━━━━━━━━━━━━━━ 13:16:19\n",
      "Accuracy: 0.9486 - Loss: 0.1517\n",
      "\n",
      "Batch 721/992 ━━━━━━━━━━━━━━━━━━━━ 13:16:30\n",
      "Accuracy: 0.9485 - Loss: 0.1523\n",
      "\n",
      "Batch 722/992 ━━━━━━━━━━━━━━━━━━━━ 13:16:40\n",
      "Accuracy: 0.9482 - Loss: 0.1525\n",
      "\n",
      "Batch 723/992 ━━━━━━━━━━━━━━━━━━━━ 13:16:50\n",
      "Accuracy: 0.9483 - Loss: 0.1525\n",
      "\n",
      "Batch 724/992 ━━━━━━━━━━━━━━━━━━━━ 13:17:01\n",
      "Accuracy: 0.9484 - Loss: 0.1523\n",
      "\n",
      "Batch 725/992 ━━━━━━━━━━━━━━━━━━━━ 13:17:11\n",
      "Accuracy: 0.9484 - Loss: 0.1521\n",
      "\n",
      "Batch 726/992 ━━━━━━━━━━━━━━━━━━━━ 13:17:22\n",
      "Accuracy: 0.9485 - Loss: 0.1521\n",
      "\n",
      "Batch 727/992 ━━━━━━━━━━━━━━━━━━━━ 13:17:32\n",
      "Accuracy: 0.9486 - Loss: 0.1520\n",
      "\n",
      "Batch 728/992 ━━━━━━━━━━━━━━━━━━━━ 13:17:42\n",
      "Accuracy: 0.9487 - Loss: 0.1519\n",
      "\n",
      "Batch 729/992 ━━━━━━━━━━━━━━━━━━━━ 13:17:53\n",
      "Accuracy: 0.9487 - Loss: 0.1517\n",
      "\n",
      "Batch 730/992 ━━━━━━━━━━━━━━━━━━━━ 13:18:04\n",
      "Accuracy: 0.9488 - Loss: 0.1517\n",
      "\n",
      "Batch 731/992 ━━━━━━━━━━━━━━━━━━━━ 13:18:14\n",
      "Accuracy: 0.9489 - Loss: 0.1517\n",
      "\n",
      "Batch 732/992 ━━━━━━━━━━━━━━━━━━━━ 13:18:25\n",
      "Accuracy: 0.9486 - Loss: 0.1527\n",
      "\n",
      "Batch 733/992 ━━━━━━━━━━━━━━━━━━━━ 13:18:35\n",
      "Accuracy: 0.9487 - Loss: 0.1525\n",
      "\n",
      "Batch 734/992 ━━━━━━━━━━━━━━━━━━━━ 13:18:46\n",
      "Accuracy: 0.9487 - Loss: 0.1523\n",
      "\n",
      "Batch 735/992 ━━━━━━━━━━━━━━━━━━━━ 13:18:56\n",
      "Accuracy: 0.9486 - Loss: 0.1524\n",
      "\n",
      "Batch 736/992 ━━━━━━━━━━━━━━━━━━━━ 13:19:07\n",
      "Accuracy: 0.9484 - Loss: 0.1528\n",
      "\n",
      "Batch 737/992 ━━━━━━━━━━━━━━━━━━━━ 13:19:17\n",
      "Accuracy: 0.9484 - Loss: 0.1526\n",
      "\n",
      "Batch 738/992 ━━━━━━━━━━━━━━━━━━━━ 13:19:27\n",
      "Accuracy: 0.9485 - Loss: 0.1524\n",
      "\n",
      "Batch 739/992 ━━━━━━━━━━━━━━━━━━━━ 13:19:38\n",
      "Accuracy: 0.9484 - Loss: 0.1533\n",
      "\n",
      "Batch 740/992 ━━━━━━━━━━━━━━━━━━━━ 13:19:48\n",
      "Accuracy: 0.9485 - Loss: 0.1534\n",
      "\n",
      "Batch 741/992 ━━━━━━━━━━━━━━━━━━━━ 13:19:59\n",
      "Accuracy: 0.9484 - Loss: 0.1535\n",
      "\n",
      "Batch 742/992 ━━━━━━━━━━━━━━━━━━━━ 13:20:09\n",
      "Accuracy: 0.9485 - Loss: 0.1534\n",
      "\n",
      "Batch 743/992 ━━━━━━━━━━━━━━━━━━━━ 13:20:20\n",
      "Accuracy: 0.9484 - Loss: 0.1541\n",
      "\n",
      "Batch 744/992 ━━━━━━━━━━━━━━━━━━━━ 13:20:31\n",
      "Accuracy: 0.9483 - Loss: 0.1541\n",
      "\n",
      "Batch 745/992 ━━━━━━━━━━━━━━━━━━━━ 13:20:41\n",
      "Accuracy: 0.9483 - Loss: 0.1539\n",
      "\n",
      "Batch 746/992 ━━━━━━━━━━━━━━━━━━━━ 13:20:51\n",
      "Accuracy: 0.9482 - Loss: 0.1541\n",
      "\n",
      "Batch 747/992 ━━━━━━━━━━━━━━━━━━━━ 13:21:02\n",
      "Accuracy: 0.9483 - Loss: 0.1540\n",
      "\n",
      "Batch 748/992 ━━━━━━━━━━━━━━━━━━━━ 13:21:12\n",
      "Accuracy: 0.9484 - Loss: 0.1541\n",
      "\n",
      "Batch 749/992 ━━━━━━━━━━━━━━━━━━━━ 13:21:22\n",
      "Accuracy: 0.9484 - Loss: 0.1540\n",
      "\n",
      "Batch 750/992 ━━━━━━━━━━━━━━━━━━━━ 13:21:33\n",
      "Accuracy: 0.9483 - Loss: 0.1541\n",
      "\n",
      "Batch 751/992 ━━━━━━━━━━━━━━━━━━━━ 13:21:43\n",
      "Accuracy: 0.9482 - Loss: 0.1540\n",
      "\n",
      "Batch 752/992 ━━━━━━━━━━━━━━━━━━━━ 13:21:53\n",
      "Accuracy: 0.9481 - Loss: 0.1540\n",
      "\n",
      "Batch 753/992 ━━━━━━━━━━━━━━━━━━━━ 13:22:04\n",
      "Accuracy: 0.9480 - Loss: 0.1548\n",
      "\n",
      "Batch 754/992 ━━━━━━━━━━━━━━━━━━━━ 13:22:15\n",
      "Accuracy: 0.9478 - Loss: 0.1558\n",
      "\n",
      "Batch 755/992 ━━━━━━━━━━━━━━━━━━━━ 13:22:25\n",
      "Accuracy: 0.9478 - Loss: 0.1556\n",
      "\n",
      "Batch 756/992 ━━━━━━━━━━━━━━━━━━━━ 13:22:36\n",
      "Accuracy: 0.9479 - Loss: 0.1555\n",
      "\n",
      "Batch 757/992 ━━━━━━━━━━━━━━━━━━━━ 13:22:46\n",
      "Accuracy: 0.9478 - Loss: 0.1555\n",
      "\n",
      "Batch 758/992 ━━━━━━━━━━━━━━━━━━━━ 13:22:57\n",
      "Accuracy: 0.9479 - Loss: 0.1554\n",
      "\n",
      "Batch 759/992 ━━━━━━━━━━━━━━━━━━━━ 13:23:07\n",
      "Accuracy: 0.9480 - Loss: 0.1553\n",
      "\n",
      "Batch 760/992 ━━━━━━━━━━━━━━━━━━━━ 13:23:17\n",
      "Accuracy: 0.9480 - Loss: 0.1551\n",
      "\n",
      "Batch 761/992 ━━━━━━━━━━━━━━━━━━━━ 13:23:27\n",
      "Accuracy: 0.9481 - Loss: 0.1550\n",
      "\n",
      "Batch 762/992 ━━━━━━━━━━━━━━━━━━━━ 13:23:38\n",
      "Accuracy: 0.9482 - Loss: 0.1551\n",
      "\n",
      "Batch 763/992 ━━━━━━━━━━━━━━━━━━━━ 13:23:48\n",
      "Accuracy: 0.9482 - Loss: 0.1549\n",
      "\n",
      "Batch 764/992 ━━━━━━━━━━━━━━━━━━━━ 13:23:59\n",
      "Accuracy: 0.9483 - Loss: 0.1548\n",
      "\n",
      "Batch 765/992 ━━━━━━━━━━━━━━━━━━━━ 13:24:10\n",
      "Accuracy: 0.9484 - Loss: 0.1546\n",
      "\n",
      "Batch 766/992 ━━━━━━━━━━━━━━━━━━━━ 13:24:20\n",
      "Accuracy: 0.9483 - Loss: 0.1547\n",
      "\n",
      "Batch 767/992 ━━━━━━━━━━━━━━━━━━━━ 13:24:31\n",
      "Accuracy: 0.9480 - Loss: 0.1553\n",
      "\n",
      "Batch 768/992 ━━━━━━━━━━━━━━━━━━━━ 13:24:41\n",
      "Accuracy: 0.9479 - Loss: 0.1553\n",
      "\n",
      "Batch 769/992 ━━━━━━━━━━━━━━━━━━━━ 13:24:52\n",
      "Accuracy: 0.9478 - Loss: 0.1553\n",
      "\n",
      "Batch 770/992 ━━━━━━━━━━━━━━━━━━━━ 13:25:02\n",
      "Accuracy: 0.9477 - Loss: 0.1559\n",
      "\n",
      "Batch 771/992 ━━━━━━━━━━━━━━━━━━━━ 13:25:12\n",
      "Accuracy: 0.9476 - Loss: 0.1560\n",
      "\n",
      "Batch 772/992 ━━━━━━━━━━━━━━━━━━━━ 13:25:23\n",
      "Accuracy: 0.9475 - Loss: 0.1562\n",
      "\n",
      "Batch 773/992 ━━━━━━━━━━━━━━━━━━━━ 13:25:33\n",
      "Accuracy: 0.9476 - Loss: 0.1561\n",
      "\n",
      "Batch 774/992 ━━━━━━━━━━━━━━━━━━━━ 13:25:44\n",
      "Accuracy: 0.9477 - Loss: 0.1560\n",
      "\n",
      "Batch 775/992 ━━━━━━━━━━━━━━━━━━━━ 13:25:54\n",
      "Accuracy: 0.9477 - Loss: 0.1559\n",
      "\n",
      "Batch 776/992 ━━━━━━━━━━━━━━━━━━━━ 13:26:05\n",
      "Accuracy: 0.9478 - Loss: 0.1558\n",
      "\n",
      "Batch 777/992 ━━━━━━━━━━━━━━━━━━━━ 13:26:15\n",
      "Accuracy: 0.9479 - Loss: 0.1558\n",
      "\n",
      "Batch 778/992 ━━━━━━━━━━━━━━━━━━━━ 13:26:26\n",
      "Accuracy: 0.9476 - Loss: 0.1561\n",
      "\n",
      "Batch 779/992 ━━━━━━━━━━━━━━━━━━━━ 13:26:36\n",
      "Accuracy: 0.9477 - Loss: 0.1559\n",
      "\n",
      "Batch 780/992 ━━━━━━━━━━━━━━━━━━━━ 13:26:47\n",
      "Accuracy: 0.9478 - Loss: 0.1558\n",
      "\n",
      "Batch 781/992 ━━━━━━━━━━━━━━━━━━━━ 13:26:57\n",
      "Accuracy: 0.9473 - Loss: 0.1564\n",
      "\n",
      "Batch 782/992 ━━━━━━━━━━━━━━━━━━━━ 13:27:08\n",
      "Accuracy: 0.9473 - Loss: 0.1565\n",
      "\n",
      "Batch 783/992 ━━━━━━━━━━━━━━━━━━━━ 13:27:18\n",
      "Accuracy: 0.9473 - Loss: 0.1564\n",
      "\n",
      "Batch 784/992 ━━━━━━━━━━━━━━━━━━━━ 13:27:28\n",
      "Accuracy: 0.9472 - Loss: 0.1569\n",
      "\n",
      "Batch 785/992 ━━━━━━━━━━━━━━━━━━━━ 13:27:39\n",
      "Accuracy: 0.9473 - Loss: 0.1568\n",
      "\n",
      "Batch 786/992 ━━━━━━━━━━━━━━━━━━━━ 13:27:49\n",
      "Accuracy: 0.9472 - Loss: 0.1572\n",
      "\n",
      "Batch 787/992 ━━━━━━━━━━━━━━━━━━━━ 13:27:59\n",
      "Accuracy: 0.9470 - Loss: 0.1573\n",
      "\n",
      "Batch 788/992 ━━━━━━━━━━━━━━━━━━━━ 13:28:10\n",
      "Accuracy: 0.9469 - Loss: 0.1576\n",
      "\n",
      "Batch 789/992 ━━━━━━━━━━━━━━━━━━━━ 13:28:21\n",
      "Accuracy: 0.9469 - Loss: 0.1577\n",
      "\n",
      "Batch 790/992 ━━━━━━━━━━━━━━━━━━━━ 13:28:32\n",
      "Accuracy: 0.9470 - Loss: 0.1577\n",
      "\n",
      "Batch 791/992 ━━━━━━━━━━━━━━━━━━━━ 13:28:43\n",
      "Accuracy: 0.9467 - Loss: 0.1579\n",
      "\n",
      "Batch 792/992 ━━━━━━━━━━━━━━━━━━━━ 13:28:53\n",
      "Accuracy: 0.9468 - Loss: 0.1577\n",
      "\n",
      "Batch 793/992 ━━━━━━━━━━━━━━━━━━━━ 13:29:03\n",
      "Accuracy: 0.9469 - Loss: 0.1576\n",
      "\n",
      "Batch 794/992 ━━━━━━━━━━━━━━━━━━━━ 13:29:14\n",
      "Accuracy: 0.9469 - Loss: 0.1576\n",
      "\n",
      "Batch 795/992 ━━━━━━━━━━━━━━━━━━━━ 13:29:24\n",
      "Accuracy: 0.9469 - Loss: 0.1580\n",
      "\n",
      "Batch 796/992 ━━━━━━━━━━━━━━━━━━━━ 13:29:35\n",
      "Accuracy: 0.9469 - Loss: 0.1580\n",
      "\n",
      "Batch 797/992 ━━━━━━━━━━━━━━━━━━━━ 13:29:45\n",
      "Accuracy: 0.9470 - Loss: 0.1579\n",
      "\n",
      "Batch 798/992 ━━━━━━━━━━━━━━━━━━━━ 13:29:55\n",
      "Accuracy: 0.9471 - Loss: 0.1578\n",
      "\n",
      "Batch 799/992 ━━━━━━━━━━━━━━━━━━━━ 13:30:06\n",
      "Accuracy: 0.9471 - Loss: 0.1578\n",
      "\n",
      "Batch 800/992 ━━━━━━━━━━━━━━━━━━━━ 13:30:17\n",
      "Accuracy: 0.9470 - Loss: 0.1580\n",
      "\n",
      "Batch 801/992 ━━━━━━━━━━━━━━━━━━━━ 13:30:27\n",
      "Accuracy: 0.9469 - Loss: 0.1584\n",
      "\n",
      "Batch 802/992 ━━━━━━━━━━━━━━━━━━━━ 13:30:37\n",
      "Accuracy: 0.9469 - Loss: 0.1585\n",
      "\n",
      "Batch 803/992 ━━━━━━━━━━━━━━━━━━━━ 13:30:48\n",
      "Accuracy: 0.9468 - Loss: 0.1586\n",
      "\n",
      "Batch 804/992 ━━━━━━━━━━━━━━━━━━━━ 13:30:58\n",
      "Accuracy: 0.9467 - Loss: 0.1588\n",
      "\n",
      "Batch 805/992 ━━━━━━━━━━━━━━━━━━━━ 13:31:09\n",
      "Accuracy: 0.9467 - Loss: 0.1588\n",
      "\n",
      "Batch 806/992 ━━━━━━━━━━━━━━━━━━━━ 13:31:19\n",
      "Accuracy: 0.9465 - Loss: 0.1589\n",
      "\n",
      "Batch 807/992 ━━━━━━━━━━━━━━━━━━━━ 13:31:29\n",
      "Accuracy: 0.9466 - Loss: 0.1589\n",
      "\n",
      "Batch 808/992 ━━━━━━━━━━━━━━━━━━━━ 13:31:40\n",
      "Accuracy: 0.9465 - Loss: 0.1590\n",
      "\n",
      "Batch 809/992 ━━━━━━━━━━━━━━━━━━━━ 13:31:50\n",
      "Accuracy: 0.9465 - Loss: 0.1588\n",
      "\n",
      "Batch 810/992 ━━━━━━━━━━━━━━━━━━━━ 13:32:00\n",
      "Accuracy: 0.9466 - Loss: 0.1588\n",
      "\n",
      "Batch 811/992 ━━━━━━━━━━━━━━━━━━━━ 13:32:11\n",
      "Accuracy: 0.9465 - Loss: 0.1588\n",
      "\n",
      "Batch 812/992 ━━━━━━━━━━━━━━━━━━━━ 13:32:22\n",
      "Accuracy: 0.9461 - Loss: 0.1593\n",
      "\n",
      "Batch 813/992 ━━━━━━━━━━━━━━━━━━━━ 13:32:32\n",
      "Accuracy: 0.9462 - Loss: 0.1591\n",
      "\n",
      "Batch 814/992 ━━━━━━━━━━━━━━━━━━━━ 13:32:43\n",
      "Accuracy: 0.9461 - Loss: 0.1592\n",
      "\n",
      "Batch 815/992 ━━━━━━━━━━━━━━━━━━━━ 13:32:53\n",
      "Accuracy: 0.9462 - Loss: 0.1591\n",
      "\n",
      "Batch 816/992 ━━━━━━━━━━━━━━━━━━━━ 13:33:04\n",
      "Accuracy: 0.9462 - Loss: 0.1589\n",
      "\n",
      "Batch 817/992 ━━━━━━━━━━━━━━━━━━━━ 13:33:14\n",
      "Accuracy: 0.9461 - Loss: 0.1593\n",
      "\n",
      "Batch 818/992 ━━━━━━━━━━━━━━━━━━━━ 13:33:24\n",
      "Accuracy: 0.9462 - Loss: 0.1592\n",
      "\n",
      "Batch 819/992 ━━━━━━━━━━━━━━━━━━━━ 13:33:35\n",
      "Accuracy: 0.9463 - Loss: 0.1591\n",
      "\n",
      "Batch 820/992 ━━━━━━━━━━━━━━━━━━━━ 13:33:45\n",
      "Accuracy: 0.9463 - Loss: 0.1590\n",
      "\n",
      "Batch 821/992 ━━━━━━━━━━━━━━━━━━━━ 13:33:56\n",
      "Accuracy: 0.9461 - Loss: 0.1594\n",
      "\n",
      "Batch 822/992 ━━━━━━━━━━━━━━━━━━━━ 13:34:06\n",
      "Accuracy: 0.9460 - Loss: 0.1595\n",
      "\n",
      "Batch 823/992 ━━━━━━━━━━━━━━━━━━━━ 13:34:17\n",
      "Accuracy: 0.9461 - Loss: 0.1594\n",
      "\n",
      "Batch 824/992 ━━━━━━━━━━━━━━━━━━━━ 13:34:28\n",
      "Accuracy: 0.9461 - Loss: 0.1593\n",
      "\n",
      "Batch 825/992 ━━━━━━━━━━━━━━━━━━━━ 13:34:38\n",
      "Accuracy: 0.9461 - Loss: 0.1600\n",
      "\n",
      "Batch 826/992 ━━━━━━━━━━━━━━━━━━━━ 13:34:48\n",
      "Accuracy: 0.9460 - Loss: 0.1605\n",
      "\n",
      "Batch 827/992 ━━━━━━━━━━━━━━━━━━━━ 13:34:59\n",
      "Accuracy: 0.9460 - Loss: 0.1603\n",
      "\n",
      "Batch 828/992 ━━━━━━━━━━━━━━━━━━━━ 13:35:09\n",
      "Accuracy: 0.9461 - Loss: 0.1601\n",
      "\n",
      "Batch 829/992 ━━━━━━━━━━━━━━━━━━━━ 13:35:19\n",
      "Accuracy: 0.9457 - Loss: 0.1606\n",
      "\n",
      "Batch 830/992 ━━━━━━━━━━━━━━━━━━━━ 13:35:30\n",
      "Accuracy: 0.9455 - Loss: 0.1607\n",
      "\n",
      "Batch 831/992 ━━━━━━━━━━━━━━━━━━━━ 13:35:40\n",
      "Accuracy: 0.9452 - Loss: 0.1612\n",
      "\n",
      "Batch 832/992 ━━━━━━━━━━━━━━━━━━━━ 13:35:51\n",
      "Accuracy: 0.9453 - Loss: 0.1612\n",
      "\n",
      "Batch 833/992 ━━━━━━━━━━━━━━━━━━━━ 13:36:01\n",
      "Accuracy: 0.9454 - Loss: 0.1610\n",
      "\n",
      "Batch 834/992 ━━━━━━━━━━━━━━━━━━━━ 13:36:12\n",
      "Accuracy: 0.9454 - Loss: 0.1609\n",
      "\n",
      "Batch 835/992 ━━━━━━━━━━━━━━━━━━━━ 13:36:23\n",
      "Accuracy: 0.9452 - Loss: 0.1617\n",
      "\n",
      "Batch 836/992 ━━━━━━━━━━━━━━━━━━━━ 13:36:33\n",
      "Accuracy: 0.9453 - Loss: 0.1617\n",
      "\n",
      "Batch 837/992 ━━━━━━━━━━━━━━━━━━━━ 13:36:43\n",
      "Accuracy: 0.9452 - Loss: 0.1619\n",
      "\n",
      "Batch 838/992 ━━━━━━━━━━━━━━━━━━━━ 13:36:54\n",
      "Accuracy: 0.9453 - Loss: 0.1617\n",
      "\n",
      "Batch 839/992 ━━━━━━━━━━━━━━━━━━━━ 13:37:04\n",
      "Accuracy: 0.9452 - Loss: 0.1619\n",
      "\n",
      "Batch 840/992 ━━━━━━━━━━━━━━━━━━━━ 13:37:15\n",
      "Accuracy: 0.9451 - Loss: 0.1621\n",
      "\n",
      "Batch 841/992 ━━━━━━━━━━━━━━━━━━━━ 13:37:26\n",
      "Accuracy: 0.9450 - Loss: 0.1622\n",
      "\n",
      "Batch 842/992 ━━━━━━━━━━━━━━━━━━━━ 13:37:37\n",
      "Accuracy: 0.9451 - Loss: 0.1620\n",
      "\n",
      "Batch 843/992 ━━━━━━━━━━━━━━━━━━━━ 13:37:49\n",
      "Accuracy: 0.9451 - Loss: 0.1620\n",
      "\n",
      "Batch 844/992 ━━━━━━━━━━━━━━━━━━━━ 13:38:00\n",
      "Accuracy: 0.9452 - Loss: 0.1619\n",
      "\n",
      "Batch 845/992 ━━━━━━━━━━━━━━━━━━━━ 13:38:11\n",
      "Accuracy: 0.9450 - Loss: 0.1625\n",
      "\n",
      "Batch 846/992 ━━━━━━━━━━━━━━━━━━━━ 13:38:22\n",
      "Accuracy: 0.9450 - Loss: 0.1624\n",
      "\n",
      "Batch 847/992 ━━━━━━━━━━━━━━━━━━━━ 13:38:32\n",
      "Accuracy: 0.9450 - Loss: 0.1624\n",
      "\n",
      "Batch 848/992 ━━━━━━━━━━━━━━━━━━━━ 13:38:43\n",
      "Accuracy: 0.9450 - Loss: 0.1626\n",
      "\n",
      "Batch 849/992 ━━━━━━━━━━━━━━━━━━━━ 13:38:57\n",
      "Accuracy: 0.9449 - Loss: 0.1628\n",
      "\n",
      "Batch 850/992 ━━━━━━━━━━━━━━━━━━━━ 13:39:11\n",
      "Accuracy: 0.9449 - Loss: 0.1629\n",
      "\n",
      "Batch 851/992 ━━━━━━━━━━━━━━━━━━━━ 13:39:25\n",
      "Accuracy: 0.9449 - Loss: 0.1630\n",
      "\n",
      "Batch 852/992 ━━━━━━━━━━━━━━━━━━━━ 13:39:37\n",
      "Accuracy: 0.9448 - Loss: 0.1631\n",
      "\n",
      "Batch 853/992 ━━━━━━━━━━━━━━━━━━━━ 13:39:49\n",
      "Accuracy: 0.9448 - Loss: 0.1633\n",
      "\n",
      "Batch 854/992 ━━━━━━━━━━━━━━━━━━━━ 13:40:00\n",
      "Accuracy: 0.9448 - Loss: 0.1632\n",
      "\n",
      "Batch 855/992 ━━━━━━━━━━━━━━━━━━━━ 13:40:10\n",
      "Accuracy: 0.9449 - Loss: 0.1630\n",
      "\n",
      "Batch 856/992 ━━━━━━━━━━━━━━━━━━━━ 13:40:20\n",
      "Accuracy: 0.9449 - Loss: 0.1629\n",
      "\n",
      "Batch 857/992 ━━━━━━━━━━━━━━━━━━━━ 13:40:31\n",
      "Accuracy: 0.9450 - Loss: 0.1628\n",
      "\n",
      "Batch 858/992 ━━━━━━━━━━━━━━━━━━━━ 13:40:41\n",
      "Accuracy: 0.9451 - Loss: 0.1627\n",
      "\n",
      "Batch 859/992 ━━━━━━━━━━━━━━━━━━━━ 13:40:54\n",
      "Accuracy: 0.9448 - Loss: 0.1628\n",
      "\n",
      "Batch 860/992 ━━━━━━━━━━━━━━━━━━━━ 13:41:09\n",
      "Accuracy: 0.9449 - Loss: 0.1628\n",
      "\n",
      "Batch 861/992 ━━━━━━━━━━━━━━━━━━━━ 13:41:23\n",
      "Accuracy: 0.9450 - Loss: 0.1627\n",
      "\n",
      "Batch 862/992 ━━━━━━━━━━━━━━━━━━━━ 13:41:34\n",
      "Accuracy: 0.9450 - Loss: 0.1626\n",
      "\n",
      "Batch 863/992 ━━━━━━━━━━━━━━━━━━━━ 13:41:46\n",
      "Accuracy: 0.9451 - Loss: 0.1624\n",
      "\n",
      "Batch 864/992 ━━━━━━━━━━━━━━━━━━━━ 13:41:58\n",
      "Accuracy: 0.9450 - Loss: 0.1625\n",
      "\n",
      "Batch 865/992 ━━━━━━━━━━━━━━━━━━━━ 13:42:09\n",
      "Accuracy: 0.9449 - Loss: 0.1626\n",
      "\n",
      "Batch 866/992 ━━━━━━━━━━━━━━━━━━━━ 13:42:21\n",
      "Accuracy: 0.9449 - Loss: 0.1626\n",
      "\n",
      "Batch 867/992 ━━━━━━━━━━━━━━━━━━━━ 13:42:32\n",
      "Accuracy: 0.9449 - Loss: 0.1626\n",
      "\n",
      "Batch 868/992 ━━━━━━━━━━━━━━━━━━━━ 13:42:44\n",
      "Accuracy: 0.9447 - Loss: 0.1629\n",
      "\n",
      "Batch 869/992 ━━━━━━━━━━━━━━━━━━━━ 13:42:56\n",
      "Accuracy: 0.9448 - Loss: 0.1627\n",
      "\n",
      "Batch 870/992 ━━━━━━━━━━━━━━━━━━━━ 13:43:08\n",
      "Accuracy: 0.9447 - Loss: 0.1629\n",
      "\n",
      "Batch 871/992 ━━━━━━━━━━━━━━━━━━━━ 13:43:19\n",
      "Accuracy: 0.9447 - Loss: 0.1628\n",
      "\n",
      "Batch 872/992 ━━━━━━━━━━━━━━━━━━━━ 13:43:33\n",
      "Accuracy: 0.9447 - Loss: 0.1628\n",
      "\n",
      "Batch 873/992 ━━━━━━━━━━━━━━━━━━━━ 13:43:43\n",
      "Accuracy: 0.9447 - Loss: 0.1626\n",
      "\n",
      "Batch 874/992 ━━━━━━━━━━━━━━━━━━━━ 13:43:54\n",
      "Accuracy: 0.9448 - Loss: 0.1625\n",
      "\n",
      "Batch 875/992 ━━━━━━━━━━━━━━━━━━━━ 13:44:06\n",
      "Accuracy: 0.9446 - Loss: 0.1626\n",
      "\n",
      "Batch 876/992 ━━━━━━━━━━━━━━━━━━━━ 13:44:17\n",
      "Accuracy: 0.9445 - Loss: 0.1627\n",
      "\n",
      "Batch 877/992 ━━━━━━━━━━━━━━━━━━━━ 13:44:32\n",
      "Accuracy: 0.9444 - Loss: 0.1628\n",
      "\n",
      "Batch 878/992 ━━━━━━━━━━━━━━━━━━━━ 13:44:46\n",
      "Accuracy: 0.9445 - Loss: 0.1626\n",
      "\n",
      "Batch 879/992 ━━━━━━━━━━━━━━━━━━━━ 13:45:02\n",
      "Accuracy: 0.9445 - Loss: 0.1625\n",
      "\n",
      "Batch 880/992 ━━━━━━━━━━━━━━━━━━━━ 13:45:13\n",
      "Accuracy: 0.9446 - Loss: 0.1624\n",
      "\n",
      "Batch 881/992 ━━━━━━━━━━━━━━━━━━━━ 13:45:24\n",
      "Accuracy: 0.9447 - Loss: 0.1624\n",
      "\n",
      "Batch 882/992 ━━━━━━━━━━━━━━━━━━━━ 13:45:36\n",
      "Accuracy: 0.9447 - Loss: 0.1623\n",
      "\n",
      "Batch 883/992 ━━━━━━━━━━━━━━━━━━━━ 13:45:48\n",
      "Accuracy: 0.9445 - Loss: 0.1626\n",
      "\n",
      "Batch 884/992 ━━━━━━━━━━━━━━━━━━━━ 13:46:00\n",
      "Accuracy: 0.9446 - Loss: 0.1624\n",
      "\n",
      "Batch 885/992 ━━━━━━━━━━━━━━━━━━━━ 13:46:13\n",
      "Accuracy: 0.9442 - Loss: 0.1630\n",
      "\n",
      "Batch 886/992 ━━━━━━━━━━━━━━━━━━━━ 13:46:26\n",
      "Accuracy: 0.9443 - Loss: 0.1629\n",
      "\n",
      "Batch 887/992 ━━━━━━━━━━━━━━━━━━━━ 13:46:40\n",
      "Accuracy: 0.9443 - Loss: 0.1627\n",
      "\n",
      "Batch 888/992 ━━━━━━━━━━━━━━━━━━━━ 13:46:52\n",
      "Accuracy: 0.9444 - Loss: 0.1627\n",
      "\n",
      "Batch 889/992 ━━━━━━━━━━━━━━━━━━━━ 13:47:04\n",
      "Accuracy: 0.9445 - Loss: 0.1626\n",
      "\n",
      "Batch 890/992 ━━━━━━━━━━━━━━━━━━━━ 13:47:17\n",
      "Accuracy: 0.9441 - Loss: 0.1632\n",
      "\n",
      "Batch 891/992 ━━━━━━━━━━━━━━━━━━━━ 13:47:28\n",
      "Accuracy: 0.9442 - Loss: 0.1631\n",
      "\n",
      "Batch 892/992 ━━━━━━━━━━━━━━━━━━━━ 13:47:42\n",
      "Accuracy: 0.9442 - Loss: 0.1631\n",
      "\n",
      "Batch 893/992 ━━━━━━━━━━━━━━━━━━━━ 13:47:54\n",
      "Accuracy: 0.9443 - Loss: 0.1630\n",
      "\n",
      "Batch 894/992 ━━━━━━━━━━━━━━━━━━━━ 13:48:09\n",
      "Accuracy: 0.9444 - Loss: 0.1629\n",
      "\n",
      "Batch 895/992 ━━━━━━━━━━━━━━━━━━━━ 13:48:24\n",
      "Accuracy: 0.9443 - Loss: 0.1630\n",
      "\n",
      "Batch 896/992 ━━━━━━━━━━━━━━━━━━━━ 13:48:39\n",
      "Accuracy: 0.9442 - Loss: 0.1641\n",
      "\n",
      "Batch 897/992 ━━━━━━━━━━━━━━━━━━━━ 13:48:54\n",
      "Accuracy: 0.9441 - Loss: 0.1643\n",
      "\n",
      "Batch 898/992 ━━━━━━━━━━━━━━━━━━━━ 13:49:06\n",
      "Accuracy: 0.9440 - Loss: 0.1644\n",
      "\n",
      "Batch 899/992 ━━━━━━━━━━━━━━━━━━━━ 13:49:17\n",
      "Accuracy: 0.9441 - Loss: 0.1644\n",
      "\n",
      "Batch 900/992 ━━━━━━━━━━━━━━━━━━━━ 13:49:28\n",
      "Accuracy: 0.9442 - Loss: 0.1642\n",
      "\n",
      "Batch 901/992 ━━━━━━━━━━━━━━━━━━━━ 13:49:42\n",
      "Accuracy: 0.9441 - Loss: 0.1642\n",
      "\n",
      "Batch 902/992 ━━━━━━━━━━━━━━━━━━━━ 13:49:54\n",
      "Accuracy: 0.9442 - Loss: 0.1642\n",
      "\n",
      "Batch 903/992 ━━━━━━━━━━━━━━━━━━━━ 13:50:09\n",
      "Accuracy: 0.9442 - Loss: 0.1641\n",
      "\n",
      "Batch 904/992 ━━━━━━━━━━━━━━━━━━━━ 13:50:23\n",
      "Accuracy: 0.9443 - Loss: 0.1641\n",
      "\n",
      "Batch 905/992 ━━━━━━━━━━━━━━━━━━━━ 13:50:34\n",
      "Accuracy: 0.9441 - Loss: 0.1645\n",
      "\n",
      "Batch 906/992 ━━━━━━━━━━━━━━━━━━━━ 13:50:45\n",
      "Accuracy: 0.9441 - Loss: 0.1644\n",
      "\n",
      "Batch 907/992 ━━━━━━━━━━━━━━━━━━━━ 13:50:58\n",
      "Accuracy: 0.9442 - Loss: 0.1642\n",
      "\n",
      "Batch 908/992 ━━━━━━━━━━━━━━━━━━━━ 13:51:12\n",
      "Accuracy: 0.9442 - Loss: 0.1641\n",
      "\n",
      "Batch 909/992 ━━━━━━━━━━━━━━━━━━━━ 13:51:26\n",
      "Accuracy: 0.9443 - Loss: 0.1641\n",
      "\n",
      "Batch 910/992 ━━━━━━━━━━━━━━━━━━━━ 13:51:41\n",
      "Accuracy: 0.9442 - Loss: 0.1643\n",
      "\n",
      "Batch 911/992 ━━━━━━━━━━━━━━━━━━━━ 13:51:56\n",
      "Accuracy: 0.9442 - Loss: 0.1651\n",
      "\n",
      "Batch 912/992 ━━━━━━━━━━━━━━━━━━━━ 13:52:11\n",
      "Accuracy: 0.9441 - Loss: 0.1651\n",
      "\n",
      "Batch 913/992 ━━━━━━━━━━━━━━━━━━━━ 13:52:24\n",
      "Accuracy: 0.9441 - Loss: 0.1652\n",
      "\n",
      "Batch 914/992 ━━━━━━━━━━━━━━━━━━━━ 13:52:34\n",
      "Accuracy: 0.9439 - Loss: 0.1657\n",
      "\n",
      "Batch 915/992 ━━━━━━━━━━━━━━━━━━━━ 13:52:44\n",
      "Accuracy: 0.9440 - Loss: 0.1656\n",
      "\n",
      "Batch 916/992 ━━━━━━━━━━━━━━━━━━━━ 13:52:54\n",
      "Accuracy: 0.9439 - Loss: 0.1660\n",
      "\n",
      "Batch 917/992 ━━━━━━━━━━━━━━━━━━━━ 13:53:04\n",
      "Accuracy: 0.9440 - Loss: 0.1661\n",
      "\n",
      "Batch 918/992 ━━━━━━━━━━━━━━━━━━━━ 13:53:15\n",
      "Accuracy: 0.9440 - Loss: 0.1659\n",
      "\n",
      "Batch 919/992 ━━━━━━━━━━━━━━━━━━━━ 13:53:25\n",
      "Accuracy: 0.9438 - Loss: 0.1661\n",
      "\n",
      "Batch 920/992 ━━━━━━━━━━━━━━━━━━━━ 13:53:35\n",
      "Accuracy: 0.9436 - Loss: 0.1662\n",
      "\n",
      "Batch 921/992 ━━━━━━━━━━━━━━━━━━━━ 13:53:46\n",
      "Accuracy: 0.9437 - Loss: 0.1661\n",
      "\n",
      "Batch 922/992 ━━━━━━━━━━━━━━━━━━━━ 13:53:56\n",
      "Accuracy: 0.9437 - Loss: 0.1660\n",
      "\n",
      "Batch 923/992 ━━━━━━━━━━━━━━━━━━━━ 13:54:06\n",
      "Accuracy: 0.9437 - Loss: 0.1661\n",
      "\n",
      "Batch 924/992 ━━━━━━━━━━━━━━━━━━━━ 13:54:16\n",
      "Accuracy: 0.9437 - Loss: 0.1661\n",
      "\n",
      "Batch 925/992 ━━━━━━━━━━━━━━━━━━━━ 13:54:26\n",
      "Accuracy: 0.9438 - Loss: 0.1660\n",
      "\n",
      "Batch 926/992 ━━━━━━━━━━━━━━━━━━━━ 13:54:37\n",
      "Accuracy: 0.9438 - Loss: 0.1659\n",
      "\n",
      "Batch 927/992 ━━━━━━━━━━━━━━━━━━━━ 13:54:47\n",
      "Accuracy: 0.9439 - Loss: 0.1657\n",
      "\n",
      "Batch 928/992 ━━━━━━━━━━━━━━━━━━━━ 13:54:57\n",
      "Accuracy: 0.9440 - Loss: 0.1657\n",
      "\n",
      "Batch 929/992 ━━━━━━━━━━━━━━━━━━━━ 13:55:07\n",
      "Accuracy: 0.9440 - Loss: 0.1656\n",
      "\n",
      "Batch 930/992 ━━━━━━━━━━━━━━━━━━━━ 13:55:17\n",
      "Accuracy: 0.9441 - Loss: 0.1655\n",
      "\n",
      "Batch 931/992 ━━━━━━━━━━━━━━━━━━━━ 13:55:27\n",
      "Accuracy: 0.9440 - Loss: 0.1658\n",
      "\n",
      "Batch 932/992 ━━━━━━━━━━━━━━━━━━━━ 13:55:37\n",
      "Accuracy: 0.9439 - Loss: 0.1658\n",
      "\n",
      "Batch 933/992 ━━━━━━━━━━━━━━━━━━━━ 13:55:48\n",
      "Accuracy: 0.9439 - Loss: 0.1658\n",
      "\n",
      "Batch 934/992 ━━━━━━━━━━━━━━━━━━━━ 13:55:58\n",
      "Accuracy: 0.9439 - Loss: 0.1656\n",
      "\n",
      "Batch 935/992 ━━━━━━━━━━━━━━━━━━━━ 13:56:08\n",
      "Accuracy: 0.9440 - Loss: 0.1655\n",
      "\n",
      "Batch 936/992 ━━━━━━━━━━━━━━━━━━━━ 13:56:18\n",
      "Accuracy: 0.9440 - Loss: 0.1654\n",
      "\n",
      "Batch 937/992 ━━━━━━━━━━━━━━━━━━━━ 13:56:29\n",
      "Accuracy: 0.9438 - Loss: 0.1654\n",
      "\n",
      "Batch 938/992 ━━━━━━━━━━━━━━━━━━━━ 13:56:40\n",
      "Accuracy: 0.9439 - Loss: 0.1655\n",
      "\n",
      "Batch 939/992 ━━━━━━━━━━━━━━━━━━━━ 13:56:51\n",
      "Accuracy: 0.9440 - Loss: 0.1654\n",
      "\n",
      "Batch 940/992 ━━━━━━━━━━━━━━━━━━━━ 13:57:02\n",
      "Accuracy: 0.9440 - Loss: 0.1653\n",
      "\n",
      "Batch 941/992 ━━━━━━━━━━━━━━━━━━━━ 13:57:13\n",
      "Accuracy: 0.9438 - Loss: 0.1658\n",
      "\n",
      "Batch 942/992 ━━━━━━━━━━━━━━━━━━━━ 13:57:23\n",
      "Accuracy: 0.9439 - Loss: 0.1658\n",
      "\n",
      "Batch 943/992 ━━━━━━━━━━━━━━━━━━━━ 13:57:36\n",
      "Accuracy: 0.9439 - Loss: 0.1657\n",
      "\n",
      "Batch 944/992 ━━━━━━━━━━━━━━━━━━━━ 13:57:47\n",
      "Accuracy: 0.9439 - Loss: 0.1657\n",
      "\n",
      "Batch 945/992 ━━━━━━━━━━━━━━━━━━━━ 13:57:57\n",
      "Accuracy: 0.9438 - Loss: 0.1657\n",
      "\n",
      "Batch 946/992 ━━━━━━━━━━━━━━━━━━━━ 13:58:09\n",
      "Accuracy: 0.9438 - Loss: 0.1657\n",
      "\n",
      "Batch 947/992 ━━━━━━━━━━━━━━━━━━━━ 13:58:22\n",
      "Accuracy: 0.9438 - Loss: 0.1658\n",
      "\n",
      "Batch 948/992 ━━━━━━━━━━━━━━━━━━━━ 13:58:35\n",
      "Accuracy: 0.9436 - Loss: 0.1660\n",
      "\n",
      "Batch 949/992 ━━━━━━━━━━━━━━━━━━━━ 13:58:46\n",
      "Accuracy: 0.9435 - Loss: 0.1661\n",
      "\n",
      "Batch 950/992 ━━━━━━━━━━━━━━━━━━━━ 13:58:57\n",
      "Accuracy: 0.9434 - Loss: 0.1662\n",
      "\n",
      "Batch 951/992 ━━━━━━━━━━━━━━━━━━━━ 13:59:09\n",
      "Accuracy: 0.9433 - Loss: 0.1664\n",
      "\n",
      "Batch 952/992 ━━━━━━━━━━━━━━━━━━━━ 13:59:21\n",
      "Accuracy: 0.9434 - Loss: 0.1662\n",
      "\n",
      "Batch 953/992 ━━━━━━━━━━━━━━━━━━━━ 13:59:32\n",
      "Accuracy: 0.9433 - Loss: 0.1662\n",
      "\n",
      "Batch 954/992 ━━━━━━━━━━━━━━━━━━━━ 13:59:43\n",
      "Accuracy: 0.9431 - Loss: 0.1664\n",
      "\n",
      "Batch 955/992 ━━━━━━━━━━━━━━━━━━━━ 13:59:53\n",
      "Accuracy: 0.9432 - Loss: 0.1663\n",
      "\n",
      "Batch 956/992 ━━━━━━━━━━━━━━━━━━━━ 14:00:04\n",
      "Accuracy: 0.9431 - Loss: 0.1668\n",
      "\n",
      "Batch 957/992 ━━━━━━━━━━━━━━━━━━━━ 14:00:14\n",
      "Accuracy: 0.9431 - Loss: 0.1670\n",
      "\n",
      "Batch 958/992 ━━━━━━━━━━━━━━━━━━━━ 14:00:29\n",
      "Accuracy: 0.9431 - Loss: 0.1669\n",
      "\n",
      "Batch 959/992 ━━━━━━━━━━━━━━━━━━━━ 14:00:45\n",
      "Accuracy: 0.9429 - Loss: 0.1675\n",
      "\n",
      "Batch 960/992 ━━━━━━━━━━━━━━━━━━━━ 14:01:01\n",
      "Accuracy: 0.9430 - Loss: 0.1674\n",
      "\n",
      "Batch 961/992 ━━━━━━━━━━━━━━━━━━━━ 14:01:15\n",
      "Accuracy: 0.9430 - Loss: 0.1672\n",
      "\n",
      "Batch 962/992 ━━━━━━━━━━━━━━━━━━━━ 14:01:30\n",
      "Accuracy: 0.9431 - Loss: 0.1672\n",
      "\n",
      "Batch 963/992 ━━━━━━━━━━━━━━━━━━━━ 14:01:45\n",
      "Accuracy: 0.9431 - Loss: 0.1670\n",
      "\n",
      "Batch 964/992 ━━━━━━━━━━━━━━━━━━━━ 14:01:59\n",
      "Accuracy: 0.9432 - Loss: 0.1670\n",
      "\n",
      "Batch 965/992 ━━━━━━━━━━━━━━━━━━━━ 14:02:13\n",
      "Accuracy: 0.9430 - Loss: 0.1675\n",
      "\n",
      "Batch 966/992 ━━━━━━━━━━━━━━━━━━━━ 14:02:23\n",
      "Accuracy: 0.9431 - Loss: 0.1674\n",
      "\n",
      "Batch 967/992 ━━━━━━━━━━━━━━━━━━━━ 14:02:33\n",
      "Accuracy: 0.9431 - Loss: 0.1673\n",
      "\n",
      "Batch 968/992 ━━━━━━━━━━━━━━━━━━━━ 14:02:43\n",
      "Accuracy: 0.9432 - Loss: 0.1672\n",
      "\n",
      "Batch 969/992 ━━━━━━━━━━━━━━━━━━━━ 14:02:53\n",
      "Accuracy: 0.9432 - Loss: 0.1672\n",
      "\n",
      "Batch 970/992 ━━━━━━━━━━━━━━━━━━━━ 14:03:03\n",
      "Accuracy: 0.9433 - Loss: 0.1670\n",
      "\n",
      "Batch 971/992 ━━━━━━━━━━━━━━━━━━━━ 14:03:13\n",
      "Accuracy: 0.9434 - Loss: 0.1669\n",
      "\n",
      "Batch 972/992 ━━━━━━━━━━━━━━━━━━━━ 14:03:25\n",
      "Accuracy: 0.9433 - Loss: 0.1669\n",
      "\n",
      "Batch 973/992 ━━━━━━━━━━━━━━━━━━━━ 14:03:37\n",
      "Accuracy: 0.9433 - Loss: 0.1668\n",
      "\n",
      "Batch 974/992 ━━━━━━━━━━━━━━━━━━━━ 14:03:51\n",
      "Accuracy: 0.9434 - Loss: 0.1668\n",
      "\n",
      "Batch 975/992 ━━━━━━━━━━━━━━━━━━━━ 14:04:03\n",
      "Accuracy: 0.9433 - Loss: 0.1673\n",
      "\n",
      "Batch 976/992 ━━━━━━━━━━━━━━━━━━━━ 14:04:15\n",
      "Accuracy: 0.9431 - Loss: 0.1676\n",
      "\n",
      "Batch 977/992 ━━━━━━━━━━━━━━━━━━━━ 14:04:26\n",
      "Accuracy: 0.9432 - Loss: 0.1674\n",
      "\n",
      "Batch 978/992 ━━━━━━━━━━━━━━━━━━━━ 14:04:36\n",
      "Accuracy: 0.9433 - Loss: 0.1673\n",
      "\n",
      "Batch 979/992 ━━━━━━━━━━━━━━━━━━━━ 14:04:50\n",
      "Accuracy: 0.9432 - Loss: 0.1673\n",
      "\n",
      "Batch 980/992 ━━━━━━━━━━━━━━━━━━━━ 14:05:04\n",
      "Accuracy: 0.9432 - Loss: 0.1672\n",
      "\n",
      "Batch 981/992 ━━━━━━━━━━━━━━━━━━━━ 14:05:18\n",
      "Accuracy: 0.9433 - Loss: 0.1671\n",
      "\n",
      "Batch 982/992 ━━━━━━━━━━━━━━━━━━━━ 14:05:33\n",
      "Accuracy: 0.9434 - Loss: 0.1670\n",
      "\n",
      "Batch 983/992 ━━━━━━━━━━━━━━━━━━━━ 14:05:47\n",
      "Accuracy: 0.9434 - Loss: 0.1669\n",
      "\n",
      "Batch 984/992 ━━━━━━━━━━━━━━━━━━━━ 14:06:02\n",
      "Accuracy: 0.9433 - Loss: 0.1669\n",
      "\n",
      "Batch 985/992 ━━━━━━━━━━━━━━━━━━━━ 14:06:17\n",
      "Accuracy: 0.9433 - Loss: 0.1670\n",
      "\n",
      "Batch 986/992 ━━━━━━━━━━━━━━━━━━━━ 14:06:32\n",
      "Accuracy: 0.9432 - Loss: 0.1670\n",
      "\n",
      "Batch 987/992 ━━━━━━━━━━━━━━━━━━━━ 14:06:46\n",
      "Accuracy: 0.9431 - Loss: 0.1670\n",
      "\n",
      "Batch 988/992 ━━━━━━━━━━━━━━━━━━━━ 14:07:01\n",
      "Accuracy: 0.9431 - Loss: 0.1669\n",
      "\n",
      "Batch 989/992 ━━━━━━━━━━━━━━━━━━━━ 14:07:13\n",
      "Accuracy: 0.9430 - Loss: 0.1670\n",
      "\n",
      "Batch 990/992 ━━━━━━━━━━━━━━━━━━━━ 14:07:25\n",
      "Accuracy: 0.9431 - Loss: 0.1669\n",
      "\n",
      "Batch 991/992 ━━━━━━━━━━━━━━━━━━━━ 14:07:38\n",
      "Accuracy: 0.9431 - Loss: 0.1667\n",
      "\n",
      "Batch 992/992 ━━━━━━━━━━━━━━━━━━━━ 14:07:50\n",
      "Accuracy: 0.9432 - Loss: 0.1666\n",
      "\n",
      "\n",
      "Epoch 8/10\n",
      "Batch 1/992 ━━━━━━━━━━━━━━━━━━━━ 14:27:37\n",
      "Accuracy: 1.0000 - Loss: 0.0022\n",
      "\n",
      "Batch 2/992 ━━━━━━━━━━━━━━━━━━━━ 14:27:52\n",
      "Accuracy: 1.0000 - Loss: 0.0747\n",
      "\n",
      "Batch 3/992 ━━━━━━━━━━━━━━━━━━━━ 14:28:07\n",
      "Accuracy: 1.0000 - Loss: 0.0661\n",
      "\n",
      "Batch 4/992 ━━━━━━━━━━━━━━━━━━━━ 14:28:19\n",
      "Accuracy: 1.0000 - Loss: 0.0723\n",
      "\n",
      "Batch 5/992 ━━━━━━━━━━━━━━━━━━━━ 14:28:33\n",
      "Accuracy: 1.0000 - Loss: 0.0722\n",
      "\n",
      "Batch 6/992 ━━━━━━━━━━━━━━━━━━━━ 14:28:44\n",
      "Accuracy: 0.9792 - Loss: 0.1033\n",
      "\n",
      "Batch 7/992 ━━━━━━━━━━━━━━━━━━━━ 14:28:54\n",
      "Accuracy: 0.9821 - Loss: 0.0997\n",
      "\n",
      "Batch 8/992 ━━━━━━━━━━━━━━━━━━━━ 14:29:08\n",
      "Accuracy: 0.9844 - Loss: 0.1032\n",
      "\n",
      "Batch 9/992 ━━━━━━━━━━━━━━━━━━━━ 14:29:21\n",
      "Accuracy: 0.9861 - Loss: 0.0923\n",
      "\n",
      "Batch 10/992 ━━━━━━━━━━━━━━━━━━━━ 14:29:33\n",
      "Accuracy: 0.9875 - Loss: 0.0879\n",
      "\n",
      "Batch 11/992 ━━━━━━━━━━━━━━━━━━━━ 14:29:46\n",
      "Accuracy: 0.9886 - Loss: 0.0918\n",
      "\n",
      "Batch 12/992 ━━━━━━━━━━━━━━━━━━━━ 14:29:58\n",
      "Accuracy: 0.9896 - Loss: 0.0880\n",
      "\n",
      "Batch 13/992 ━━━━━━━━━━━━━━━━━━━━ 14:30:08\n",
      "Accuracy: 0.9808 - Loss: 0.1028\n",
      "\n",
      "Batch 14/992 ━━━━━━━━━━━━━━━━━━━━ 14:30:19\n",
      "Accuracy: 0.9821 - Loss: 0.1061\n",
      "\n",
      "Batch 15/992 ━━━━━━━━━━━━━━━━━━━━ 14:30:30\n",
      "Accuracy: 0.9667 - Loss: 0.1155\n",
      "\n",
      "Batch 16/992 ━━━━━━━━━━━━━━━━━━━━ 14:30:41\n",
      "Accuracy: 0.9609 - Loss: 0.1275\n",
      "\n",
      "Batch 17/992 ━━━━━━━━━━━━━━━━━━━━ 14:30:52\n",
      "Accuracy: 0.9632 - Loss: 0.1257\n",
      "\n",
      "Batch 18/992 ━━━━━━━━━━━━━━━━━━━━ 14:31:04\n",
      "Accuracy: 0.9583 - Loss: 0.1281\n",
      "\n",
      "Batch 19/992 ━━━━━━━━━━━━━━━━━━━━ 14:31:17\n",
      "Accuracy: 0.9605 - Loss: 0.1261\n",
      "\n",
      "Batch 20/992 ━━━━━━━━━━━━━━━━━━━━ 14:31:30\n",
      "Accuracy: 0.9625 - Loss: 0.1216\n",
      "\n",
      "Batch 21/992 ━━━━━━━━━━━━━━━━━━━━ 14:31:44\n",
      "Accuracy: 0.9643 - Loss: 0.1224\n",
      "\n",
      "Batch 22/992 ━━━━━━━━━━━━━━━━━━━━ 14:31:55\n",
      "Accuracy: 0.9659 - Loss: 0.1182\n",
      "\n",
      "Batch 23/992 ━━━━━━━━━━━━━━━━━━━━ 14:32:07\n",
      "Accuracy: 0.9620 - Loss: 0.1211\n",
      "\n",
      "Batch 24/992 ━━━━━━━━━━━━━━━━━━━━ 14:32:19\n",
      "Accuracy: 0.9531 - Loss: 0.1327\n",
      "\n",
      "Batch 25/992 ━━━━━━━━━━━━━━━━━━━━ 14:32:32\n",
      "Accuracy: 0.9450 - Loss: 0.1422\n",
      "\n",
      "Batch 26/992 ━━━━━━━━━━━━━━━━━━━━ 14:32:44\n",
      "Accuracy: 0.9375 - Loss: 0.1532\n",
      "\n",
      "Batch 27/992 ━━━━━━━━━━━━━━━━━━━━ 14:32:56\n",
      "Accuracy: 0.9398 - Loss: 0.1493\n",
      "\n",
      "Batch 28/992 ━━━━━━━━━━━━━━━━━━━━ 14:33:07\n",
      "Accuracy: 0.9420 - Loss: 0.1481\n",
      "\n",
      "Batch 29/992 ━━━━━━━━━━━━━━━━━━━━ 14:33:17\n",
      "Accuracy: 0.9440 - Loss: 0.1442\n",
      "\n",
      "Batch 30/992 ━━━━━━━━━━━━━━━━━━━━ 14:33:28\n",
      "Accuracy: 0.9417 - Loss: 0.1459\n",
      "\n",
      "Batch 31/992 ━━━━━━━━━━━━━━━━━━━━ 14:33:38\n",
      "Accuracy: 0.9435 - Loss: 0.1481\n",
      "\n",
      "Batch 32/992 ━━━━━━━━━━━━━━━━━━━━ 14:33:49\n",
      "Accuracy: 0.9453 - Loss: 0.1439\n",
      "\n",
      "Batch 33/992 ━━━━━━━━━━━━━━━━━━━━ 14:33:59\n",
      "Accuracy: 0.9432 - Loss: 0.1476\n",
      "\n",
      "Batch 34/992 ━━━━━━━━━━━━━━━━━━━━ 14:34:10\n",
      "Accuracy: 0.9412 - Loss: 0.1559\n",
      "\n",
      "Batch 35/992 ━━━━━━━━━━━━━━━━━━━━ 14:34:25\n",
      "Accuracy: 0.9429 - Loss: 0.1537\n",
      "\n",
      "Batch 36/992 ━━━━━━━━━━━━━━━━━━━━ 14:34:37\n",
      "Accuracy: 0.9410 - Loss: 0.1544\n",
      "\n",
      "Batch 37/992 ━━━━━━━━━━━━━━━━━━━━ 14:34:48\n",
      "Accuracy: 0.9426 - Loss: 0.1536\n",
      "\n",
      "Batch 38/992 ━━━━━━━━━━━━━━━━━━━━ 14:35:00\n",
      "Accuracy: 0.9441 - Loss: 0.1511\n",
      "\n",
      "Batch 39/992 ━━━━━━━━━━━━━━━━━━━━ 14:35:11\n",
      "Accuracy: 0.9455 - Loss: 0.1476\n",
      "\n",
      "Batch 40/992 ━━━━━━━━━━━━━━━━━━━━ 14:35:25\n",
      "Accuracy: 0.9469 - Loss: 0.1470\n",
      "\n",
      "Batch 41/992 ━━━━━━━━━━━━━━━━━━━━ 14:35:40\n",
      "Accuracy: 0.9482 - Loss: 0.1443\n",
      "\n",
      "Batch 42/992 ━━━━━━━━━━━━━━━━━━━━ 14:35:54\n",
      "Accuracy: 0.9494 - Loss: 0.1420\n",
      "\n",
      "Batch 43/992 ━━━━━━━━━━━━━━━━━━━━ 14:36:08\n",
      "Accuracy: 0.9477 - Loss: 0.1455\n",
      "\n",
      "Batch 44/992 ━━━━━━━━━━━━━━━━━━━━ 14:36:21\n",
      "Accuracy: 0.9489 - Loss: 0.1428\n",
      "\n",
      "Batch 45/992 ━━━━━━━━━━━━━━━━━━━━ 14:36:32\n",
      "Accuracy: 0.9500 - Loss: 0.1402\n",
      "\n",
      "Batch 46/992 ━━━━━━━━━━━━━━━━━━━━ 14:36:45\n",
      "Accuracy: 0.9511 - Loss: 0.1373\n",
      "\n",
      "Batch 47/992 ━━━━━━━━━━━━━━━━━━━━ 14:36:59\n",
      "Accuracy: 0.9495 - Loss: 0.1377\n",
      "\n",
      "Batch 48/992 ━━━━━━━━━━━━━━━━━━━━ 14:37:14\n",
      "Accuracy: 0.9505 - Loss: 0.1362\n",
      "\n",
      "Batch 49/992 ━━━━━━━━━━━━━━━━━━━━ 14:37:29\n",
      "Accuracy: 0.9515 - Loss: 0.1350\n",
      "\n",
      "Batch 50/992 ━━━━━━━━━━━━━━━━━━━━ 14:37:42\n",
      "Accuracy: 0.9525 - Loss: 0.1330\n",
      "\n",
      "Batch 51/992 ━━━━━━━━━━━━━━━━━━━━ 14:37:52\n",
      "Accuracy: 0.9534 - Loss: 0.1319\n",
      "\n",
      "Batch 52/992 ━━━━━━━━━━━━━━━━━━━━ 14:38:02\n",
      "Accuracy: 0.9519 - Loss: 0.1339\n",
      "\n",
      "Batch 53/992 ━━━━━━━━━━━━━━━━━━━━ 14:38:12\n",
      "Accuracy: 0.9528 - Loss: 0.1317\n",
      "\n",
      "Batch 54/992 ━━━━━━━━━━━━━━━━━━━━ 14:38:23\n",
      "Accuracy: 0.9537 - Loss: 0.1297\n",
      "\n",
      "Batch 55/992 ━━━━━━━━━━━━━━━━━━━━ 14:38:35\n",
      "Accuracy: 0.9545 - Loss: 0.1286\n",
      "\n",
      "Batch 56/992 ━━━━━━━━━━━━━━━━━━━━ 14:38:50\n",
      "Accuracy: 0.9554 - Loss: 0.1264\n",
      "\n",
      "Batch 57/992 ━━━━━━━━━━━━━━━━━━━━ 14:39:03\n",
      "Accuracy: 0.9561 - Loss: 0.1246\n",
      "\n",
      "Batch 58/992 ━━━━━━━━━━━━━━━━━━━━ 14:39:17\n",
      "Accuracy: 0.9569 - Loss: 0.1243\n",
      "\n",
      "Batch 59/992 ━━━━━━━━━━━━━━━━━━━━ 14:39:30\n",
      "Accuracy: 0.9576 - Loss: 0.1239\n",
      "\n",
      "Batch 60/992 ━━━━━━━━━━━━━━━━━━━━ 14:39:41\n",
      "Accuracy: 0.9583 - Loss: 0.1229\n",
      "\n",
      "Batch 61/992 ━━━━━━━━━━━━━━━━━━━━ 14:39:53\n",
      "Accuracy: 0.9590 - Loss: 0.1225\n",
      "\n",
      "Batch 62/992 ━━━━━━━━━━━━━━━━━━━━ 14:40:05\n",
      "Accuracy: 0.9597 - Loss: 0.1229\n",
      "\n",
      "Batch 63/992 ━━━━━━━━━━━━━━━━━━━━ 14:40:17\n",
      "Accuracy: 0.9603 - Loss: 0.1221\n",
      "\n",
      "Batch 64/992 ━━━━━━━━━━━━━━━━━━━━ 14:40:28\n",
      "Accuracy: 0.9609 - Loss: 0.1205\n",
      "\n",
      "Batch 65/992 ━━━━━━━━━━━━━━━━━━━━ 14:40:39\n",
      "Accuracy: 0.9615 - Loss: 0.1201\n",
      "\n",
      "Batch 66/992 ━━━━━━━━━━━━━━━━━━━━ 14:40:50\n",
      "Accuracy: 0.9621 - Loss: 0.1193\n",
      "\n",
      "Batch 67/992 ━━━━━━━━━━━━━━━━━━━━ 14:41:01\n",
      "Accuracy: 0.9627 - Loss: 0.1204\n",
      "\n",
      "Batch 68/992 ━━━━━━━━━━━━━━━━━━━━ 14:41:11\n",
      "Accuracy: 0.9632 - Loss: 0.1196\n",
      "\n",
      "Batch 69/992 ━━━━━━━━━━━━━━━━━━━━ 14:41:22\n",
      "Accuracy: 0.9638 - Loss: 0.1192\n",
      "\n",
      "Batch 70/992 ━━━━━━━━━━━━━━━━━━━━ 14:41:32\n",
      "Accuracy: 0.9643 - Loss: 0.1176\n",
      "\n",
      "Batch 71/992 ━━━━━━━━━━━━━━━━━━━━ 14:41:42\n",
      "Accuracy: 0.9648 - Loss: 0.1161\n",
      "\n",
      "Batch 72/992 ━━━━━━━━━━━━━━━━━━━━ 14:41:52\n",
      "Accuracy: 0.9653 - Loss: 0.1159\n",
      "\n",
      "Batch 73/992 ━━━━━━━━━━━━━━━━━━━━ 14:42:03\n",
      "Accuracy: 0.9640 - Loss: 0.1167\n",
      "\n",
      "Batch 74/992 ━━━━━━━━━━━━━━━━━━━━ 14:42:13\n",
      "Accuracy: 0.9645 - Loss: 0.1153\n",
      "\n",
      "Batch 75/992 ━━━━━━━━━━━━━━━━━━━━ 14:42:24\n",
      "Accuracy: 0.9650 - Loss: 0.1154\n",
      "\n",
      "Batch 76/992 ━━━━━━━━━━━━━━━━━━━━ 14:42:35\n",
      "Accuracy: 0.9638 - Loss: 0.1161\n",
      "\n",
      "Batch 77/992 ━━━━━━━━━━━━━━━━━━━━ 14:42:45\n",
      "Accuracy: 0.9643 - Loss: 0.1157\n",
      "\n",
      "Batch 78/992 ━━━━━━━━━━━━━━━━━━━━ 14:42:56\n",
      "Accuracy: 0.9647 - Loss: 0.1146\n",
      "\n",
      "Batch 79/992 ━━━━━━━━━━━━━━━━━━━━ 14:43:06\n",
      "Accuracy: 0.9652 - Loss: 0.1150\n",
      "\n",
      "Batch 80/992 ━━━━━━━━━━━━━━━━━━━━ 14:43:17\n",
      "Accuracy: 0.9656 - Loss: 0.1141\n",
      "\n",
      "Batch 81/992 ━━━━━━━━━━━━━━━━━━━━ 14:43:28\n",
      "Accuracy: 0.9660 - Loss: 0.1133\n",
      "\n",
      "Batch 82/992 ━━━━━━━━━━━━━━━━━━━━ 14:43:38\n",
      "Accuracy: 0.9665 - Loss: 0.1121\n",
      "\n",
      "Batch 83/992 ━━━━━━━━━━━━━━━━━━━━ 14:43:49\n",
      "Accuracy: 0.9669 - Loss: 0.1123\n",
      "\n",
      "Batch 84/992 ━━━━━━━━━━━━━━━━━━━━ 14:43:59\n",
      "Accuracy: 0.9673 - Loss: 0.1125\n",
      "\n",
      "Batch 85/992 ━━━━━━━━━━━━━━━━━━━━ 14:44:09\n",
      "Accuracy: 0.9676 - Loss: 0.1113\n",
      "\n",
      "Batch 86/992 ━━━━━━━━━━━━━━━━━━━━ 14:44:20\n",
      "Accuracy: 0.9680 - Loss: 0.1107\n",
      "\n",
      "Batch 87/992 ━━━━━━━━━━━━━━━━━━━━ 14:44:31\n",
      "Accuracy: 0.9684 - Loss: 0.1114\n",
      "\n",
      "Batch 88/992 ━━━━━━━━━━━━━━━━━━━━ 14:44:41\n",
      "Accuracy: 0.9688 - Loss: 0.1106\n",
      "\n",
      "Batch 89/992 ━━━━━━━━━━━━━━━━━━━━ 14:44:51\n",
      "Accuracy: 0.9677 - Loss: 0.1118\n",
      "\n",
      "Batch 90/992 ━━━━━━━━━━━━━━━━━━━━ 14:45:02\n",
      "Accuracy: 0.9681 - Loss: 0.1107\n",
      "\n",
      "Batch 91/992 ━━━━━━━━━━━━━━━━━━━━ 14:45:12\n",
      "Accuracy: 0.9670 - Loss: 0.1113\n",
      "\n",
      "Batch 92/992 ━━━━━━━━━━━━━━━━━━━━ 14:45:22\n",
      "Accuracy: 0.9674 - Loss: 0.1106\n",
      "\n",
      "Batch 93/992 ━━━━━━━━━━━━━━━━━━━━ 14:45:33\n",
      "Accuracy: 0.9677 - Loss: 0.1111\n",
      "\n",
      "Batch 94/992 ━━━━━━━━━━━━━━━━━━━━ 14:45:43\n",
      "Accuracy: 0.9681 - Loss: 0.1114\n",
      "\n",
      "Batch 95/992 ━━━━━━━━━━━━━━━━━━━━ 14:45:53\n",
      "Accuracy: 0.9658 - Loss: 0.1139\n",
      "\n",
      "Batch 96/992 ━━━━━━━━━━━━━━━━━━━━ 14:46:03\n",
      "Accuracy: 0.9661 - Loss: 0.1136\n",
      "\n",
      "Batch 97/992 ━━━━━━━━━━━━━━━━━━━━ 14:46:14\n",
      "Accuracy: 0.9665 - Loss: 0.1138\n",
      "\n",
      "Batch 98/992 ━━━━━━━━━━━━━━━━━━━━ 14:46:25\n",
      "Accuracy: 0.9656 - Loss: 0.1166\n",
      "\n",
      "Batch 99/992 ━━━━━━━━━━━━━━━━━━━━ 14:46:36\n",
      "Accuracy: 0.9659 - Loss: 0.1155\n",
      "\n",
      "Batch 100/992 ━━━━━━━━━━━━━━━━━━━━ 14:46:47\n",
      "Accuracy: 0.9663 - Loss: 0.1145\n",
      "\n",
      "Batch 101/992 ━━━━━━━━━━━━━━━━━━━━ 14:46:57\n",
      "Accuracy: 0.9666 - Loss: 0.1146\n",
      "\n",
      "Batch 102/992 ━━━━━━━━━━━━━━━━━━━━ 14:47:07\n",
      "Accuracy: 0.9669 - Loss: 0.1144\n",
      "\n",
      "Batch 103/992 ━━━━━━━━━━━━━━━━━━━━ 14:47:18\n",
      "Accuracy: 0.9672 - Loss: 0.1135\n",
      "\n",
      "Batch 104/992 ━━━━━━━━━━━━━━━━━━━━ 14:47:28\n",
      "Accuracy: 0.9675 - Loss: 0.1129\n",
      "\n",
      "Batch 105/992 ━━━━━━━━━━━━━━━━━━━━ 14:47:39\n",
      "Accuracy: 0.9679 - Loss: 0.1119\n",
      "\n",
      "Batch 106/992 ━━━━━━━━━━━━━━━━━━━━ 14:47:49\n",
      "Accuracy: 0.9658 - Loss: 0.1155\n",
      "\n",
      "Batch 107/992 ━━━━━━━━━━━━━━━━━━━━ 14:47:59\n",
      "Accuracy: 0.9661 - Loss: 0.1146\n",
      "\n",
      "Batch 108/992 ━━━━━━━━━━━━━━━━━━━━ 14:48:10\n",
      "Accuracy: 0.9653 - Loss: 0.1156\n",
      "\n",
      "Batch 109/992 ━━━━━━━━━━━━━━━━━━━━ 14:48:21\n",
      "Accuracy: 0.9656 - Loss: 0.1149\n",
      "\n",
      "Batch 110/992 ━━━━━━━━━━━━━━━━━━━━ 14:48:31\n",
      "Accuracy: 0.9659 - Loss: 0.1142\n",
      "\n",
      "Batch 111/992 ━━━━━━━━━━━━━━━━━━━━ 14:48:42\n",
      "Accuracy: 0.9662 - Loss: 0.1132\n",
      "\n",
      "Batch 112/992 ━━━━━━━━━━━━━━━━━━━━ 14:48:52\n",
      "Accuracy: 0.9665 - Loss: 0.1128\n",
      "\n",
      "Batch 113/992 ━━━━━━━━━━━━━━━━━━━━ 14:49:02\n",
      "Accuracy: 0.9668 - Loss: 0.1124\n",
      "\n",
      "Batch 114/992 ━━━━━━━━━━━━━━━━━━━━ 14:49:12\n",
      "Accuracy: 0.9671 - Loss: 0.1115\n",
      "\n",
      "Batch 115/992 ━━━━━━━━━━━━━━━━━━━━ 14:49:23\n",
      "Accuracy: 0.9674 - Loss: 0.1110\n",
      "\n",
      "Batch 116/992 ━━━━━━━━━━━━━━━━━━━━ 14:49:33\n",
      "Accuracy: 0.9677 - Loss: 0.1101\n",
      "\n",
      "Batch 117/992 ━━━━━━━━━━━━━━━━━━━━ 14:49:43\n",
      "Accuracy: 0.9679 - Loss: 0.1096\n",
      "\n",
      "Batch 118/992 ━━━━━━━━━━━━━━━━━━━━ 14:49:53\n",
      "Accuracy: 0.9682 - Loss: 0.1091\n",
      "\n",
      "Batch 119/992 ━━━━━━━━━━━━━━━━━━━━ 14:50:04\n",
      "Accuracy: 0.9685 - Loss: 0.1088\n",
      "\n",
      "Batch 120/992 ━━━━━━━━━━━━━━━━━━━━ 14:50:14\n",
      "Accuracy: 0.9688 - Loss: 0.1081\n",
      "\n",
      "Batch 121/992 ━━━━━━━━━━━━━━━━━━━━ 14:50:25\n",
      "Accuracy: 0.9690 - Loss: 0.1074\n",
      "\n",
      "Batch 122/992 ━━━━━━━━━━━━━━━━━━━━ 14:50:35\n",
      "Accuracy: 0.9682 - Loss: 0.1101\n",
      "\n",
      "Batch 123/992 ━━━━━━━━━━━━━━━━━━━━ 14:50:46\n",
      "Accuracy: 0.9685 - Loss: 0.1100\n",
      "\n",
      "Batch 124/992 ━━━━━━━━━━━━━━━━━━━━ 14:50:56\n",
      "Accuracy: 0.9688 - Loss: 0.1095\n",
      "\n",
      "Batch 125/992 ━━━━━━━━━━━━━━━━━━━━ 14:51:06\n",
      "Accuracy: 0.9690 - Loss: 0.1088\n",
      "\n",
      "Batch 126/992 ━━━━━━━━━━━━━━━━━━━━ 14:51:16\n",
      "Accuracy: 0.9692 - Loss: 0.1081\n",
      "\n",
      "Batch 127/992 ━━━━━━━━━━━━━━━━━━━━ 14:51:27\n",
      "Accuracy: 0.9685 - Loss: 0.1116\n",
      "\n",
      "Batch 128/992 ━━━━━━━━━━━━━━━━━━━━ 14:51:37\n",
      "Accuracy: 0.9678 - Loss: 0.1134\n",
      "\n",
      "Batch 129/992 ━━━━━━━━━━━━━━━━━━━━ 14:51:47\n",
      "Accuracy: 0.9680 - Loss: 0.1131\n",
      "\n",
      "Batch 130/992 ━━━━━━━━━━━━━━━━━━━━ 14:51:57\n",
      "Accuracy: 0.9683 - Loss: 0.1127\n",
      "\n",
      "Batch 131/992 ━━━━━━━━━━━━━━━━━━━━ 14:52:07\n",
      "Accuracy: 0.9685 - Loss: 0.1133\n",
      "\n",
      "Batch 132/992 ━━━━━━━━━━━━━━━━━━━━ 14:52:18\n",
      "Accuracy: 0.9688 - Loss: 0.1126\n",
      "\n",
      "Batch 133/992 ━━━━━━━━━━━━━━━━━━━━ 14:52:29\n",
      "Accuracy: 0.9690 - Loss: 0.1125\n",
      "\n",
      "Batch 134/992 ━━━━━━━━━━━━━━━━━━━━ 14:52:39\n",
      "Accuracy: 0.9692 - Loss: 0.1130\n",
      "\n",
      "Batch 135/992 ━━━━━━━━━━━━━━━━━━━━ 14:52:50\n",
      "Accuracy: 0.9694 - Loss: 0.1127\n",
      "\n",
      "Batch 136/992 ━━━━━━━━━━━━━━━━━━━━ 14:53:03\n",
      "Accuracy: 0.9697 - Loss: 0.1124\n",
      "\n",
      "Batch 137/992 ━━━━━━━━━━━━━━━━━━━━ 14:53:17\n",
      "Accuracy: 0.9699 - Loss: 0.1118\n",
      "\n",
      "Batch 138/992 ━━━━━━━━━━━━━━━━━━━━ 14:53:29\n",
      "Accuracy: 0.9701 - Loss: 0.1112\n",
      "\n",
      "Batch 139/992 ━━━━━━━━━━━━━━━━━━━━ 14:53:39\n",
      "Accuracy: 0.9694 - Loss: 0.1116\n",
      "\n",
      "Batch 140/992 ━━━━━━━━━━━━━━━━━━━━ 14:53:50\n",
      "Accuracy: 0.9688 - Loss: 0.1139\n",
      "\n",
      "Batch 141/992 ━━━━━━━━━━━━━━━━━━━━ 14:54:01\n",
      "Accuracy: 0.9690 - Loss: 0.1132\n",
      "\n",
      "Batch 142/992 ━━━━━━━━━━━━━━━━━━━━ 14:54:12\n",
      "Accuracy: 0.9683 - Loss: 0.1146\n",
      "\n",
      "Batch 143/992 ━━━━━━━━━━━━━━━━━━━━ 14:54:23\n",
      "Accuracy: 0.9677 - Loss: 0.1161\n",
      "\n",
      "Batch 144/992 ━━━━━━━━━━━━━━━━━━━━ 14:54:34\n",
      "Accuracy: 0.9679 - Loss: 0.1158\n",
      "\n",
      "Batch 145/992 ━━━━━━━━━━━━━━━━━━━━ 14:54:44\n",
      "Accuracy: 0.9672 - Loss: 0.1162\n",
      "\n",
      "Batch 146/992 ━━━━━━━━━━━━━━━━━━━━ 14:54:55\n",
      "Accuracy: 0.9675 - Loss: 0.1158\n",
      "\n",
      "Batch 147/992 ━━━━━━━━━━━━━━━━━━━━ 14:55:06\n",
      "Accuracy: 0.9668 - Loss: 0.1182\n",
      "\n",
      "Batch 148/992 ━━━━━━━━━━━━━━━━━━━━ 14:55:16\n",
      "Accuracy: 0.9671 - Loss: 0.1181\n",
      "\n",
      "Batch 149/992 ━━━━━━━━━━━━━━━━━━━━ 14:55:28\n",
      "Accuracy: 0.9673 - Loss: 0.1174\n",
      "\n",
      "Batch 150/992 ━━━━━━━━━━━━━━━━━━━━ 14:55:38\n",
      "Accuracy: 0.9675 - Loss: 0.1170\n",
      "\n",
      "Batch 151/992 ━━━━━━━━━━━━━━━━━━━━ 14:55:49\n",
      "Accuracy: 0.9677 - Loss: 0.1166\n",
      "\n",
      "Batch 152/992 ━━━━━━━━━━━━━━━━━━━━ 14:56:00\n",
      "Accuracy: 0.9679 - Loss: 0.1167\n",
      "\n",
      "Batch 153/992 ━━━━━━━━━━━━━━━━━━━━ 14:56:12\n",
      "Accuracy: 0.9681 - Loss: 0.1159\n",
      "\n",
      "Batch 154/992 ━━━━━━━━━━━━━━━━━━━━ 14:56:27\n",
      "Accuracy: 0.9675 - Loss: 0.1162\n",
      "\n",
      "Batch 155/992 ━━━━━━━━━━━━━━━━━━━━ 14:56:42\n",
      "Accuracy: 0.9677 - Loss: 0.1158\n",
      "\n",
      "Batch 156/992 ━━━━━━━━━━━━━━━━━━━━ 14:56:53\n",
      "Accuracy: 0.9671 - Loss: 0.1184\n",
      "\n",
      "Batch 157/992 ━━━━━━━━━━━━━━━━━━━━ 14:57:03\n",
      "Accuracy: 0.9666 - Loss: 0.1211\n",
      "\n",
      "Batch 158/992 ━━━━━━━━━━━━━━━━━━━━ 14:57:14\n",
      "Accuracy: 0.9668 - Loss: 0.1212\n",
      "\n",
      "Batch 159/992 ━━━━━━━━━━━━━━━━━━━━ 14:57:25\n",
      "Accuracy: 0.9670 - Loss: 0.1206\n",
      "\n",
      "Batch 160/992 ━━━━━━━━━━━━━━━━━━━━ 14:57:36\n",
      "Accuracy: 0.9672 - Loss: 0.1202\n",
      "\n",
      "Batch 161/992 ━━━━━━━━━━━━━━━━━━━━ 14:57:47\n",
      "Accuracy: 0.9674 - Loss: 0.1198\n",
      "\n",
      "Batch 162/992 ━━━━━━━━━━━━━━━━━━━━ 14:58:03\n",
      "Accuracy: 0.9676 - Loss: 0.1193\n",
      "\n",
      "Batch 163/992 ━━━━━━━━━━━━━━━━━━━━ 14:58:15\n",
      "Accuracy: 0.9678 - Loss: 0.1189\n",
      "\n",
      "Batch 164/992 ━━━━━━━━━━━━━━━━━━━━ 14:58:26\n",
      "Accuracy: 0.9672 - Loss: 0.1188\n",
      "\n",
      "Batch 165/992 ━━━━━━━━━━━━━━━━━━━━ 14:58:36\n",
      "Accuracy: 0.9674 - Loss: 0.1181\n",
      "\n",
      "Batch 166/992 ━━━━━━━━━━━━━━━━━━━━ 14:58:47\n",
      "Accuracy: 0.9676 - Loss: 0.1176\n",
      "\n",
      "Batch 167/992 ━━━━━━━━━━━━━━━━━━━━ 14:58:57\n",
      "Accuracy: 0.9678 - Loss: 0.1172\n",
      "\n",
      "Batch 168/992 ━━━━━━━━━━━━━━━━━━━━ 14:59:07\n",
      "Accuracy: 0.9680 - Loss: 0.1166\n",
      "\n",
      "Batch 169/992 ━━━━━━━━━━━━━━━━━━━━ 14:59:17\n",
      "Accuracy: 0.9675 - Loss: 0.1184\n",
      "\n",
      "Batch 170/992 ━━━━━━━━━━━━━━━━━━━━ 14:59:28\n",
      "Accuracy: 0.9669 - Loss: 0.1187\n",
      "\n",
      "Batch 171/992 ━━━━━━━━━━━━━━━━━━━━ 14:59:39\n",
      "Accuracy: 0.9671 - Loss: 0.1182\n",
      "\n",
      "Batch 172/992 ━━━━━━━━━━━━━━━━━━━━ 14:59:50\n",
      "Accuracy: 0.9673 - Loss: 0.1176\n",
      "\n",
      "Batch 173/992 ━━━━━━━━━━━━━━━━━━━━ 15:00:01\n",
      "Accuracy: 0.9675 - Loss: 0.1170\n",
      "\n",
      "Batch 174/992 ━━━━━━━━━━━━━━━━━━━━ 15:00:12\n",
      "Accuracy: 0.9677 - Loss: 0.1164\n",
      "\n",
      "Batch 175/992 ━━━━━━━━━━━━━━━━━━━━ 15:00:23\n",
      "Accuracy: 0.9671 - Loss: 0.1169\n",
      "\n",
      "Batch 176/992 ━━━━━━━━━━━━━━━━━━━━ 15:00:36\n",
      "Accuracy: 0.9673 - Loss: 0.1168\n",
      "\n",
      "Batch 177/992 ━━━━━━━━━━━━━━━━━━━━ 15:00:49\n",
      "Accuracy: 0.9675 - Loss: 0.1162\n",
      "\n",
      "Batch 178/992 ━━━━━━━━━━━━━━━━━━━━ 15:01:00\n",
      "Accuracy: 0.9677 - Loss: 0.1158\n",
      "\n",
      "Batch 179/992 ━━━━━━━━━━━━━━━━━━━━ 15:01:11\n",
      "Accuracy: 0.9672 - Loss: 0.1165\n",
      "\n",
      "Batch 180/992 ━━━━━━━━━━━━━━━━━━━━ 15:01:22\n",
      "Accuracy: 0.9674 - Loss: 0.1160\n",
      "\n",
      "Batch 181/992 ━━━━━━━━━━━━━━━━━━━━ 15:01:35\n",
      "Accuracy: 0.9675 - Loss: 0.1155\n",
      "\n",
      "Batch 182/992 ━━━━━━━━━━━━━━━━━━━━ 15:01:48\n",
      "Accuracy: 0.9670 - Loss: 0.1155\n",
      "\n",
      "Batch 183/992 ━━━━━━━━━━━━━━━━━━━━ 15:02:00\n",
      "Accuracy: 0.9672 - Loss: 0.1153\n",
      "\n",
      "Batch 184/992 ━━━━━━━━━━━━━━━━━━━━ 15:02:12\n",
      "Accuracy: 0.9674 - Loss: 0.1147\n",
      "\n",
      "Batch 185/992 ━━━━━━━━━━━━━━━━━━━━ 15:02:24\n",
      "Accuracy: 0.9669 - Loss: 0.1152\n",
      "\n",
      "Batch 186/992 ━━━━━━━━━━━━━━━━━━━━ 15:02:39\n",
      "Accuracy: 0.9671 - Loss: 0.1147\n",
      "\n",
      "Batch 187/992 ━━━━━━━━━━━━━━━━━━━━ 15:02:50\n",
      "Accuracy: 0.9666 - Loss: 0.1155\n",
      "\n",
      "Batch 188/992 ━━━━━━━━━━━━━━━━━━━━ 15:03:01\n",
      "Accuracy: 0.9668 - Loss: 0.1152\n",
      "\n",
      "Batch 189/992 ━━━━━━━━━━━━━━━━━━━━ 15:03:13\n",
      "Accuracy: 0.9669 - Loss: 0.1149\n",
      "\n",
      "Batch 190/992 ━━━━━━━━━━━━━━━━━━━━ 15:03:23\n",
      "Accuracy: 0.9671 - Loss: 0.1149\n",
      "\n",
      "Batch 191/992 ━━━━━━━━━━━━━━━━━━━━ 15:03:35\n",
      "Accuracy: 0.9673 - Loss: 0.1146\n",
      "\n",
      "Batch 192/992 ━━━━━━━━━━━━━━━━━━━━ 15:03:48\n",
      "Accuracy: 0.9674 - Loss: 0.1142\n",
      "\n",
      "Batch 193/992 ━━━━━━━━━━━━━━━━━━━━ 15:04:07\n",
      "Accuracy: 0.9676 - Loss: 0.1137\n",
      "\n",
      "Batch 194/992 ━━━━━━━━━━━━━━━━━━━━ 15:04:19\n",
      "Accuracy: 0.9678 - Loss: 0.1135\n",
      "\n",
      "Batch 195/992 ━━━━━━━━━━━━━━━━━━━━ 15:04:32\n",
      "Accuracy: 0.9679 - Loss: 0.1129\n",
      "\n",
      "Batch 196/992 ━━━━━━━━━━━━━━━━━━━━ 15:04:44\n",
      "Accuracy: 0.9681 - Loss: 0.1124\n",
      "\n",
      "Batch 197/992 ━━━━━━━━━━━━━━━━━━━━ 15:04:56\n",
      "Accuracy: 0.9676 - Loss: 0.1124\n",
      "\n",
      "Batch 198/992 ━━━━━━━━━━━━━━━━━━━━ 15:05:08\n",
      "Accuracy: 0.9678 - Loss: 0.1119\n",
      "\n",
      "Batch 199/992 ━━━━━━━━━━━━━━━━━━━━ 15:05:19\n",
      "Accuracy: 0.9673 - Loss: 0.1125\n",
      "\n",
      "Batch 200/992 ━━━━━━━━━━━━━━━━━━━━ 15:05:29\n",
      "Accuracy: 0.9675 - Loss: 0.1120\n",
      "\n",
      "Batch 201/992 ━━━━━━━━━━━━━━━━━━━━ 15:05:40\n",
      "Accuracy: 0.9670 - Loss: 0.1135\n",
      "\n",
      "Batch 202/992 ━━━━━━━━━━━━━━━━━━━━ 15:05:51\n",
      "Accuracy: 0.9672 - Loss: 0.1130\n",
      "\n",
      "Batch 203/992 ━━━━━━━━━━━━━━━━━━━━ 15:06:04\n",
      "Accuracy: 0.9667 - Loss: 0.1133\n",
      "\n",
      "Batch 204/992 ━━━━━━━━━━━━━━━━━━━━ 15:06:17\n",
      "Accuracy: 0.9669 - Loss: 0.1137\n",
      "\n",
      "Batch 205/992 ━━━━━━━━━━━━━━━━━━━━ 15:06:29\n",
      "Accuracy: 0.9671 - Loss: 0.1135\n",
      "\n",
      "Batch 206/992 ━━━━━━━━━━━━━━━━━━━━ 15:06:40\n",
      "Accuracy: 0.9672 - Loss: 0.1130\n",
      "\n",
      "Batch 207/992 ━━━━━━━━━━━━━━━━━━━━ 15:06:51\n",
      "Accuracy: 0.9674 - Loss: 0.1125\n",
      "\n",
      "Batch 208/992 ━━━━━━━━━━━━━━━━━━━━ 15:07:03\n",
      "Accuracy: 0.9675 - Loss: 0.1121\n",
      "\n",
      "Batch 209/992 ━━━━━━━━━━━━━━━━━━━━ 15:07:15\n",
      "Accuracy: 0.9677 - Loss: 0.1119\n",
      "\n",
      "Batch 210/992 ━━━━━━━━━━━━━━━━━━━━ 15:07:26\n",
      "Accuracy: 0.9679 - Loss: 0.1117\n",
      "\n",
      "Batch 211/992 ━━━━━━━━━━━━━━━━━━━━ 15:07:37\n",
      "Accuracy: 0.9680 - Loss: 0.1112\n",
      "\n",
      "Batch 212/992 ━━━━━━━━━━━━━━━━━━━━ 15:07:47\n",
      "Accuracy: 0.9682 - Loss: 0.1107\n",
      "\n",
      "Batch 213/992 ━━━━━━━━━━━━━━━━━━━━ 15:07:58\n",
      "Accuracy: 0.9677 - Loss: 0.1115\n",
      "\n",
      "Batch 214/992 ━━━━━━━━━━━━━━━━━━━━ 15:08:08\n",
      "Accuracy: 0.9679 - Loss: 0.1110\n",
      "\n",
      "Batch 215/992 ━━━━━━━━━━━━━━━━━━━━ 15:08:19\n",
      "Accuracy: 0.9680 - Loss: 0.1105\n",
      "\n",
      "Batch 216/992 ━━━━━━━━━━━━━━━━━━━━ 15:08:30\n",
      "Accuracy: 0.9682 - Loss: 0.1102\n",
      "\n",
      "Batch 217/992 ━━━━━━━━━━━━━━━━━━━━ 15:08:40\n",
      "Accuracy: 0.9677 - Loss: 0.1117\n",
      "\n",
      "Batch 218/992 ━━━━━━━━━━━━━━━━━━━━ 15:08:51\n",
      "Accuracy: 0.9679 - Loss: 0.1119\n",
      "\n",
      "Batch 219/992 ━━━━━━━━━━━━━━━━━━━━ 15:09:01\n",
      "Accuracy: 0.9680 - Loss: 0.1118\n",
      "\n",
      "Batch 220/992 ━━━━━━━━━━━━━━━━━━━━ 15:09:12\n",
      "Accuracy: 0.9676 - Loss: 0.1119\n",
      "\n",
      "Batch 221/992 ━━━━━━━━━━━━━━━━━━━━ 15:09:26\n",
      "Accuracy: 0.9678 - Loss: 0.1116\n",
      "\n",
      "Batch 222/992 ━━━━━━━━━━━━━━━━━━━━ 15:09:39\n",
      "Accuracy: 0.9679 - Loss: 0.1116\n",
      "\n",
      "Batch 223/992 ━━━━━━━━━━━━━━━━━━━━ 15:09:52\n",
      "Accuracy: 0.9680 - Loss: 0.1114\n",
      "\n",
      "Batch 224/992 ━━━━━━━━━━━━━━━━━━━━ 15:10:03\n",
      "Accuracy: 0.9682 - Loss: 0.1110\n",
      "\n",
      "Batch 225/992 ━━━━━━━━━━━━━━━━━━━━ 15:10:14\n",
      "Accuracy: 0.9683 - Loss: 0.1105\n",
      "\n",
      "Batch 226/992 ━━━━━━━━━━━━━━━━━━━━ 15:10:25\n",
      "Accuracy: 0.9685 - Loss: 0.1103\n",
      "\n",
      "Batch 227/992 ━━━━━━━━━━━━━━━━━━━━ 15:10:37\n",
      "Accuracy: 0.9686 - Loss: 0.1099\n",
      "\n",
      "Batch 228/992 ━━━━━━━━━━━━━━━━━━━━ 15:10:47\n",
      "Accuracy: 0.9688 - Loss: 0.1097\n",
      "\n",
      "Batch 229/992 ━━━━━━━━━━━━━━━━━━━━ 15:10:58\n",
      "Accuracy: 0.9683 - Loss: 0.1113\n",
      "\n",
      "Batch 230/992 ━━━━━━━━━━━━━━━━━━━━ 15:11:09\n",
      "Accuracy: 0.9685 - Loss: 0.1113\n",
      "\n",
      "Batch 231/992 ━━━━━━━━━━━━━━━━━━━━ 15:11:20\n",
      "Accuracy: 0.9686 - Loss: 0.1112\n",
      "\n",
      "Batch 232/992 ━━━━━━━━━━━━━━━━━━━━ 15:11:31\n",
      "Accuracy: 0.9688 - Loss: 0.1109\n",
      "\n",
      "Batch 233/992 ━━━━━━━━━━━━━━━━━━━━ 15:11:42\n",
      "Accuracy: 0.9689 - Loss: 0.1106\n",
      "\n",
      "Batch 234/992 ━━━━━━━━━━━━━━━━━━━━ 15:11:53\n",
      "Accuracy: 0.9690 - Loss: 0.1106\n",
      "\n",
      "Batch 235/992 ━━━━━━━━━━━━━━━━━━━━ 15:12:04\n",
      "Accuracy: 0.9691 - Loss: 0.1102\n",
      "\n",
      "Batch 236/992 ━━━━━━━━━━━━━━━━━━━━ 15:12:14\n",
      "Accuracy: 0.9693 - Loss: 0.1099\n",
      "\n",
      "Batch 237/992 ━━━━━━━━━━━━━━━━━━━━ 15:12:25\n",
      "Accuracy: 0.9694 - Loss: 0.1099\n",
      "\n",
      "Batch 238/992 ━━━━━━━━━━━━━━━━━━━━ 15:12:36\n",
      "Accuracy: 0.9690 - Loss: 0.1109\n",
      "\n",
      "Batch 239/992 ━━━━━━━━━━━━━━━━━━━━ 15:12:47\n",
      "Accuracy: 0.9691 - Loss: 0.1105\n",
      "\n",
      "Batch 240/992 ━━━━━━━━━━━━━━━━━━━━ 15:12:59\n",
      "Accuracy: 0.9688 - Loss: 0.1106\n",
      "\n",
      "Batch 241/992 ━━━━━━━━━━━━━━━━━━━━ 15:13:10\n",
      "Accuracy: 0.9689 - Loss: 0.1105\n",
      "\n",
      "Batch 242/992 ━━━━━━━━━━━━━━━━━━━━ 15:13:22\n",
      "Accuracy: 0.9690 - Loss: 0.1102\n",
      "\n",
      "Batch 243/992 ━━━━━━━━━━━━━━━━━━━━ 15:13:32\n",
      "Accuracy: 0.9686 - Loss: 0.1107\n",
      "\n",
      "Batch 244/992 ━━━━━━━━━━━━━━━━━━━━ 15:13:43\n",
      "Accuracy: 0.9688 - Loss: 0.1102\n",
      "\n",
      "Batch 245/992 ━━━━━━━━━━━━━━━━━━━━ 15:13:54\n",
      "Accuracy: 0.9689 - Loss: 0.1099\n",
      "\n",
      "Batch 246/992 ━━━━━━━━━━━━━━━━━━━━ 15:14:05\n",
      "Accuracy: 0.9690 - Loss: 0.1099\n",
      "\n",
      "Batch 247/992 ━━━━━━━━━━━━━━━━━━━━ 15:14:15\n",
      "Accuracy: 0.9691 - Loss: 0.1095\n",
      "\n",
      "Batch 248/992 ━━━━━━━━━━━━━━━━━━━━ 15:14:26\n",
      "Accuracy: 0.9693 - Loss: 0.1092\n",
      "\n",
      "Batch 249/992 ━━━━━━━━━━━━━━━━━━━━ 15:14:37\n",
      "Accuracy: 0.9694 - Loss: 0.1092\n",
      "\n",
      "Batch 250/992 ━━━━━━━━━━━━━━━━━━━━ 15:14:48\n",
      "Accuracy: 0.9695 - Loss: 0.1092\n",
      "\n",
      "Batch 251/992 ━━━━━━━━━━━━━━━━━━━━ 15:14:59\n",
      "Accuracy: 0.9696 - Loss: 0.1088\n",
      "\n",
      "Batch 252/992 ━━━━━━━━━━━━━━━━━━━━ 15:15:10\n",
      "Accuracy: 0.9697 - Loss: 0.1087\n",
      "\n",
      "Batch 253/992 ━━━━━━━━━━━━━━━━━━━━ 15:15:22\n",
      "Accuracy: 0.9699 - Loss: 0.1084\n",
      "\n",
      "Batch 254/992 ━━━━━━━━━━━━━━━━━━━━ 15:15:33\n",
      "Accuracy: 0.9700 - Loss: 0.1082\n",
      "\n",
      "Batch 255/992 ━━━━━━━━━━━━━━━━━━━━ 15:15:44\n",
      "Accuracy: 0.9696 - Loss: 0.1098\n",
      "\n",
      "Batch 256/992 ━━━━━━━━━━━━━━━━━━━━ 15:15:55\n",
      "Accuracy: 0.9692 - Loss: 0.1107\n",
      "\n",
      "Batch 257/992 ━━━━━━━━━━━━━━━━━━━━ 15:16:06\n",
      "Accuracy: 0.9694 - Loss: 0.1103\n",
      "\n",
      "Batch 258/992 ━━━━━━━━━━━━━━━━━━━━ 15:16:18\n",
      "Accuracy: 0.9690 - Loss: 0.1104\n",
      "\n",
      "Batch 259/992 ━━━━━━━━━━━━━━━━━━━━ 15:16:30\n",
      "Accuracy: 0.9681 - Loss: 0.1115\n",
      "\n",
      "Batch 260/992 ━━━━━━━━━━━━━━━━━━━━ 15:16:43\n",
      "Accuracy: 0.9678 - Loss: 0.1121\n",
      "\n",
      "Batch 261/992 ━━━━━━━━━━━━━━━━━━━━ 15:16:55\n",
      "Accuracy: 0.9674 - Loss: 0.1123\n",
      "\n",
      "Batch 262/992 ━━━━━━━━━━━━━━━━━━━━ 15:17:07\n",
      "Accuracy: 0.9676 - Loss: 0.1120\n",
      "\n",
      "Batch 263/992 ━━━━━━━━━━━━━━━━━━━━ 15:17:19\n",
      "Accuracy: 0.9677 - Loss: 0.1119\n",
      "\n",
      "Batch 264/992 ━━━━━━━━━━━━━━━━━━━━ 15:17:32\n",
      "Accuracy: 0.9678 - Loss: 0.1117\n",
      "\n",
      "Batch 265/992 ━━━━━━━━━━━━━━━━━━━━ 15:17:47\n",
      "Accuracy: 0.9679 - Loss: 0.1113\n",
      "\n",
      "Batch 266/992 ━━━━━━━━━━━━━━━━━━━━ 15:17:58\n",
      "Accuracy: 0.9676 - Loss: 0.1120\n",
      "\n",
      "Batch 267/992 ━━━━━━━━━━━━━━━━━━━━ 15:18:09\n",
      "Accuracy: 0.9677 - Loss: 0.1118\n",
      "\n",
      "Batch 268/992 ━━━━━━━━━━━━━━━━━━━━ 15:18:20\n",
      "Accuracy: 0.9678 - Loss: 0.1115\n",
      "\n",
      "Batch 269/992 ━━━━━━━━━━━━━━━━━━━━ 15:18:30\n",
      "Accuracy: 0.9679 - Loss: 0.1113\n",
      "\n",
      "Batch 270/992 ━━━━━━━━━━━━━━━━━━━━ 15:18:42\n",
      "Accuracy: 0.9681 - Loss: 0.1111\n",
      "\n",
      "Batch 271/992 ━━━━━━━━━━━━━━━━━━━━ 15:18:52\n",
      "Accuracy: 0.9682 - Loss: 0.1112\n",
      "\n",
      "Batch 272/992 ━━━━━━━━━━━━━━━━━━━━ 15:19:03\n",
      "Accuracy: 0.9683 - Loss: 0.1111\n",
      "\n",
      "Batch 273/992 ━━━━━━━━━━━━━━━━━━━━ 15:19:13\n",
      "Accuracy: 0.9679 - Loss: 0.1117\n",
      "\n",
      "Batch 274/992 ━━━━━━━━━━━━━━━━━━━━ 15:19:24\n",
      "Accuracy: 0.9681 - Loss: 0.1117\n",
      "\n",
      "Batch 275/992 ━━━━━━━━━━━━━━━━━━━━ 15:19:34\n",
      "Accuracy: 0.9677 - Loss: 0.1117\n",
      "\n",
      "Batch 276/992 ━━━━━━━━━━━━━━━━━━━━ 15:19:45\n",
      "Accuracy: 0.9678 - Loss: 0.1114\n",
      "\n",
      "Batch 277/992 ━━━━━━━━━━━━━━━━━━━━ 15:19:55\n",
      "Accuracy: 0.9680 - Loss: 0.1110\n",
      "\n",
      "Batch 278/992 ━━━━━━━━━━━━━━━━━━━━ 15:20:06\n",
      "Accuracy: 0.9672 - Loss: 0.1117\n",
      "\n",
      "Batch 279/992 ━━━━━━━━━━━━━━━━━━━━ 15:20:16\n",
      "Accuracy: 0.9673 - Loss: 0.1113\n",
      "\n",
      "Batch 280/992 ━━━━━━━━━━━━━━━━━━━━ 15:20:26\n",
      "Accuracy: 0.9665 - Loss: 0.1120\n",
      "\n",
      "Batch 281/992 ━━━━━━━━━━━━━━━━━━━━ 15:20:37\n",
      "Accuracy: 0.9666 - Loss: 0.1118\n",
      "\n",
      "Batch 282/992 ━━━━━━━━━━━━━━━━━━━━ 15:20:48\n",
      "Accuracy: 0.9663 - Loss: 0.1121\n",
      "\n",
      "Batch 283/992 ━━━━━━━━━━━━━━━━━━━━ 15:20:59\n",
      "Accuracy: 0.9664 - Loss: 0.1118\n",
      "\n",
      "Batch 284/992 ━━━━━━━━━━━━━━━━━━━━ 15:21:10\n",
      "Accuracy: 0.9657 - Loss: 0.1124\n",
      "\n",
      "Batch 285/992 ━━━━━━━━━━━━━━━━━━━━ 15:21:20\n",
      "Accuracy: 0.9658 - Loss: 0.1121\n",
      "\n",
      "Batch 286/992 ━━━━━━━━━━━━━━━━━━━━ 15:21:31\n",
      "Accuracy: 0.9655 - Loss: 0.1122\n",
      "\n",
      "Batch 287/992 ━━━━━━━━━━━━━━━━━━━━ 15:21:42\n",
      "Accuracy: 0.9656 - Loss: 0.1119\n",
      "\n",
      "Batch 288/992 ━━━━━━━━━━━━━━━━━━━━ 15:21:52\n",
      "Accuracy: 0.9657 - Loss: 0.1119\n",
      "\n",
      "Batch 289/992 ━━━━━━━━━━━━━━━━━━━━ 15:22:03\n",
      "Accuracy: 0.9654 - Loss: 0.1125\n",
      "\n",
      "Batch 290/992 ━━━━━━━━━━━━━━━━━━━━ 15:22:13\n",
      "Accuracy: 0.9651 - Loss: 0.1128\n",
      "\n",
      "Batch 291/992 ━━━━━━━━━━━━━━━━━━━━ 15:22:24\n",
      "Accuracy: 0.9652 - Loss: 0.1130\n",
      "\n",
      "Batch 292/992 ━━━━━━━━━━━━━━━━━━━━ 15:22:35\n",
      "Accuracy: 0.9649 - Loss: 0.1132\n",
      "\n",
      "Batch 293/992 ━━━━━━━━━━━━━━━━━━━━ 15:22:45\n",
      "Accuracy: 0.9650 - Loss: 0.1128\n",
      "\n",
      "Batch 294/992 ━━━━━━━━━━━━━━━━━━━━ 15:22:56\n",
      "Accuracy: 0.9651 - Loss: 0.1125\n",
      "\n",
      "Batch 295/992 ━━━━━━━━━━━━━━━━━━━━ 15:23:06\n",
      "Accuracy: 0.9653 - Loss: 0.1122\n",
      "\n",
      "Batch 296/992 ━━━━━━━━━━━━━━━━━━━━ 15:23:17\n",
      "Accuracy: 0.9654 - Loss: 0.1121\n",
      "\n",
      "Batch 297/992 ━━━━━━━━━━━━━━━━━━━━ 15:23:28\n",
      "Accuracy: 0.9655 - Loss: 0.1118\n",
      "\n",
      "Batch 298/992 ━━━━━━━━━━━━━━━━━━━━ 15:23:39\n",
      "Accuracy: 0.9656 - Loss: 0.1116\n",
      "\n",
      "Batch 299/992 ━━━━━━━━━━━━━━━━━━━━ 15:23:49\n",
      "Accuracy: 0.9657 - Loss: 0.1115\n",
      "\n",
      "Batch 300/992 ━━━━━━━━━━━━━━━━━━━━ 15:24:00\n",
      "Accuracy: 0.9658 - Loss: 0.1111\n",
      "\n",
      "Batch 301/992 ━━━━━━━━━━━━━━━━━━━━ 15:24:10\n",
      "Accuracy: 0.9659 - Loss: 0.1111\n",
      "\n",
      "Batch 302/992 ━━━━━━━━━━━━━━━━━━━━ 15:24:21\n",
      "Accuracy: 0.9661 - Loss: 0.1111\n",
      "\n",
      "Batch 303/992 ━━━━━━━━━━━━━━━━━━━━ 15:24:32\n",
      "Accuracy: 0.9658 - Loss: 0.1117\n",
      "\n",
      "Batch 304/992 ━━━━━━━━━━━━━━━━━━━━ 15:24:43\n",
      "Accuracy: 0.9655 - Loss: 0.1117\n",
      "\n",
      "Batch 305/992 ━━━━━━━━━━━━━━━━━━━━ 15:24:54\n",
      "Accuracy: 0.9656 - Loss: 0.1114\n",
      "\n",
      "Batch 306/992 ━━━━━━━━━━━━━━━━━━━━ 15:25:05\n",
      "Accuracy: 0.9657 - Loss: 0.1112\n",
      "\n",
      "Batch 307/992 ━━━━━━━━━━━━━━━━━━━━ 15:25:16\n",
      "Accuracy: 0.9658 - Loss: 0.1108\n",
      "\n",
      "Batch 308/992 ━━━━━━━━━━━━━━━━━━━━ 15:25:27\n",
      "Accuracy: 0.9659 - Loss: 0.1105\n",
      "\n",
      "Batch 309/992 ━━━━━━━━━━━━━━━━━━━━ 15:25:39\n",
      "Accuracy: 0.9660 - Loss: 0.1104\n",
      "\n",
      "Batch 310/992 ━━━━━━━━━━━━━━━━━━━━ 15:25:52\n",
      "Accuracy: 0.9661 - Loss: 0.1102\n",
      "\n",
      "Batch 311/992 ━━━━━━━━━━━━━━━━━━━━ 15:26:03\n",
      "Accuracy: 0.9662 - Loss: 0.1101\n",
      "\n",
      "Batch 312/992 ━━━━━━━━━━━━━━━━━━━━ 15:26:13\n",
      "Accuracy: 0.9655 - Loss: 0.1106\n",
      "\n",
      "Batch 313/992 ━━━━━━━━━━━━━━━━━━━━ 15:26:23\n",
      "Accuracy: 0.9657 - Loss: 0.1106\n",
      "\n",
      "Batch 314/992 ━━━━━━━━━━━━━━━━━━━━ 15:26:34\n",
      "Accuracy: 0.9658 - Loss: 0.1105\n",
      "\n",
      "Batch 315/992 ━━━━━━━━━━━━━━━━━━━━ 15:26:44\n",
      "Accuracy: 0.9655 - Loss: 0.1106\n",
      "\n",
      "Batch 316/992 ━━━━━━━━━━━━━━━━━━━━ 15:26:54\n",
      "Accuracy: 0.9656 - Loss: 0.1106\n",
      "\n",
      "Batch 317/992 ━━━━━━━━━━━━━━━━━━━━ 15:27:04\n",
      "Accuracy: 0.9657 - Loss: 0.1103\n",
      "\n",
      "Batch 318/992 ━━━━━━━━━━━━━━━━━━━━ 15:27:15\n",
      "Accuracy: 0.9658 - Loss: 0.1100\n",
      "\n",
      "Batch 319/992 ━━━━━━━━━━━━━━━━━━━━ 15:27:25\n",
      "Accuracy: 0.9659 - Loss: 0.1098\n",
      "\n",
      "Batch 320/992 ━━━━━━━━━━━━━━━━━━━━ 15:27:35\n",
      "Accuracy: 0.9660 - Loss: 0.1099\n",
      "\n",
      "Batch 321/992 ━━━━━━━━━━━━━━━━━━━━ 15:27:45\n",
      "Accuracy: 0.9653 - Loss: 0.1106\n",
      "\n",
      "Batch 322/992 ━━━━━━━━━━━━━━━━━━━━ 15:27:55\n",
      "Accuracy: 0.9655 - Loss: 0.1103\n",
      "\n",
      "Batch 323/992 ━━━━━━━━━━━━━━━━━━━━ 15:28:06\n",
      "Accuracy: 0.9656 - Loss: 0.1102\n",
      "\n",
      "Batch 324/992 ━━━━━━━━━━━━━━━━━━━━ 15:28:16\n",
      "Accuracy: 0.9653 - Loss: 0.1102\n",
      "\n",
      "Batch 325/992 ━━━━━━━━━━━━━━━━━━━━ 15:28:26\n",
      "Accuracy: 0.9654 - Loss: 0.1101\n",
      "\n",
      "Batch 326/992 ━━━━━━━━━━━━━━━━━━━━ 15:28:36\n",
      "Accuracy: 0.9655 - Loss: 0.1100\n",
      "\n",
      "Batch 327/992 ━━━━━━━━━━━━━━━━━━━━ 15:28:47\n",
      "Accuracy: 0.9656 - Loss: 0.1097\n",
      "\n",
      "Batch 328/992 ━━━━━━━━━━━━━━━━━━━━ 15:28:57\n",
      "Accuracy: 0.9649 - Loss: 0.1114\n",
      "\n",
      "Batch 329/992 ━━━━━━━━━━━━━━━━━━━━ 15:29:07\n",
      "Accuracy: 0.9650 - Loss: 0.1113\n",
      "\n",
      "Batch 330/992 ━━━━━━━━━━━━━━━━━━━━ 15:29:17\n",
      "Accuracy: 0.9652 - Loss: 0.1112\n",
      "\n",
      "Batch 331/992 ━━━━━━━━━━━━━━━━━━━━ 15:29:32\n",
      "Accuracy: 0.9653 - Loss: 0.1110\n",
      "\n",
      "Batch 332/992 ━━━━━━━━━━━━━━━━━━━━ 15:29:46\n",
      "Accuracy: 0.9654 - Loss: 0.1107\n",
      "\n",
      "Batch 333/992 ━━━━━━━━━━━━━━━━━━━━ 15:29:59\n",
      "Accuracy: 0.9655 - Loss: 0.1106\n",
      "\n",
      "Batch 334/992 ━━━━━━━━━━━━━━━━━━━━ 15:30:09\n",
      "Accuracy: 0.9656 - Loss: 0.1105\n",
      "\n",
      "Batch 335/992 ━━━━━━━━━━━━━━━━━━━━ 15:30:20\n",
      "Accuracy: 0.9657 - Loss: 0.1103\n",
      "\n",
      "Batch 336/992 ━━━━━━━━━━━━━━━━━━━━ 15:30:31\n",
      "Accuracy: 0.9650 - Loss: 0.1116\n",
      "\n",
      "Batch 337/992 ━━━━━━━━━━━━━━━━━━━━ 15:30:41\n",
      "Accuracy: 0.9651 - Loss: 0.1113\n",
      "\n",
      "Batch 338/992 ━━━━━━━━━━━━━━━━━━━━ 15:30:52\n",
      "Accuracy: 0.9652 - Loss: 0.1112\n",
      "\n",
      "Batch 339/992 ━━━━━━━━━━━━━━━━━━━━ 15:31:02\n",
      "Accuracy: 0.9653 - Loss: 0.1109\n",
      "\n",
      "Batch 340/992 ━━━━━━━━━━━━━━━━━━━━ 15:31:12\n",
      "Accuracy: 0.9654 - Loss: 0.1107\n",
      "\n",
      "Batch 341/992 ━━━━━━━━━━━━━━━━━━━━ 15:31:22\n",
      "Accuracy: 0.9652 - Loss: 0.1107\n",
      "\n",
      "Batch 342/992 ━━━━━━━━━━━━━━━━━━━━ 15:31:33\n",
      "Accuracy: 0.9653 - Loss: 0.1104\n",
      "\n",
      "Batch 343/992 ━━━━━━━━━━━━━━━━━━━━ 15:31:43\n",
      "Accuracy: 0.9654 - Loss: 0.1104\n",
      "\n",
      "Batch 344/992 ━━━━━━━━━━━━━━━━━━━━ 15:31:53\n",
      "Accuracy: 0.9655 - Loss: 0.1103\n",
      "\n",
      "Batch 345/992 ━━━━━━━━━━━━━━━━━━━━ 15:32:04\n",
      "Accuracy: 0.9656 - Loss: 0.1100\n",
      "\n",
      "Batch 346/992 ━━━━━━━━━━━━━━━━━━━━ 15:32:15\n",
      "Accuracy: 0.9657 - Loss: 0.1098\n",
      "\n",
      "Batch 347/992 ━━━━━━━━━━━━━━━━━━━━ 15:32:25\n",
      "Accuracy: 0.9658 - Loss: 0.1096\n",
      "\n",
      "Batch 348/992 ━━━━━━━━━━━━━━━━━━━━ 15:32:35\n",
      "Accuracy: 0.9659 - Loss: 0.1095\n",
      "\n",
      "Batch 349/992 ━━━━━━━━━━━━━━━━━━━━ 15:32:46\n",
      "Accuracy: 0.9660 - Loss: 0.1092\n",
      "\n",
      "Batch 350/992 ━━━━━━━━━━━━━━━━━━━━ 15:32:57\n",
      "Accuracy: 0.9661 - Loss: 0.1091\n",
      "\n",
      "Batch 351/992 ━━━━━━━━━━━━━━━━━━━━ 15:33:08\n",
      "Accuracy: 0.9662 - Loss: 0.1089\n",
      "\n",
      "Batch 352/992 ━━━━━━━━━━━━━━━━━━━━ 15:33:19\n",
      "Accuracy: 0.9663 - Loss: 0.1087\n",
      "\n",
      "Batch 353/992 ━━━━━━━━━━━━━━━━━━━━ 15:33:31\n",
      "Accuracy: 0.9664 - Loss: 0.1085\n",
      "\n",
      "Batch 354/992 ━━━━━━━━━━━━━━━━━━━━ 15:33:43\n",
      "Accuracy: 0.9665 - Loss: 0.1085\n",
      "\n",
      "Batch 355/992 ━━━━━━━━━━━━━━━━━━━━ 15:33:56\n",
      "Accuracy: 0.9665 - Loss: 0.1085\n",
      "\n",
      "Batch 356/992 ━━━━━━━━━━━━━━━━━━━━ 15:34:08\n",
      "Accuracy: 0.9663 - Loss: 0.1088\n",
      "\n",
      "Batch 357/992 ━━━━━━━━━━━━━━━━━━━━ 15:34:23\n",
      "Accuracy: 0.9664 - Loss: 0.1086\n",
      "\n",
      "Batch 358/992 ━━━━━━━━━━━━━━━━━━━━ 15:34:36\n",
      "Accuracy: 0.9661 - Loss: 0.1093\n",
      "\n",
      "Batch 359/992 ━━━━━━━━━━━━━━━━━━━━ 15:34:49\n",
      "Accuracy: 0.9662 - Loss: 0.1090\n",
      "\n",
      "Batch 360/992 ━━━━━━━━━━━━━━━━━━━━ 15:35:03\n",
      "Accuracy: 0.9660 - Loss: 0.1091\n",
      "\n",
      "Batch 361/992 ━━━━━━━━━━━━━━━━━━━━ 15:35:15\n",
      "Accuracy: 0.9661 - Loss: 0.1090\n",
      "\n",
      "Batch 362/992 ━━━━━━━━━━━━━━━━━━━━ 15:35:25\n",
      "Accuracy: 0.9662 - Loss: 0.1089\n",
      "\n",
      "Batch 363/992 ━━━━━━━━━━━━━━━━━━━━ 15:35:35\n",
      "Accuracy: 0.9659 - Loss: 0.1090\n",
      "\n",
      "Batch 364/992 ━━━━━━━━━━━━━━━━━━━━ 15:35:47\n",
      "Accuracy: 0.9660 - Loss: 0.1090\n",
      "\n",
      "Batch 365/992 ━━━━━━━━━━━━━━━━━━━━ 15:36:00\n",
      "Accuracy: 0.9658 - Loss: 0.1092\n",
      "\n",
      "Batch 366/992 ━━━━━━━━━━━━━━━━━━━━ 15:36:12\n",
      "Accuracy: 0.9658 - Loss: 0.1090\n",
      "\n",
      "Batch 367/992 ━━━━━━━━━━━━━━━━━━━━ 15:36:23\n",
      "Accuracy: 0.9659 - Loss: 0.1089\n",
      "\n",
      "Batch 368/992 ━━━━━━━━━━━━━━━━━━━━ 15:36:34\n",
      "Accuracy: 0.9660 - Loss: 0.1087\n",
      "\n",
      "Batch 369/992 ━━━━━━━━━━━━━━━━━━━━ 15:36:47\n",
      "Accuracy: 0.9661 - Loss: 0.1085\n",
      "\n",
      "Batch 370/992 ━━━━━━━━━━━━━━━━━━━━ 15:37:00\n",
      "Accuracy: 0.9662 - Loss: 0.1082\n",
      "\n",
      "Batch 371/992 ━━━━━━━━━━━━━━━━━━━━ 15:37:12\n",
      "Accuracy: 0.9660 - Loss: 0.1083\n",
      "\n",
      "Batch 372/992 ━━━━━━━━━━━━━━━━━━━━ 15:37:22\n",
      "Accuracy: 0.9657 - Loss: 0.1085\n",
      "\n",
      "Batch 373/992 ━━━━━━━━━━━━━━━━━━━━ 15:37:36\n",
      "Accuracy: 0.9655 - Loss: 0.1087\n",
      "\n",
      "Batch 374/992 ━━━━━━━━━━━━━━━━━━━━ 15:37:47\n",
      "Accuracy: 0.9649 - Loss: 0.1091\n",
      "\n",
      "Batch 375/992 ━━━━━━━━━━━━━━━━━━━━ 15:37:59\n",
      "Accuracy: 0.9650 - Loss: 0.1089\n",
      "\n",
      "Batch 376/992 ━━━━━━━━━━━━━━━━━━━━ 15:38:13\n",
      "Accuracy: 0.9651 - Loss: 0.1087\n",
      "\n",
      "Batch 377/992 ━━━━━━━━━━━━━━━━━━━━ 15:38:23\n",
      "Accuracy: 0.9652 - Loss: 0.1084\n",
      "\n",
      "Batch 378/992 ━━━━━━━━━━━━━━━━━━━━ 15:38:33\n",
      "Accuracy: 0.9653 - Loss: 0.1082\n",
      "\n",
      "Batch 379/992 ━━━━━━━━━━━━━━━━━━━━ 15:38:45\n",
      "Accuracy: 0.9647 - Loss: 0.1087\n",
      "\n",
      "Batch 380/992 ━━━━━━━━━━━━━━━━━━━━ 15:38:57\n",
      "Accuracy: 0.9645 - Loss: 0.1092\n",
      "\n",
      "Batch 381/992 ━━━━━━━━━━━━━━━━━━━━ 15:39:10\n",
      "Accuracy: 0.9646 - Loss: 0.1091\n",
      "\n",
      "Batch 382/992 ━━━━━━━━━━━━━━━━━━━━ 15:39:24\n",
      "Accuracy: 0.9647 - Loss: 0.1088\n",
      "\n",
      "Batch 383/992 ━━━━━━━━━━━━━━━━━━━━ 15:39:35\n",
      "Accuracy: 0.9648 - Loss: 0.1086\n",
      "\n",
      "Batch 384/992 ━━━━━━━━━━━━━━━━━━━━ 15:39:46\n",
      "Accuracy: 0.9648 - Loss: 0.1084\n",
      "\n",
      "Batch 385/992 ━━━━━━━━━━━━━━━━━━━━ 15:39:59\n",
      "Accuracy: 0.9649 - Loss: 0.1081\n",
      "\n",
      "Batch 386/992 ━━━━━━━━━━━━━━━━━━━━ 15:40:09\n",
      "Accuracy: 0.9650 - Loss: 0.1079\n",
      "\n",
      "Batch 387/992 ━━━━━━━━━━━━━━━━━━━━ 15:40:20\n",
      "Accuracy: 0.9651 - Loss: 0.1077\n",
      "\n",
      "Batch 388/992 ━━━━━━━━━━━━━━━━━━━━ 15:40:35\n",
      "Accuracy: 0.9652 - Loss: 0.1075\n",
      "\n",
      "Batch 389/992 ━━━━━━━━━━━━━━━━━━━━ 15:40:49\n",
      "Accuracy: 0.9653 - Loss: 0.1072\n",
      "\n",
      "Batch 390/992 ━━━━━━━━━━━━━━━━━━━━ 15:41:01\n",
      "Accuracy: 0.9654 - Loss: 0.1074\n",
      "\n",
      "Batch 391/992 ━━━━━━━━━━━━━━━━━━━━ 15:41:14\n",
      "Accuracy: 0.9648 - Loss: 0.1086\n",
      "\n",
      "Batch 392/992 ━━━━━━━━━━━━━━━━━━━━ 15:41:24\n",
      "Accuracy: 0.9649 - Loss: 0.1086\n",
      "\n",
      "Batch 393/992 ━━━━━━━━━━━━━━━━━━━━ 15:41:36\n",
      "Accuracy: 0.9650 - Loss: 0.1084\n",
      "\n",
      "Batch 394/992 ━━━━━━━━━━━━━━━━━━━━ 15:41:46\n",
      "Accuracy: 0.9651 - Loss: 0.1082\n",
      "\n",
      "Batch 395/992 ━━━━━━━━━━━━━━━━━━━━ 15:41:57\n",
      "Accuracy: 0.9652 - Loss: 0.1083\n",
      "\n",
      "Batch 396/992 ━━━━━━━━━━━━━━━━━━━━ 15:42:08\n",
      "Accuracy: 0.9653 - Loss: 0.1081\n",
      "\n",
      "Batch 397/992 ━━━━━━━━━━━━━━━━━━━━ 15:42:20\n",
      "Accuracy: 0.9654 - Loss: 0.1079\n",
      "\n",
      "Batch 398/992 ━━━━━━━━━━━━━━━━━━━━ 15:42:35\n",
      "Accuracy: 0.9655 - Loss: 0.1077\n",
      "\n",
      "Batch 399/992 ━━━━━━━━━━━━━━━━━━━━ 15:42:48\n",
      "Accuracy: 0.9655 - Loss: 0.1076\n",
      "\n",
      "Batch 400/992 ━━━━━━━━━━━━━━━━━━━━ 15:43:01\n",
      "Accuracy: 0.9656 - Loss: 0.1074\n",
      "\n",
      "Batch 401/992 ━━━━━━━━━━━━━━━━━━━━ 15:43:15\n",
      "Accuracy: 0.9657 - Loss: 0.1071\n",
      "\n",
      "Batch 402/992 ━━━━━━━━━━━━━━━━━━━━ 15:43:27\n",
      "Accuracy: 0.9658 - Loss: 0.1072\n",
      "\n",
      "Batch 403/992 ━━━━━━━━━━━━━━━━━━━━ 15:43:38\n",
      "Accuracy: 0.9656 - Loss: 0.1076\n",
      "\n",
      "Batch 404/992 ━━━━━━━━━━━━━━━━━━━━ 15:43:48\n",
      "Accuracy: 0.9657 - Loss: 0.1076\n",
      "\n",
      "Batch 405/992 ━━━━━━━━━━━━━━━━━━━━ 15:43:59\n",
      "Accuracy: 0.9657 - Loss: 0.1074\n",
      "\n",
      "Batch 406/992 ━━━━━━━━━━━━━━━━━━━━ 15:44:09\n",
      "Accuracy: 0.9658 - Loss: 0.1073\n",
      "\n",
      "Batch 407/992 ━━━━━━━━━━━━━━━━━━━━ 15:44:19\n",
      "Accuracy: 0.9656 - Loss: 0.1074\n",
      "\n",
      "Batch 408/992 ━━━━━━━━━━━━━━━━━━━━ 15:44:30\n",
      "Accuracy: 0.9657 - Loss: 0.1071\n",
      "\n",
      "Batch 409/992 ━━━━━━━━━━━━━━━━━━━━ 15:44:40\n",
      "Accuracy: 0.9658 - Loss: 0.1071\n",
      "\n",
      "Batch 410/992 ━━━━━━━━━━━━━━━━━━━━ 15:44:51\n",
      "Accuracy: 0.9659 - Loss: 0.1069\n",
      "\n",
      "Batch 411/992 ━━━━━━━━━━━━━━━━━━━━ 15:45:01\n",
      "Accuracy: 0.9659 - Loss: 0.1067\n",
      "\n",
      "Batch 412/992 ━━━━━━━━━━━━━━━━━━━━ 15:45:11\n",
      "Accuracy: 0.9660 - Loss: 0.1065\n",
      "\n",
      "Batch 413/992 ━━━━━━━━━━━━━━━━━━━━ 15:45:22\n",
      "Accuracy: 0.9661 - Loss: 0.1063\n",
      "\n",
      "Batch 414/992 ━━━━━━━━━━━━━━━━━━━━ 15:45:33\n",
      "Accuracy: 0.9659 - Loss: 0.1067\n",
      "\n",
      "Batch 415/992 ━━━━━━━━━━━━━━━━━━━━ 15:45:44\n",
      "Accuracy: 0.9657 - Loss: 0.1073\n",
      "\n",
      "Batch 416/992 ━━━━━━━━━━━━━━━━━━━━ 15:45:59\n",
      "Accuracy: 0.9657 - Loss: 0.1073\n",
      "\n",
      "Batch 417/992 ━━━━━━━━━━━━━━━━━━━━ 15:46:13\n",
      "Accuracy: 0.9658 - Loss: 0.1070\n",
      "\n",
      "Batch 418/992 ━━━━━━━━━━━━━━━━━━━━ 15:46:30\n",
      "Accuracy: 0.9659 - Loss: 0.1068\n",
      "\n",
      "Batch 419/992 ━━━━━━━━━━━━━━━━━━━━ 15:46:43\n",
      "Accuracy: 0.9660 - Loss: 0.1066\n",
      "\n",
      "Batch 420/992 ━━━━━━━━━━━━━━━━━━━━ 15:46:53\n",
      "Accuracy: 0.9661 - Loss: 0.1063\n",
      "\n",
      "Batch 421/992 ━━━━━━━━━━━━━━━━━━━━ 15:47:03\n",
      "Accuracy: 0.9662 - Loss: 0.1063\n",
      "\n",
      "Batch 422/992 ━━━━━━━━━━━━━━━━━━━━ 15:47:13\n",
      "Accuracy: 0.9662 - Loss: 0.1065\n",
      "\n",
      "Batch 423/992 ━━━━━━━━━━━━━━━━━━━━ 15:47:25\n",
      "Accuracy: 0.9663 - Loss: 0.1062\n",
      "\n",
      "Batch 424/992 ━━━━━━━━━━━━━━━━━━━━ 15:47:39\n",
      "Accuracy: 0.9664 - Loss: 0.1061\n",
      "\n",
      "Batch 425/992 ━━━━━━━━━━━━━━━━━━━━ 15:47:54\n",
      "Accuracy: 0.9662 - Loss: 0.1064\n",
      "\n",
      "Batch 426/992 ━━━━━━━━━━━━━━━━━━━━ 15:48:06\n",
      "Accuracy: 0.9663 - Loss: 0.1062\n",
      "\n",
      "Batch 427/992 ━━━━━━━━━━━━━━━━━━━━ 15:48:21\n",
      "Accuracy: 0.9660 - Loss: 0.1064\n",
      "\n",
      "Batch 428/992 ━━━━━━━━━━━━━━━━━━━━ 15:48:36\n",
      "Accuracy: 0.9661 - Loss: 0.1062\n",
      "\n",
      "Batch 429/992 ━━━━━━━━━━━━━━━━━━━━ 15:48:50\n",
      "Accuracy: 0.9662 - Loss: 0.1061\n",
      "\n",
      "Batch 430/992 ━━━━━━━━━━━━━━━━━━━━ 15:49:00\n",
      "Accuracy: 0.9663 - Loss: 0.1058\n",
      "\n",
      "Batch 431/992 ━━━━━━━━━━━━━━━━━━━━ 15:49:13\n",
      "Accuracy: 0.9664 - Loss: 0.1057\n",
      "\n",
      "Batch 432/992 ━━━━━━━━━━━━━━━━━━━━ 15:49:26\n",
      "Accuracy: 0.9664 - Loss: 0.1055\n",
      "\n",
      "Batch 433/992 ━━━━━━━━━━━━━━━━━━━━ 15:49:38\n",
      "Accuracy: 0.9665 - Loss: 0.1055\n",
      "\n",
      "Batch 434/992 ━━━━━━━━━━━━━━━━━━━━ 15:49:48\n",
      "Accuracy: 0.9666 - Loss: 0.1052\n",
      "\n",
      "Batch 435/992 ━━━━━━━━━━━━━━━━━━━━ 15:49:59\n",
      "Accuracy: 0.9664 - Loss: 0.1056\n",
      "\n",
      "Batch 436/992 ━━━━━━━━━━━━━━━━━━━━ 15:50:09\n",
      "Accuracy: 0.9665 - Loss: 0.1055\n",
      "\n",
      "Batch 437/992 ━━━━━━━━━━━━━━━━━━━━ 15:50:20\n",
      "Accuracy: 0.9665 - Loss: 0.1053\n",
      "\n",
      "Batch 438/992 ━━━━━━━━━━━━━━━━━━━━ 15:50:30\n",
      "Accuracy: 0.9666 - Loss: 0.1051\n",
      "\n",
      "Batch 439/992 ━━━━━━━━━━━━━━━━━━━━ 15:50:41\n",
      "Accuracy: 0.9667 - Loss: 0.1049\n",
      "\n",
      "Batch 440/992 ━━━━━━━━━━━━━━━━━━━━ 15:50:52\n",
      "Accuracy: 0.9668 - Loss: 0.1047\n",
      "\n",
      "Batch 441/992 ━━━━━━━━━━━━━━━━━━━━ 15:51:04\n",
      "Accuracy: 0.9668 - Loss: 0.1045\n",
      "\n",
      "Batch 442/992 ━━━━━━━━━━━━━━━━━━━━ 15:51:15\n",
      "Accuracy: 0.9669 - Loss: 0.1045\n",
      "\n",
      "Batch 443/992 ━━━━━━━━━━━━━━━━━━━━ 15:51:25\n",
      "Accuracy: 0.9670 - Loss: 0.1044\n",
      "\n",
      "Batch 444/992 ━━━━━━━━━━━━━━━━━━━━ 15:51:35\n",
      "Accuracy: 0.9671 - Loss: 0.1042\n",
      "\n",
      "Batch 445/992 ━━━━━━━━━━━━━━━━━━━━ 15:51:46\n",
      "Accuracy: 0.9671 - Loss: 0.1041\n",
      "\n",
      "Batch 446/992 ━━━━━━━━━━━━━━━━━━━━ 15:51:56\n",
      "Accuracy: 0.9672 - Loss: 0.1041\n",
      "\n",
      "Batch 447/992 ━━━━━━━━━━━━━━━━━━━━ 15:52:07\n",
      "Accuracy: 0.9673 - Loss: 0.1039\n",
      "\n",
      "Batch 448/992 ━━━━━━━━━━━━━━━━━━━━ 15:52:17\n",
      "Accuracy: 0.9674 - Loss: 0.1037\n",
      "\n",
      "Batch 449/992 ━━━━━━━━━━━━━━━━━━━━ 15:52:29\n",
      "Accuracy: 0.9674 - Loss: 0.1035\n",
      "\n",
      "Batch 450/992 ━━━━━━━━━━━━━━━━━━━━ 15:52:41\n",
      "Accuracy: 0.9675 - Loss: 0.1033\n",
      "\n",
      "Batch 451/992 ━━━━━━━━━━━━━━━━━━━━ 15:52:54\n",
      "Accuracy: 0.9673 - Loss: 0.1035\n",
      "\n",
      "Batch 452/992 ━━━━━━━━━━━━━━━━━━━━ 15:53:06\n",
      "Accuracy: 0.9674 - Loss: 0.1035\n",
      "\n",
      "Batch 453/992 ━━━━━━━━━━━━━━━━━━━━ 15:53:18\n",
      "Accuracy: 0.9674 - Loss: 0.1034\n",
      "\n",
      "Batch 454/992 ━━━━━━━━━━━━━━━━━━━━ 15:53:30\n",
      "Accuracy: 0.9675 - Loss: 0.1032\n",
      "\n",
      "Batch 455/992 ━━━━━━━━━━━━━━━━━━━━ 15:53:43\n",
      "Accuracy: 0.9676 - Loss: 0.1030\n",
      "\n",
      "Batch 456/992 ━━━━━━━━━━━━━━━━━━━━ 15:53:56\n",
      "Accuracy: 0.9677 - Loss: 0.1028\n",
      "\n",
      "Batch 457/992 ━━━━━━━━━━━━━━━━━━━━ 15:54:08\n",
      "Accuracy: 0.9677 - Loss: 0.1027\n",
      "\n",
      "Batch 458/992 ━━━━━━━━━━━━━━━━━━━━ 15:54:20\n",
      "Accuracy: 0.9678 - Loss: 0.1026\n",
      "\n",
      "Batch 459/992 ━━━━━━━━━━━━━━━━━━━━ 15:54:32\n",
      "Accuracy: 0.9679 - Loss: 0.1025\n",
      "\n",
      "Batch 460/992 ━━━━━━━━━━━━━━━━━━━━ 15:54:43\n",
      "Accuracy: 0.9679 - Loss: 0.1026\n",
      "\n",
      "Batch 461/992 ━━━━━━━━━━━━━━━━━━━━ 15:54:54\n",
      "Accuracy: 0.9680 - Loss: 0.1023\n",
      "\n",
      "Batch 462/992 ━━━━━━━━━━━━━━━━━━━━ 15:55:05\n",
      "Accuracy: 0.9681 - Loss: 0.1025\n",
      "\n",
      "Batch 463/992 ━━━━━━━━━━━━━━━━━━━━ 15:55:17\n",
      "Accuracy: 0.9679 - Loss: 0.1032\n",
      "\n",
      "Batch 464/992 ━━━━━━━━━━━━━━━━━━━━ 15:55:30\n",
      "Accuracy: 0.9679 - Loss: 0.1030\n",
      "\n",
      "Batch 465/992 ━━━━━━━━━━━━━━━━━━━━ 15:55:41\n",
      "Accuracy: 0.9677 - Loss: 0.1031\n",
      "\n",
      "Batch 466/992 ━━━━━━━━━━━━━━━━━━━━ 15:55:51\n",
      "Accuracy: 0.9678 - Loss: 0.1031\n",
      "\n",
      "Batch 467/992 ━━━━━━━━━━━━━━━━━━━━ 15:56:02\n",
      "Accuracy: 0.9679 - Loss: 0.1033\n",
      "\n",
      "Batch 468/992 ━━━━━━━━━━━━━━━━━━━━ 15:56:13\n",
      "Accuracy: 0.9679 - Loss: 0.1033\n",
      "\n",
      "Batch 469/992 ━━━━━━━━━━━━━━━━━━━━ 15:56:23\n",
      "Accuracy: 0.9680 - Loss: 0.1033\n",
      "\n",
      "Batch 470/992 ━━━━━━━━━━━━━━━━━━━━ 15:56:33\n",
      "Accuracy: 0.9681 - Loss: 0.1033\n",
      "\n",
      "Batch 471/992 ━━━━━━━━━━━━━━━━━━━━ 15:56:44\n",
      "Accuracy: 0.9679 - Loss: 0.1034\n",
      "\n",
      "Batch 472/992 ━━━━━━━━━━━━━━━━━━━━ 15:56:54\n",
      "Accuracy: 0.9680 - Loss: 0.1032\n",
      "\n",
      "Batch 473/992 ━━━━━━━━━━━━━━━━━━━━ 15:57:05\n",
      "Accuracy: 0.9678 - Loss: 0.1033\n",
      "\n",
      "Batch 474/992 ━━━━━━━━━━━━━━━━━━━━ 15:57:15\n",
      "Accuracy: 0.9678 - Loss: 0.1031\n",
      "\n",
      "Batch 475/992 ━━━━━━━━━━━━━━━━━━━━ 15:57:26\n",
      "Accuracy: 0.9676 - Loss: 0.1033\n",
      "\n",
      "Batch 476/992 ━━━━━━━━━━━━━━━━━━━━ 15:57:36\n",
      "Accuracy: 0.9677 - Loss: 0.1032\n",
      "\n",
      "Batch 477/992 ━━━━━━━━━━━━━━━━━━━━ 15:57:47\n",
      "Accuracy: 0.9672 - Loss: 0.1035\n",
      "\n",
      "Batch 478/992 ━━━━━━━━━━━━━━━━━━━━ 15:57:58\n",
      "Accuracy: 0.9671 - Loss: 0.1038\n",
      "\n",
      "Batch 479/992 ━━━━━━━━━━━━━━━━━━━━ 15:58:09\n",
      "Accuracy: 0.9669 - Loss: 0.1042\n",
      "\n",
      "Batch 480/992 ━━━━━━━━━━━━━━━━━━━━ 15:58:20\n",
      "Accuracy: 0.9669 - Loss: 0.1041\n",
      "\n",
      "Batch 481/992 ━━━━━━━━━━━━━━━━━━━━ 15:58:30\n",
      "Accuracy: 0.9670 - Loss: 0.1040\n",
      "\n",
      "Batch 482/992 ━━━━━━━━━━━━━━━━━━━━ 15:58:40\n",
      "Accuracy: 0.9671 - Loss: 0.1039\n",
      "\n",
      "Batch 483/992 ━━━━━━━━━━━━━━━━━━━━ 15:58:51\n",
      "Accuracy: 0.9669 - Loss: 0.1041\n",
      "\n",
      "Batch 484/992 ━━━━━━━━━━━━━━━━━━━━ 15:59:02\n",
      "Accuracy: 0.9669 - Loss: 0.1039\n",
      "\n",
      "Batch 485/992 ━━━━━━━━━━━━━━━━━━━━ 15:59:13\n",
      "Accuracy: 0.9668 - Loss: 0.1040\n",
      "\n",
      "Batch 486/992 ━━━━━━━━━━━━━━━━━━━━ 15:59:23\n",
      "Accuracy: 0.9668 - Loss: 0.1040\n",
      "\n",
      "Batch 487/992 ━━━━━━━━━━━━━━━━━━━━ 15:59:34\n",
      "Accuracy: 0.9669 - Loss: 0.1039\n",
      "\n",
      "Batch 488/992 ━━━━━━━━━━━━━━━━━━━━ 15:59:44\n",
      "Accuracy: 0.9667 - Loss: 0.1044\n",
      "\n",
      "Batch 489/992 ━━━━━━━━━━━━━━━━━━━━ 15:59:55\n",
      "Accuracy: 0.9665 - Loss: 0.1046\n",
      "\n",
      "Batch 490/992 ━━━━━━━━━━━━━━━━━━━━ 16:00:06\n",
      "Accuracy: 0.9663 - Loss: 0.1053\n",
      "\n",
      "Batch 491/992 ━━━━━━━━━━━━━━━━━━━━ 16:00:17\n",
      "Accuracy: 0.9661 - Loss: 0.1054\n",
      "\n",
      "Batch 492/992 ━━━━━━━━━━━━━━━━━━━━ 16:00:27\n",
      "Accuracy: 0.9662 - Loss: 0.1052\n",
      "\n",
      "Batch 493/992 ━━━━━━━━━━━━━━━━━━━━ 16:00:38\n",
      "Accuracy: 0.9663 - Loss: 0.1050\n",
      "\n",
      "Batch 494/992 ━━━━━━━━━━━━━━━━━━━━ 16:00:48\n",
      "Accuracy: 0.9663 - Loss: 0.1048\n",
      "\n",
      "Batch 495/992 ━━━━━━━━━━━━━━━━━━━━ 16:00:59\n",
      "Accuracy: 0.9662 - Loss: 0.1050\n",
      "\n",
      "Batch 496/992 ━━━━━━━━━━━━━━━━━━━━ 16:01:13\n",
      "Accuracy: 0.9662 - Loss: 0.1048\n",
      "\n",
      "Batch 497/992 ━━━━━━━━━━━━━━━━━━━━ 16:01:24\n",
      "Accuracy: 0.9663 - Loss: 0.1047\n",
      "\n",
      "Batch 498/992 ━━━━━━━━━━━━━━━━━━━━ 16:01:35\n",
      "Accuracy: 0.9664 - Loss: 0.1045\n",
      "\n",
      "Batch 499/992 ━━━━━━━━━━━━━━━━━━━━ 16:01:45\n",
      "Accuracy: 0.9664 - Loss: 0.1044\n",
      "\n",
      "Batch 500/992 ━━━━━━━━━━━━━━━━━━━━ 16:01:55\n",
      "Accuracy: 0.9663 - Loss: 0.1047\n",
      "\n",
      "Batch 501/992 ━━━━━━━━━━━━━━━━━━━━ 16:02:06\n",
      "Accuracy: 0.9663 - Loss: 0.1045\n",
      "\n",
      "Batch 502/992 ━━━━━━━━━━━━━━━━━━━━ 16:02:17\n",
      "Accuracy: 0.9664 - Loss: 0.1047\n",
      "\n",
      "Batch 503/992 ━━━━━━━━━━━━━━━━━━━━ 16:02:27\n",
      "Accuracy: 0.9665 - Loss: 0.1045\n",
      "\n",
      "Batch 504/992 ━━━━━━━━━━━━━━━━━━━━ 16:02:37\n",
      "Accuracy: 0.9663 - Loss: 0.1047\n",
      "\n",
      "Batch 505/992 ━━━━━━━━━━━━━━━━━━━━ 16:02:48\n",
      "Accuracy: 0.9663 - Loss: 0.1047\n",
      "\n",
      "Batch 506/992 ━━━━━━━━━━━━━━━━━━━━ 16:02:59\n",
      "Accuracy: 0.9664 - Loss: 0.1045\n",
      "\n",
      "Batch 507/992 ━━━━━━━━━━━━━━━━━━━━ 16:03:10\n",
      "Accuracy: 0.9662 - Loss: 0.1050\n",
      "\n",
      "Batch 508/992 ━━━━━━━━━━━━━━━━━━━━ 16:03:20\n",
      "Accuracy: 0.9663 - Loss: 0.1048\n",
      "\n",
      "Batch 509/992 ━━━━━━━━━━━━━━━━━━━━ 16:03:31\n",
      "Accuracy: 0.9661 - Loss: 0.1048\n",
      "\n",
      "Batch 510/992 ━━━━━━━━━━━━━━━━━━━━ 16:03:41\n",
      "Accuracy: 0.9662 - Loss: 0.1047\n",
      "\n",
      "Batch 511/992 ━━━━━━━━━━━━━━━━━━━━ 16:03:52\n",
      "Accuracy: 0.9662 - Loss: 0.1047\n",
      "\n",
      "Batch 512/992 ━━━━━━━━━━━━━━━━━━━━ 16:04:02\n",
      "Accuracy: 0.9663 - Loss: 0.1045\n",
      "\n",
      "Batch 513/992 ━━━━━━━━━━━━━━━━━━━━ 16:04:13\n",
      "Accuracy: 0.9661 - Loss: 0.1047\n",
      "\n",
      "Batch 514/992 ━━━━━━━━━━━━━━━━━━━━ 16:04:23\n",
      "Accuracy: 0.9662 - Loss: 0.1045\n",
      "\n",
      "Batch 515/992 ━━━━━━━━━━━━━━━━━━━━ 16:04:34\n",
      "Accuracy: 0.9663 - Loss: 0.1044\n",
      "\n",
      "Batch 516/992 ━━━━━━━━━━━━━━━━━━━━ 16:04:44\n",
      "Accuracy: 0.9661 - Loss: 0.1048\n",
      "\n",
      "Batch 517/992 ━━━━━━━━━━━━━━━━━━━━ 16:04:55\n",
      "Accuracy: 0.9657 - Loss: 0.1053\n",
      "\n",
      "Batch 518/992 ━━━━━━━━━━━━━━━━━━━━ 16:05:06\n",
      "Accuracy: 0.9655 - Loss: 0.1059\n",
      "\n",
      "Batch 519/992 ━━━━━━━━━━━━━━━━━━━━ 16:05:17\n",
      "Accuracy: 0.9656 - Loss: 0.1057\n",
      "\n",
      "Batch 520/992 ━━━━━━━━━━━━━━━━━━━━ 16:05:28\n",
      "Accuracy: 0.9656 - Loss: 0.1056\n",
      "\n",
      "Batch 521/992 ━━━━━━━━━━━━━━━━━━━━ 16:05:38\n",
      "Accuracy: 0.9657 - Loss: 0.1055\n",
      "\n",
      "Batch 522/992 ━━━━━━━━━━━━━━━━━━━━ 16:05:49\n",
      "Accuracy: 0.9653 - Loss: 0.1061\n",
      "\n",
      "Batch 523/992 ━━━━━━━━━━━━━━━━━━━━ 16:05:59\n",
      "Accuracy: 0.9649 - Loss: 0.1070\n",
      "\n",
      "Batch 524/992 ━━━━━━━━━━━━━━━━━━━━ 16:06:10\n",
      "Accuracy: 0.9649 - Loss: 0.1068\n",
      "\n",
      "Batch 525/992 ━━━━━━━━━━━━━━━━━━━━ 16:06:20\n",
      "Accuracy: 0.9645 - Loss: 0.1076\n",
      "\n",
      "Batch 526/992 ━━━━━━━━━━━━━━━━━━━━ 16:06:31\n",
      "Accuracy: 0.9646 - Loss: 0.1076\n",
      "\n",
      "Batch 527/992 ━━━━━━━━━━━━━━━━━━━━ 16:06:42\n",
      "Accuracy: 0.9647 - Loss: 0.1075\n",
      "\n",
      "Batch 528/992 ━━━━━━━━━━━━━━━━━━━━ 16:06:52\n",
      "Accuracy: 0.9647 - Loss: 0.1073\n",
      "\n",
      "Batch 529/992 ━━━━━━━━━━━━━━━━━━━━ 16:07:03\n",
      "Accuracy: 0.9646 - Loss: 0.1077\n",
      "\n",
      "Batch 530/992 ━━━━━━━━━━━━━━━━━━━━ 16:07:14\n",
      "Accuracy: 0.9644 - Loss: 0.1080\n",
      "\n",
      "Batch 531/992 ━━━━━━━━━━━━━━━━━━━━ 16:07:25\n",
      "Accuracy: 0.9645 - Loss: 0.1078\n",
      "\n",
      "Batch 532/992 ━━━━━━━━━━━━━━━━━━━━ 16:07:35\n",
      "Accuracy: 0.9645 - Loss: 0.1077\n",
      "\n",
      "Batch 533/992 ━━━━━━━━━━━━━━━━━━━━ 16:07:46\n",
      "Accuracy: 0.9644 - Loss: 0.1083\n",
      "\n",
      "Batch 534/992 ━━━━━━━━━━━━━━━━━━━━ 16:07:56\n",
      "Accuracy: 0.9644 - Loss: 0.1083\n",
      "\n",
      "Batch 535/992 ━━━━━━━━━━━━━━━━━━━━ 16:08:07\n",
      "Accuracy: 0.9643 - Loss: 0.1084\n",
      "\n",
      "Batch 536/992 ━━━━━━━━━━━━━━━━━━━━ 16:08:17\n",
      "Accuracy: 0.9641 - Loss: 0.1089\n",
      "\n",
      "Batch 537/992 ━━━━━━━━━━━━━━━━━━━━ 16:08:28\n",
      "Accuracy: 0.9637 - Loss: 0.1094\n",
      "\n",
      "Batch 538/992 ━━━━━━━━━━━━━━━━━━━━ 16:08:38\n",
      "Accuracy: 0.9638 - Loss: 0.1094\n",
      "\n",
      "Batch 539/992 ━━━━━━━━━━━━━━━━━━━━ 16:08:49\n",
      "Accuracy: 0.9638 - Loss: 0.1092\n",
      "\n",
      "Batch 540/992 ━━━━━━━━━━━━━━━━━━━━ 16:09:00\n",
      "Accuracy: 0.9637 - Loss: 0.1094\n",
      "\n",
      "Batch 541/992 ━━━━━━━━━━━━━━━━━━━━ 16:09:10\n",
      "Accuracy: 0.9637 - Loss: 0.1094\n",
      "\n",
      "Batch 542/992 ━━━━━━━━━━━━━━━━━━━━ 16:09:21\n",
      "Accuracy: 0.9638 - Loss: 0.1092\n",
      "\n",
      "Batch 543/992 ━━━━━━━━━━━━━━━━━━━━ 16:09:31\n",
      "Accuracy: 0.9639 - Loss: 0.1091\n",
      "\n",
      "Batch 544/992 ━━━━━━━━━━━━━━━━━━━━ 16:09:42\n",
      "Accuracy: 0.9637 - Loss: 0.1093\n",
      "\n",
      "Batch 545/992 ━━━━━━━━━━━━━━━━━━━━ 16:09:53\n",
      "Accuracy: 0.9635 - Loss: 0.1098\n",
      "\n",
      "Batch 546/992 ━━━━━━━━━━━━━━━━━━━━ 16:10:03\n",
      "Accuracy: 0.9636 - Loss: 0.1097\n",
      "\n",
      "Batch 547/992 ━━━━━━━━━━━━━━━━━━━━ 16:10:14\n",
      "Accuracy: 0.9634 - Loss: 0.1100\n",
      "\n",
      "Batch 548/992 ━━━━━━━━━━━━━━━━━━━━ 16:10:27\n",
      "Accuracy: 0.9633 - Loss: 0.1101\n",
      "\n",
      "Batch 549/992 ━━━━━━━━━━━━━━━━━━━━ 16:10:39\n",
      "Accuracy: 0.9631 - Loss: 0.1104\n",
      "\n",
      "Batch 550/992 ━━━━━━━━━━━━━━━━━━━━ 16:10:51\n",
      "Accuracy: 0.9632 - Loss: 0.1102\n",
      "\n",
      "Batch 551/992 ━━━━━━━━━━━━━━━━━━━━ 16:11:02\n",
      "Accuracy: 0.9630 - Loss: 0.1119\n",
      "\n",
      "Batch 552/992 ━━━━━━━━━━━━━━━━━━━━ 16:11:13\n",
      "Accuracy: 0.9629 - Loss: 0.1122\n",
      "\n",
      "Batch 553/992 ━━━━━━━━━━━━━━━━━━━━ 16:11:24\n",
      "Accuracy: 0.9629 - Loss: 0.1122\n",
      "\n",
      "Batch 554/992 ━━━━━━━━━━━━━━━━━━━━ 16:11:34\n",
      "Accuracy: 0.9628 - Loss: 0.1124\n",
      "\n",
      "Batch 555/992 ━━━━━━━━━━━━━━━━━━━━ 16:11:44\n",
      "Accuracy: 0.9628 - Loss: 0.1122\n",
      "\n",
      "Batch 556/992 ━━━━━━━━━━━━━━━━━━━━ 16:11:55\n",
      "Accuracy: 0.9627 - Loss: 0.1122\n",
      "\n",
      "Batch 557/992 ━━━━━━━━━━━━━━━━━━━━ 16:12:06\n",
      "Accuracy: 0.9625 - Loss: 0.1123\n",
      "\n",
      "Batch 558/992 ━━━━━━━━━━━━━━━━━━━━ 16:12:16\n",
      "Accuracy: 0.9624 - Loss: 0.1128\n",
      "\n",
      "Batch 559/992 ━━━━━━━━━━━━━━━━━━━━ 16:12:27\n",
      "Accuracy: 0.9622 - Loss: 0.1132\n",
      "\n",
      "Batch 560/992 ━━━━━━━━━━━━━━━━━━━━ 16:12:37\n",
      "Accuracy: 0.9623 - Loss: 0.1130\n",
      "\n",
      "Batch 561/992 ━━━━━━━━━━━━━━━━━━━━ 16:12:47\n",
      "Accuracy: 0.9619 - Loss: 0.1137\n",
      "\n",
      "Batch 562/992 ━━━━━━━━━━━━━━━━━━━━ 16:12:58\n",
      "Accuracy: 0.9620 - Loss: 0.1138\n",
      "\n",
      "Batch 563/992 ━━━━━━━━━━━━━━━━━━━━ 16:13:09\n",
      "Accuracy: 0.9620 - Loss: 0.1138\n",
      "\n",
      "Batch 564/992 ━━━━━━━━━━━━━━━━━━━━ 16:13:21\n",
      "Accuracy: 0.9621 - Loss: 0.1136\n",
      "\n",
      "Batch 565/992 ━━━━━━━━━━━━━━━━━━━━ 16:13:36\n",
      "Accuracy: 0.9622 - Loss: 0.1134\n",
      "\n",
      "Batch 566/992 ━━━━━━━━━━━━━━━━━━━━ 16:13:50\n",
      "Accuracy: 0.9622 - Loss: 0.1132\n",
      "\n",
      "Batch 567/992 ━━━━━━━━━━━━━━━━━━━━ 16:14:03\n",
      "Accuracy: 0.9619 - Loss: 0.1139\n",
      "\n",
      "Batch 568/992 ━━━━━━━━━━━━━━━━━━━━ 16:14:16\n",
      "Accuracy: 0.9617 - Loss: 0.1140\n",
      "\n",
      "Batch 569/992 ━━━━━━━━━━━━━━━━━━━━ 16:14:27\n",
      "Accuracy: 0.9618 - Loss: 0.1140\n",
      "\n",
      "Batch 570/992 ━━━━━━━━━━━━━━━━━━━━ 16:14:40\n",
      "Accuracy: 0.9618 - Loss: 0.1139\n",
      "\n",
      "Batch 571/992 ━━━━━━━━━━━━━━━━━━━━ 16:14:51\n",
      "Accuracy: 0.9617 - Loss: 0.1141\n",
      "\n",
      "Batch 572/992 ━━━━━━━━━━━━━━━━━━━━ 16:15:03\n",
      "Accuracy: 0.9615 - Loss: 0.1141\n",
      "\n",
      "Batch 573/992 ━━━━━━━━━━━━━━━━━━━━ 16:15:19\n",
      "Accuracy: 0.9616 - Loss: 0.1139\n",
      "\n",
      "Batch 574/992 ━━━━━━━━━━━━━━━━━━━━ 16:15:34\n",
      "Accuracy: 0.9617 - Loss: 0.1138\n",
      "\n",
      "Batch 575/992 ━━━━━━━━━━━━━━━━━━━━ 16:15:46\n",
      "Accuracy: 0.9617 - Loss: 0.1136\n",
      "\n",
      "Batch 576/992 ━━━━━━━━━━━━━━━━━━━━ 16:16:01\n",
      "Accuracy: 0.9618 - Loss: 0.1135\n",
      "\n",
      "Batch 577/992 ━━━━━━━━━━━━━━━━━━━━ 16:16:14\n",
      "Accuracy: 0.9619 - Loss: 0.1134\n",
      "\n",
      "Batch 578/992 ━━━━━━━━━━━━━━━━━━━━ 16:16:26\n",
      "Accuracy: 0.9619 - Loss: 0.1133\n",
      "\n",
      "Batch 579/992 ━━━━━━━━━━━━━━━━━━━━ 16:16:36\n",
      "Accuracy: 0.9620 - Loss: 0.1133\n",
      "\n",
      "Batch 580/992 ━━━━━━━━━━━━━━━━━━━━ 16:16:47\n",
      "Accuracy: 0.9621 - Loss: 0.1131\n",
      "\n",
      "Batch 581/992 ━━━━━━━━━━━━━━━━━━━━ 16:16:59\n",
      "Accuracy: 0.9619 - Loss: 0.1134\n",
      "\n",
      "Batch 582/992 ━━━━━━━━━━━━━━━━━━━━ 16:17:09\n",
      "Accuracy: 0.9620 - Loss: 0.1132\n",
      "\n",
      "Batch 583/992 ━━━━━━━━━━━━━━━━━━━━ 16:17:20\n",
      "Accuracy: 0.9620 - Loss: 0.1131\n",
      "\n",
      "Batch 584/992 ━━━━━━━━━━━━━━━━━━━━ 16:17:31\n",
      "Accuracy: 0.9621 - Loss: 0.1130\n",
      "\n",
      "Batch 585/992 ━━━━━━━━━━━━━━━━━━━━ 16:17:42\n",
      "Accuracy: 0.9622 - Loss: 0.1129\n",
      "\n",
      "Batch 586/992 ━━━━━━━━━━━━━━━━━━━━ 16:17:55\n",
      "Accuracy: 0.9620 - Loss: 0.1130\n",
      "\n",
      "Batch 587/992 ━━━━━━━━━━━━━━━━━━━━ 16:18:08\n",
      "Accuracy: 0.9621 - Loss: 0.1129\n",
      "\n",
      "Batch 588/992 ━━━━━━━━━━━━━━━━━━━━ 16:18:22\n",
      "Accuracy: 0.9619 - Loss: 0.1130\n",
      "\n",
      "Batch 589/992 ━━━━━━━━━━━━━━━━━━━━ 16:18:33\n",
      "Accuracy: 0.9620 - Loss: 0.1129\n",
      "\n",
      "Batch 590/992 ━━━━━━━━━━━━━━━━━━━━ 16:18:44\n",
      "Accuracy: 0.9621 - Loss: 0.1128\n",
      "\n",
      "Batch 591/992 ━━━━━━━━━━━━━━━━━━━━ 16:18:55\n",
      "Accuracy: 0.9619 - Loss: 0.1134\n",
      "\n",
      "Batch 592/992 ━━━━━━━━━━━━━━━━━━━━ 16:19:06\n",
      "Accuracy: 0.9620 - Loss: 0.1133\n",
      "\n",
      "Batch 593/992 ━━━━━━━━━━━━━━━━━━━━ 16:19:17\n",
      "Accuracy: 0.9621 - Loss: 0.1131\n",
      "\n",
      "Batch 594/992 ━━━━━━━━━━━━━━━━━━━━ 16:19:28\n",
      "Accuracy: 0.9619 - Loss: 0.1132\n",
      "\n",
      "Batch 595/992 ━━━━━━━━━━━━━━━━━━━━ 16:19:38\n",
      "Accuracy: 0.9620 - Loss: 0.1132\n",
      "\n",
      "Batch 596/992 ━━━━━━━━━━━━━━━━━━━━ 16:19:49\n",
      "Accuracy: 0.9620 - Loss: 0.1131\n",
      "\n",
      "Batch 597/992 ━━━━━━━━━━━━━━━━━━━━ 16:20:00\n",
      "Accuracy: 0.9621 - Loss: 0.1130\n",
      "\n",
      "Batch 598/992 ━━━━━━━━━━━━━━━━━━━━ 16:20:10\n",
      "Accuracy: 0.9622 - Loss: 0.1129\n",
      "\n",
      "Batch 599/992 ━━━━━━━━━━━━━━━━━━━━ 16:20:21\n",
      "Accuracy: 0.9622 - Loss: 0.1128\n",
      "\n",
      "Batch 600/992 ━━━━━━━━━━━━━━━━━━━━ 16:20:34\n",
      "Accuracy: 0.9621 - Loss: 0.1130\n",
      "\n",
      "Batch 601/992 ━━━━━━━━━━━━━━━━━━━━ 16:20:45\n",
      "Accuracy: 0.9619 - Loss: 0.1133\n",
      "\n",
      "Batch 602/992 ━━━━━━━━━━━━━━━━━━━━ 16:20:57\n",
      "Accuracy: 0.9620 - Loss: 0.1132\n",
      "\n",
      "Batch 603/992 ━━━━━━━━━━━━━━━━━━━━ 16:21:08\n",
      "Accuracy: 0.9621 - Loss: 0.1131\n",
      "\n",
      "Batch 604/992 ━━━━━━━━━━━━━━━━━━━━ 16:21:19\n",
      "Accuracy: 0.9619 - Loss: 0.1132\n",
      "\n",
      "Batch 605/992 ━━━━━━━━━━━━━━━━━━━━ 16:21:30\n",
      "Accuracy: 0.9620 - Loss: 0.1130\n",
      "\n",
      "Batch 606/992 ━━━━━━━━━━━━━━━━━━━━ 16:21:40\n",
      "Accuracy: 0.9620 - Loss: 0.1128\n",
      "\n",
      "Batch 607/992 ━━━━━━━━━━━━━━━━━━━━ 16:21:51\n",
      "Accuracy: 0.9621 - Loss: 0.1127\n",
      "\n",
      "Batch 608/992 ━━━━━━━━━━━━━━━━━━━━ 16:22:02\n",
      "Accuracy: 0.9622 - Loss: 0.1126\n",
      "\n",
      "Batch 609/992 ━━━━━━━━━━━━━━━━━━━━ 16:22:12\n",
      "Accuracy: 0.9622 - Loss: 0.1124\n",
      "\n",
      "Batch 610/992 ━━━━━━━━━━━━━━━━━━━━ 16:22:23\n",
      "Accuracy: 0.9619 - Loss: 0.1126\n",
      "\n",
      "Batch 611/992 ━━━━━━━━━━━━━━━━━━━━ 16:22:34\n",
      "Accuracy: 0.9619 - Loss: 0.1125\n",
      "\n",
      "Batch 612/992 ━━━━━━━━━━━━━━━━━━━━ 16:22:45\n",
      "Accuracy: 0.9618 - Loss: 0.1129\n",
      "\n",
      "Batch 613/992 ━━━━━━━━━━━━━━━━━━━━ 16:22:56\n",
      "Accuracy: 0.9617 - Loss: 0.1129\n",
      "\n",
      "Batch 614/992 ━━━━━━━━━━━━━━━━━━━━ 16:23:09\n",
      "Accuracy: 0.9615 - Loss: 0.1131\n",
      "\n",
      "Batch 615/992 ━━━━━━━━━━━━━━━━━━━━ 16:23:20\n",
      "Accuracy: 0.9614 - Loss: 0.1131\n",
      "\n",
      "Batch 616/992 ━━━━━━━━━━━━━━━━━━━━ 16:23:31\n",
      "Accuracy: 0.9614 - Loss: 0.1131\n",
      "\n",
      "Batch 617/992 ━━━━━━━━━━━━━━━━━━━━ 16:23:41\n",
      "Accuracy: 0.9615 - Loss: 0.1129\n",
      "\n",
      "Batch 618/992 ━━━━━━━━━━━━━━━━━━━━ 16:23:52\n",
      "Accuracy: 0.9616 - Loss: 0.1128\n",
      "\n",
      "Batch 619/992 ━━━━━━━━━━━━━━━━━━━━ 16:24:02\n",
      "Accuracy: 0.9616 - Loss: 0.1127\n",
      "\n",
      "Batch 620/992 ━━━━━━━━━━━━━━━━━━━━ 16:24:13\n",
      "Accuracy: 0.9613 - Loss: 0.1133\n",
      "\n",
      "Batch 621/992 ━━━━━━━━━━━━━━━━━━━━ 16:24:23\n",
      "Accuracy: 0.9614 - Loss: 0.1132\n",
      "\n",
      "Batch 622/992 ━━━━━━━━━━━━━━━━━━━━ 16:24:34\n",
      "Accuracy: 0.9614 - Loss: 0.1132\n",
      "\n",
      "Batch 623/992 ━━━━━━━━━━━━━━━━━━━━ 16:24:45\n",
      "Accuracy: 0.9613 - Loss: 0.1134\n",
      "\n",
      "Batch 624/992 ━━━━━━━━━━━━━━━━━━━━ 16:24:55\n",
      "Accuracy: 0.9611 - Loss: 0.1135\n",
      "\n",
      "Batch 625/992 ━━━━━━━━━━━━━━━━━━━━ 16:25:06\n",
      "Accuracy: 0.9610 - Loss: 0.1138\n",
      "\n",
      "Batch 626/992 ━━━━━━━━━━━━━━━━━━━━ 16:25:17\n",
      "Accuracy: 0.9611 - Loss: 0.1138\n",
      "\n",
      "Batch 627/992 ━━━━━━━━━━━━━━━━━━━━ 16:25:28\n",
      "Accuracy: 0.9609 - Loss: 0.1142\n",
      "\n",
      "Batch 628/992 ━━━━━━━━━━━━━━━━━━━━ 16:25:39\n",
      "Accuracy: 0.9608 - Loss: 0.1142\n",
      "\n",
      "Batch 629/992 ━━━━━━━━━━━━━━━━━━━━ 16:25:50\n",
      "Accuracy: 0.9609 - Loss: 0.1141\n",
      "\n",
      "Batch 630/992 ━━━━━━━━━━━━━━━━━━━━ 16:26:01\n",
      "Accuracy: 0.9609 - Loss: 0.1141\n",
      "\n",
      "Batch 631/992 ━━━━━━━━━━━━━━━━━━━━ 16:26:14\n",
      "Accuracy: 0.9610 - Loss: 0.1140\n",
      "\n",
      "Batch 632/992 ━━━━━━━━━━━━━━━━━━━━ 16:26:26\n",
      "Accuracy: 0.9610 - Loss: 0.1138\n",
      "\n",
      "Batch 633/992 ━━━━━━━━━━━━━━━━━━━━ 16:26:37\n",
      "Accuracy: 0.9607 - Loss: 0.1145\n",
      "\n",
      "Batch 634/992 ━━━━━━━━━━━━━━━━━━━━ 16:26:47\n",
      "Accuracy: 0.9608 - Loss: 0.1147\n",
      "\n",
      "Batch 635/992 ━━━━━━━━━━━━━━━━━━━━ 16:26:58\n",
      "Accuracy: 0.9606 - Loss: 0.1152\n",
      "\n",
      "Batch 636/992 ━━━━━━━━━━━━━━━━━━━━ 16:27:09\n",
      "Accuracy: 0.9605 - Loss: 0.1155\n",
      "\n",
      "Batch 637/992 ━━━━━━━━━━━━━━━━━━━━ 16:27:20\n",
      "Accuracy: 0.9606 - Loss: 0.1154\n",
      "\n",
      "Batch 638/992 ━━━━━━━━━━━━━━━━━━━━ 16:27:32\n",
      "Accuracy: 0.9606 - Loss: 0.1153\n",
      "\n",
      "Batch 639/992 ━━━━━━━━━━━━━━━━━━━━ 16:27:44\n",
      "Accuracy: 0.9603 - Loss: 0.1158\n",
      "\n",
      "Batch 640/992 ━━━━━━━━━━━━━━━━━━━━ 16:27:55\n",
      "Accuracy: 0.9604 - Loss: 0.1157\n",
      "\n",
      "Batch 641/992 ━━━━━━━━━━━━━━━━━━━━ 16:28:06\n",
      "Accuracy: 0.9604 - Loss: 0.1159\n",
      "\n",
      "Batch 642/992 ━━━━━━━━━━━━━━━━━━━━ 16:28:17\n",
      "Accuracy: 0.9601 - Loss: 0.1163\n",
      "\n",
      "Batch 643/992 ━━━━━━━━━━━━━━━━━━━━ 16:28:28\n",
      "Accuracy: 0.9600 - Loss: 0.1164\n",
      "\n",
      "Batch 644/992 ━━━━━━━━━━━━━━━━━━━━ 16:28:40\n",
      "Accuracy: 0.9600 - Loss: 0.1163\n",
      "\n",
      "Batch 645/992 ━━━━━━━━━━━━━━━━━━━━ 16:28:51\n",
      "Accuracy: 0.9601 - Loss: 0.1162\n",
      "\n",
      "Batch 646/992 ━━━━━━━━━━━━━━━━━━━━ 16:29:02\n",
      "Accuracy: 0.9599 - Loss: 0.1162\n",
      "\n",
      "Batch 647/992 ━━━━━━━━━━━━━━━━━━━━ 16:29:13\n",
      "Accuracy: 0.9594 - Loss: 0.1166\n",
      "\n",
      "Batch 648/992 ━━━━━━━━━━━━━━━━━━━━ 16:29:25\n",
      "Accuracy: 0.9595 - Loss: 0.1167\n",
      "\n",
      "Batch 649/992 ━━━━━━━━━━━━━━━━━━━━ 16:29:36\n",
      "Accuracy: 0.9596 - Loss: 0.1165\n",
      "\n",
      "Batch 650/992 ━━━━━━━━━━━━━━━━━━━━ 16:29:46\n",
      "Accuracy: 0.9596 - Loss: 0.1165\n",
      "\n",
      "Batch 651/992 ━━━━━━━━━━━━━━━━━━━━ 16:29:58\n",
      "Accuracy: 0.9597 - Loss: 0.1163\n",
      "\n",
      "Batch 652/992 ━━━━━━━━━━━━━━━━━━━━ 16:30:09\n",
      "Accuracy: 0.9595 - Loss: 0.1163\n",
      "\n",
      "Batch 653/992 ━━━━━━━━━━━━━━━━━━━━ 16:30:20\n",
      "Accuracy: 0.9596 - Loss: 0.1162\n",
      "\n",
      "Batch 654/992 ━━━━━━━━━━━━━━━━━━━━ 16:30:31\n",
      "Accuracy: 0.9597 - Loss: 0.1162\n",
      "\n",
      "Batch 655/992 ━━━━━━━━━━━━━━━━━━━━ 16:30:42\n",
      "Accuracy: 0.9597 - Loss: 0.1161\n",
      "\n",
      "Batch 656/992 ━━━━━━━━━━━━━━━━━━━━ 16:30:53\n",
      "Accuracy: 0.9596 - Loss: 0.1164\n",
      "\n",
      "Batch 657/992 ━━━━━━━━━━━━━━━━━━━━ 16:31:06\n",
      "Accuracy: 0.9597 - Loss: 0.1162\n",
      "\n",
      "Batch 658/992 ━━━━━━━━━━━━━━━━━━━━ 16:31:18\n",
      "Accuracy: 0.9597 - Loss: 0.1161\n",
      "\n",
      "Batch 659/992 ━━━━━━━━━━━━━━━━━━━━ 16:31:30\n",
      "Accuracy: 0.9596 - Loss: 0.1162\n",
      "\n",
      "Batch 660/992 ━━━━━━━━━━━━━━━━━━━━ 16:31:41\n",
      "Accuracy: 0.9595 - Loss: 0.1163\n",
      "\n",
      "Batch 661/992 ━━━━━━━━━━━━━━━━━━━━ 16:31:54\n",
      "Accuracy: 0.9595 - Loss: 0.1162\n",
      "\n",
      "Batch 662/992 ━━━━━━━━━━━━━━━━━━━━ 16:32:06\n",
      "Accuracy: 0.9596 - Loss: 0.1161\n",
      "\n",
      "Batch 663/992 ━━━━━━━━━━━━━━━━━━━━ 16:32:19\n",
      "Accuracy: 0.9595 - Loss: 0.1166\n",
      "\n",
      "Batch 664/992 ━━━━━━━━━━━━━━━━━━━━ 16:32:30\n",
      "Accuracy: 0.9593 - Loss: 0.1168\n",
      "\n",
      "Batch 665/992 ━━━━━━━━━━━━━━━━━━━━ 16:32:40\n",
      "Accuracy: 0.9594 - Loss: 0.1167\n",
      "\n",
      "Batch 666/992 ━━━━━━━━━━━━━━━━━━━━ 16:32:50\n",
      "Accuracy: 0.9595 - Loss: 0.1165\n",
      "\n",
      "Batch 667/992 ━━━━━━━━━━━━━━━━━━━━ 16:33:01\n",
      "Accuracy: 0.9595 - Loss: 0.1164\n",
      "\n",
      "Batch 668/992 ━━━━━━━━━━━━━━━━━━━━ 16:33:12\n",
      "Accuracy: 0.9594 - Loss: 0.1168\n",
      "\n",
      "Batch 669/992 ━━━━━━━━━━━━━━━━━━━━ 16:33:22\n",
      "Accuracy: 0.9593 - Loss: 0.1168\n",
      "\n",
      "Batch 670/992 ━━━━━━━━━━━━━━━━━━━━ 16:33:33\n",
      "Accuracy: 0.9591 - Loss: 0.1169\n",
      "\n",
      "Batch 671/992 ━━━━━━━━━━━━━━━━━━━━ 16:33:43\n",
      "Accuracy: 0.9592 - Loss: 0.1168\n",
      "\n",
      "Batch 672/992 ━━━━━━━━━━━━━━━━━━━━ 16:33:53\n",
      "Accuracy: 0.9591 - Loss: 0.1169\n",
      "\n",
      "Batch 673/992 ━━━━━━━━━━━━━━━━━━━━ 16:34:03\n",
      "Accuracy: 0.9591 - Loss: 0.1167\n",
      "\n",
      "Batch 674/992 ━━━━━━━━━━━━━━━━━━━━ 16:34:14\n",
      "Accuracy: 0.9592 - Loss: 0.1166\n",
      "\n",
      "Batch 675/992 ━━━━━━━━━━━━━━━━━━━━ 16:34:24\n",
      "Accuracy: 0.9591 - Loss: 0.1166\n",
      "\n",
      "Batch 676/992 ━━━━━━━━━━━━━━━━━━━━ 16:34:34\n",
      "Accuracy: 0.9591 - Loss: 0.1165\n",
      "\n",
      "Batch 677/992 ━━━━━━━━━━━━━━━━━━━━ 16:34:45\n",
      "Accuracy: 0.9592 - Loss: 0.1167\n",
      "\n",
      "Batch 678/992 ━━━━━━━━━━━━━━━━━━━━ 16:34:55\n",
      "Accuracy: 0.9593 - Loss: 0.1166\n",
      "\n",
      "Batch 679/992 ━━━━━━━━━━━━━━━━━━━━ 16:35:06\n",
      "Accuracy: 0.9591 - Loss: 0.1167\n",
      "\n",
      "Batch 680/992 ━━━━━━━━━━━━━━━━━━━━ 16:35:16\n",
      "Accuracy: 0.9590 - Loss: 0.1170\n",
      "\n",
      "Batch 681/992 ━━━━━━━━━━━━━━━━━━━━ 16:35:27\n",
      "Accuracy: 0.9589 - Loss: 0.1175\n",
      "\n",
      "Batch 682/992 ━━━━━━━━━━━━━━━━━━━━ 16:35:37\n",
      "Accuracy: 0.9589 - Loss: 0.1175\n",
      "\n",
      "Batch 683/992 ━━━━━━━━━━━━━━━━━━━━ 16:35:47\n",
      "Accuracy: 0.9590 - Loss: 0.1174\n",
      "\n",
      "Batch 684/992 ━━━━━━━━━━━━━━━━━━━━ 16:35:57\n",
      "Accuracy: 0.9591 - Loss: 0.1173\n",
      "\n",
      "Batch 685/992 ━━━━━━━━━━━━━━━━━━━━ 16:36:09\n",
      "Accuracy: 0.9591 - Loss: 0.1172\n",
      "\n",
      "Batch 686/992 ━━━━━━━━━━━━━━━━━━━━ 16:36:20\n",
      "Accuracy: 0.9592 - Loss: 0.1171\n",
      "\n",
      "Batch 687/992 ━━━━━━━━━━━━━━━━━━━━ 16:36:32\n",
      "Accuracy: 0.9592 - Loss: 0.1170\n",
      "\n",
      "Batch 688/992 ━━━━━━━━━━━━━━━━━━━━ 16:36:43\n",
      "Accuracy: 0.9593 - Loss: 0.1169\n",
      "\n",
      "Batch 689/992 ━━━━━━━━━━━━━━━━━━━━ 16:36:54\n",
      "Accuracy: 0.9594 - Loss: 0.1168\n",
      "\n",
      "Batch 690/992 ━━━━━━━━━━━━━━━━━━━━ 16:37:05\n",
      "Accuracy: 0.9591 - Loss: 0.1173\n",
      "\n",
      "Batch 691/992 ━━━━━━━━━━━━━━━━━━━━ 16:37:16\n",
      "Accuracy: 0.9591 - Loss: 0.1173\n",
      "\n",
      "Batch 692/992 ━━━━━━━━━━━━━━━━━━━━ 16:37:28\n",
      "Accuracy: 0.9590 - Loss: 0.1173\n",
      "\n",
      "Batch 693/992 ━━━━━━━━━━━━━━━━━━━━ 16:37:40\n",
      "Accuracy: 0.9589 - Loss: 0.1178\n",
      "\n",
      "Batch 694/992 ━━━━━━━━━━━━━━━━━━━━ 16:37:51\n",
      "Accuracy: 0.9589 - Loss: 0.1177\n",
      "\n",
      "Batch 695/992 ━━━━━━━━━━━━━━━━━━━━ 16:38:02\n",
      "Accuracy: 0.9590 - Loss: 0.1176\n",
      "\n",
      "Batch 696/992 ━━━━━━━━━━━━━━━━━━━━ 16:38:12\n",
      "Accuracy: 0.9589 - Loss: 0.1177\n",
      "\n",
      "Batch 697/992 ━━━━━━━━━━━━━━━━━━━━ 16:38:22\n",
      "Accuracy: 0.9589 - Loss: 0.1176\n",
      "\n",
      "Batch 698/992 ━━━━━━━━━━━━━━━━━━━━ 16:38:32\n",
      "Accuracy: 0.9590 - Loss: 0.1176\n",
      "\n",
      "Batch 699/992 ━━━━━━━━━━━━━━━━━━━━ 16:38:43\n",
      "Accuracy: 0.9590 - Loss: 0.1176\n",
      "\n",
      "Batch 700/992 ━━━━━━━━━━━━━━━━━━━━ 16:38:53\n",
      "Accuracy: 0.9591 - Loss: 0.1175\n",
      "\n",
      "Batch 701/992 ━━━━━━━━━━━━━━━━━━━━ 16:39:04\n",
      "Accuracy: 0.9592 - Loss: 0.1174\n",
      "\n",
      "Batch 702/992 ━━━━━━━━━━━━━━━━━━━━ 16:39:14\n",
      "Accuracy: 0.9590 - Loss: 0.1176\n",
      "\n",
      "Batch 703/992 ━━━━━━━━━━━━━━━━━━━━ 16:39:25\n",
      "Accuracy: 0.9589 - Loss: 0.1179\n",
      "\n",
      "Batch 704/992 ━━━━━━━━━━━━━━━━━━━━ 16:39:35\n",
      "Accuracy: 0.9588 - Loss: 0.1180\n",
      "\n",
      "Batch 705/992 ━━━━━━━━━━━━━━━━━━━━ 16:39:45\n",
      "Accuracy: 0.9589 - Loss: 0.1178\n",
      "\n",
      "Batch 706/992 ━━━━━━━━━━━━━━━━━━━━ 16:39:56\n",
      "Accuracy: 0.9589 - Loss: 0.1177\n",
      "\n",
      "Batch 707/992 ━━━━━━━━━━━━━━━━━━━━ 16:40:06\n",
      "Accuracy: 0.9590 - Loss: 0.1177\n",
      "\n",
      "Batch 708/992 ━━━━━━━━━━━━━━━━━━━━ 16:40:16\n",
      "Accuracy: 0.9590 - Loss: 0.1175\n",
      "\n",
      "Batch 709/992 ━━━━━━━━━━━━━━━━━━━━ 16:40:26\n",
      "Accuracy: 0.9591 - Loss: 0.1174\n",
      "\n",
      "Batch 710/992 ━━━━━━━━━━━━━━━━━━━━ 16:40:37\n",
      "Accuracy: 0.9592 - Loss: 0.1173\n",
      "\n",
      "Batch 711/992 ━━━━━━━━━━━━━━━━━━━━ 16:40:47\n",
      "Accuracy: 0.9592 - Loss: 0.1172\n",
      "\n",
      "Batch 712/992 ━━━━━━━━━━━━━━━━━━━━ 16:40:57\n",
      "Accuracy: 0.9593 - Loss: 0.1171\n",
      "\n",
      "Batch 713/992 ━━━━━━━━━━━━━━━━━━━━ 16:41:08\n",
      "Accuracy: 0.9593 - Loss: 0.1169\n",
      "\n",
      "Batch 714/992 ━━━━━━━━━━━━━━━━━━━━ 16:41:19\n",
      "Accuracy: 0.9594 - Loss: 0.1168\n",
      "\n",
      "Batch 715/992 ━━━━━━━━━━━━━━━━━━━━ 16:41:29\n",
      "Accuracy: 0.9594 - Loss: 0.1167\n",
      "\n",
      "Batch 716/992 ━━━━━━━━━━━━━━━━━━━━ 16:41:39\n",
      "Accuracy: 0.9593 - Loss: 0.1167\n",
      "\n",
      "Batch 717/992 ━━━━━━━━━━━━━━━━━━━━ 16:41:49\n",
      "Accuracy: 0.9594 - Loss: 0.1167\n",
      "\n",
      "Batch 718/992 ━━━━━━━━━━━━━━━━━━━━ 16:42:00\n",
      "Accuracy: 0.9593 - Loss: 0.1169\n",
      "\n",
      "Batch 719/992 ━━━━━━━━━━━━━━━━━━━━ 16:42:10\n",
      "Accuracy: 0.9593 - Loss: 0.1169\n",
      "\n",
      "Batch 720/992 ━━━━━━━━━━━━━━━━━━━━ 16:42:20\n",
      "Accuracy: 0.9592 - Loss: 0.1173\n",
      "\n",
      "Batch 721/992 ━━━━━━━━━━━━━━━━━━━━ 16:42:30\n",
      "Accuracy: 0.9593 - Loss: 0.1172\n",
      "\n",
      "Batch 722/992 ━━━━━━━━━━━━━━━━━━━━ 16:42:40\n",
      "Accuracy: 0.9593 - Loss: 0.1170\n",
      "\n",
      "Batch 723/992 ━━━━━━━━━━━━━━━━━━━━ 16:42:51\n",
      "Accuracy: 0.9594 - Loss: 0.1169\n",
      "\n",
      "Batch 724/992 ━━━━━━━━━━━━━━━━━━━━ 16:43:01\n",
      "Accuracy: 0.9594 - Loss: 0.1168\n",
      "\n",
      "Batch 725/992 ━━━━━━━━━━━━━━━━━━━━ 16:43:11\n",
      "Accuracy: 0.9595 - Loss: 0.1167\n",
      "\n",
      "Batch 726/992 ━━━━━━━━━━━━━━━━━━━━ 16:43:22\n",
      "Accuracy: 0.9595 - Loss: 0.1166\n",
      "\n",
      "Batch 727/992 ━━━━━━━━━━━━━━━━━━━━ 16:43:32\n",
      "Accuracy: 0.9594 - Loss: 0.1168\n",
      "\n",
      "Batch 728/992 ━━━━━━━━━━━━━━━━━━━━ 16:43:43\n",
      "Accuracy: 0.9595 - Loss: 0.1168\n",
      "\n",
      "Batch 729/992 ━━━━━━━━━━━━━━━━━━━━ 16:43:53\n",
      "Accuracy: 0.9595 - Loss: 0.1168\n",
      "\n",
      "Batch 730/992 ━━━━━━━━━━━━━━━━━━━━ 16:44:03\n",
      "Accuracy: 0.9596 - Loss: 0.1169\n",
      "\n",
      "Batch 731/992 ━━━━━━━━━━━━━━━━━━━━ 16:44:13\n",
      "Accuracy: 0.9596 - Loss: 0.1167\n",
      "\n",
      "Batch 732/992 ━━━━━━━━━━━━━━━━━━━━ 16:44:23\n",
      "Accuracy: 0.9597 - Loss: 0.1166\n",
      "\n",
      "Batch 733/992 ━━━━━━━━━━━━━━━━━━━━ 16:44:33\n",
      "Accuracy: 0.9598 - Loss: 0.1165\n",
      "\n",
      "Batch 734/992 ━━━━━━━━━━━━━━━━━━━━ 16:44:44\n",
      "Accuracy: 0.9598 - Loss: 0.1165\n",
      "\n",
      "Batch 735/992 ━━━━━━━━━━━━━━━━━━━━ 16:44:54\n",
      "Accuracy: 0.9599 - Loss: 0.1164\n",
      "\n",
      "Batch 736/992 ━━━━━━━━━━━━━━━━━━━━ 16:45:04\n",
      "Accuracy: 0.9599 - Loss: 0.1164\n",
      "\n",
      "Batch 737/992 ━━━━━━━━━━━━━━━━━━━━ 16:45:15\n",
      "Accuracy: 0.9598 - Loss: 0.1166\n",
      "\n",
      "Batch 738/992 ━━━━━━━━━━━━━━━━━━━━ 16:45:25\n",
      "Accuracy: 0.9599 - Loss: 0.1165\n",
      "\n",
      "Batch 739/992 ━━━━━━━━━━━━━━━━━━━━ 16:45:36\n",
      "Accuracy: 0.9599 - Loss: 0.1164\n",
      "\n",
      "Batch 740/992 ━━━━━━━━━━━━━━━━━━━━ 16:45:46\n",
      "Accuracy: 0.9600 - Loss: 0.1163\n",
      "\n",
      "Batch 741/992 ━━━━━━━━━━━━━━━━━━━━ 16:45:56\n",
      "Accuracy: 0.9600 - Loss: 0.1163\n",
      "\n",
      "Batch 742/992 ━━━━━━━━━━━━━━━━━━━━ 16:46:07\n",
      "Accuracy: 0.9601 - Loss: 0.1161\n",
      "\n",
      "Batch 743/992 ━━━━━━━━━━━━━━━━━━━━ 16:46:17\n",
      "Accuracy: 0.9601 - Loss: 0.1160\n",
      "\n",
      "Batch 744/992 ━━━━━━━━━━━━━━━━━━━━ 16:46:27\n",
      "Accuracy: 0.9600 - Loss: 0.1168\n",
      "\n",
      "Batch 745/992 ━━━━━━━━━━━━━━━━━━━━ 16:46:37\n",
      "Accuracy: 0.9601 - Loss: 0.1167\n",
      "\n",
      "Batch 746/992 ━━━━━━━━━━━━━━━━━━━━ 16:46:47\n",
      "Accuracy: 0.9601 - Loss: 0.1165\n",
      "\n",
      "Batch 747/992 ━━━━━━━━━━━━━━━━━━━━ 16:46:58\n",
      "Accuracy: 0.9602 - Loss: 0.1165\n",
      "\n",
      "Batch 748/992 ━━━━━━━━━━━━━━━━━━━━ 16:47:08\n",
      "Accuracy: 0.9602 - Loss: 0.1164\n",
      "\n",
      "Batch 749/992 ━━━━━━━━━━━━━━━━━━━━ 16:47:19\n",
      "Accuracy: 0.9603 - Loss: 0.1162\n",
      "\n",
      "Batch 750/992 ━━━━━━━━━━━━━━━━━━━━ 16:47:29\n",
      "Accuracy: 0.9603 - Loss: 0.1161\n",
      "\n",
      "Batch 751/992 ━━━━━━━━━━━━━━━━━━━━ 16:47:39\n",
      "Accuracy: 0.9604 - Loss: 0.1159\n",
      "\n",
      "Batch 752/992 ━━━━━━━━━━━━━━━━━━━━ 16:47:49\n",
      "Accuracy: 0.9604 - Loss: 0.1158\n",
      "\n",
      "Batch 753/992 ━━━━━━━━━━━━━━━━━━━━ 16:48:00\n",
      "Accuracy: 0.9605 - Loss: 0.1157\n",
      "\n",
      "Batch 754/992 ━━━━━━━━━━━━━━━━━━━━ 16:48:10\n",
      "Accuracy: 0.9605 - Loss: 0.1156\n",
      "\n",
      "Batch 755/992 ━━━━━━━━━━━━━━━━━━━━ 16:48:20\n",
      "Accuracy: 0.9606 - Loss: 0.1156\n",
      "\n",
      "Batch 756/992 ━━━━━━━━━━━━━━━━━━━━ 16:48:30\n",
      "Accuracy: 0.9605 - Loss: 0.1158\n",
      "\n",
      "Batch 757/992 ━━━━━━━━━━━━━━━━━━━━ 16:48:40\n",
      "Accuracy: 0.9605 - Loss: 0.1157\n",
      "\n",
      "Batch 758/992 ━━━━━━━━━━━━━━━━━━━━ 16:48:51\n",
      "Accuracy: 0.9606 - Loss: 0.1158\n",
      "\n",
      "Batch 759/992 ━━━━━━━━━━━━━━━━━━━━ 16:49:01\n",
      "Accuracy: 0.9606 - Loss: 0.1156\n",
      "\n",
      "Batch 760/992 ━━━━━━━━━━━━━━━━━━━━ 16:49:11\n",
      "Accuracy: 0.9607 - Loss: 0.1155\n",
      "\n",
      "Batch 761/992 ━━━━━━━━━━━━━━━━━━━━ 16:49:22\n",
      "Accuracy: 0.9606 - Loss: 0.1155\n",
      "\n",
      "Batch 762/992 ━━━━━━━━━━━━━━━━━━━━ 16:49:32\n",
      "Accuracy: 0.9606 - Loss: 0.1154\n",
      "\n",
      "Batch 763/992 ━━━━━━━━━━━━━━━━━━━━ 16:49:42\n",
      "Accuracy: 0.9607 - Loss: 0.1153\n",
      "\n",
      "Batch 764/992 ━━━━━━━━━━━━━━━━━━━━ 16:49:52\n",
      "Accuracy: 0.9607 - Loss: 0.1152\n",
      "\n",
      "Batch 765/992 ━━━━━━━━━━━━━━━━━━━━ 16:50:03\n",
      "Accuracy: 0.9608 - Loss: 0.1151\n",
      "\n",
      "Batch 766/992 ━━━━━━━━━━━━━━━━━━━━ 16:50:13\n",
      "Accuracy: 0.9607 - Loss: 0.1154\n",
      "\n",
      "Batch 767/992 ━━━━━━━━━━━━━━━━━━━━ 16:50:23\n",
      "Accuracy: 0.9607 - Loss: 0.1152\n",
      "\n",
      "Batch 768/992 ━━━━━━━━━━━━━━━━━━━━ 16:50:33\n",
      "Accuracy: 0.9608 - Loss: 0.1151\n",
      "\n",
      "Batch 769/992 ━━━━━━━━━━━━━━━━━━━━ 16:50:43\n",
      "Accuracy: 0.9608 - Loss: 0.1150\n",
      "\n",
      "Batch 770/992 ━━━━━━━━━━━━━━━━━━━━ 16:50:53\n",
      "Accuracy: 0.9607 - Loss: 0.1151\n",
      "\n",
      "Batch 771/992 ━━━━━━━━━━━━━━━━━━━━ 16:51:03\n",
      "Accuracy: 0.9608 - Loss: 0.1151\n",
      "\n",
      "Batch 772/992 ━━━━━━━━━━━━━━━━━━━━ 16:51:14\n",
      "Accuracy: 0.9607 - Loss: 0.1151\n",
      "\n",
      "Batch 773/992 ━━━━━━━━━━━━━━━━━━━━ 16:51:24\n",
      "Accuracy: 0.9607 - Loss: 0.1151\n",
      "\n",
      "Batch 774/992 ━━━━━━━━━━━━━━━━━━━━ 16:51:35\n",
      "Accuracy: 0.9608 - Loss: 0.1150\n",
      "\n",
      "Batch 775/992 ━━━━━━━━━━━━━━━━━━━━ 16:51:45\n",
      "Accuracy: 0.9608 - Loss: 0.1149\n",
      "\n",
      "Batch 776/992 ━━━━━━━━━━━━━━━━━━━━ 16:51:55\n",
      "Accuracy: 0.9609 - Loss: 0.1148\n",
      "\n",
      "Batch 777/992 ━━━━━━━━━━━━━━━━━━━━ 16:52:05\n",
      "Accuracy: 0.9609 - Loss: 0.1147\n",
      "\n",
      "Batch 778/992 ━━━━━━━━━━━━━━━━━━━━ 16:52:15\n",
      "Accuracy: 0.9610 - Loss: 0.1146\n",
      "\n",
      "Batch 779/992 ━━━━━━━━━━━━━━━━━━━━ 16:52:26\n",
      "Accuracy: 0.9610 - Loss: 0.1145\n",
      "\n",
      "Batch 780/992 ━━━━━━━━━━━━━━━━━━━━ 16:52:36\n",
      "Accuracy: 0.9611 - Loss: 0.1144\n",
      "\n",
      "Batch 781/992 ━━━━━━━━━━━━━━━━━━━━ 16:52:46\n",
      "Accuracy: 0.9611 - Loss: 0.1144\n",
      "\n",
      "Batch 782/992 ━━━━━━━━━━━━━━━━━━━━ 16:52:56\n",
      "Accuracy: 0.9612 - Loss: 0.1143\n",
      "\n",
      "Batch 783/992 ━━━━━━━━━━━━━━━━━━━━ 16:53:06\n",
      "Accuracy: 0.9612 - Loss: 0.1143\n",
      "\n",
      "Batch 784/992 ━━━━━━━━━━━━━━━━━━━━ 16:53:17\n",
      "Accuracy: 0.9613 - Loss: 0.1144\n",
      "\n",
      "Batch 785/992 ━━━━━━━━━━━━━━━━━━━━ 16:53:28\n",
      "Accuracy: 0.9613 - Loss: 0.1143\n",
      "\n",
      "Batch 786/992 ━━━━━━━━━━━━━━━━━━━━ 16:53:38\n",
      "Accuracy: 0.9612 - Loss: 0.1144\n",
      "\n",
      "Batch 787/992 ━━━━━━━━━━━━━━━━━━━━ 16:53:48\n",
      "Accuracy: 0.9612 - Loss: 0.1143\n",
      "\n",
      "Batch 788/992 ━━━━━━━━━━━━━━━━━━━━ 16:53:58\n",
      "Accuracy: 0.9611 - Loss: 0.1144\n",
      "\n",
      "Batch 789/992 ━━━━━━━━━━━━━━━━━━━━ 16:54:09\n",
      "Accuracy: 0.9609 - Loss: 0.1148\n",
      "\n",
      "Batch 790/992 ━━━━━━━━━━━━━━━━━━━━ 16:54:19\n",
      "Accuracy: 0.9608 - Loss: 0.1149\n",
      "\n",
      "Batch 791/992 ━━━━━━━━━━━━━━━━━━━━ 16:54:29\n",
      "Accuracy: 0.9608 - Loss: 0.1148\n",
      "\n",
      "Batch 792/992 ━━━━━━━━━━━━━━━━━━━━ 16:54:39\n",
      "Accuracy: 0.9609 - Loss: 0.1148\n",
      "\n",
      "Batch 793/992 ━━━━━━━━━━━━━━━━━━━━ 16:54:50\n",
      "Accuracy: 0.9606 - Loss: 0.1151\n",
      "\n",
      "Batch 794/992 ━━━━━━━━━━━━━━━━━━━━ 16:55:00\n",
      "Accuracy: 0.9605 - Loss: 0.1159\n",
      "\n",
      "Batch 795/992 ━━━━━━━━━━━━━━━━━━━━ 16:55:10\n",
      "Accuracy: 0.9605 - Loss: 0.1158\n",
      "\n",
      "Batch 796/992 ━━━━━━━━━━━━━━━━━━━━ 16:55:21\n",
      "Accuracy: 0.9603 - Loss: 0.1160\n",
      "\n",
      "Batch 797/992 ━━━━━━━━━━━━━━━━━━━━ 16:55:31\n",
      "Accuracy: 0.9603 - Loss: 0.1160\n",
      "\n",
      "Batch 798/992 ━━━━━━━━━━━━━━━━━━━━ 16:55:42\n",
      "Accuracy: 0.9604 - Loss: 0.1160\n",
      "\n",
      "Batch 799/992 ━━━━━━━━━━━━━━━━━━━━ 16:55:52\n",
      "Accuracy: 0.9604 - Loss: 0.1161\n",
      "\n",
      "Batch 800/992 ━━━━━━━━━━━━━━━━━━━━ 16:56:02\n",
      "Accuracy: 0.9605 - Loss: 0.1160\n",
      "\n",
      "Batch 801/992 ━━━━━━━━━━━━━━━━━━━━ 16:56:12\n",
      "Accuracy: 0.9605 - Loss: 0.1158\n",
      "\n",
      "Batch 802/992 ━━━━━━━━━━━━━━━━━━━━ 16:56:23\n",
      "Accuracy: 0.9606 - Loss: 0.1159\n",
      "\n",
      "Batch 803/992 ━━━━━━━━━━━━━━━━━━━━ 16:56:33\n",
      "Accuracy: 0.9606 - Loss: 0.1157\n",
      "\n",
      "Batch 804/992 ━━━━━━━━━━━━━━━━━━━━ 16:56:44\n",
      "Accuracy: 0.9607 - Loss: 0.1156\n",
      "\n",
      "Batch 805/992 ━━━━━━━━━━━━━━━━━━━━ 16:56:54\n",
      "Accuracy: 0.9607 - Loss: 0.1158\n",
      "\n",
      "Batch 806/992 ━━━━━━━━━━━━━━━━━━━━ 16:57:04\n",
      "Accuracy: 0.9608 - Loss: 0.1158\n",
      "\n",
      "Batch 807/992 ━━━━━━━━━━━━━━━━━━━━ 16:57:14\n",
      "Accuracy: 0.9608 - Loss: 0.1157\n",
      "\n",
      "Batch 808/992 ━━━━━━━━━━━━━━━━━━━━ 16:57:25\n",
      "Accuracy: 0.9609 - Loss: 0.1157\n",
      "\n",
      "Batch 809/992 ━━━━━━━━━━━━━━━━━━━━ 16:57:35\n",
      "Accuracy: 0.9609 - Loss: 0.1156\n",
      "\n",
      "Batch 810/992 ━━━━━━━━━━━━━━━━━━━━ 16:57:45\n",
      "Accuracy: 0.9610 - Loss: 0.1155\n",
      "\n",
      "Batch 811/992 ━━━━━━━━━━━━━━━━━━━━ 16:57:56\n",
      "Accuracy: 0.9609 - Loss: 0.1156\n",
      "\n",
      "Batch 812/992 ━━━━━━━━━━━━━━━━━━━━ 16:58:06\n",
      "Accuracy: 0.9607 - Loss: 0.1158\n",
      "\n",
      "Batch 813/992 ━━━━━━━━━━━━━━━━━━━━ 16:58:16\n",
      "Accuracy: 0.9608 - Loss: 0.1157\n",
      "\n",
      "Batch 814/992 ━━━━━━━━━━━━━━━━━━━━ 16:58:26\n",
      "Accuracy: 0.9607 - Loss: 0.1158\n",
      "\n",
      "Batch 815/992 ━━━━━━━━━━━━━━━━━━━━ 16:58:37\n",
      "Accuracy: 0.9607 - Loss: 0.1157\n",
      "\n",
      "Batch 816/992 ━━━━━━━━━━━━━━━━━━━━ 16:58:47\n",
      "Accuracy: 0.9606 - Loss: 0.1163\n",
      "\n",
      "Batch 817/992 ━━━━━━━━━━━━━━━━━━━━ 16:58:57\n",
      "Accuracy: 0.9605 - Loss: 0.1168\n",
      "\n",
      "Batch 818/992 ━━━━━━━━━━━━━━━━━━━━ 16:59:07\n",
      "Accuracy: 0.9606 - Loss: 0.1167\n",
      "\n",
      "Batch 819/992 ━━━━━━━━━━━━━━━━━━━━ 16:59:18\n",
      "Accuracy: 0.9605 - Loss: 0.1169\n",
      "\n",
      "Batch 820/992 ━━━━━━━━━━━━━━━━━━━━ 16:59:28\n",
      "Accuracy: 0.9605 - Loss: 0.1169\n",
      "\n",
      "Batch 821/992 ━━━━━━━━━━━━━━━━━━━━ 16:59:39\n",
      "Accuracy: 0.9604 - Loss: 0.1169\n",
      "\n",
      "Batch 822/992 ━━━━━━━━━━━━━━━━━━━━ 16:59:49\n",
      "Accuracy: 0.9605 - Loss: 0.1168\n",
      "\n",
      "Batch 823/992 ━━━━━━━━━━━━━━━━━━━━ 16:59:59\n",
      "Accuracy: 0.9604 - Loss: 0.1169\n",
      "\n",
      "Batch 824/992 ━━━━━━━━━━━━━━━━━━━━ 17:00:09\n",
      "Accuracy: 0.9604 - Loss: 0.1167\n",
      "\n",
      "Batch 825/992 ━━━━━━━━━━━━━━━━━━━━ 17:00:19\n",
      "Accuracy: 0.9605 - Loss: 0.1166\n",
      "\n",
      "Batch 826/992 ━━━━━━━━━━━━━━━━━━━━ 17:00:30\n",
      "Accuracy: 0.9604 - Loss: 0.1167\n",
      "\n",
      "Batch 827/992 ━━━━━━━━━━━━━━━━━━━━ 17:00:40\n",
      "Accuracy: 0.9601 - Loss: 0.1171\n",
      "\n",
      "Batch 828/992 ━━━━━━━━━━━━━━━━━━━━ 17:00:50\n",
      "Accuracy: 0.9600 - Loss: 0.1172\n",
      "\n",
      "Batch 829/992 ━━━━━━━━━━━━━━━━━━━━ 17:01:00\n",
      "Accuracy: 0.9600 - Loss: 0.1171\n",
      "\n",
      "Batch 830/992 ━━━━━━━━━━━━━━━━━━━━ 17:01:10\n",
      "Accuracy: 0.9601 - Loss: 0.1170\n",
      "\n",
      "Batch 831/992 ━━━━━━━━━━━━━━━━━━━━ 17:01:21\n",
      "Accuracy: 0.9601 - Loss: 0.1168\n",
      "\n",
      "Batch 832/992 ━━━━━━━━━━━━━━━━━━━━ 17:01:32\n",
      "Accuracy: 0.9602 - Loss: 0.1168\n",
      "\n",
      "Batch 833/992 ━━━━━━━━━━━━━━━━━━━━ 17:01:42\n",
      "Accuracy: 0.9602 - Loss: 0.1167\n",
      "\n",
      "Batch 834/992 ━━━━━━━━━━━━━━━━━━━━ 17:01:52\n",
      "Accuracy: 0.9603 - Loss: 0.1167\n",
      "\n",
      "Batch 835/992 ━━━━━━━━━━━━━━━━━━━━ 17:02:02\n",
      "Accuracy: 0.9603 - Loss: 0.1166\n",
      "\n",
      "Batch 836/992 ━━━━━━━━━━━━━━━━━━━━ 17:02:12\n",
      "Accuracy: 0.9602 - Loss: 0.1168\n",
      "\n",
      "Batch 837/992 ━━━━━━━━━━━━━━━━━━━━ 17:02:23\n",
      "Accuracy: 0.9603 - Loss: 0.1167\n",
      "\n",
      "Batch 838/992 ━━━━━━━━━━━━━━━━━━━━ 17:02:33\n",
      "Accuracy: 0.9602 - Loss: 0.1168\n",
      "\n",
      "Batch 839/992 ━━━━━━━━━━━━━━━━━━━━ 17:02:43\n",
      "Accuracy: 0.9602 - Loss: 0.1167\n",
      "\n",
      "Batch 840/992 ━━━━━━━━━━━━━━━━━━━━ 17:02:53\n",
      "Accuracy: 0.9603 - Loss: 0.1166\n",
      "\n",
      "Batch 841/992 ━━━━━━━━━━━━━━━━━━━━ 17:03:04\n",
      "Accuracy: 0.9603 - Loss: 0.1166\n",
      "\n",
      "Batch 842/992 ━━━━━━━━━━━━━━━━━━━━ 17:03:14\n",
      "Accuracy: 0.9604 - Loss: 0.1165\n",
      "\n",
      "Batch 843/992 ━━━━━━━━━━━━━━━━━━━━ 17:03:25\n",
      "Accuracy: 0.9604 - Loss: 0.1164\n",
      "\n",
      "Batch 844/992 ━━━━━━━━━━━━━━━━━━━━ 17:03:35\n",
      "Accuracy: 0.9605 - Loss: 0.1163\n",
      "\n",
      "Batch 845/992 ━━━━━━━━━━━━━━━━━━━━ 17:03:45\n",
      "Accuracy: 0.9605 - Loss: 0.1162\n",
      "\n",
      "Batch 846/992 ━━━━━━━━━━━━━━━━━━━━ 17:03:56\n",
      "Accuracy: 0.9605 - Loss: 0.1161\n",
      "\n",
      "Batch 847/992 ━━━━━━━━━━━━━━━━━━━━ 17:04:06\n",
      "Accuracy: 0.9606 - Loss: 0.1161\n",
      "\n",
      "Batch 848/992 ━━━━━━━━━━━━━━━━━━━━ 17:04:16\n",
      "Accuracy: 0.9606 - Loss: 0.1159\n",
      "\n",
      "Batch 849/992 ━━━━━━━━━━━━━━━━━━━━ 17:04:26\n",
      "Accuracy: 0.9607 - Loss: 0.1158\n",
      "\n",
      "Batch 850/992 ━━━━━━━━━━━━━━━━━━━━ 17:04:37\n",
      "Accuracy: 0.9606 - Loss: 0.1159\n",
      "\n",
      "Batch 851/992 ━━━━━━━━━━━━━━━━━━━━ 17:04:47\n",
      "Accuracy: 0.9606 - Loss: 0.1159\n",
      "\n",
      "Batch 852/992 ━━━━━━━━━━━━━━━━━━━━ 17:04:57\n",
      "Accuracy: 0.9607 - Loss: 0.1158\n",
      "\n",
      "Batch 853/992 ━━━━━━━━━━━━━━━━━━━━ 17:05:07\n",
      "Accuracy: 0.9607 - Loss: 0.1158\n",
      "\n",
      "Batch 854/992 ━━━━━━━━━━━━━━━━━━━━ 17:05:18\n",
      "Accuracy: 0.9608 - Loss: 0.1157\n",
      "\n",
      "Batch 855/992 ━━━━━━━━━━━━━━━━━━━━ 17:05:28\n",
      "Accuracy: 0.9607 - Loss: 0.1160\n",
      "\n",
      "Batch 856/992 ━━━━━━━━━━━━━━━━━━━━ 17:05:39\n",
      "Accuracy: 0.9607 - Loss: 0.1159\n",
      "\n",
      "Batch 857/992 ━━━━━━━━━━━━━━━━━━━━ 17:05:49\n",
      "Accuracy: 0.9608 - Loss: 0.1158\n",
      "\n",
      "Batch 858/992 ━━━━━━━━━━━━━━━━━━━━ 17:06:00\n",
      "Accuracy: 0.9608 - Loss: 0.1158\n",
      "\n",
      "Batch 859/992 ━━━━━━━━━━━━━━━━━━━━ 17:06:10\n",
      "Accuracy: 0.9609 - Loss: 0.1158\n",
      "\n",
      "Batch 860/992 ━━━━━━━━━━━━━━━━━━━━ 17:06:20\n",
      "Accuracy: 0.9609 - Loss: 0.1156\n",
      "\n",
      "Batch 861/992 ━━━━━━━━━━━━━━━━━━━━ 17:06:31\n",
      "Accuracy: 0.9609 - Loss: 0.1156\n",
      "\n",
      "Batch 862/992 ━━━━━━━━━━━━━━━━━━━━ 17:06:41\n",
      "Accuracy: 0.9610 - Loss: 0.1155\n",
      "\n",
      "Batch 863/992 ━━━━━━━━━━━━━━━━━━━━ 17:06:51\n",
      "Accuracy: 0.9610 - Loss: 0.1154\n",
      "\n",
      "Batch 864/992 ━━━━━━━━━━━━━━━━━━━━ 17:07:04\n",
      "Accuracy: 0.9609 - Loss: 0.1154\n",
      "\n",
      "Batch 865/992 ━━━━━━━━━━━━━━━━━━━━ 17:07:15\n",
      "Accuracy: 0.9608 - Loss: 0.1162\n",
      "\n",
      "Batch 866/992 ━━━━━━━━━━━━━━━━━━━━ 17:07:26\n",
      "Accuracy: 0.9609 - Loss: 0.1161\n",
      "\n",
      "Batch 867/992 ━━━━━━━━━━━━━━━━━━━━ 17:07:37\n",
      "Accuracy: 0.9609 - Loss: 0.1161\n",
      "\n",
      "Batch 868/992 ━━━━━━━━━━━━━━━━━━━━ 17:07:47\n",
      "Accuracy: 0.9610 - Loss: 0.1160\n",
      "\n",
      "Batch 869/992 ━━━━━━━━━━━━━━━━━━━━ 17:07:58\n",
      "Accuracy: 0.9610 - Loss: 0.1159\n",
      "\n",
      "Batch 870/992 ━━━━━━━━━━━━━━━━━━━━ 17:08:09\n",
      "Accuracy: 0.9608 - Loss: 0.1162\n",
      "\n",
      "Batch 871/992 ━━━━━━━━━━━━━━━━━━━━ 17:08:20\n",
      "Accuracy: 0.9607 - Loss: 0.1164\n",
      "\n",
      "Batch 872/992 ━━━━━━━━━━━━━━━━━━━━ 17:08:31\n",
      "Accuracy: 0.9606 - Loss: 0.1168\n",
      "\n",
      "Batch 873/992 ━━━━━━━━━━━━━━━━━━━━ 17:08:41\n",
      "Accuracy: 0.9606 - Loss: 0.1166\n",
      "\n",
      "Batch 874/992 ━━━━━━━━━━━━━━━━━━━━ 17:08:52\n",
      "Accuracy: 0.9607 - Loss: 0.1167\n",
      "\n",
      "Batch 875/992 ━━━━━━━━━━━━━━━━━━━━ 17:09:03\n",
      "Accuracy: 0.9607 - Loss: 0.1167\n",
      "\n",
      "Batch 876/992 ━━━━━━━━━━━━━━━━━━━━ 17:09:15\n",
      "Accuracy: 0.9608 - Loss: 0.1166\n",
      "\n",
      "Batch 877/992 ━━━━━━━━━━━━━━━━━━━━ 17:09:27\n",
      "Accuracy: 0.9607 - Loss: 0.1168\n",
      "\n",
      "Batch 878/992 ━━━━━━━━━━━━━━━━━━━━ 17:09:39\n",
      "Accuracy: 0.9607 - Loss: 0.1167\n",
      "\n",
      "Batch 879/992 ━━━━━━━━━━━━━━━━━━━━ 17:09:51\n",
      "Accuracy: 0.9608 - Loss: 0.1165\n",
      "\n",
      "Batch 880/992 ━━━━━━━━━━━━━━━━━━━━ 17:10:01\n",
      "Accuracy: 0.9608 - Loss: 0.1167\n",
      "\n",
      "Batch 881/992 ━━━━━━━━━━━━━━━━━━━━ 17:10:13\n",
      "Accuracy: 0.9608 - Loss: 0.1168\n",
      "\n",
      "Batch 882/992 ━━━━━━━━━━━━━━━━━━━━ 17:10:26\n",
      "Accuracy: 0.9609 - Loss: 0.1167\n",
      "\n",
      "Batch 883/992 ━━━━━━━━━━━━━━━━━━━━ 17:10:37\n",
      "Accuracy: 0.9609 - Loss: 0.1166\n",
      "\n",
      "Batch 884/992 ━━━━━━━━━━━━━━━━━━━━ 17:10:51\n",
      "Accuracy: 0.9610 - Loss: 0.1165\n",
      "\n",
      "Batch 885/992 ━━━━━━━━━━━━━━━━━━━━ 17:11:05\n",
      "Accuracy: 0.9610 - Loss: 0.1165\n",
      "\n",
      "Batch 886/992 ━━━━━━━━━━━━━━━━━━━━ 17:11:18\n",
      "Accuracy: 0.9609 - Loss: 0.1167\n",
      "\n",
      "Batch 887/992 ━━━━━━━━━━━━━━━━━━━━ 17:11:32\n",
      "Accuracy: 0.9610 - Loss: 0.1166\n",
      "\n",
      "Batch 888/992 ━━━━━━━━━━━━━━━━━━━━ 17:11:44\n",
      "Accuracy: 0.9610 - Loss: 0.1165\n",
      "\n",
      "Batch 889/992 ━━━━━━━━━━━━━━━━━━━━ 17:11:56\n",
      "Accuracy: 0.9611 - Loss: 0.1164\n",
      "\n",
      "Batch 890/992 ━━━━━━━━━━━━━━━━━━━━ 17:12:07\n",
      "Accuracy: 0.9610 - Loss: 0.1165\n",
      "\n",
      "Batch 891/992 ━━━━━━━━━━━━━━━━━━━━ 17:12:18\n",
      "Accuracy: 0.9610 - Loss: 0.1164\n",
      "\n",
      "Batch 892/992 ━━━━━━━━━━━━━━━━━━━━ 17:12:30\n",
      "Accuracy: 0.9610 - Loss: 0.1164\n",
      "\n",
      "Batch 893/992 ━━━━━━━━━━━━━━━━━━━━ 17:12:41\n",
      "Accuracy: 0.9611 - Loss: 0.1164\n",
      "\n",
      "Batch 894/992 ━━━━━━━━━━━━━━━━━━━━ 17:12:52\n",
      "Accuracy: 0.9610 - Loss: 0.1164\n",
      "\n",
      "Batch 895/992 ━━━━━━━━━━━━━━━━━━━━ 17:13:02\n",
      "Accuracy: 0.9610 - Loss: 0.1164\n",
      "\n",
      "Batch 896/992 ━━━━━━━━━━━━━━━━━━━━ 17:13:13\n",
      "Accuracy: 0.9609 - Loss: 0.1165\n",
      "\n",
      "Batch 897/992 ━━━━━━━━━━━━━━━━━━━━ 17:13:24\n",
      "Accuracy: 0.9610 - Loss: 0.1165\n",
      "\n",
      "Batch 898/992 ━━━━━━━━━━━━━━━━━━━━ 17:13:34\n",
      "Accuracy: 0.9609 - Loss: 0.1168\n",
      "\n",
      "Batch 899/992 ━━━━━━━━━━━━━━━━━━━━ 17:13:47\n",
      "Accuracy: 0.9608 - Loss: 0.1169\n",
      "\n",
      "Batch 900/992 ━━━━━━━━━━━━━━━━━━━━ 17:13:59\n",
      "Accuracy: 0.9608 - Loss: 0.1169\n",
      "\n",
      "Batch 901/992 ━━━━━━━━━━━━━━━━━━━━ 17:14:11\n",
      "Accuracy: 0.9607 - Loss: 0.1170\n",
      "\n",
      "Batch 902/992 ━━━━━━━━━━━━━━━━━━━━ 17:14:22\n",
      "Accuracy: 0.9608 - Loss: 0.1169\n",
      "\n",
      "Batch 903/992 ━━━━━━━━━━━━━━━━━━━━ 17:14:33\n",
      "Accuracy: 0.9608 - Loss: 0.1168\n",
      "\n",
      "Batch 904/992 ━━━━━━━━━━━━━━━━━━━━ 17:14:43\n",
      "Accuracy: 0.9609 - Loss: 0.1168\n",
      "\n",
      "Batch 905/992 ━━━━━━━━━━━━━━━━━━━━ 17:14:54\n",
      "Accuracy: 0.9609 - Loss: 0.1168\n",
      "\n",
      "Batch 906/992 ━━━━━━━━━━━━━━━━━━━━ 17:15:05\n",
      "Accuracy: 0.9610 - Loss: 0.1168\n",
      "\n",
      "Batch 907/992 ━━━━━━━━━━━━━━━━━━━━ 17:15:15\n",
      "Accuracy: 0.9610 - Loss: 0.1168\n",
      "\n",
      "Batch 908/992 ━━━━━━━━━━━━━━━━━━━━ 17:15:26\n",
      "Accuracy: 0.9610 - Loss: 0.1169\n",
      "\n",
      "Batch 909/992 ━━━━━━━━━━━━━━━━━━━━ 17:15:38\n",
      "Accuracy: 0.9611 - Loss: 0.1168\n",
      "\n",
      "Batch 910/992 ━━━━━━━━━━━━━━━━━━━━ 17:15:50\n",
      "Accuracy: 0.9611 - Loss: 0.1167\n",
      "\n",
      "Batch 911/992 ━━━━━━━━━━━━━━━━━━━━ 17:16:01\n",
      "Accuracy: 0.9610 - Loss: 0.1171\n",
      "\n",
      "Batch 912/992 ━━━━━━━━━━━━━━━━━━━━ 17:16:11\n",
      "Accuracy: 0.9609 - Loss: 0.1173\n",
      "\n",
      "Batch 913/992 ━━━━━━━━━━━━━━━━━━━━ 17:16:21\n",
      "Accuracy: 0.9610 - Loss: 0.1172\n",
      "\n",
      "Batch 914/992 ━━━━━━━━━━━━━━━━━━━━ 17:16:33\n",
      "Accuracy: 0.9610 - Loss: 0.1171\n",
      "\n",
      "Batch 915/992 ━━━━━━━━━━━━━━━━━━━━ 17:16:44\n",
      "Accuracy: 0.9611 - Loss: 0.1170\n",
      "\n",
      "Batch 916/992 ━━━━━━━━━━━━━━━━━━━━ 17:16:54\n",
      "Accuracy: 0.9610 - Loss: 0.1170\n",
      "\n",
      "Batch 917/992 ━━━━━━━━━━━━━━━━━━━━ 17:17:05\n",
      "Accuracy: 0.9610 - Loss: 0.1169\n",
      "\n",
      "Batch 918/992 ━━━━━━━━━━━━━━━━━━━━ 17:17:16\n",
      "Accuracy: 0.9611 - Loss: 0.1169\n",
      "\n",
      "Batch 919/992 ━━━━━━━━━━━━━━━━━━━━ 17:17:28\n",
      "Accuracy: 0.9610 - Loss: 0.1170\n",
      "\n",
      "Batch 920/992 ━━━━━━━━━━━━━━━━━━━━ 17:17:39\n",
      "Accuracy: 0.9610 - Loss: 0.1169\n",
      "\n",
      "Batch 921/992 ━━━━━━━━━━━━━━━━━━━━ 17:17:50\n",
      "Accuracy: 0.9609 - Loss: 0.1169\n",
      "\n",
      "Batch 922/992 ━━━━━━━━━━━━━━━━━━━━ 17:18:01\n",
      "Accuracy: 0.9610 - Loss: 0.1169\n",
      "\n",
      "Batch 923/992 ━━━━━━━━━━━━━━━━━━━━ 17:18:12\n",
      "Accuracy: 0.9610 - Loss: 0.1168\n",
      "\n",
      "Batch 924/992 ━━━━━━━━━━━━━━━━━━━━ 17:18:22\n",
      "Accuracy: 0.9609 - Loss: 0.1170\n",
      "\n",
      "Batch 925/992 ━━━━━━━━━━━━━━━━━━━━ 17:18:32\n",
      "Accuracy: 0.9609 - Loss: 0.1169\n",
      "\n",
      "Batch 926/992 ━━━━━━━━━━━━━━━━━━━━ 17:18:43\n",
      "Accuracy: 0.9610 - Loss: 0.1168\n",
      "\n",
      "Batch 927/992 ━━━━━━━━━━━━━━━━━━━━ 17:18:54\n",
      "Accuracy: 0.9610 - Loss: 0.1166\n",
      "\n",
      "Batch 928/992 ━━━━━━━━━━━━━━━━━━━━ 17:19:06\n",
      "Accuracy: 0.9611 - Loss: 0.1165\n",
      "\n",
      "Batch 929/992 ━━━━━━━━━━━━━━━━━━━━ 17:19:19\n",
      "Accuracy: 0.9611 - Loss: 0.1165\n",
      "\n",
      "Batch 930/992 ━━━━━━━━━━━━━━━━━━━━ 17:19:30\n",
      "Accuracy: 0.9612 - Loss: 0.1164\n",
      "\n",
      "Batch 931/992 ━━━━━━━━━━━━━━━━━━━━ 17:19:41\n",
      "Accuracy: 0.9612 - Loss: 0.1163\n",
      "\n",
      "Batch 932/992 ━━━━━━━━━━━━━━━━━━━━ 17:19:52\n",
      "Accuracy: 0.9612 - Loss: 0.1163\n",
      "\n",
      "Batch 933/992 ━━━━━━━━━━━━━━━━━━━━ 17:20:03\n",
      "Accuracy: 0.9613 - Loss: 0.1162\n",
      "\n",
      "Batch 934/992 ━━━━━━━━━━━━━━━━━━━━ 17:20:14\n",
      "Accuracy: 0.9613 - Loss: 0.1161\n",
      "\n",
      "Batch 935/992 ━━━━━━━━━━━━━━━━━━━━ 17:20:26\n",
      "Accuracy: 0.9614 - Loss: 0.1160\n",
      "\n",
      "Batch 936/992 ━━━━━━━━━━━━━━━━━━━━ 17:20:37\n",
      "Accuracy: 0.9613 - Loss: 0.1162\n",
      "\n",
      "Batch 937/992 ━━━━━━━━━━━━━━━━━━━━ 17:20:49\n",
      "Accuracy: 0.9612 - Loss: 0.1163\n",
      "\n",
      "Batch 938/992 ━━━━━━━━━━━━━━━━━━━━ 17:21:01\n",
      "Accuracy: 0.9610 - Loss: 0.1166\n",
      "\n",
      "Batch 939/992 ━━━━━━━━━━━━━━━━━━━━ 17:21:12\n",
      "Accuracy: 0.9610 - Loss: 0.1165\n",
      "\n",
      "Batch 940/992 ━━━━━━━━━━━━━━━━━━━━ 17:21:24\n",
      "Accuracy: 0.9610 - Loss: 0.1164\n",
      "\n",
      "Batch 941/992 ━━━━━━━━━━━━━━━━━━━━ 17:21:36\n",
      "Accuracy: 0.9608 - Loss: 0.1167\n",
      "\n",
      "Batch 942/992 ━━━━━━━━━━━━━━━━━━━━ 17:21:49\n",
      "Accuracy: 0.9607 - Loss: 0.1168\n",
      "\n",
      "Batch 943/992 ━━━━━━━━━━━━━━━━━━━━ 17:22:02\n",
      "Accuracy: 0.9608 - Loss: 0.1167\n",
      "\n",
      "Batch 944/992 ━━━━━━━━━━━━━━━━━━━━ 17:22:14\n",
      "Accuracy: 0.9607 - Loss: 0.1169\n",
      "\n",
      "Batch 945/992 ━━━━━━━━━━━━━━━━━━━━ 17:22:26\n",
      "Accuracy: 0.9604 - Loss: 0.1172\n",
      "\n",
      "Batch 946/992 ━━━━━━━━━━━━━━━━━━━━ 17:22:38\n",
      "Accuracy: 0.9604 - Loss: 0.1174\n",
      "\n",
      "Batch 947/992 ━━━━━━━━━━━━━━━━━━━━ 17:22:49\n",
      "Accuracy: 0.9604 - Loss: 0.1173\n",
      "\n",
      "Batch 948/992 ━━━━━━━━━━━━━━━━━━━━ 17:23:01\n",
      "Accuracy: 0.9604 - Loss: 0.1172\n",
      "\n",
      "Batch 949/992 ━━━━━━━━━━━━━━━━━━━━ 17:23:13\n",
      "Accuracy: 0.9605 - Loss: 0.1172\n",
      "\n",
      "Batch 950/992 ━━━━━━━━━━━━━━━━━━━━ 17:23:24\n",
      "Accuracy: 0.9605 - Loss: 0.1172\n",
      "\n",
      "Batch 951/992 ━━━━━━━━━━━━━━━━━━━━ 17:23:36\n",
      "Accuracy: 0.9603 - Loss: 0.1175\n",
      "\n",
      "Batch 952/992 ━━━━━━━━━━━━━━━━━━━━ 17:23:48\n",
      "Accuracy: 0.9602 - Loss: 0.1175\n",
      "\n",
      "Batch 953/992 ━━━━━━━━━━━━━━━━━━━━ 17:23:58\n",
      "Accuracy: 0.9603 - Loss: 0.1174\n",
      "\n",
      "Batch 954/992 ━━━━━━━━━━━━━━━━━━━━ 17:24:09\n",
      "Accuracy: 0.9603 - Loss: 0.1174\n",
      "\n",
      "Batch 955/992 ━━━━━━━━━━━━━━━━━━━━ 17:24:21\n",
      "Accuracy: 0.9603 - Loss: 0.1172\n",
      "\n",
      "Batch 956/992 ━━━━━━━━━━━━━━━━━━━━ 17:24:35\n",
      "Accuracy: 0.9604 - Loss: 0.1172\n",
      "\n",
      "Batch 957/992 ━━━━━━━━━━━━━━━━━━━━ 17:24:46\n",
      "Accuracy: 0.9604 - Loss: 0.1171\n",
      "\n",
      "Batch 958/992 ━━━━━━━━━━━━━━━━━━━━ 17:24:57\n",
      "Accuracy: 0.9603 - Loss: 0.1172\n",
      "\n",
      "Batch 959/992 ━━━━━━━━━━━━━━━━━━━━ 17:25:07\n",
      "Accuracy: 0.9602 - Loss: 0.1175\n",
      "\n",
      "Batch 960/992 ━━━━━━━━━━━━━━━━━━━━ 17:25:17\n",
      "Accuracy: 0.9603 - Loss: 0.1174\n",
      "\n",
      "Batch 961/992 ━━━━━━━━━━━━━━━━━━━━ 17:25:27\n",
      "Accuracy: 0.9602 - Loss: 0.1177\n",
      "\n",
      "Batch 962/992 ━━━━━━━━━━━━━━━━━━━━ 17:25:37\n",
      "Accuracy: 0.9602 - Loss: 0.1177\n",
      "\n",
      "Batch 963/992 ━━━━━━━━━━━━━━━━━━━━ 17:25:49\n",
      "Accuracy: 0.9603 - Loss: 0.1176\n",
      "\n",
      "Batch 964/992 ━━━━━━━━━━━━━━━━━━━━ 17:26:01\n",
      "Accuracy: 0.9602 - Loss: 0.1180\n",
      "\n",
      "Batch 965/992 ━━━━━━━━━━━━━━━━━━━━ 17:26:12\n",
      "Accuracy: 0.9602 - Loss: 0.1179\n",
      "\n",
      "Batch 966/992 ━━━━━━━━━━━━━━━━━━━━ 17:26:24\n",
      "Accuracy: 0.9603 - Loss: 0.1178\n",
      "\n",
      "Batch 967/992 ━━━━━━━━━━━━━━━━━━━━ 17:26:35\n",
      "Accuracy: 0.9603 - Loss: 0.1177\n",
      "\n",
      "Batch 968/992 ━━━━━━━━━━━━━━━━━━━━ 17:26:47\n",
      "Accuracy: 0.9604 - Loss: 0.1177\n",
      "\n",
      "Batch 969/992 ━━━━━━━━━━━━━━━━━━━━ 17:26:58\n",
      "Accuracy: 0.9603 - Loss: 0.1178\n",
      "\n",
      "Batch 970/992 ━━━━━━━━━━━━━━━━━━━━ 17:27:10\n",
      "Accuracy: 0.9603 - Loss: 0.1178\n",
      "\n",
      "Batch 971/992 ━━━━━━━━━━━━━━━━━━━━ 17:27:22\n",
      "Accuracy: 0.9604 - Loss: 0.1176\n",
      "\n",
      "Batch 972/992 ━━━━━━━━━━━━━━━━━━━━ 17:27:33\n",
      "Accuracy: 0.9603 - Loss: 0.1176\n",
      "\n",
      "Batch 973/992 ━━━━━━━━━━━━━━━━━━━━ 17:27:45\n",
      "Accuracy: 0.9603 - Loss: 0.1176\n",
      "\n",
      "Batch 974/992 ━━━━━━━━━━━━━━━━━━━━ 17:27:56\n",
      "Accuracy: 0.9602 - Loss: 0.1177\n",
      "\n",
      "Batch 975/992 ━━━━━━━━━━━━━━━━━━━━ 17:28:08\n",
      "Accuracy: 0.9603 - Loss: 0.1176\n",
      "\n",
      "Batch 976/992 ━━━━━━━━━━━━━━━━━━━━ 17:28:20\n",
      "Accuracy: 0.9602 - Loss: 0.1177\n",
      "\n",
      "Batch 977/992 ━━━━━━━━━━━━━━━━━━━━ 17:28:31\n",
      "Accuracy: 0.9601 - Loss: 0.1178\n",
      "\n",
      "Batch 978/992 ━━━━━━━━━━━━━━━━━━━━ 17:28:41\n",
      "Accuracy: 0.9601 - Loss: 0.1177\n",
      "\n",
      "Batch 979/992 ━━━━━━━━━━━━━━━━━━━━ 17:28:51\n",
      "Accuracy: 0.9600 - Loss: 0.1183\n",
      "\n",
      "Batch 980/992 ━━━━━━━━━━━━━━━━━━━━ 17:29:03\n",
      "Accuracy: 0.9601 - Loss: 0.1182\n",
      "\n",
      "Batch 981/992 ━━━━━━━━━━━━━━━━━━━━ 17:29:15\n",
      "Accuracy: 0.9601 - Loss: 0.1181\n",
      "\n",
      "Batch 982/992 ━━━━━━━━━━━━━━━━━━━━ 17:29:26\n",
      "Accuracy: 0.9602 - Loss: 0.1180\n",
      "\n",
      "Batch 983/992 ━━━━━━━━━━━━━━━━━━━━ 17:29:38\n",
      "Accuracy: 0.9602 - Loss: 0.1179\n",
      "\n",
      "Batch 984/992 ━━━━━━━━━━━━━━━━━━━━ 17:29:51\n",
      "Accuracy: 0.9601 - Loss: 0.1181\n",
      "\n",
      "Batch 985/992 ━━━━━━━━━━━━━━━━━━━━ 17:30:03\n",
      "Accuracy: 0.9599 - Loss: 0.1185\n",
      "\n",
      "Batch 986/992 ━━━━━━━━━━━━━━━━━━━━ 17:30:14\n",
      "Accuracy: 0.9599 - Loss: 0.1185\n",
      "\n",
      "Batch 987/992 ━━━━━━━━━━━━━━━━━━━━ 17:30:25\n",
      "Accuracy: 0.9600 - Loss: 0.1184\n",
      "\n",
      "Batch 988/992 ━━━━━━━━━━━━━━━━━━━━ 17:30:36\n",
      "Accuracy: 0.9599 - Loss: 0.1185\n",
      "\n",
      "Batch 989/992 ━━━━━━━━━━━━━━━━━━━━ 17:30:47\n",
      "Accuracy: 0.9597 - Loss: 0.1192\n",
      "\n",
      "Batch 990/992 ━━━━━━━━━━━━━━━━━━━━ 17:30:58\n",
      "Accuracy: 0.9597 - Loss: 0.1191\n",
      "\n",
      "Batch 991/992 ━━━━━━━━━━━━━━━━━━━━ 17:31:10\n",
      "Accuracy: 0.9598 - Loss: 0.1191\n",
      "\n",
      "Batch 992/992 ━━━━━━━━━━━━━━━━━━━━ 17:31:20\n",
      "Accuracy: 0.9598 - Loss: 0.1190\n",
      "\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1347s\u001b[0m 4s/step - accuracy: 0.8633 - loss: 0.3976\n",
      "Test Loss: 0.3958311975002289, Test Accuracy: 0.8649193644523621\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1441s\u001b[0m 5s/step\n",
      "Precision: 0.8664, Recall: 0.8649, F1 Score: 0.8653\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Dense, GlobalAveragePooling2D, LayerNormalization,\n",
    "    Dropout, Conv2D, Add, Multiply, Layer, Concatenate,\n",
    "    MultiHeadAttention, DepthwiseConv2D\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime\n",
    "from tensorflow.keras import backend as K  # Import Keras backend for shape handling\n",
    "\n",
    "# Parameters\n",
    "num_classes = 6\n",
    "image_size = (224, 224)\n",
    "dropout_rate = 0.1\n",
    "batch_size = 8  # Adjusted batch size to have 793 steps per epoch\n",
    "epochs = 10\n",
    "learning_rate = 1e-4\n",
    "window_size = 7  # Swin Transformer window size\n",
    "\n",
    "# Attention-Guided Feature Fusion (AGFF) Layer with Feature Calibration\n",
    "class AttentionGuidedFeatureFusion(Layer):\n",
    "    def __init__(self, conv_channels=768, swin_channels=768, **kwargs):\n",
    "        super(AttentionGuidedFeatureFusion, self).__init__(**kwargs)\n",
    "        self.conv_channels = conv_channels\n",
    "        self.swin_channels = swin_channels\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Adjust num_channels to account for projection\n",
    "        num_channels = (self.conv_channels // 2) + (self.swin_channels // 2)  # 384 + 384 = 768\n",
    "        self.conv_spatial = Conv2D(1, kernel_size=1, activation='sigmoid', name='conv_spatial')\n",
    "        self.dense1 = Dense(num_channels // 8, activation='relu', name='dense1')  # 768 // 8 = 96\n",
    "        self.dense2 = Dense(num_channels, activation='sigmoid', name='dense2')      # 768\n",
    "\n",
    "        # Feature calibration layers\n",
    "        self.norm1 = LayerNormalization(epsilon=1e-6, name='norm1')\n",
    "        self.norm2 = LayerNormalization(epsilon=1e-6, name='norm2')\n",
    "        self.proj_conv = Dense(self.conv_channels // 2, activation='relu', name='proj_conv')  # 384\n",
    "        self.proj_swin = Dense(self.swin_channels // 2, activation='relu', name='proj_swin')  # 384\n",
    "        self.weight_conv = self.add_weight(name='weight_conv', shape=(1,), initializer='ones', trainable=True)\n",
    "        self.weight_swin = self.add_weight(name='weight_swin', shape=(1,), initializer='ones', trainable=True)\n",
    "\n",
    "        super(AttentionGuidedFeatureFusion, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        conv_features, swin_features = inputs\n",
    "\n",
    "        # Feature Calibration\n",
    "        conv_features = self.norm1(conv_features)\n",
    "        swin_features = self.norm2(swin_features)\n",
    "\n",
    "        conv_features = self.proj_conv(conv_features)  # [B,7,7,384]\n",
    "        swin_features = self.proj_swin(swin_features)  # [B,7,7,384]\n",
    "\n",
    "        # Re-weighting\n",
    "        conv_features = conv_features * self.weight_conv\n",
    "        swin_features = swin_features * self.weight_swin\n",
    "\n",
    "        # Concatenate calibrated features\n",
    "        combined_features = Concatenate(name='concatenate')([conv_features, swin_features])  # [B,7,7,768]\n",
    "\n",
    "        # Spatial Attention Mechanism\n",
    "        spatial_attention = self.conv_spatial(combined_features)  # [B,7,7,1]\n",
    "        spatial_attention = Multiply(name='spatial_attention')([spatial_attention, combined_features])  # [B,7,7,768]\n",
    "\n",
    "        # Channel Attention Mechanism\n",
    "        channel_attention = GlobalAveragePooling2D(name='global_avg_pool')(combined_features)  # [B,768]\n",
    "        channel_attention = self.dense1(channel_attention)  # [B,96]\n",
    "        channel_attention = self.dense2(channel_attention)  # [B,768]\n",
    "        channel_attention = tf.expand_dims(tf.expand_dims(channel_attention, 1), 1)  # [B,1,1,768]\n",
    "        channel_attention = Multiply(name='channel_attention')([combined_features, channel_attention])  # [B,7,7,768]\n",
    "\n",
    "        # Combine spatial and channel attention outputs\n",
    "        fused_output = Add(name='fused_output')([spatial_attention, channel_attention])  # [B,7,7,768]\n",
    "        return fused_output\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(AttentionGuidedFeatureFusion, self).get_config()\n",
    "        config.update({\n",
    "            \"conv_channels\": self.conv_channels,\n",
    "            \"swin_channels\": self.swin_channels\n",
    "        })\n",
    "        return config\n",
    "\n",
    "# Complete Swin Transformer Implementation\n",
    "class SwinTransformerBlock(Layer):\n",
    "    def __init__(self, dim, num_heads, window_size=7, mlp_ratio=4., dropout=0., **kwargs):\n",
    "        super(SwinTransformerBlock, self).__init__(**kwargs)\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "\n",
    "        self.norm1 = LayerNormalization(epsilon=1e-6, name='norm1')\n",
    "        self.attn = MultiHeadAttention(num_heads=num_heads, key_dim=dim // num_heads, name='multi_head_attn')\n",
    "        self.dropout1 = Dropout(dropout, name='dropout1')\n",
    "        self.norm2 = LayerNormalization(epsilon=1e-6, name='norm2')\n",
    "        self.mlp = tf.keras.Sequential([\n",
    "            Dense(int(dim * mlp_ratio), activation='gelu', name='mlp_dense1'),\n",
    "            Dense(dim, name='mlp_dense2'),\n",
    "        ], name='mlp')\n",
    "        self.dropout2 = Dropout(dropout, name='dropout2')\n",
    "\n",
    "    def call(self, x):\n",
    "        # x: [B, H, W, C]\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.attn(x, x)  # [B, H, W, C]\n",
    "        x = self.dropout1(x)\n",
    "        x = Add(name='attn_add')([shortcut, x])\n",
    "\n",
    "        # MLP\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.mlp(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = Add(name='mlp_add')([shortcut, x])\n",
    "\n",
    "        return x\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(SwinTransformerBlock, self).get_config()\n",
    "        config.update({\n",
    "            \"dim\": self.dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"window_size\": self.window_size,\n",
    "            \"mlp_ratio\": self.mlp_ratio,\n",
    "            \"dropout\": self.dropout1.rate\n",
    "        })\n",
    "        return config\n",
    "\n",
    "class SwinTransformerStage(Layer):\n",
    "    def __init__(self, dim, num_heads, window_size=7, num_blocks=2, mlp_ratio=4., dropout=0., downsample=True, **kwargs):\n",
    "        super(SwinTransformerStage, self).__init__(**kwargs)\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.num_blocks = num_blocks\n",
    "        self.downsample = downsample\n",
    "\n",
    "        # Register each block as an attribute\n",
    "        for i in range(num_blocks):\n",
    "            block = SwinTransformerBlock(dim, num_heads, window_size, mlp_ratio, dropout, name=f'swin_block_{i}')\n",
    "            self.__setattr__(f'swin_block_{i}', block)\n",
    "\n",
    "        self.norm = LayerNormalization(epsilon=1e-6, name='stage_norm')\n",
    "        if self.downsample:\n",
    "            self.downsample_layer = Conv2D(dim * 2, kernel_size=2, strides=2, padding='same', name='stage_downsample')\n",
    "        else:\n",
    "            self.downsample_layer = None\n",
    "\n",
    "    def call(self, x):\n",
    "        # x: [B, H*W, C]\n",
    "        B = tf.shape(x)[0]\n",
    "        N = tf.shape(x)[1]\n",
    "        C = tf.shape(x)[2]\n",
    "        H = tf.cast(tf.math.sqrt(tf.cast(N, tf.float32)), tf.int32)\n",
    "        W = H  # assuming square\n",
    "\n",
    "        x = tf.reshape(x, [B, H, W, C])  # [B, H, W, C]\n",
    "\n",
    "        for i in range(self.num_blocks):\n",
    "            block = self.__getattribute__(f'swin_block_{i}')\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        if self.downsample:\n",
    "            x = self.downsample_layer(x)\n",
    "            H = H // 2\n",
    "            W = W // 2\n",
    "            C = self.dim * 2\n",
    "            x = tf.reshape(x, [B, H * W, C])\n",
    "        else:\n",
    "            x = tf.reshape(x, [B, H * W, C])\n",
    "\n",
    "        return x\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(SwinTransformerStage, self).get_config()\n",
    "        config.update({\n",
    "            \"dim\": self.dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"window_size\": self.window_size,\n",
    "            \"num_blocks\": self.num_blocks,\n",
    "            \"mlp_ratio\": self.mlp_ratio,\n",
    "            \"dropout\": self.dropout1.rate if hasattr(self, 'dropout1') else 0.,\n",
    "            \"downsample\": self.downsample\n",
    "        })\n",
    "        return config\n",
    "\n",
    "class SwinTransformer(Layer):\n",
    "    def __init__(self, embed_dim=96, depths=[2, 2, 6, 2], num_heads=[3, 6, 12, 24],\n",
    "                 window_size=7, mlp_ratio=4., dropout=0., **kwargs):\n",
    "        super(SwinTransformer, self).__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.depths = depths\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.patch_embed = Conv2D(embed_dim, kernel_size=4, strides=4, padding='same', name='patch_embed')  # Patch Embedding\n",
    "\n",
    "        # Register each stage as a separate attribute\n",
    "        self.stages = []\n",
    "        for i in range(len(depths)):\n",
    "            # Prevent downsampling in the last stage\n",
    "            downsample = True if i < len(depths) - 1 else False\n",
    "            stage = SwinTransformerStage(\n",
    "                dim=embed_dim * (2 ** i),\n",
    "                num_heads=num_heads[i],\n",
    "                window_size=window_size,\n",
    "                num_blocks=depths[i],\n",
    "                mlp_ratio=mlp_ratio,\n",
    "                dropout=dropout,\n",
    "                downsample=downsample,\n",
    "                name=f'swin_stage_{i}'\n",
    "            )\n",
    "            self.stages.append(stage)\n",
    "            self.__setattr__(f'swin_stage_{i}', stage)  # Register the stage\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.patch_embed(x)  # [B, H/4, W/4, embed_dim]\n",
    "        B = tf.shape(x)[0]\n",
    "        H = tf.shape(x)[1]\n",
    "        W = tf.shape(x)[2]\n",
    "        C = tf.shape(x)[3]\n",
    "        x = tf.reshape(x, [B, H * W, C])  # [B, H*W, C]\n",
    "\n",
    "        for stage in self.stages:\n",
    "            x = stage(x)\n",
    "\n",
    "        # Calculate final H and W based on downsampling\n",
    "        final_stage = self.stages[-1]\n",
    "        if final_stage.downsample:\n",
    "            final_H = H // (2 ** (len(self.stages)))\n",
    "            final_W = W // (2 ** (len(self.stages)))\n",
    "            final_C = final_stage.dim * 2\n",
    "        else:\n",
    "            final_H = H // (2 ** (len(self.stages) - 1))\n",
    "            final_W = W // (2 ** (len(self.stages) - 1))\n",
    "            final_C = final_stage.dim\n",
    "\n",
    "        x = tf.reshape(x, [B, final_H, final_W, final_C])  # [B, H_final, W_final, C_final]\n",
    "        return x\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(SwinTransformer, self).get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"depths\": self.depths,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"window_size\": self.window_size,\n",
    "            \"mlp_ratio\": self.mlp_ratio,\n",
    "            \"dropout\": self.dropout\n",
    "        })\n",
    "        return config\n",
    "\n",
    "# Complete ConvNeXt Implementation\n",
    "class ConvNeXtBlock(Layer):\n",
    "    def __init__(self, filters, drop_path=0., layer_scale_init_value=1e-6, **kwargs):\n",
    "        super(ConvNeXtBlock, self).__init__(**kwargs)\n",
    "        self.norm = LayerNormalization(epsilon=1e-6, name='convnext_block_norm')\n",
    "        self.dwconv = DepthwiseConv2D(kernel_size=7, padding='same', name='convnext_block_dwconv')  # Depthwise Conv\n",
    "        self.pwconv1 = Conv2D(filters * 4, kernel_size=1, activation='gelu', name='convnext_block_pwconv1')  # Pointwise Conv 1\n",
    "        self.pwconv2 = Conv2D(filters, kernel_size=1, name='convnext_block_pwconv2')  # Pointwise Conv 2\n",
    "        self.drop_path = Dropout(drop_path, name='convnext_block_drop_path') if drop_path > 0. else lambda x: x\n",
    "        # Initialize gamma as a trainable scalar per channel\n",
    "        self.gamma = self.add_weight(\n",
    "            shape=(filters,),\n",
    "            initializer=tf.keras.initializers.Constant(layer_scale_init_value),\n",
    "            trainable=True,\n",
    "            name='gamma'\n",
    "        )\n",
    "\n",
    "    def call(self, x):\n",
    "        input = x\n",
    "        x = self.norm(x)\n",
    "        x = self.dwconv(x)\n",
    "        x = self.pwconv1(x)\n",
    "        x = self.pwconv2(x)\n",
    "        # Reshape gamma for broadcasting\n",
    "        gamma = tf.reshape(self.gamma, (1, 1, 1, -1))\n",
    "        x = x * gamma\n",
    "        x = self.drop_path(x)\n",
    "        return Add(name='convnext_block_add')([input, x])\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(ConvNeXtBlock, self).get_config()\n",
    "        config.update({\n",
    "            \"filters\": self.gamma.shape[-1],\n",
    "            \"drop_path\": self.drop_path.rate if isinstance(self.drop_path, Dropout) else 0.,\n",
    "            \"layer_scale_init_value\": K.get_value(self.gamma)[0]  # Assumes uniform initialization\n",
    "        })\n",
    "        return config\n",
    "\n",
    "class ConvNeXtStage(Layer):\n",
    "    def __init__(self, filters, depth, drop_path_rates, layer_scale_init_value=1e-6, downsample=True, **kwargs):\n",
    "        super(ConvNeXtStage, self).__init__(**kwargs)\n",
    "        self.filters = filters\n",
    "        self.depth = depth\n",
    "        self.downsample = downsample\n",
    "\n",
    "        # Register each block as an attribute\n",
    "        for i, dp in enumerate(drop_path_rates):\n",
    "            block = ConvNeXtBlock(filters, drop_path=dp, layer_scale_init_value=layer_scale_init_value, name=f'convnext_stage_{filters}_block_{i}')\n",
    "            self.__setattr__(f'convnext_stage_{filters}_block_{i}', block)\n",
    "\n",
    "        if self.downsample:\n",
    "            self.downsample_layer = Conv2D(filters * 2, kernel_size=2, strides=2, padding='same', name=f'convnext_stage_{filters}_downsample')\n",
    "        else:\n",
    "            self.downsample_layer = None\n",
    "\n",
    "    def call(self, x):\n",
    "        for i in range(self.depth):\n",
    "            block = self.__getattribute__(f'convnext_stage_{self.filters}_block_{i}')\n",
    "            x = block(x)\n",
    "        if self.downsample_layer:\n",
    "            x = self.downsample_layer(x)\n",
    "        return x\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(ConvNeXtStage, self).get_config()\n",
    "        config.update({\n",
    "            \"filters\": self.filters,\n",
    "            \"depth\": self.depth,\n",
    "            \"drop_path_rates\": [self.__getattribute__(f'convnext_stage_{self.filters}_block_{i}').drop_path.rate for i in range(self.depth)],\n",
    "            \"layer_scale_init_value\": K.get_value(self.__getattribute__(f'convnext_stage_{self.filters}_block_0').gamma)[0],\n",
    "            \"downsample\": self.downsample\n",
    "        })\n",
    "        return config\n",
    "\n",
    "class ConvNeXt(Layer):\n",
    "    def __init__(self, depths=[3, 3, 9, 3], dims=[96, 192, 384, 768], drop_path_rate=0.2, layer_scale_init_value=1e-6, **kwargs):\n",
    "        super(ConvNeXt, self).__init__(**kwargs)\n",
    "        self.depths = depths\n",
    "        self.dims = dims\n",
    "        self.drop_path_rate = drop_path_rate\n",
    "        self.layer_scale_init_value = layer_scale_init_value\n",
    "        self.stem = Conv2D(dims[0], kernel_size=4, strides=4, padding='same', name='convnext_stem')  # Initial embedding\n",
    "\n",
    "        # Calculate drop path rates\n",
    "        total_blocks = sum(depths)\n",
    "        dp_rates = np.linspace(0, drop_path_rate, total_blocks).tolist()\n",
    "        dp_idx = 0\n",
    "\n",
    "        # Register each stage as an attribute\n",
    "        for i, (depth, dim) in enumerate(zip(depths, dims)):\n",
    "            drop_path_rates = dp_rates[dp_idx:dp_idx + depth]\n",
    "            downsample = True if i < len(depths) - 1 else False\n",
    "            stage = ConvNeXtStage(\n",
    "                filters=dim,\n",
    "                depth=depth,\n",
    "                drop_path_rates=drop_path_rates,\n",
    "                layer_scale_init_value=layer_scale_init_value,\n",
    "                downsample=downsample,\n",
    "                name=f'convnext_stage_{dim}'\n",
    "            )\n",
    "            self.__setattr__(f'convnext_stage_{dim}', stage)\n",
    "            dp_idx += depth\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.stem(x)  # Shape: [B, H/4, W/4, 96]\n",
    "        for dim in self.dims:\n",
    "            stage = self.__getattribute__(f'convnext_stage_{dim}')\n",
    "            x = stage(x)\n",
    "        return x  # Final feature map with 768 channels\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(ConvNeXt, self).get_config()\n",
    "        config.update({\n",
    "            \"depths\": self.depths,\n",
    "            \"dims\": self.dims,\n",
    "            \"drop_path_rate\": self.drop_path_rate,\n",
    "            \"layer_scale_init_value\": self.layer_scale_init_value\n",
    "        })\n",
    "        return config\n",
    "\n",
    "# Feature Alignment Layer (unchanged)\n",
    "class FeatureAlignmentLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(FeatureAlignmentLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        conv_features, swin_features = inputs\n",
    "        conv_shape = tf.shape(conv_features)[1:3]\n",
    "        swin_features = tf.image.resize(swin_features, size=conv_shape)\n",
    "        return swin_features\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(FeatureAlignmentLayer, self).get_config()\n",
    "        return config\n",
    "\n",
    "def create_convnext_swin_model(input_shape, num_classes):\n",
    "    inputs = Input(shape=input_shape, name='input_layer')\n",
    "\n",
    "    # ConvNeXt Backbone\n",
    "    convnext = ConvNeXt(name='convnext_backbone')\n",
    "    conv_features = convnext(inputs)  # Shape: (None, 7, 7, 768)\n",
    "\n",
    "    # Swin Transformer Backbone\n",
    "    swin_transformer = SwinTransformer(name='swin_transformer_backbone')\n",
    "    swin_features = swin_transformer(inputs)  # Output shape: [B, 7, 7, 768]\n",
    "\n",
    "    # Align swin_features to conv_features\n",
    "    swin_features = FeatureAlignmentLayer(name='feature_alignment')([conv_features, swin_features])\n",
    "\n",
    "    # Since both ConvNeXt and SwinTransformer output 768 channels, no adjustment needed\n",
    "    # Apply Attention-Guided Feature Fusion (AGFF) with Feature Calibration\n",
    "    fused_features = AttentionGuidedFeatureFusion(conv_channels=768, swin_channels=768, name='agff')([conv_features, swin_features])\n",
    "\n",
    "    x = LayerNormalization(epsilon=1e-6, name='final_norm')(fused_features)\n",
    "    x = GlobalAveragePooling2D(name='global_avg_pool')(x)\n",
    "    x = Dropout(dropout_rate, name='dropout_final')(x)\n",
    "    outputs = Dense(num_classes, activation=\"softmax\", name='output_layer')(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs, name='ConvNeXt_SwinTransformer_Model')\n",
    "    return model\n",
    "\n",
    "# Compile Model\n",
    "input_shape = (224, 224, 3)\n",
    "model = create_convnext_swin_model(input_shape, num_classes)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# Data Loading and Preprocessing\n",
    "csv_path = r\"C:\\Users\\Jaber\\OneDrive - University of Florida\\Educational\\GitHub\\Ultrasound_Fetal\\Data\\FETAL_PLANES_DB_data.csv\"\n",
    "df = pd.read_csv(csv_path, delimiter=\";\")\n",
    "\n",
    "# Shuffle the DataFrame\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Add .png extension to each image name\n",
    "df[\"Image_name\"] = df[\"Image_name\"].apply(lambda x: f\"{x}.png\")\n",
    "\n",
    "# Split into train+val and test sets\n",
    "train_val_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Image data generator with train-validation split\n",
    "datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    "\n",
    "train_gen = datagen.flow_from_dataframe(\n",
    "    dataframe=train_val_df,\n",
    "    directory=r\"C:\\Users\\Jaber\\OneDrive - University of Florida\\Educational\\GitHub\\Ultrasound_Fetal\\Data\\Images\",\n",
    "    x_col=\"Image_name\",\n",
    "    y_col=\"Plane\",\n",
    "    target_size=image_size,\n",
    "    class_mode=\"sparse\",\n",
    "    batch_size=batch_size,\n",
    "    subset=\"training\",\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_gen = datagen.flow_from_dataframe(\n",
    "    dataframe=train_val_df,\n",
    "    directory=r\"C:\\Users\\Jaber\\OneDrive - University of Florida\\Educational\\GitHub\\Ultrasound_Fetal\\Data\\Images\",\n",
    "    x_col=\"Image_name\",\n",
    "    y_col=\"Plane\",\n",
    "    target_size=image_size,\n",
    "    class_mode=\"sparse\",\n",
    "    batch_size=batch_size,\n",
    "    subset=\"validation\",\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Test data generator\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_gen = test_datagen.flow_from_dataframe(\n",
    "    dataframe=test_df,\n",
    "    directory=r\"C:\\Users\\Jaber\\OneDrive - University of Florida\\Educational\\GitHub\\Ultrasound_Fetal\\Data\\Images\",\n",
    "    x_col=\"Image_name\",\n",
    "    y_col=\"Plane\",\n",
    "    target_size=image_size,\n",
    "    class_mode=\"sparse\",\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Simplified Metrics Callback\n",
    "class MetricsCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, total_batches):\n",
    "        super().__init__()\n",
    "        self.total_batches = total_batches\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{self.params['epochs']}\")\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        logs = logs or {}\n",
    "        accuracy = logs.get('accuracy', 0)\n",
    "        loss = logs.get('loss', 0)\n",
    "        current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "        print(f\"Batch {batch+1}/{self.total_batches} ━━━━━━━━━━━━━━━━━━━━ {current_time}\")\n",
    "        print(f\"Accuracy: {accuracy:.4f} - Loss: {loss:.4f}\\n\")\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(MetricsCallback, self).get_config()\n",
    "        config.update({\"total_batches\": self.total_batches})\n",
    "        return config\n",
    "\n",
    "# Calculate total_batches for training\n",
    "total_train_batches = train_gen.samples // train_gen.batch_size\n",
    "if train_gen.samples % train_gen.batch_size != 0:\n",
    "    total_train_batches += 1\n",
    "\n",
    "# Early Stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Training\n",
    "history = model.fit(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    epochs=epochs,\n",
    "    callbacks=[MetricsCallback(total_train_batches), early_stopping],\n",
    "    verbose=0  # Set verbose to 0 to suppress default output\n",
    ")\n",
    "\n",
    "# Evaluate on Test Data\n",
    "test_loss, test_accuracy = model.evaluate(test_gen)\n",
    "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "# Calculate Metrics on Test Set\n",
    "test_steps = test_gen.samples // test_gen.batch_size + int(test_gen.samples % test_gen.batch_size != 0)\n",
    "test_preds = model.predict(test_gen, steps=test_steps)\n",
    "test_labels = test_gen.classes\n",
    "\n",
    "# Convert predictions to label format\n",
    "test_preds = np.argmax(test_preds, axis=1)\n",
    "\n",
    "precision = precision_score(test_labels, test_preds, average='weighted')\n",
    "recall = recall_score(test_labels, test_preds, average='weighted')\n",
    "f1 = f1_score(test_labels, test_preds, average='weighted')\n",
    "print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Visualization of Training History\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history.get('val_accuracy'), label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history.get('val_loss'), label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d48eff-b6c0-43d0-99b3-7da88c38e351",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
